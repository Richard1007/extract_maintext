Number,Body
20,"Why Salespeople Avoid Big-Whale Sales Opportunities Contrary to the intuition that salespeople gravitate toward big-whale sales opportunities, in reality they often avoid them. To study this phenomenon, the authors integrate contingent decision-making and conservation-of-resources theories to develop and test a framework of salespeople's decision making when prospecting. Study 1 reveals that the performance impact of salesperson initial judgment of opportunity magnitude follows an inverted U-shape, indicating that salespeople's avoidance of large opportunities results from rational benefit–cost analyses due to their conservation of resources. Interestingly, salespeople use a calibration decision-making strategy (i.e., calculating expected benefits by accounting for conversion uncertainty) at the portfolio rather than prospect level, in solution- but not product-selling contexts. Ignoring this calibration effect can lead to under- or overestimation of conversion rates of up to 100%. Study 2 shows that salespeople's past performance success and experience bias this calibration. Simulations reveal that when high performers or inexperienced salespeople believe their portfolio magnitude is large and conversion uncertainty low, they are less concerned about resource conservation and improve their quota attainment by 50%. Study 3 confirms the theoretical mechanism. These findings shed new light on salespeople's decision making and suggest ways for sales professionals to improve effectiveness when prospecting.Keywords: salesperson judgment; uncertainty; solution selling; prospecting; conservation-of-resources theoryCentral to a firm's customer acquisition is salesperson prospecting, which involves identifying sales opportunities among potential customers. Prior research on salesperson prospecting has underscored its importance not only for firms' customer relationship management (CRM) but also for salesperson performance (e.g., [48]). However, more than 40% of salespeople report that prospecting is challenging and full of uncertainty, taking on average 25% of their time ([ 8]). Whereas some practitioners emphasize the pursuit of large prospects because these ""big whales"" help firms and salespeople achieve rapid sales growth, others warn against prioritizing such prospects because they can easily drain salesperson and company resources ([32]). Moreover, ""in the time it takes to land one major deal, [the salesperson] could have closed five smaller deals"" ([19]). Although practitioners appear to recognize salespeople's benefit–cost trade-offs when prospecting, academic research has not systematically examined this important phenomenon.Marketing research on salesperson prospecting has developed along two major streams. One stream focuses on salesperson judgment of market opportunities, such as market demand for a new brand, customer needs, and expected performance (e.g., [27]; [31]; [66]). This research stream shows a linear positive relationship between customer demand judgment and salesperson performance. The other stream emphasizes the role of salespeople as decision makers in dealing with various types of uncertainty, such as salespeople's general risk aversion, context-specific uncertainty, or salesperson idiosyncratic characteristics (e.g., [ 2]; [10]; [41]; [55]; [60]). Although these research streams provide useful insights into the information salespeople use in their decision making, three important research gaps exist.First, when prospecting, salespeople typically identify multiple potential opportunities but pursue only some of them. However, research is scant on the potentially curvilinear impact of salesperson judgment of opportunity magnitude—defined as a salesperson's judgment of the size of an opportunity—on sales performance. Although anecdotal evidence suggests that salespeople focus on large opportunities, other sources allude to major drawbacks in pursuing them. Opening this black box can be useful for improving companies' prospecting effectiveness. Second, there is a lack of understanding of how conversion uncertainty affects salespeople's decision making when prospecting. A focus on conversion uncertainty is important, because prospecting is costly for the firm and for salespeople. Third, little is known about how such decision making varies between salespeople and selling contexts. Knowledge of these contingencies help sales managers to effectively manage salesperson prospecting behavior.To address these gaps, we seek answers to three key questions. First, what is salespeople's benefit–cost trade-off after they form an initial judgment of opportunity magnitude, and how does this affect their sales performance? The focus on initial judgments is based on prior research that underscores the importance of a primacy effect in both decision making and salesperson–customer interactions ([17]; [27]). Second, how does a salesperson's initial judgments of opportunity conversion uncertainty change the sales performance outcome of the benefit–cost analysis? Given that salespeople's compensation generally depends on conversion, understanding the effect of opportunity conversion uncertainty, or a salesperson's initial judgment of the likelihood to convert an opportunity into a deal, is important. Third, what are important boundary conditions of the effects of these initial judgments? We focus on two sets of moderators: ( 1) the selling context (i.e., product vs. solution selling) and ( 2) key salesperson characteristics (i.e., past performance success and salesperson experience). In doing so, we also explore the role of information level (i.e., prospect and portfolio levels) in salesperson decision making.To answer our questions, we develop and test a contingency framework of salespeople's decision making when prospecting for market opportunities in a sequence of three studies. For theoretical foundation, we integrate research on contingency decision making ([46]) and conservation of resources (COR) ([30]). We augment these theories with field notes from in-depth interviews with sales professionals. While Studies 1 and 2 rely on multisource field data, Study 3 is a scenario-based experiment to provide evidence of the benefit–cost analysis as the underlying mechanism. Together, this multimethod approach allows us to rigorously triangulate the effects and unpack the theoretical mechanisms.This research makes several contributions. First, we contribute to the emerging literature on salesperson judgment and decision making when prospecting by unpacking the underlying decision process. We provide theoretical arguments and strong empirical evidence that explains why salespeople avoid big-whale prospects. Specifically, we show that, based on their initial judgment of opportunity magnitude, salespeople conduct a benefit–cost analysis under resource constraints to decide which opportunity to pursue. This analysis results in an inverted U-shaped relationship between magnitude and performance. Spotlight analyses show that a one-standard-deviation (SD) increase in opportunity magnitude lowers salespeople's conversion rate by 10%.Second, we provide insights into the effect of conversion uncertainty on the salesperson decision-making process when prospecting. The results show that when selling solutions, salespeople use a calibration decision-making strategy, in which the effects of opportunity magnitude are conditional on conversion uncertainty. However, this strategy occurs only at the portfolio level, underscoring the role of salesperson portfolio as a decision-making context. Ignoring the calibration effect in estimating performance outcomes may lead to under- or overestimation of conversion rates of up to 100%. When selling products, salespeople use a compensatory decision-making strategy that accounts for the effects of portfolio magnitude and conversion uncertainty in an additive manner. These findings extend prior work (e.g., [60]) on uncertainty in personal selling and salesperson decision making.Third, we provide empirical evidence for how, in a solution-selling context, the calibration effect varies depending on salesperson past performance success and experience. The results suggest that when faced with high levels of conversion uncertainty, high performers and inexperienced salespeople perform much worse because their resource-conserving tendency makes them more sensitive to the cost increases associated with uncertainty. Simulations reveal that their quota attainment can suffer by as much as 50%. These insights extend prior research focusing on the salesperson–customer dyad in business-to-business (B2B) marketing and retail encounters (e.g., [27]; [43]). Background Literature and Conceptual FrameworkSalespeople are generally assigned to a territory or a customer segment, and their sales opportunities can be self-generated or assigned ([48]). Within a given period, they move these sales opportunities through a funnel from prospects to closed sales deals. At any given time, salespeople form a judgment of the magnitude of specific sales prospects, with a certain level of conversion uncertainty. Prior research on salesperson prospecting provides useful insights into why salespeople fail to follow sales leads, how their judgment of opportunities linearly influences their performance, and how they deal with uncertainty. However, it has not examined why and when salespeople pursue or avoid big opportunities. To shed light on these issues, we view salesperson prospecting as decision making under resource constraints. In this section, we first briefly review the relevant literature and then present our conceptual framework. Prospecting as Decision Making Under Resource Constraints Decision-making frameworksTwo major decision-making frameworks are the benefit–cost framework ([ 6]) and perceptual frameworks, such as prospect theory ([59]). In their review of these two frameworks, [46] posit that the former provides insights into rational decision making under multiple alternatives while the latter is useful in explaining cognitive biases and heuristics in decision making. In their review, they also underscore task and individual characteristics, such as willingness to bear uncertainty, as important contingencies of individual decision making. COR theoryUnlike the majority of general decision-making frameworks that assume away any resource constraint, COR theory emphasizes that people ""strive to retain, protect, and build resources and that what is threatening to them is the potential or actual loss of these valued resources"" ([30], p. 513). Furthermore, people must invest resources to gain resources, and those who experience a lack of resources attempt to conserve remaining resources ([26]). We argue that COR theory is particularly relevant in the context of B2B salespeople's prospecting for three reasons. First, unlike simple, low-effort choices between two lotteries, the pursuit of a prospect is costly—salespeople need to invest their time, effort, and resources in converting prospects into sales ([48]). Second, uncertainty in prospecting brings salespeople's resource constraints to the fore. Unlike gambling, which can be replayed, a forgone sales opportunity might be gone for good, and a failure to convert opportunities represents a loss of resources. Thus, salespeople need to balance between risk seeking and COR. Third, salespeople differ in terms of resource constraints ([48]). Contingency Framework of Salesperson Decision Making When ProspectingWe integrate decision-making frameworks with COR theory to propose a contingency framework of salesperson decision making when prospecting. Our framework focuses on the initial judgments of sales opportunities in terms of magnitude and conversion uncertainty. First, although salespeople encounter multiple market opportunities, they only invest their resources into converting some of them. The benefit–cost framework suggests that this decision is based on rational benefit–cost analyses of opportunity magnitude before action ([ 6]). COR theory offers a similar explanation that the pursuit of an opportunity is a trade-off between resource acquisition (e.g., the expected benefits of a sale) and resource conservation (e.g., the costs of resources expended on pursuing the opportunity). Second, salespeople make this decision under uncertainty. In line with contingency decision-making frameworks and COR theory, we expect that salespeople's benefit–cost analysis of opportunity magnitude is contingent on conversion uncertainty. This is because uncertainty prevents action by obfuscating ""whether the potential reward of action is worth the potential costs"" ([40], p. 139; see also [26]; [46]).Third, task and personal factors represent additional contingencies that distort the rational benefit–cost analyses. We focus on two sets of contingencies. The first is the selling context (i.e., product vs. solution selling), in which a product denotes a physical object that can be sold in a transactional way (e.g., lamps) and a solution refers to a product-service system (e.g., smart lighting) that requires a relational process and tailoring. Solution selling represents a more uncertain task than product selling ([57]; [60]). Examining the selling context is important because many firms that shift from product to solution selling often struggle to cope with the inherent greater uncertainty (e.g., [18]; [60]). The second set includes two salesperson characteristics related to the propensity to conserve resources under uncertainty. Past performance success refers to salesperson quota attainment in the previous quota period. Salesperson experience refers to a salesperson's time in the sales territory, with the company, and in the sales profession ([ 2]). We focus on these two moderators because prior research suggests these factors are related to how salespeople deal with uncertainty and conserve resources ([26]; [30]; [48]). Overview of StudiesTo test our conceptual framework, we conducted three studies using multiple methods. We provide an overview of our conceptual framework, hypotheses, and the studies in Figure 1. Study 1 focuses on how salesperson initial judgments of opportunity magnitude determine the actual conversion of a prospect into a sale, which in the aggregate influences the salesperson conversion rate at the portfolio level. In doing so, we also investigate how salespeople calibrate opportunity magnitude for opportunity conversion uncertainty and whether such calibration differs between product and solution selling. In Study 2, we examine the heterogeneity of such calibration effect, with a focus on salespeople's past performance success and experience. The dependent variable in Study 2 is salesperson quota achievement, which is theoretically connected with the conversion rate examined in Study 1. In Study 3, a scenario-based experiment, we elucidate the underlying benefit–cost mechanism and the role of resource slack. Table 1 summarizes the key concepts in our framework and corresponding operational measures.Graph: Figure 1. A contingency framework of salesperson decision making when prospecting and overview of three studies.GraphTable 1. Overview of Key Concepts and Operationalizations in Studies 1 and 2. Key ConceptsDescription and Conceptual MeaningConceptual FoundationsOperationalizationStudy 1Study 2Representative StudiesPerformance OutcomeSales performanceDegree to which the salesperson obtains a desired outcomeAhearne et al. (2010)Prospect-level performance: A binary measure, where 0 reflects no deal and 1 reflects that a deal has been made (objective likelihood of conversion)✓Smith, Gopalakrishna, and Chatterjee (2006); Mayberry, Boles, and Donthu (2018)Portfolio-level performance: The ratio of prospects that are successfully turned into deals in a salesperson's portfolio (objective conversion rate)✓Own operationalizationSalesperson performance: Percentage of quota attainment✓Ahearne et al. (2010)Initial Cues and Judgment FormationOpportunity magnitudeThe size of a potential sales optionKumar, Petersen, and Leone (2013)Prospect magnitude: Log-transformed salesperson initial judgment of revenue for a prospect (i.e., deal magnitude in $)✓Mayberry, Boles, and Donthu (2018)Portfolio baseline magnitude: The mean of all prospects' magnitude in a salesperson' portfolio✓Own operationalizationPortfolio magnitude: Reflective four-item construct capturing a solution-selling salesperson's initial estimates in terms of the size of order intake, sales volume, revenue, and profits of their entire portfolio.✓Van der Borgh, De Jong, and Nijssen (2019)Opportunity conversion uncertaintyThe subjective likelihood of being able to convert an opportunity into a desired sales outcomeMcMullen and Shepherd (2006)Prospect conversion uncertainty: Categorical measure differentiating among high, medium, and low levels of likelihood to convert a prospect into a paying customer within six months (−1, 0, and 1), based on salesperson initial judgment.✓Own operationalizationPortfolio baseline conversion uncertainty: The average of prospect conversion uncertainty across all the prospects in a salesperson's portfolio✓Own operationalizationPortfolio conversion uncertainty: Reflective four-item construct capturing a salesperson's initial judgment of uncertainty for realizing anticipated outcomes for solution selling for their entire portfolio (in terms of size of order intake, sales volume, revenue, and profits)✓Own operationalizationContingenciesSalesperson characteristicsDifferences in motivation, attitude, or risk propensity that determine whether a salesperson is willing to bear uncertainty or notMcMullen and Shepherd (2006); Payne, Bettman, and Johnson (1992)Past performance success: The percentage of quota achieved in the previous quota cycle✓Mayberry, Boles, and Donthu (2018)Salesperson experience: Composite measure consisting of three measures of sales experience (i.e., time in sales territory, time with the company, and time in the sales profession). We z-scored these scores and averaged them to form an overall experience index.✓Ahearne et al. (2010)Task characteristicsVarious dimensions, descriptors, or attributes of a particular organizational position that determine task execution and/or outcomesPayne, Bettman, and Johnson (1992)Binary measure indicating whether a prospect requires product selling or solution selling✓Mayberry, Boles, and Donthu (2018)Information levelsDenotes the reference class of judgments, distinguishing between judgment about the specific case or the aggregate of multiple casesSniezek and Buckley (1995)Data are separated into information at the single case level (prospect information) and aggregate level (portfolio baseline information)✓Own operationalization  Study 1: Understanding the Interplay Between Opportunity Magnitude and Conversion UncertaintyStudy 1 examines the interaction effect between opportunity magnitude and conversion uncertainty in product- and solution-selling contexts at both the prospect and portfolio levels. We supplement our theoretical development for this study with verbatim quotes from a qualitative study of seven salespeople (for a description of respondents, see Web Appendix W1). Study 1 Hypothesis Development Benefit–cost analysis of opportunity magnitudeWe predict that the effect of opportunity magnitude on salesperson performance follows an inverted U-shaped relationship. This is due to two countervailing underlying mechanisms: a linear positive effect from potential benefits of pursuing an opportunity and a curvilinear effect from potential costs of such pursuit. We follow [24] recommendation to visually summarize this benefit–cost analysis in Figure 2, Panel A. On the one hand, the higher the magnitude of the opportunity, the greater the extrinsic and intrinsic benefits of pursuing a sizable opportunity. Extrinsic benefits take the form of potential compensation and recognition from the selling firm, whereas intrinsic benefits include the potential enjoyment in pursuing sizable opportunities and the learning associated with the pursuit ([ 7]; [15]).Graph: Figure 2. Illustration of theoretical arguments for the inverted U-shaped effect and the calibration effect.On the other hand, pursuing a sizable opportunity entails substantial explicit and implicit costs. Salespeople incur explicit costs when pursing an opportunity because they need to invest resources, such as time and effort ([48]). Implicit costs refer to the opportunity costs of such pursuit because when pursuing an opportunity, salespeople must forgo other opportunities ([54]). As the opportunity magnitude increases, both explicit and implicit costs accelerate significantly because salespeople are constrained by limited resources and information-processing capacity ([30]; [48]; [55]). One senior salesperson of a software company explained this issue succinctly:Big opportunities? Um, the pros. The prospect of earning a load of money.... Second, obviously it's also more satisfying or fulfilling.... So, it's more complex, which is also like a nice challenge. Plus, you learn the most from complex deals and bigger deals.... But it costs a lot of time and whatever time you spend on one deal you cannot reinvest anymore in smaller deals. So, there's always a trade-off.Therefore, when both potential benefits and costs are considered, the effect of opportunity magnitude on salesperson performance will incrementally increase at first, but after a threshold, the costs to act on moderate to large opportunities outweigh their benefits. Thus, H1:  All else being equal, salesperson judgment of opportunity magnitude has a curvilinear, inverted U-shaped effect on salesperson performance. Opportunity magnitude–conversion uncertainty calibrationBy itself, conversion uncertainty can be a source of benefits for salespeople because uncertainty stimulates positive feelings and excitement ([50]). However, conversion uncertainty also increases costs of pursuing—both explicit costs (i.e., salespeople need to exert greater effort to convert highly uncertain opportunities) and implicit costs (i.e., highly uncertain opportunities carry higher opportunity costs). Under the compensatory decision-making strategy, salespeople assess the benefits and costs of opportunity magnitude and conversion uncertainty in an additive manner. However, our interviews suggest that salespeople at times calibrate for conversion uncertainty in a multiplicative rather than additive manner. For example, one solution-selling salesperson was very clear on his decision-making strategy to deal with conversion uncertainty:When I take a look at a deal that has a very high certainty, so say you've a 90% closing chance, but it's very small in size, and you have a very big deal that has a small closing chance. Yeah, I can just multiply it [size by uncertainty] and see where I get the most buck for my uncertainty, so to speak. Even though it's very simple, it's pretty effective.We refer to this multiplicative strategy as the calibration hypothesis, such that the inverted U-shaped relationship between opportunity magnitude and salesperson performance in H1 is contingent on conversion uncertainty ([40]). We again follow [24] recommendation to visually summarize how conversion uncertainty influences the benefit and cost functions in our arguments in Figure 2, Panel B.In terms of the benefit function, the (previously noted) solution-selling salesperson's calculus is consistent with both expectancy theory and COR theory ([30]; [64]). These theories suggest that salespeople calculate the expected benefits of an action by multiplying its benefits by the success odds (i.e., expected benefits = magnitude-based benefits × conversion uncertainty). Thus, when conversion uncertainty is high, the expected benefits of pursuing a large opportunity are lower, making it less motivating to pursue. Importantly, this calculus is universal, without evoking any individual characteristics as contingencies. Therefore, conversion uncertainty weakens the slope of the benefit line, shifting the inverted U-shaped effect of opportunity magnitude on salesperson performance to the left.How do salespeople calibrate for conversion uncertainty in assessing the costs of pursuing an opportunity? As mentioned previously, conversion uncertainty can be positively stimulating for risk seekers but harmfully costly for people who want to conserve resources. In this regard, prior research indicates that salespeople are heterogeneous in their risk-seeking behavior for various reasons, such as their past performance success and their capability (e.g., [42]). Given this heterogeneity, the cost function can swing in either direction, and thus we predict that, in the aggregate, opportunity conversion uncertainty may appear as not having an influence on the cost function. For the leftward shifting effect to occur, opportunity conversion uncertainty only needs to shift the benefit function downward and does not need to change the shape of the cost function ([24]). Thus, H2:  Opportunity conversion uncertainty moderates the effect of opportunity magnitude on salesperson performance, such that it shifts the inverted U-shaped effect of opportunity magnitude on salesperson performance to the left. Solution- versus product-selling taskPrior research on decision making suggests that, under high levels of situational uncertainty, people search for a relevant reference class to calibrate their judgments ([23]; [33]). However, [60] suggest that, beyond outcome uncertainty, such as conversion uncertainty, solution selling has a higher level of need and process uncertainty than product selling. We argue that it is this difference in overall situational uncertainty that causes salespeople to calibrate for conversion uncertainty differently when selling products versus solutions. Specifically, because need and process uncertainties are higher in solution selling, salespeople do not have a reliable frame of reference to count on. By contrast, because need and process uncertainties are lower in product selling, salespeople can confidently draw from their knowledge of customer needs and requirements, the sales process, and product configurations to deal with conversion uncertainty. Therefore, compared with product-selling salespeople, solution-selling salespeople are more sensitive to conversion uncertainty and tend to calibrate for this uncertainty more intensely. Thus, we expect the weakening effect of conversion uncertainty on the benefits function predicted in H2 to be stronger in solution- than product-selling contexts. H3:  The leftward shifting effect of opportunity conversion uncertainty on the inverted U-shaped relationship between opportunity magnitude and salesperson performance is stronger in solution- than product-selling contexts. Institutional ContextWe collected data from a Fortune Global 500 firm, a market leader in lighting products and solutions for enterprise customers; at the time of data collection, the company generated more than $25 billion annually in total revenue. The company provides a broad portfolio of lighting offerings, ranging from products (e.g., luminaires, lighting electronics, horticulture lighting) to system solutions (e.g., connected, smart luminaires; lighting management software). Customers come from various industries, such as food and fashion retail, health care, education, sports, municipalities, hospitality, infrastructure, and manufacturing. For its field-based sales approach, the company relies primarily on direct sales, and salespeople are subject to the same compensation and incentive scheme. Salespeople obtain a fixed yearly salary plus commission (maximum 30% of the fixed salary). To explore the impact of initial judgments of opportunity magnitude and conversion uncertainty, we gathered archival data from the company's sales force automation (SFA) system for all prospects within one market. For every prospect, we obtained transaction-level records from January 2016 to May 2017, including initial estimates of opportunity magnitude (i.e., prospect deal size) and conversion uncertainty, updated estimates, and the final sales outcome. The SFA data cover 12,988 B2B prospects, handled by 173 salespeople, who logged 110,278 events in total. We provide supplemental information about the research context and the SFA data in Web Appendix W2. Manifest VariablesBecause we are interested in the performance impact of a salesperson's initial judgments, we aggregated the event-level data to the prospect level. This approach allows us to estimate a two-level model in which prospect-level data (case-specific; within salesperson) are nested within portfolio-level data (baseline; between salesperson). Focal variablesA unique feature of Study 1 is that we leverage the company's SFA data to operationalize the key variables. We measure opportunity magnitude as the salesperson's initial point estimate of a prospect in terms of revenue. Following [62], we log-transform the measure to correct for right-skewness. We measure opportunity conversion uncertainty as a categorical measure that captures the probability of converting a prospect into a deal within six months. We coded these categories as −1 (low), 0 (medium), and 1 (high) to facilitate interpretation and enhance model parsimony ([16]). We measure prospect-level performance as a binary measure that indicates the actual conversion of a prospect at the end of the sales cycle (0 = no deal, 1 = a deal). Portfolio-level performance indicates the salesperson's portfolio-level conversion rate, aggregated from their prospect-level actual conversion. Control variablesTo obtain unbiased estimates, we control for the nonlinear effects of uncertainty by including a square term ([20]). To zero in on the effects of initial judgments, we control for several time-related dynamics. Specifically, we control for five (e.g., [ 1]). First, we control for duration of a sales cycle by including the sales cycle length ([39]). Second, we control for frequency of leads by including workload ([48]), measured as the total number of leads under a salesperson's wing during the assessment. Previous studies have shown that workload affects judgments (e.g., [22]). Third, we control for the frequency of uncertainty updates (i.e., process uncertainty), which reflects the total number of changes a salesperson has made after the initial uncertainty estimate. It reflects the doubt inherent in the sales process and filters out variation in the dependent variable after salespeople made their initial judgments, which is the focus of the article. Fourth, we control for accuracy and recency effects by including the difference between the initial and final estimates for opportunity magnitude and uncertainty (magnitude accuracy = [last estimate − first estimate]; conversion uncertainty accuracy = [first estimate − last estimate]). Fifth, we control for timing- and sequence-related dynamics by including time fixed effects. We use dummy variables to account for the prospect's industry (i.e., public, office, retail, and other). We also control for the potentially curvilinear effect of uncertainty because prior research suggests that people respond more rigorously to two ends of the uncertainty continuum than to moderate levels of uncertainty ([ 2]; [55]; [67]). Web Appendix W3 provides sample descriptives and the correlation matrix. Empirical Strategy Levels of analysis and centeringAs is true in many B2B selling contexts, salespeople are responsible for a portfolio of prospects. In the ""Study 1 Hypothesis Development"" subsection, we use the term ""opportunity"" without specifying whether this opportunity is at the prospect or portfolio level. However, previous studies in decision making (e.g., [33]; [58], [59]) show that people ( 1) can leverage two levels of information (case-specific and base-rate) and ( 2) use a reference point. From a multilevel perspective, portfolio baseline judgments are essentially salesperson-level constructs that capture between-salesperson variation, serving the function of the base-rate information about the sales territory. Prospect-level judgments reflect within-salesperson judgment about specific prospects relative to each salesperson's portfolio baseline judgments ([12]). Because assuming that an effect existing at a higher level will generalize to a lower level (or vice versa) can be erroneous ([13]), we employ multilevel modeling techniques to estimate the impact of a salesperson's initial judgments of opportunity magnitude and conversion uncertainty on performance and test the effect at the prospect (case-specific) and portfolio (baseline) levels. Conceptually, people tend to rely heavily on the mean value in their decision making ([28])—akin to a salesperson's baseline. Therefore, in Study 1, we examine the portfolio average magnitude and average conversion uncertainty as the reference points at the portfolio level and refer to these as portfolio baseline magnitude and conversion uncertainty.We specify a multilevel model using Mplus 8.3 ([44]). To allow for unbiased estimates at the between and within levels, we decompose the manifest variables into uncorrelated latent ""between"" and ""within"" components ([47]). Latent means of focal variables are estimated at the between level, while ""pure"" within-person effects are estimated in the within-level portion of the model. This specification is necessary for teasing apart prospect- and portfolio-level effects ([47]). EstimationThe complexity of the model and the use of a binary dependent variable did not allow use of robust maximum likelihood estimation techniques because of a lack of model convergence. As an alternative, we employed Bayesian estimation techniques and therefore specified a Bayesian multilevel probit model. We estimate two sets of models for the pooled (no separation of selling contexts), product-selling, and solution-selling data, respectively. Models 1–3 are main-effects-only models, and Models 4–6 are the interaction-effects models. All the manifest variables are standardized to aid in interpretation, with the exception of conversion uncertainty. We provide details on the model specification and estimation in Web Appendix W4. Endogeneity considerationsThe effect of opportunity magnitude and conversion uncertainty on performance may be endogenous, because common unobserved factors may influence both predictors and outcomes in our model (due to, e.g., simultaneity, measurement error, omitted variables). Following prior studies ([22]; [49]; [63]), we address endogeneity in three ways: ( 1) by adopting a rich data-modeling approach, ( 2) by controlling for endogeneity due to omitted variables, and ( 3) by checking for endogeneity due to selection bias. Because we consider a multilevel setting, we need to address endogeneity at each level ([37]). We correct for Level 1 endogeneity using a control function procedure ([49]) and check for robustness with an instrument-free Gaussian copula approach ([45]). We control for Level 2 (cross-level) endogeneity in our multilevel latent covariate model by allowing a correlation between random intercepts and slopes ([ 3]). A direct test of the random-effects assumption (Wald  χ12   = 2.226, p = .136) indicates that Level 2 endogeneity is not a concern ([ 3]). Web Appendix W4 provides further details of model-free evidence of inverted U-shaped relationship, robustness checks of adding higher-order terms and seasonal variation to the empirical model, and endogeneity corrections. ResultsWe present the results of our tests for H1 and H2 for solution selling at the portfolio level before discussing the differences between solution selling and product selling (H3). In the ""Discussion"" section, we explore differences across portfolio and prospect levels. Benefit–cost analysis of opportunity magnitudeTable 2 shows the results of the analyses. To test H1, we follow a rigorous three-step procedure ([24]). First, we find a significant, negative effect of (opportunity magnitude)2 on salesperson performance for solution selling (Model 3: γ02 = −.142, p < .001). We plot this effect in Panel A of Figure 3, which shows an inverted U-shape. Second, we formally test that the marginal effects on the left side of the turning point of the inverted U-shape are positive and significant and those on the right side of the turning point are negative and significant. Mathematically, we test whether γ01 + 2γ02XL is positive and significant and γ01 + 2γ02XH is negative and significant, where XL and XH represent low and high values of opportunity magnitude within the data range, respectively. For portfolio baseline magnitude, the results confirm this pattern (for details, see Web Appendix W5). Third, we examine whether the turning point (i.e., X) is located within the data range. Taking the first derivative of the Level 2 equation specified for Model 3 and setting it to zero yields a turning point X of −γ01/2γ02. We found that the turning point is 2.52 SD below the mean value and within the data range (Xsolution = −2.52 SD; 95% confidence interval [CI] = [−4.15, −1.38]). Overall, these results confirm an inverted U-shaped relationship between opportunity magnitude and salesperson performance for solution selling, in support of H1.Graph: Figure 3. Study 1: inverted U-shape and the moderating effect of opportunity conversion uncertainty in solution selling.GraphTable 2. Study 1—Results of Multilevel Probit Analyses: Effect of Initial Judgment on Performance Outcomes. Step 1Step 2aHyp.Model 1: PooledModel 2: ProductsModel 3: SolutionsModel 4: PooledModel 5: ProductsModel 6: SolutionsbSDbSDbSDbSDbSDbSDL2: DV = Portfolio-Level PerformanceMagnitude (γ01)−.536***.149−.301*.162−.716**.232−.562***.178−.377*.198−.240.303Magnitude2 (γ02)−.264***.056−.225**.066−.142***.045−.261***.056−.234***.069−.161***.048H1Uncertainty (γ04)−1.136***.249−1.026***.269−1.541**.481−1.248***.262−1.086***.279−1.556***.466Magnitude × Uncertainty (γ03)——————.051.158.108.175−.663**.227H2/H3L1: DV = Prospect-Level PerformanceMagnitude (β1j)−.456***.075−.410***.062−.536***.155−.393***.052−.414***.065−.484***.159Magnitude2 (β2j)−.142***.026−.158***.022−.098**.038−.123***.019−.157***.022−.129***.043Uncertainty (β4j)−.932***.112−.877***.105−.571*.258−.911***.094−.885***.104−.510***.259Magnitude × Uncertainty (β3j)——————−.017.040.011.045−.053.086ControlsUncertainty2 (L2).323.275.249.3041.100*.515.394***.273.251.309.929*.508Uncertainty2 (L1)−.093.080−.129.083−.167.228−.058.077−.126.079−.235.237Process uncertainty−.270***.016−.288***.017−.204***.040−.276***.016−.290***.017−.207***.040Sales cycle length.092***.021.088***.023.015.142.087***.020.091***.022.069.144Magnitude accuracy−.047***.012−.026*.013−.095***.035−.047***.013−.027*.013−.096**.035Uncertainty accuracy.820***.028.835***.031.675***.073.819***.028.834***.031.684***.073Workload−.004.008.002.008−.008.019−.002.006.003.007−.010.019ε^Magnitudeb.020.039−.006.038.168.119−.023.032−.008.039.114.124Magnitude × ε^Magnitude−.006.015−.011*.016.096*.058−.038**.016−.012.017.149**.066ε^Uncertainty.062*.030.015.028−.095.070.000.027.015.029−.112.071Uncertainty × ε^Uncertainty.109***.021.106*.018.191***.047.095***.016.103***.020.194***.048IMR−1.291***.084−1.416***.088−2.292***.593−1.027***.079−1.423***.087−2.112***.592Industry fixed effectsYesYesYesYesYesYesTime fixed effectsYesYesYesYesYesYesConstant (γ00)1.364***.2021.282***.1611.465.8331.199***.1561.220***.1731.101.866Pseudo-R2: L2/L1.795/.554.645/.545.805.716.801/.509.678/.543.831/.713n (prospects)/N (salespeople)12,988/17310,991/1661,997/12112,988/17310,991/1661,997/121 5 *p < .05.1 **p < .01.2 ***p < .001; unstandardized coefficients.6 a To determine the extent of bias in our estimates ([24]), we also tested an extension of Models 4–6, in which we included Magnitude2 × Uncertainty at L2 (γ05) and L1 (β5j). The results for these models showed nonsignificant effects. The results show that neither interaction is significant (  γ05   = .064, p > .05;  β5j   = .002, p > .05).7 b We also tested the robustness of our findings by controlling for endogeneity using the instrument-free Gaussian copula approach. Findings are similar when compared with the control function approach reported in this table. For details of the results of these additional analyses, see Web Appendix W4.8 Notes: L2 = portfolio level; L1 = prospect level. Magnitude = opportunity magnitude; Uncertainty = opportunity conversion uncertainty. IMR = inverse Mills ratio. Opportunity magnitude–conversion uncertainty calibration hypothesisTo test H2, we add the interaction term (magnitude × uncertainty)ij to the equation. The results of Model 6 in Table 2 confirm that the interaction is significant and negative in the solution-selling context (γ03 = −.663, p <.01). However, we cannot determine significance from the estimated interaction term alone in a nonlinear model (probit) with nonlinear interaction terms ([65]). Therefore, we formally test how the turning point changes as conversion uncertainty changes. To do so, we derive the turning point ""magnitude*"" (X*) by setting the first derivative of Model 6's equation with respect to X to zero. Then, we take the derivative of the turning point with respect to conversion uncertainty (Z) to show how the turning point changes as conversion uncertainty changes, yielding δX*/δZ = (−γ02 γ05)/[2(γ02)2]. We find that this term is significant and negative (b = −2.032, p <.01), in support of H2. Solution- versus product-selling task hypothesisTo test H3, we compare the results for the two selling tasks (see Table 2). At the portfolio level, in contrast with the significant and negative interaction in the solution-selling context (Model 6: γ03 = −.663, p <.01), we find no interaction effect of opportunity magnitude and conversion uncertainty on salesperson performance for product selling (Model 5: γ03 = .108, p >.10). Testing the difference between solution versus product selling reveals significant differences at the portfolio level (Δ[γ03_Solution; γ03_Product] = .771, p <.01), in support of H3. DiscussionStudy 1 provides evidence of an inverted U-shaped relationship between opportunity magnitude and sales performance, across levels and selling context. However, we only find evidence of the calibration hypothesis for the solution-selling context at the portfolio level. This suggests that salespeople respond differently to opportunities of different magnitude, depending on the baseline conversion uncertainty of their portfolio of solution prospects. To illustrate the impact of this effect, we ran a simple counterfactual analysis and compared the calibration model with a compensatory model (i.e., fixing the interaction coefficient γ03 to zero). The results show that under certain conditions the compensatory model over- or underestimates conversion rates by almost 100% (e.g., predict 100% conversion, 0% ""true"" value). For large opportunity magnitude (>∼$26.750), the compensatory model overestimates conversion rates by up to 30% for high conversion uncertainty but underestimates conversion rates by up to 90% for low conversion uncertainty. Overall, these results highlight the importance of the calibration model.We also found that the information level (i.e., prospect- and portfolio-level) matters. While prior research on decision making generally focuses on two simple choices, it might be cognitively impossible for salespeople to constantly make decisions at the prospect level while juggling a portfolio of prospects in their sales funnel. Consistent with this notion, our findings in Table 2 suggest that salespeople adopt a simpler compensatory decision-making strategy at the prospect level (i.e., that accounts for conversion uncertainty in an additive/subtractive manner) but rely on a more complex decision-making strategy at the portfolio level (i.e., a calibration strategy that accounts for conversion uncertainty in an interactive manner, using portfolio baseline information) for solution selling. In practice, salespeople generally have an idea about this so-called portfolio baseline in their assigned territory, such as the average magnitude and uncertainty of their portfolio. They then assess individual opportunities relative to this baseline and prioritize accordingly ([53]; [58]). Study 2: Examining the Heterogeneity of the Calibration Effect in Solution SellingAlthough Study 1 shows that salespeople calibrate for opportunity conversion uncertainty when selling solutions at the portfolio level, it does not investigate how salespeople differ in their calibration. Study 2 focuses on salespeople's past performance success and experience as two key boundary conditions that bias their rational calibration of benefit–cost analyses. Empirically, a test of these contingencies is a three-way interaction test of the two-way interaction in H2. Study 2 Hypothesis Development Past performance successWe first focus on how past performance success influences the way salespeople calibrate their benefit function for conversion uncertainty differently. Compared with salespeople with low past performance, those with high past performance have a higher sense of competence. As a result, they are more risk-seeking and view uncertain opportunities as challenging and intrinsically motivating ([42]; [56]). To these high performers, the intrinsic benefits associated with high conversion uncertainty may outweigh the potential loss of extrinsic benefits. By contrast, salespeople with low past performance repulse opportunities that have high conversion uncertainty. This aversion arises because these highly uncertain opportunities not only threaten their potential extrinsic benefits (e.g., losing compensation and rewards) but also represent an unreliable path to achieve intrinsic benefits (e.g., bolstering their lack of competence; see [15]). Thus, the downward shift of the benefit function created by opportunity conversion uncertainty (predicted in H2) is weaker among salespeople whose past performance success is high (vs. low).In terms of the cost function, being successful in the prior period induces high performers to be more sensitive to opportunity conversion uncertainty for two reasons. First, consistent with COR theory, they are likely to slow down to conserve their resources to reduce stress ([30]). Empirical evidence shows that people tend to hold back after achieving a goal before working on the next goal (e.g., a resetting period; [11]; [34]). Second, high performers are more sensitive to high implicit costs associated with high conversion uncertainty because opportunities that can be converted with certainty allow them to maintain their status (e.g., [38]). Therefore, for high performers, opportunity conversion uncertainty is likely to shift their cost function upward more strongly. This upward shift is especially strong when opportunity magnitude is large because large opportunities require them to invest much more resources. By contrast, while salespeople whose past performance was less successful are also sensitive to the opportunity costs associated with high conversion uncertainty, their main concern is to prove themselves to the firm. Therefore, these poor performers need to exert greater efforts and cannot afford to conserve their resources. As a result, poor performers' cost function shifts upward less strongly when opportunity conversion uncertainty is high.Taken together, compared with salespeople who are low past performers, high performers view opportunities with high conversion uncertainty as more beneficial but also more costly. In prospecting, although all salespeople have limited resources ([48]), high performers are more inclined to conserve their resources than poor performers. Thus, for high past performers, we predict that the upward shifting effect of conversion uncertainty on the cost function will outweigh its downward yet weaker shifting effect on the benefit function. As a result, the expected net benefits of pursuing an opportunity will be lower when conversion uncertainty is high, causing the inverted U-shaped relationship between opportunity magnitude and salesperson performance to shift more strongly to the left ([24]). H4:  The greater salesperson past performance success, the stronger is the leftward shifting effect of opportunity conversion uncertainty on the inverted U-shaped relationship between opportunity magnitude and salesperson performance. Salesperson experienceWe argue that because experienced salespeople differ from less experienced salespeople in terms of resources, they calibrate their benefits and costs under conversion uncertainty differently. First, they have better network-based resources in the form of relationships they have built over time. Second, they are more knowledgeable about various aspects of the sales process (e.g., customers, the market, the competition, the company), another resource critical for success in prospecting ([48]).In terms of the benefit function, the resources accumulated over time make experienced salespeople believe they are more capable, resulting in more risk-seeking behavior ([42]; [56]). For them, the challenge associated with uncertain opportunities can be intrinsically motivating. Because experienced salespeople are more strongly motivated by intrinsic than extrinsic benefits (e.g., [14]), the intrinsic benefits associated with high conversion uncertainty may outweigh the potential loss of extrinsic benefits. By contrast, the lack of capability and resources makes inexperienced salespeople more concerned about potential losses of both intrinsic and extrinsic benefits at high levels of opportunity conversion uncertainty ([42]). Therefore, the downward shifting effect created by opportunity conversion uncertainty on the benefit function is stronger among inexperienced salespeople than experienced ones.In terms of the cost function, the abundance of aforementioned resources makes experienced salespeople less concerned about COR when pursuing opportunities with high conversion uncertainty. Conversely, given their lack of resources, inexperienced salespeople are more concerned about conserving their limited resources and are more sensitive to the costs associated with high conversion uncertainty ([26]; [30]). Thus, opportunity conversion uncertainty is likely to create a weaker upward shift of the cost function among experienced than inexperienced salespeople. Taking the benefit and cost effects together, experienced salespeople expect greater net benefits when conversion uncertainty is high. For them, the inverted U-shaped relationship between opportunity magnitude and salesperson performance shifts less strongly to the left ([24]). H5:  The greater salesperson experience, the weaker is the leftward shifting effect of opportunity conversion uncertainty on the inverted U-shaped relationship between opportunity magnitude and salesperson performance. Institutional ContextIn Study 2, we corroborate Study 1's findings and examine the postulated boundary conditions of salespeople's calibration for conversion uncertainty in solution selling at the portfolio level. We collected data from the sales organization of a large firm ($16.4 billion in total revenue per year). The firm, which operates in the B2B market, provides information and technology solutions (e.g., workspace systems, data center solutions, managed services, security) to customers in industries such as finance, government, education, transport, service, retail, and media. Field-based salespeople are grouped according to the industries the firm serves, with each assigned a territory. All the salespeople are subject to the same compensation and incentive scheme and obtain a fixed yearly salary plus commission (with a progressive plan for all sales beyond quota). Using a survey instrument, we collected information about the salespeople's perceptions of their portfolios. Of the 248 salespeople, 211 completed the questionnaire (85% response rate). Consistent with the length of the average sales cycle, we collected objective salesperson performance from the firm's records six months after the survey. MeasuresIn Study 2, we examine the portfolio magnitude and conversion uncertainty in the aggregate at the portfolio level. Thus, portfolio magnitude corresponds to opportunity magnitude, and portfolio conversion uncertainty corresponds to opportunity conversion uncertainty. Focal variablesTo measure portfolio magnitude, we use the expected customer demand scale from [61]. The scale has four items that cover salespeople's judgment of the opportunity magnitude in terms of order intake, sales volume, revenue, and profits for the solutions in their portfolio. To measure portfolio conversion uncertainty, we developed a new scale that asks salespeople to assess their degree of (un)certainty about the portfolio magnitude. We inversely coded the scores to obtain uncertainty scores. We obtained salesperson past performance success (in the previous quota cycle) and salesperson performance from company databases. We used the percentage of quota achievement, as previous studies indicate that it accurately captures measurable task performance output while accounting for situational factors ([ 2]). Following previous studies (e.g., [ 2]), we operationalize salesperson experience as a composite measure consisting of three separate measures of experience: time in sales territory, time with the company, and time in the sales profession. Control variablesWe control for nonlinear effects of uncertainty by including a squared term ([20]). Dummy variables account for salespeople's industry. We also account for individual characteristics that may influence their judgments (i.e., age and workload). Finally, we control for salesperson trait competitiveness, measured with a scale from [ 9]. Web Appendix W6 provides measurement scales and descriptives of Study 2. Empirical Strategy Validation of measurement modelA confirmatory factor analysis of the measures indicated good model fit (  χ412   = 88.108, p < .01; comparative fit index = .950; Tucker–Lewis index = .933; root mean square error of approximation = .074; square root mean residual = .045; [ 5]). The scales achieved sufficient reliability, with composite reliabilities between.77 and.90 and average variances extracted exceeding.50 for all constructs, indicating reliability. The average variance extracted of each construct exceeds the average variance shared with any other construct, providing evidence of discriminant validity. In addition, all factor loadings are significant (p < .01) and have standardized values ranging from.65 to.91, thus demonstrating convergent validity of the constructs. To examine the effects of opportunity magnitude, conversion uncertainty, past performance success, and salesperson experience on salesperson performance, we specified a multilevel model in Mplus 8.3 ([44]) to control for the nesting of the data. We provide the model specification in Web Appendix W7. Endogeneity considerationsThe effect of salesperson opportunity magnitude and conversion uncertainty on sales performance may be spurious as a result of omitted variables (e.g., group-level factors) and correlation between independent variables and the error terms. For example, a sales manager's and coworkers' judgments may influence a salesperson's judgments and performance outcomes. To control for possible endogeneity in our analyses, we adopted [21] control function procedure. Web Appendix W7 provides further details. ResultsWe present the results of our retests of the main effects of the opportunity magnitude (H1) and calibration (H2) hypotheses for solution selling at the portfolio level. We then report the findings regarding the boundary conditions of past performance and salesperson experience (H4 and H5). Main effect of opportunity magnitudeWe report the results in Table 3. To retest H1 about the inverted U-shaped effect of opportunity magnitude on salesperson performance, we again rely on the three-step approach. First, in line with our results from Study 1, we find that opportunity magnitude2 has a significant, negative effect (Model 7: ζ2 = −.059, p < .05). Second, we formally test marginal effects. We find that the slope is positive and significant for low values of opportunity magnitude and negative and significant for high values (see Web Appendix W5). Third, we calculate the turning point. The estimated turning point is just above the average of the opportunity magnitude scale (i.e., mean + .36 = 3.55), with an estimated CI well within the data range (95% CIraw score = [3.02, 4.78]). These results confirm the inverted U-shaped relationship between opportunity magnitude and salesperson performance, corroborating H1.GraphTable 3. Study 2: Results (Solution-Selling Context). PortfolioRobust Maximum Likelihood EstimatesHyp.Model 7Model 8Model 9Model 10bSDbSDbSDbSDMagnitude (ζ1).042.043.040.043.057.041.098**.038Magnitude2 (ζ2)−.059*.026−.074**.029−.075**.028−.116***.030H1Uncertainty (ζ3)−.087**.037−.088*.038−.121*.056−.098.059Past performance success (ζ4).040.027.042.028.037.039.034.037Salesperson experience (ζ5).015.078.029.078.071.108.035.109Moderation EffectsMagnitude × Uncertainty (ζ6)——−.044*a.025−.044.027−.066*.028H2Magnitude2 × Uncertainty (ζ7)————.026.016.018.013Magnitude × Past perf. success (ζ8)————−.051.037−.026.034Uncertainty × Past perf. success (ζ9)————−.054.037−.012.043Magnitude2 × Past perf. success (ζ10)————.007.026−.023.032Magn. × Uncert. × Past perf. success (ζ11)——————−.058*.027H4Magn.2 × Uncert. × Past perf. success (ζ12)——————−.022.016Magnitude × Salesperson exp. (ζ13)————−.107*.064−.142*.059Uncertainty × Salesperson exp. (ζ14)————.041.039.150*.079Magnitude2 × Salesperson exp. (ζ15)————.024.041.075*.033Magn. × Uncert. × Salesperson exp. (ζ16)——————.016.044H5Magn.2 × Uncert. × Salesperson exp. (ζ17)——————−.070**.024ControlsAge.019.049.017.048.015.051.014.056Workload−.123***.038−.119***.037−.113**.038−.101**.038Trait competitiveness.029.041.037.040.033.040.054.040Dummy Govt. & Edu.−.018.147−.020.148−.049.123−.010.142Dummy Industry & Transport−.074.110−.075.112−.134.107−.102.130Dummy Services, Retail & Media.014.110.006.112−.031.105.044.136Uncertainty2.054**.020.041*.020.035.022.027.022Constant1.826***.0871.841***.0891.890***.0791.876***.100Pseudo-R2.161.171.208.250Log-likelihood−800.078−798.029−793.395−788.232Log-likelihood χ2 diff. test (d.f.)b—5.07(1)*11.33(7)344.20(4) ***cN (salespeople)211211211211 9 *p < .05.3 **p < .01.4 ***p < .001; unstandardized coefficients.10 a To determine the extent of bias in our estimates ([24]), we also examined an extension of Model 8 in which we added Magnitude2 × Uncertainty (ζ7) to our equation. The results show that the coefficient of this interaction is not statistically different from zero (ζ7 = .018, p > .10) and does not improve model fit (χ21 = .660, p > .10). See also Web Appendix W7.11 b When using the MLR estimator in Mplus, a log-likelihood difference test statistic is calculated using log-likelihoods and scaling correction factors for the null and alternative models.12 c Model fit of Model 10 is also significantly better than that of Model 8 (χ21 = 41.97, p < .001).13 Notes: Magnitude = opportunity magnitude; uncertainty = opportunity conversion uncertainty. Moderating role of conversion uncertaintyTo retest H2, we add ζ6(magnitude × uncertainty)jh to the equation and test its significance. Model 8 in Table 3 shows that the interaction is significant and negative (ζ6 = −.044, p < .05). We estimate the change in the turning point using the same approach we reported in Study 1. The result again confirms H2, as the change in the turning point is negative and significant (b = −.301, p < .05). Salesperson characteristics as moderatorsTo test the moderating role of past performance success and salesperson experience, we extend Model 8's equation and test ζ11 and ζ16. Model 10 in Table 3 shows that ζ11 is negative and significant (ζ11 = −.058, p < .05), in support of H4. By contrast, ζ16 is not significant (ζ16 = .016, p > .10) and thus does not support a shifting effect, as postulated in H5. Instead, we find a significant, negative curvilinear moderating effect of salesperson experience (ζ17 = −.070, p < .01), suggesting a flipping effect.We performed several additional robustness checks of our results. First, we used the Wilcoxon rank-sum test (p = .270) to compare respondents and nonrespondents. The tests showed no significant differences between respondents and nonrespondents, alleviating concerns about self-selection bias in our sample. Second, results from Ramsey's regression error specification test (RESET) (χ2 = 1.14, p = .285) alleviate concerns about omitted variables. Third, the maximum variance inflation factors is 3.62, well below the threshold value of 10 ([25]), indicating no multicollinearity issues. Fourth, to test heteroskedasticity we conducted the Cameron–Trivedi test (p = .337) and Breusch–Pagan test (p = .269), neither of which was significant, thus alleviating concerns about heteroskedasticity in our results. DiscussionThe results of Study 2 not only corroborate the key findings of Study 1 in a different context but also reveal the boundary conditions of salesperson calibration. Figure 4, Panel A, reveals that high past performance success triggers COR, whereas low past performance success provokes more risk-seeking behavior. Salespeople with high past performance success perform best under low uncertainty, whereas those with low past performance success do better under high uncertainty (see right-hand and left-hand sides, respectively). Figure 4, Panel B, shows that highly experienced salespeople perform best under the most challenging situations (low/moderate opportunity magnitude; high conversion uncertainty). However, the left-hand side shows that for inexperienced salespeople, high conversion uncertainty dampens quota achievement significantly. These findings suggest that less experienced salespeople tend to conserve resources under high conversion uncertainty, whereas highly experienced salespeople are more willing to bear uncertainty because they have more resources available.Graph: Figure 4. Study 2: three-way moderating effects (opportunity magnitude × opportunity conversion uncertainty × salesperson characteristics). Study 3: Unpacking the Benefit–Cost Mechanisms in ProspectingStudy 3, a scenario-based experiment, has three objectives. First, we replicate the inverted U-shaped effect of opportunity magnitude on sales performance in a controlled setting. Second, we unpack the underlying benefit–cost mechanism of this effect assumed in Studies 1 and 2. Third, we examine the role of resource slack to show the appropriateness of using COR theory. Study 3 Hypothesis DevelopmentAccording to COR theory, salespeople with limited resources are more likely to conserve them than those with abundant resources ([26]; [30]). For the former, small opportunities do not provide significant resources, whereas large opportunities are prohibitively resource straining. Thus, the indirect negative effect of opportunity magnitude on willingness to pursue an opportunity through costs is amplified when salesperson resource slack is limited. For these salespeople, the total indirect effect of opportunity magnitude through benefits and costs follows an inverted U-shape. By contrast, salespeople with high resource slack are motivated to pursue larger opportunities because they have the resources and, by mobilizing them, can gain even more resources ([26]). For these salespeople, the total indirect effect of opportunity magnitude through benefits and costs is convex. Thus, H6:  Salesperson resource slack buffers the negative effect of costs on salesperson willingness to pursue an opportunity. As a corollary, the total indirect effect of opportunity magnitude on salesperson willingness to pursue an opportunity follows an inverted U-shape only when salesperson resource slack is low. Method and Results Sample and procedureGiven the consistent findings of an inverted U-shape across levels, in Study 3 we focus on the prospect level. We partnered with a prominent market research firm to access a diverse panel of salespeople from various industries. The research firm randomly recruited 216 experienced salespeople (64% 36–45 years of age, 62% male, 53% in the information technology industry) for our between-subjects experiment.We then randomly assigned them to one of five scenarios. Each scenario informed participants that they were assigned a territory where the typical revenue of a prospect was $50,000. We included this portfolio baseline information to ensure the design matches with Study 1 and real-life selling contexts. They identified a new sales lead (Prospect A) with a specific opportunity magnitude. In line with data from Study 1, we set the opportunity magnitude at five levels: $1,000, $10,000, $50,000, $250,000, and $1,000,000. After participants read the scenario, we assessed their willingness to pursue Prospect A, anticipated costs, and anticipated benefits on a seven-point scale (1 = ""strongly disagree,"" and 7 = ""strongly agree"") and their resource slack for prospecting activities. We used the natural variation of salespeople's resource slack in their jobs, as previous research shows that resource slack affects people's framing of costs and benefits in decision making ([68]). Post hoc tests indicated that resource slack was not differently distributed between treatment groups (F = .30; p > .10), thereby providing evidence that the manipulation itself did not affect participants' perceptions of resource slack. We included the manipulation check, attention and realism checks and demographic questions. ResultsAnalysis of variance revealed significant between group differences in willingness to pursue the prospect (F( 4, 211) = 2.835, p = .025). Specifically, willingness to pursue is significantly greater (p < .05) in the $50,000 condition (5.50) than in the small ($1,000; 4.75) or large ($1,000,000; 4.96) conditions. Thus, we replicate the inverted U-shaped effect of opportunity magnitude found in Studies 1 and 2. We then specified the path model of the Study 3 panel in Figure 1. We find that the effect of costs on willingness to pursue is contingent on resource slack (b = .242, p < .01). To test the mediating benefit–cost mechanism and the COR effect, we examined the ""instantaneous conditional indirect effect"" of opportunity magnitude on willingness to pursue, with salesperson resource slack as the moderator ([29]). The results show that when resource slack is low, the total indirect effect of opportunity magnitude through the two mediators is only significant at moderate levels of opportunity magnitude (θopp.mag=2 = .111, p < .05). When resource slack is high, moderate to high levels of opportunity magnitude translate significantly into willingness to pursue (θopp.mag=5 = .202, p < .01). These results lend support to H6 and our contention that, under resource constraints, the inverted U-shaped effect of opportunity magnitude operates through the benefit–cost mechanism. For further details, see Web Appendix W8. General DiscussionIntegrating decision-making and COR theories, we develop and test a framework of salesperson decision making when prospecting in three multimethod studies. Together, the empirical evidence explains why salespeople avoid big-whale sales opportunities. Theoretical ContributionsOur research stems from the idea that salespeople differ from participants in studies that focus on low-effort, constraint-free, and repeatable choices ([36]). First, the potential benefits—extrinsic and/or intrinsic—of salespeople's decisions are consequential rather than trivial. Second, given their resource constraints and the ephemeral nature of sales opportunities, their costs—explicit and/or implicit—are not negligible. Third, their decision-making context abounds with uncertainties. Our findings confirm and provide novel insights into the theoretical importance of these differences for research on salespeople's decision making, especially when prospecting. Benefit–cost analysis in salesperson prospectingWe provide strong empirical evidence that in deciding on which opportunities to pursue, salespeople conduct a benefit–cost analysis based on their initial judgment of opportunity magnitude. We show that the relationship between initial judgment of opportunity magnitude and actual conversion follows an inverted U-shape, regardless of selling task (product vs. solution selling) and information level (prospect vs. portfolio). This finding debunks the intuition that salespeople gravitate toward big-whale opportunities, an insight that extends current understanding of salesperson prospecting behavior. Our result also confirms that salespeople's initial judgment of opportunity magnitude exerts a strong impact on their subsequent behavior and performance, even after controlling for transient phases. This finding complements prior research on salesperson intuition ([27]) and on primacy and anchoring effects ([58]). Salesperson calibration for conversion uncertaintyWe also found that solution-selling salespeople take into consideration opportunity conversion uncertainty in their benefit–cost analysis. Due to this calibration, the inverted U-shaped relationship between opportunity magnitude and performance shifts to the left. This shift implies that salespeople are generally more risk-seeking when opportunity magnitude ranges from small to moderate and risk-averse when opportunity magnitude is large. The counterfactual analyses we conducted show that the calibration effect reduces misspecification of conversion rates by up to 100%, when compared with the estimates from a compensatory decision strategy in which uncertainty is simply factored in as an extra cost. This finding provides a more nuanced understanding of the differences between salespeople's decision-making strategies (i.e., calibration vs. compensatory) when prospecting. Furthermore, it joins two separate streams of research on salesperson decision making, one focusing on salesperson judgment of demands and the other on uncertainty. Selling contexts and calibrationCompared with product selling, solution selling is full of uncertainties (e.g., need, process, outcome; [60]). Although these uncertainties are likely to influence salesperson behavior, salesperson behavior and decision making in solution selling has not received much academic research. We contribute to the literature by showing that salespeople indeed use different decision-making strategies in solution selling versus product selling. Specifically, they rely on a calibration decision-making strategy only in solution selling and only at the portfolio level. At the prospect level, regardless of the selling context, salespeople assess each individual opportunity relative to their portfolio baseline in terms of magnitude and conversion uncertainty using a compensatory strategy in which a large magnitude can make up for high uncertainty (and vice versa). Salesperson characteristics and calibration in solution sellingWe find that salesperson past performance success and salesperson experience are important contingencies of salespeople's decision-making process under uncertainty (i.e., calibration). The interaction plots (Figure 4) suggest that salespeople who have achieved past performance success and/or have low experience tend to conserve their resources and become more risk averse when selling solutions. They perform better under low or average than high conversion uncertainty conditions. Experienced salespeople are able to overcome this cautious approach.Our findings also address the contrast between the COR perspective ([30]) and the risk-seeking perspective based on research on risky choice, such as gambling ([56]). While the latter perspective is not specific to the selling context, the COR argument is uniquely relevant to the personal selling context, as it accounts for the notions that ( 1) salespeople are subject to resource constraints and their efforts are costly and ( 2) salespeople need to conserve resources to avoid stress in the long run ([48]). Therefore, researchers who apply decision-making theories to the context of personal selling will benefit from accounting for the uniqueness of salespeople as decision makers. Information level and calibrationOur findings highlight a dual information-processing framework in salesperson decision making when prospecting for solution-selling opportunities ([51]; [58]). Under these conditions, we find that salesperson performance is a function of two processes. At the prospect level, salespeople rely on a simple compensatory model in their decision making, such that a large magnitude can make up for high uncertainty (and vice versa). At the portfolio level, they integrate information about both the magnitude and uncertainty of the prospects they targeted in a more complex calibration model. In this decision-making strategy, conversion uncertainty interacts with opportunity magnitude in driving salesperson portfolio performance. This insight is a meaningful step toward a better understanding of salespeople's prioritization of resources and the importance of considering salesperson characteristics in prospecting. It also sheds first light on potential differences between findings at the prospect level and those at the portfolio level (i.e., a lack of homology) and calls for additional multilevel research of this kind. Managerial ImplicationsOur findings provide both managers and salespeople with several new insights into salesperson decision making when prospecting. We underscore key performance implications of our findings by simulating several what-if analyses using the parameters from our results. Managing salespeople's avoidance of large opportunitiesThe results from three studies consistently show that, all else being equal, salespeople are likely to gravitate toward medium-sized opportunities, leaving smaller and larger opportunities unattended. Using simulated data from Study 1, we find that a salesperson with a prospect whose magnitude equals their baseline opportunity magnitude of ∼$26,750 will have an 86.5% probability of successful conversion. Yet receiving a new prospect that is 1 SD larger in terms of magnitude (∼$143,250) will decrease the conversion odds by more than 15%. This effect is due to the propensity to conserve resources when there are constraints, as Study 3 further shows that the anticipated costs only affect salespeople's pursuit of large prospects when operating under resource constraints.Thus, to assuage salespeople's avoidance of big-whale deals, managers can leverage their firms' CRM databases. Specifically, a manager can use historical CRM data to calculate the baseline estimates of opportunity magnitude (and conversion uncertainty) for each salesperson. Then, the manager can use this information to match marketing-generated prospects with a salesperson's portfolio baseline, because a large difference in opportunity magnitude between a new opportunity and the salesperson baseline is demotivating and decreases conversion success. Furthermore, when necessary, managers should alter salespeople's benefit–cost calculus when prospecting. For example, they should provide salespeople who work on relatively large opportunities with extra benefits (both extrinsic and intrinsic) and additional resources (to relax the resource constraints), thereby increasing the likelihood of conversion. In addition, managers could pair salespeople with peers with larger portfolio baselines to create ad hoc sales teams to follow up. Such a temporary arrangement can reduce the costs for the focal salespeople. Salesperson calibration for conversion uncertainty in solution sellingOur findings show that salespeople's decision-making strategy differs between solution and product selling. For product selling, salespeople rely on a compensatory decision-making strategy at both prospect and portfolio levels. For solution selling, however, they rely on a calibration decision-making strategy, which is noncompensatory in nature, with conversion uncertainty acting as the calibrator of opportunity magnitude at the portfolio level. This calibration effect underscores the important role of portfolio-level information, in terms of both magnitude and conversion uncertainty, in salesperson decision making for solutions. Thus, sales managers should pay close attention to this important ""between-salespeople"" difference when reallocating prospects for maximum effect. Continuing with the previous example from the simulated data, a sales manager could intuitively decide to allocate the solution-selling prospect of ∼$143,250 to a salesperson with a portfolio baseline magnitude of about the same size (i.e., ∼$143,250). However, if this salesperson's portfolio baseline conversion uncertainty is 1 SD higher, the probability of closing the deal decreases by 36.2%. To reduce conversion uncertainty, managers can play an active role by, for example, using more behavior-based control to curtail salespeople's pursuit of highly uncertain opportunities and providing them with more frequent feedback. Firms can also leverage advanced sales analytics capabilities to decrease uncertainty in opportunity costs and train salespeople how to use the information in their prospecting decisions. Boundary conditions of conversion uncertainty calibration for solution sellingOur results indicate that past performance success and experience can alter the way salespeople calibrate for conversion uncertainty. Thus, these two variables are important for managers as well as salespeople. In a post hoc analysis, we used Study 2's results to predict salesperson quota achievement under various combinations of levels of salespeople's past performance success and experience (±1 SD as high and low values). Drawing on the results summarized in Table 4, Panel A, we derive the most effective managerial actions for managing salesperson prospecting in Table 4, Panel B. Three insights are worth noting. First, regardless of past performance success, salespeople's quota attainment is the worst when they gravitate toward highly certain but small opportunities. Second, salespeople who performed well in the past are most likely to ""hit"" quota again when the portfolio opportunity is large and conversion uncertainty is low (96%). Nevertheless, these high performers become average performers when conversion uncertainty and portfolio magnitude are average (ranging from 50% to 79%). Therefore, an effective way to manage high performers' prospecting is to give them a large portfolio but also help them reduce conversion uncertainty. This combination allows them to conserve resources while also maintaining high levels of performance. By contrast, salespeople who performed poorly in the past can achieve a quota attainment as high as 83% when they have a large portfolio and conversion uncertainty is high. The increase in opportunity magnitude is more motivating to these poor performers—they are willing to exert greater efforts without conserving resources to prove themselves. Thus, an effective, but perhaps counterintuitive, way to manage poor performers' prospecting is to give them a larger, more uncertain portfolio to challenge them.GraphTable 4. Study 2: Managerial Insights into Boundary Conditions of Salesperson Prospecting in Solution Selling. A: Post Hoc Analysis for Study 2Moderating Role of Salesperson Past Performance Success (H4)Low Past Performance Success (−1 SD)High Past Performance Success (+1 SD)Conversion UncertaintyConversion UncertaintyMagnitude1 SD LowerAverage1 SD Higher1 SD LowerAverage1 SD Higher1 SD lower36%40%48%39%50%64%Average68%67%71%84%81%79%1 SD higher67%76%83%96%70%50% Table 4. Study 2: Managerial Insights into Boundary Conditions of Salesperson Prospecting in Solution Selling. Moderating Role of Salesperson Experience (H5)Low Salesperson Experience (−1 SD)High Salesperson Experience (+1 SD)Conversion UncertaintyConversion UncertaintyMagnitude1 SD LowerAverage1 SD Higher1 SD LowerAverage1 SD Higher1 SD lower29%28%27%54%76%106%Average106%75%53%53%75%106%1 SD higher118%84%60%60%62%64% Table 4. Study 2: Managerial Insights into Boundary Conditions of Salesperson Prospecting in Solution Selling. B: Managerial TakeawaysObservation from DataSuggested Managerial ActionRegardless of past performance success, salespeople's quota attainment is worst when they gravitate toward highly certain but small opportunities.Point out the importance of ""bread-and-butter"" prospects, as salespeople may ignore them while such prospects could be of strategic importance.Salespeople with greater past performance success perform relatively well when portfolio opportunity is large and conversion uncertainty is low.Give these salespeople a large portfolio but also help them reduce conversion uncertainty (to help them conserve resources).Salespeople with lower past performance success tend to perform better for relatively larger and more uncertain portfolios.Give these salespeople a larger, more uncertain portfolio to challenge them, while also providing opportunities to recuperate from poor past performance.Inexperienced salespeople perform better for relatively larger, but certain portfolios.Reduce conversion uncertainty (e.g., via information provision, training) and provide them with larger portfolios.Experienced salespeople tend to perform especially well for relatively small to moderate portfolio magnitude with relatively high conversion uncertainty.Challenge these salespeople with opportunities that have high conversion uncertainty, while ensuring the portfolio itself is not too large. 14 Notes: 100% = on-target performance. Shaded boxes reflect higher levels of quota achievement.Third, when conversion uncertainty is reduced, inexperienced salespeople who handle a large portfolio can go from zero to hero, as their quota attainment increases from 60% to 118%. Inexperienced salespeople also achieve low quota (under 30%) when their portfolio is small, regardless of conversion uncertainty. Therefore, an effective way to manage inexperienced salespeople's prospecting is to reduce conversion uncertainty and provide them with ample opportunities. By contrast, experienced salespeople do not perform well when their portfolio opportunity is large, regardless of conversion uncertainty (range: 60%–64%). However, they thrive under high conversion uncertainty and when their portfolio is moderate in size, with a quota attainment exceeding 100%. Thus, an effective way to manage experienced salespeople is to challenge them with opportunities that have high conversion uncertainty, while ensuring the portfolio opportunity itself is not too large.What do these results mean for salespeople? Our results show that salespeople need to be cognizant of potential biases created by their past performance success and experience. This is because these biases can significantly improve or impair their sales performance, as indicated by the aforementioned potential gains and losses in quota attainment. By changing the benefit–cost analysis and reducing factors that drive conversion uncertainty (e.g., learn from peers, ask managers for support, form ad hoc teams), salespeople can become more effective in closing big-whale deals and hitting their targets despite conversion uncertainty. Limitations and Future Research DirectionsWhile our research covers three empirical contexts and our data came from multiple sources, this article has several limitations. First, although opportunity magnitude and conversion uncertainty are two of the most important factors in salesperson decision making, they are by no means the only factors. In our research, we included several contingencies and control variables to account for heterogeneity. Nevertheless, we urge further research to consider other aspects as contingencies of salesperson calibration, such as salesperson perceptual accuracy in forming judgments of opportunity magnitude and uncertainty, customer characteristics, competition, and the source of the sales leads ([46]; [48]). Future research could also explore how managers can influence salesperson calibration (e.g., through incentives, by changing baseline conversion uncertainty via altering the composition of self-generated and assigned leads). Moreover, the nature of uncertainty itself and how it affects judgment and decision making could be further explored. Second, we focus on portfolio baseline magnitude and conversion uncertainty as the frames of reference for how salespeople form relative comparisons of prospects in their portfolios. Although this focus is both theoretically and empirically justified, further research could examine other reference points as suggested in the judgment–decision making literature (e.g., sales goals, status quo, minimum requirement). Other measures of magnitude, such as customer lifetime value, could also be examined.Third, we control for several time-related effects in Study 2 but did not examine the dynamics. Although the first impression generally serves as the anchor point, people adjust their anchors as they receive new information ([58]). Further research could examine how salespeople update their judgment of uncertainty over time by exploring how this effect manifests itself in salesperson prospecting. Fourth, we focus on salesperson past performance success and salesperson experience as moderators, but other moderators may exist, such as control systems and training. Finally, although theoretical arguments exist in support of the moderating role of salespeople's past performance success and experience, future research could explicitly test how these contingencies influence the underlying benefit–costs analysis. Supplemental Materialsj-pdf-1-jmx-10.1177_00222429211037336 - Supplemental material for Why Salespeople Avoid Big-Whale Sales OpportunitiesSupplemental material, sj-pdf-1-jmx-10.1177_00222429211037336 for Why Salespeople Avoid Big-Whale Sales Opportunities by Juan Xu, Michel van der Borgh, Edwin J. Nijssen, and Son K. Lam in Journal of Marketing  "
1,"Analyzing the Cultural Contradictions of Authenticity: Theoretical and Managerial Insights from the Market Logic of Conscious Capitalism This research analyzes the cultural contradictions of authenticity as they pertain to the actions of consumers and marketers. The authors' conceptualization diverges from the conventional assumption that the ambiguity manifest in the concept of authenticity can be resolved by identifying an essential set of defining attributes or by conceptualizing it as a continuum. Using a semiotic approach, the authors identify a general system of structural relationships and ambiguous classifications that organize the meanings through which authenticity is understood and contested in a given market context. They demonstrate the contextually adaptable nature of this framework by analyzing the authenticity contradictions generated by the cultural tensions between ""conscious capitalism""—a market logic that encompasses both global brands and small independent businesses, such as a farm-to-table restaurant or an organic food co-op—and the elitist critique. The Slow Food movement provides a case study for analyzing how consumers, producers, and entrepreneurs who identify with conscious capitalist ideals understand these disauthenticating, elitist associations and the strategies they use to counter them. The authors conclude by discussing implications of the analysis for theories of authenticity and for managing the authenticity challenges facing conscious capitalist brands.Keywords: authenticity; brand image; conscious capitalism; ethical consumerism; consumer identity; market logics; semioticsConsumers crave authenticity—so much so that their quest for authenticity is considered ""one of the cornerstones of contemporary marketing"" ([11], p. 21). This has created an enormous challenge for the field, considering that marketing itself is typically considered inherently inauthentic. —[67], p. 1)In the field of marketing, little doubt exits that ""authenticity"" is highly desired by consumers and thereby is a crucially important strategic resource for marketing management. Consumers are more likely to form stronger emotional attachments to a brand, business, or tourist site they perceive as being authentic ([20]; [30]; [57]; [86]) and to incorporate these market resources into their identities ([ 7]; [11]; [45]). On the managerial side, [31], p. 610) conclude that authenticity is ""the most rare and coveted asset in the contemporary branding landscape."" Their assertion is supported by an array of studies indicating that authenticity is integral to the enhancement of brand equity ([60]), effective brand extensions ([82]), persuasive marketing communications ([ 5]), success in relationship marketing ([22]), and emotionally engaging person and celebrity brands ([31]; [87]).Although there is a clear consensus that authenticity profoundly matters to both consumers and marketers, the marketing literature also presents a recurrent concern that authenticity is a nebulous concept that has eluded precise definition ([ 5]). [67], p. 2) proclaim that this conceptual ambiguity poses a significant barrier to creating ""a coherent theory of authenticity."" Accordingly, they aim to redress this dilemma by presenting a general definition of authenticity based on six key perceptual components. In contrast, [81], p. 3) proposes that ""authenticity is a polysemous and multilayered concept"" and thus ""it might not [emphasis added] be helpful to compress the wealth of disparate meanings associated with the concept into a single definition.""As Södergren further notes in his meta-analysis, ""the majority of the research [on authenticity] has focused on characteristics that distinguish the 'real thing' from the fake"" (p. 11). To further elaborate on this conceptual tendency, marketers' efforts to define authenticity almost invariably invoke some variant of genuineness, such as brands (via their management teams) staying true to ideals of timeless tradition, heritage, craftsmanship, and quality (see also [ 6]; [82]). In this spirit, [55], p. 30) propose that the authenticity of a brand's social media communications hinges on perceptions of honesty, sincerity, and being ""real."" [ 7] similarly contend that the core cultural meanings of authenticity are truth, genuineness, and reality. [67] comprehensive definition of authenticity also incorporates a series of veracity-oriented constructs, such as originality (i.e., not being a copy), accuracy (i.e., being true to others), and integrity (i.e., being true to oneself).While the analytic goal of distinguishing the authentic from the inauthentic makes intuitive sense, it is a Sisyphean undertaking that attempts to specify an ambiguous cultural category by referring to other semantic terms whose meanings are also contextually contingent and malleable (i.e., honesty, sincerity, originality, genuineness, and truthfulness). Furthermore, informing marketing managers that their brand lacks authenticity because consumers see it as being unoriginal, insincere, or dishonest offers little guidance on how to resolve the deeper cultural tensions that drive these unfavorable perceptions. Rather than a checklist of definitional attributes, we argue that marketing managers need an analytic approach that can enable them to answer questions such as ( 1) why is their brand or business susceptible to certain kinds of authenticity challenges?, ( 2) what cultural meanings and contradictions underlie those challenges?, and ( 3) what responses could they take to mitigate the disauthenticating associations that ensue from these tensions?Returning to our opening vignette, we can reframe [67], p. 1) statement that marketers face an ""enormous challenge"" because their profession is ""typically seen as inauthentic"" (see also [ 5]) as a realization that marketing, as a business practice, also occupies an ambiguous cultural position. On the one hand, marketing aims to advocate for the needs (and voices) of customers ([41]) and, yet, it is also means for companies to enhance their profits and market share. This tension readily gives rise to concerns that short-term (and potentially exploitive) profitability goals might take priority over serving customers' best interests. Accordingly, consumers are inundated with cultural narratives (ranging from journalistic reports about deceptive marketing tactics to portrayals of unscrupulous marketers by entertainment media) that encourage cynicism and distrust toward marketers' branding claims and persuasive communications ([32]; [45]; [66]).However, the specific cultural meanings and associations that lead to perceptions of authenticity or inauthenticity vary across brands and markets. For example, consumers are likely to deploy different configurations of meanings, beliefs, and evaluative norms when judging the authenticity of a high-fashion retailer ([23]), a café owner who promotes their establishment as a home-away-from-home ([20]) or a global brand that positions itself as an advocate for environmental justice (e.g., Patagonia; [49]).In this article, we explain and demonstrate how the semiotic square ([40]) can be used to systematically analyze such culturally heterogeneous authenticity contradictions and to develop contextually appropriate responses to the specific authenticity challenges that arise in a given market. The semiotic square is an analytic tool that has often been used to delineate cultural meanings and semantic contradictions that are manifest in both consumer perceptions and marketing strategies ([29]; [36]; [50]; [51]; [54]; [68]; [69]). From a semiotic perspective, the cultural categories of the authentic and the inauthentic are not just contrasting or oppositional terms. Rather, they are anchor points in a broader network of relationships through which the authenticity of a given brand, business, brand ambassador, social media influencer, and the like is culturally constructed and potentially contested.Our market context is conscious capitalism, which refers to a ""way of thinking about capitalism and business that better reflects where we are in the human journey, the state of our world today, and the innate potential of business to make a positive impact on the world"" ([62], p. 273). Conscious capitalism is particularly vulnerable to the broader authenticity–inauthenticity tension that all marketers confront to varying degrees. Therefore, it serves as a very relevant and informative context for our analysis.Conscious capitalism's key premise is that capitalism's societal purpose has, historically, been defined too narrowly (i.e., maximizing shareholder wealth and optimizing consumers' market choices) and, accordingly, its society-enhancing potential remains greatly underutilized. Rather than grafting a social mission onto a traditional profit-maximization model, as per conventional corporate social responsibility approaches, proponents of conscious capitalism contend that businesses should place value-driven goals and social consciousness at the core of their institutional missions ([62]).By aiming to redefine the nature and function of capitalism, conscious capitalism can be analyzed as a market logic that transcends its iconic brands (e.g., Patagonia, Starbucks, TOMS, Whole Foods) or socially conscious businesses (e.g., a cooperatively owned, fair trade, local coffee shop). As discussed by [27], pp. 40–42), a market logic is an integrated network of meanings, values, and norms that provide ( 1) principles that can guide thoughts, actions, and preferences; ( 2) vocabularies of motivation and justification; and ( 3) material and symbolic resources for constructing an identity (such as being an ethical consumer or a purpose-driven business owner).Conscious capitalism organizes a constellation of ideologically aligned brands and an even larger network of businesses that have different scales of operation and serve different roles in the supply chain. Thus, consumers who support this array of brands and enterprises have access to a set of normative principles to guide their purchase choices (e.g., locally sourced materials are preferred over imported ones, plastic product packaging should be avoided); they learn an intricate system of terms and codes (e.g., ""postconsumer recycled content,"" third-party certification labels such as the Rainforest Alliance or Certified Carbon Neutral); and they can express their socially conscious sensibilities through an array of consumption practices—wearing a Patagonia fleece, driving an electric car, shopping at a farmers' market, brandishing a reusable Whole Foods' canvas tote bag, buying fair trade chocolate, or supporting a farm-to-table restaurant.In the general public discourse, however, the authenticity of conscious capitalist brands and businesses, and their consumer supporters, is frequently called into question. These authenticity challenges are sufficiently problematic that leading proponents of conscious capitalism feel compelled to address them:There is a growing network of people building their companies based on the idea that business is about more than making a profit. It's about higher purpose ... and the innate potential of business to make a positive impact on the world.... But one of the most predictable responses we get from people when we mention the idea of conscious capitalism is, ""That's an oxymoron!"" ([61])Conscious capitalism's authenticity challenges hinge on a cultural tension between the profit-maximizing ethos of capitalism and the ennobling idea that capitalist enterprises can serve higher societal and moral purposes that supersede commercial interests ([ 3]; [33]). In this vein, critics often suggest that conscious capitalism deploys the language of sustainability and other socially beneficial goals for the instrumental purpose of catering to higher-income consumers who will pay a premium to imbue their consumption practices with an aura of moral virtue:Thus, the rise of social enterprises [i.e., conscious capitalist enterprises] has been met with hostility, particularly toward its authenticity and its sustainable impact. If their goods and services continue to be priced as they are, is the sustainable movement only for the demographic that can afford it? ([14])[70], p. 73) similarly argue that conscious capitalism is more promotional hyperbole than a viable business reality and, further, add this reservation: ""It is important to note that the firms associated with the Conscious Capitalism movement are far from a random sample of American businesses: In fact, a great many sell relatively expensive products to relatively affluent, socially- or health-conscious consumers.""This incredulous and, at times, adversarial public response to conscious capitalism has not arisen ex nihilo. Rather, it draws from a cultural narrative that we characterize as the ""elitist critique."" As historian [34] elaborates, the political charge of elitism has evolved from its classic populist roots, which railed against the undue power wielded by the captains of industry and affluent political insiders, to an antipathy toward the intellectual class (who may not be unduly wealthy or politically powerful). Through this shift, the charge of elitism was distanced from its origins in economic conflicts between the working class and the owners of capital (and their management intermediaries) and became repositioned in a culture war rift whereby ""the 'elite' could be identified by its liberal ideas, coastal real estate, and highbrow consumer preferences"" ([34], emphasis added).We investigate how the specific authenticity challenges posed by the elitist critique of conscious capitalism are negotiated by consumers and producers in the context of the Slow Food movement ([89]). Slow Food encompasses an array of ideologically aligned brands, enterprises (farm-to-table restaurants, artisan producers, and organic and free-range farmers), consumption practices (e.g., shopping at a farmers' market or a local co-op), and goods and services (e.g., an heirloom tomato, grass-fed beef, a class in fermentation techniques). Slow Food's signature issues and social change goals are grounded in the market logic of conscious capitalism, including local sourcing, fair wages for workers, sustainable modes of production, environmental awareness and habitat protection, and a broader project of redressing societal ills through the coordinated actions of socially conscious businesses and consumers (see [73], [74]). The elitist critique has also become part and parcel of Slow Food's brand image, and it poses salient authenticity challenges for Slow Food's producers, entrepreneurs, and consumers.In the following sections, we first discuss the key analytic premises of the semiotic square. Next, we develop a semiotic conceptualization of authenticity that maps out its structural contradictions (and ambiguous classifications). We use this analytic framework to explicate the ways in which the elitist critique gives a particular cultural form to the authenticity contradictions plaguing the market logic of conscious capitalism. We then profile the authenticating strategies that Slow Food advocates (consumers, producers, and restauranteurs) use to counter these disauthenticating elitist associations. We conclude by discussing the implications of this analysis for theories of authenticity and for managing the authenticity challenges facing conscious capitalist enterprises. The Semiotics of Authenticity The Semiotic Square as an Analytic ToolFrom a semiotic perspective ([40]), the meaning and categorical boundaries of a given concept are defined through relations to what it is not. For example, the cultural meanings of masculinity have been historically established through contrasts to those that have defined femininity and the related nexus of ever-changing ideals, values, and practices through which this binary contrast has been culturally articulated and transformed over time ([50]). These structural relations give rise to ambiguous categories whose associated cultural meanings can become points of contestation and debate, such as in the cases of ""metrosexuals"" ([78]), stay-at-home dads ([19]), or the ongoing controversies sparked by the category of transgender athletes ([12]).The binary opposition between authenticity and inauthenticity presents a similar arrangement of contradictions and ambiguous classifications. Consequently, we propose that authenticity is not a set of discrete properties that distinguish the genuine from the fake—but, rather, an ongoing process of managing a network of contingent relationships. In some markets, for some brands and enterprises, these contingencies may be more stable, whereas in others, they may become more culturally contested and, thus, unstable. We suggest that conscious capitalist brands and businesses, owing to the elitist critique, exemplify this latter and more managerially challenging case.Figure 1 presents a semiotic square representation of authenticity. In this article, we use the ""contradictions of authenticity"" as an integrative term that encompasses the structural relations among the semiotic 3Cs (contrariety, complementarity, and contradictory relations).Graph: Figure 1. A semiotic model of the authenticity–inauthenticity opposition.The horizontal arrows represent contrariety relations. These relations are roughly analogous to the standard binary oppositions that anchor semantic differential scales. However, relations of contrariety further indicate that the meaning of a term is defined through a relationship to its binary contrast (e.g., good is understood relative to evil). Accordingly, the meaning of authenticity is always contingent on the operative meaning of inauthenticity, and vice versa. We refer to the authentic ↔ inauthentic contrariety as the primary contrariety relation because it represents the dominant tension that, in turn, sets the complementary terms for the secondary contrariety relation (i.e., not inauthentic ↔ not authentic).The vertical arrows represent complementarity relations. Such conceptual pairings are compatible and noncontradictory (but are not necessarily synonymous or interchangeable). For example, ""not inauthentic"" is congruent with the dominant term, authentic. However, this classification also harbors other connotations and, thus, ambiguous meanings. For example, imagine a painting created by a famous artist, say Picasso, who at the time was a fledgling beginner, imitating the style of another painter. Because the painting does not evince Picasso's quintessential artistic motifs, its authenticity becomes ambiguous (and debatable)—that is, at what point in his career does a painting by Picasso truly become a ""Picasso""? The term ""not inauthentic"" conveys this type of ambiguity.The diagonal arrows represent contradictory relations. These relations indicate that any entity or action deemed to be authentic (or inauthentic) will harbor some qualities that can be judged as contradicting such an assessment. As an illustration, let us again consider the idea of artistic authenticity. From a conventional standpoint, the authenticity of an artist, even a renowned one, can always be challenged on the grounds that their creations exhibit properties that are derivative of other genres, styles, or artistic predecessors (authentic ↔ not authentic). Conversely, the art world's postmodern movement disavows the idea of artistic originality and, instead, celebrates that all artistic productions are, in some sense, a reworking of something prior. As exemplified by Andy Warhol's replications of iconic cultural images (Coca-Cola bottles, Campbell Soup cans, the face of Marilyn Monroe), postmodern art is also heralded for its capacity to surprise and inspire revelatory aesthetic experiences through its creative (and often ironic) uses of repetition, collage, assemblage, montage, and bricolage (inauthentic ↔ not inauthentic) ([43]).[ 4] A Semiotic Conceptualization of AuthenticityIn Figure 1, the cloud-like drawings represent the specific cultural meanings that give contextual form to the contradictions of authenticity. For purposes of our analysis, the relevant meaning systems are the market logic of conscious capitalism and the elitist critique. This system of semiotic relationships gives rise to four emergent (and ambiguous) classifications, each harboring latent contradictions. In discussing these ambiguous categories, we first illustrate them in more general terms and then address their manifestations in the context of conscious capitalism and the elitist critique. Authentic + not inauthenticThis complementarity relation corresponds to what [39] discuss as indexical authenticity. In this usage, an index refers to a given object or behavior—for example, the actions of a whitewater raft guide, handprints in front of Grauman's Chinese Theater, a painting, or a branded good. Indexes are classified as authentic when they are believed to possess a factual and spatiotemporal connection to some validating condition. For example, consumers will judge the actions of a whitewater raft guide as authentic if they are believed to reflect an inner passion for the outdoors (rather than being a calculated performance done for remunerative purposes; [ 2]). Similarly, consumers will typically deem a branded good to be authentic when they believe its design, production, and quality certification has proceeded under the auspices of those who own or manage the brand of note.As these examples suggest, perceptions of indexical authenticity can be more or less certain. As an example of higher certainty, Prada certifies the genuineness of its handbags by assigning each a unique and traceable serial number that is documented on an authenticity card. On the less certain side, customers have to infer the indexical authenticity of a whitewater raft guide's passion for the outdoors or a retail associate's expressions of friendliness and interpersonal concern. In these cases, consumers' judgements about the authenticity (or inauthenticity) of a marketer's actions (or the actions of other consumers) depend on their inferences about underlying motivations and intent.The elitist critique provides a constellation of culturally shared meanings and rationales that support disconfirming suppositions about the indexical authenticity of conscious capitalist enterprises and their consumer followers. These disauthenticating associations directly correspond to the ambiguous categories emerging from the primary contrariety (authentic ↔ inauthentic), the secondary contrariety (not inauthentic ↔ not authentic), and the complementarity relation of inauthentic ↔ not authentic relations. Authentic + inauthenticThis ambiguous classification corresponds to seemingly oxymoronic constructions such as authentic reproductions—or, in semiotic vernacular, ""iconic authenticity"" ([39]). In this usage, the icon is an object that is a known facsimile of an original referent and that is appreciated for its mimetic properties, as in the case of a comedian doing an uncanny impression of a celebrity. For the category of iconic authenticity, the ensuing goal is to present a compelling sense of verisimilitude through a meticulous recreation of the original referents' characteristics. Iconic authenticity is pursued by, among others, members of the cosplay community ([80]) and consumers who perform in historical recreations, such as Civil War reenactments ([17]). In a different market application, iconic authenticity would also be highly relevant to a budget-conscious consumer who wants to buy a convincing counterfeit version of an expensive designer brand.When situated in the context of the elitist critique, the ""authentic + inauthentic"" category assumes less favorable meanings of moral pretentiousness and hypocrisy. In this disauthenticating cultural frame, affluent (and typically left-leaning) consumers use conscious capitalist brands and goods to distinguish themselves from the price-conscious mainstream and their socioeconomic peers who display affluence through more ostentatious lifestyle choices ([24]). By claiming the mantle of moral virtue, such consumers can pursue social distinction in an otherwise orthodox manner—that is, through material displays of refined tastes ([ 9]; [44])—while appearing to disavow materialism and status consciousness.For example, during its heyday as a cultural icon, the Toyota Prius inspired oppositional brand communities (Muñiz and O'Guinn 2001) who referred to the vehicle (and its drivers) as ""the pious."" This epithet suggested that Prius drivers evinced a self-aggrandizing ""holier than thou"" stance that amplified the moral merits of their automotive preferences relative to those who made different choices ([59]). In a similar cultural vein, [13] note that ecofriendly consumers who purchase organic foods, drive electric luxury cars, and use natural cleaning products typically lead lifestyles that carry a much higher carbon footprint than lower-income consumers who live in smaller housing units, rely on public transport, and seldom fly. Seen in this critical light, such ecoconscious (affluent) consumers are virtue signaling ([24]; [42]; [90]), but their pretense of moral superiority is not warranted by these symbolic acts. Not inauthentic + not authenticThis ambiguous classification highlights that perceptions of genuineness (often taken as the sine qua non of authenticity) are a necessary but not insufficient condition for ascribing this honorific appellation to an object or action. That is, an entity or action may be deemed as genuine (i.e., not fake) but lack the perceived aesthetic or moral virtues needed to be classified as ""authentic,"" or, conversely, to have its authenticity challenged. Thus, we can have marketplace conditions where authenticity, in its full moral and aestheticized sense, is not a relevant cultural category.To illustrate, barring extenuating circumstances, consumers seldom venerate conventional mass-produced goods (e.g., a Big Mac, a Gillette disposable razor) for their authenticity because they lack potent associations with rarefied aesthetic ideals. Conversely, such items are not typically classified as inauthentic either (assuming that they are not knock-off products), with companies often promoting the standardized nature of their branded offerings—and the resulting performance consistency—as value-added benefits.In the context of the elitist critique, the ""not inauthentic + not authentic"" classification suggests that middle-class consumers who support conscious capitalist brands and enterprises are, owing to their class privileges, inherently ""not authentic."" This disauthenticating implication hinges not on conscious intent but on the systemic advantages afforded by their relatively privileged socioeconomic position. Rather than being hypocritical per se (i.e., authentic + inauthentic), the implication is that such consumers may genuinely believe that conscious capitalism is a viable means to create a more equitable and just society. However, their genuine belief is an ideological one, steeped in their internalized class interests. Middle-class consumers' ideological affinity for the market logic of conscious capitalism allows them to lead a materially privileged lifestyle in a guilt-free manner ([92]). By purchasing brands and goods that signify a heightened social consciousness (e.g., fair trade coffee; TOMS shoes; organic, locally sourced foods), they can feel symbolically absolved from culpability in the perpetuation of socioeconomic inequalities. Consequently, their habituated class predilections also create an ideological blind spot toward the exclusionary signals that conscious capitalist ideals and values convey to those who lack the economic and cultural resources needed to fully participate in a middle-class lifestyle ([47]). Inauthentic + not authenticThis ambiguous classification suggests that conscious capitalists' products, services, and brands are, to use[66] term, ""gimmicks"" that always promise more than they can deliver. [21], p. 668) further discuss issues relevant to this classification in their typology of transactions. Among their designations of deceitful transactions (i.e., scams), they list ""fraud"" and ""confidence games."" In the former condition, a disingenuous party misrepresents their intentions to an unsuspecting partner; in the latter condition, the scammer actively enrolls their target in the ruse, such as in catfishing and pyramid schemes.The inauthentic + not authentic classification implies a manipulative opportunism whereby an unethical agent feigns genuineness to extract ill-gotten gains from another. In the context of conscious capitalism, this disauthenticating association is most germane to the marketer side of the exchange. As one well-known example, the business ethics journalist Jon Entine accused the pioneering conscious capitalist brand The Body Shop, and its founder Anita Riddick, of fraudulent misrepresentation. According to [25], Riddick stole the brand concept from a local entrepreneur and fabricated an authenticating origin story about traveling the world searching for natural skin care and hair treatments. His exposé further contended that The Body Shop significantly overstated the percentage of profits that it donated to philanthropic causes. Though Riddick formally denied these charges, the authenticity challenges posed by these accusations, as well as others that subsequently followed, continued to plague the brand ([76]). After years of underperformance, relative to the brand's prescandal pinnacle, The Body Shop undertook a revitalizing strategy that its management characterized as an activist revamp ([77]).The ""inauthentic + not authentic"" classification can also cast more nuanced doubts on the authenticity of conscious capitalist entrepreneurs. Although such conscious capitalist entrepreneurs would not be committing overt acts of fraud (i.e., they are not lying about their business practices per se), the disauthenticating implication is that they are cynically espousing higher-order civic ideals to serve commercial ends, such as charging a premium to their consumers or driving higher stock valuations. This disauthenticating association can arise, for example, when the founder/chief executive of a privately owned conscious capitalist brand sells its rights to a larger corporate entity. Such a backlash arose when Gene Kahn—the founder of Cascadian Farms—sold his business to General Mills. Many leading voices in the organic food community lambasted Kahn's integrity, condemning him as a Boomer sellout and warning that the brand's corporate ownership would not stay true to the higher-order values that originally galvanized the organic food movement (see [75]). Research ProceduresTo investigate how Slow Food advocates negotiate the elitist critique of conscious capitalism and its disauthenticating connotations, we recruited informants from a Slow Food chapter located in a metropolitan area of the Midwestern United States using informational flyers, contacts made at local chapter meetings, and snowballing referrals. We conducted interviews at public locations such as coffee shops or at Slow Food–sponsored events, with exception of two that respectively occurred in these participants' domestic residence and private work office. Interviewees were paid $20 in appreciation for their time. Interviews were audiotaped and ranged from one to four hours in duration, yielding 830 double-spaced pages of verbatim text. All participant names are pseudonyms.Of our 19 interviews, 8 were conducted with chapter organizers, 5 with Slow Food producers and entrepreneurs, and 6 with Slow Food advocates who had volunteered their time to different outreach activities (for our participants' profiles, see Table 1). Most of our Slow Food organizers and consumer advocates are college graduates employed in professional occupations and hail from middle- and upper-middle-class families. Among the entrepreneurs, Dave, Leslie, Maggie, and Tom are also college graduates. This demographic profile matches the membership ranks of Slow Food USA, which skews toward middle-class professionals ([16]).GraphTable 1. Participant Profiles. PseudonymGenderAge (Years)EducationOccupationSlow Food RoleAaronM28Ph.D.Research scientistMiddle-class advocateAlexM28Some collegeArtisan cheese makerEntrepreneurAmandaF31MBASmall business ownerChapter organizerBenM61M.S.Pursuing Ph.D.Middle-class advocateBrendaF44B.A.Former organic farmer, at-home momMiddle-class advocateCarolineF56B.A.Government employeeChapter organizerChrisM47B.ASoftware designerChapter organizerChristinaF44M.S.Nurse practitionerChapter organizerDaveM30B.A.Organic farmerEntrepreneurErinF29M.S.E-commerce food entrepreneurChapter organizerHankM59M.S.Interior designer, former chefMiddle-class advocateHeidiF58B.A.Professional chefMiddle-class advocateJaneF68Associate's degreeHolistic medicine practitionerMiddle-class advocateKevinM42B.S.Software designerChapter organizerLeslieF23B.SOrganic farmerEntrepreneurMaggieF34B.S.Free range farmerEntrepreneurPaulaF49M.S.Retail buyer and jewelry makerChapter organizerRichardM34B.S.Computer systems analystChapter organizerTomM35M.A.Farm-to-table restauranteurEntrepreneur 1 Notes: M = male; F = female; B.A. = bachelor of arts; B.S. = bachelor of science; M.A. = master of arts; MBA = master of business administration; M.S. = master of science; Ph.D. = doctor of philosophy.Following the conventions of in-depth phenomenological interviewing ([85]), our participants largely determined the course of the dialogue. The interviewer relied on follow-up probes to elicit more detailed accounts of the informants' experiences and viewpoints and to ensure that various aspects of food production, distribution, and consumption were covered. Procedurally, our interpretation developed through an iterative process of creating, challenging, and reworking provisional understandings by tacking back and forth between individual transcripts and the broader data set ([84]). We then pivoted to another level of hermeneutic tacking that entailed iterations between these emic narratives and theoretical concepts, which led us to the application of the semiotic square and our resulting focus on the elitist critique and corresponding strategies for countering the authenticity challenges posed by the cultural contradictions manifest in this market system. Contextual Background The Slow Food MovementAs an institutional entity, Slow Food is a transnational organization encompassing 1,500 local chapters plus numerous subsidiary organizations. Beyond its formal institutional boundaries, Slow Food's culinary practices, values, and activist goals organize ideological and economic alliances among a globally diffused network of food writers (such as Mark Bittman and Michael Pollan), consumers, producers, merchants, and restauranteurs (including celebrity chefs Alice Waters and Jamie Oliver). As [18], p. 131) writes, ""The phrase 'slow food' strikes a chord among the public not because it is the name of an organization but because it reflects a series of desires, interests and concerns.""Slow Food discourses valorize meals that are traditionally prepared with fresh ingredients as unique sources of pleasurable experiences that can mobilize consumers to resist the industrialized system of food production. Over the years, the Slow Food movement has embraced a broader conscious capitalist agenda that advocates for sustainable production, environmental protection, and social justice (i.e., fighting hunger, advocating for living wages for agricultural workers; see [16]; [89]). The Elitist Critique of the Slow Food MovementLike other conscious capitalist exemplars, Slow Food has also been plagued by charges of elitism from its inception in 1986 when its founder, Carlo Petrini, organized a series of public protests over the opening of a McDonald's in the heart of Rome (see [89]). This ignominious view of Slow Food finds ready expression in both academic analyses (e.g., Guthman 2007; [56]) as well as journalistic accounts, such as [38], who states that ""none of the aggressive, judgmental pitches of the movement have ever been proven. The power of its association with the economic elite has.""From this skeptical standpoint, Slow Food's exalted rhetoric of sustainable diets, biodiversity, and socially conscious eating (see https://www.slowfood.com/about-us/our-philosophy/) is a guise for privileging upper-middle-class tastes over the dietary practices of less affluent (and lower-cultural-capital) consumer segments (see [53]; [56]). Even Slow Food's ardent proponents, such as food writer Annie Levy, concede that a tacit elitism has hindered the cultural diffusion of its core principles:—The revered Alice Waters once said, ""when we eat food that is fast, cheap, and easy, we digest those very values."" What are the judgments contained in this kind of statement? She intends, I believe, to critique the values of a food system that doesn't care about its conditions or effects on people and the environment. But the words suggest that if you eat fast, cheap, and easy you become fast, cheap, and easy—language many women might recognize as shaming. Isn't this how it really sounds to someone who enjoys such food, or is caught in situations in which it might seem the best available option? ([58])Slow Food encourages consumers to shift their culinary tastes away from fast food and industrialized fare (including the oft-demonized category of junk food) ([16]; [83]). Such admonitions can imply a moralistic hectoring and an invidious comparison with those whose food tastes and practices are more orthodox. These elitist associations often cross into other sociocultural spheres, such as the controversy sparked by former First Lady Michelle Obama's school lunch initiative, which was institutionalized through the Healthy, Hunger-Free Kids Act of 2010. By explicitly disavowing fast food and processed foods, the revised school lunch guidelines dovetailed with Slow Food's mobilizing agenda—an alliance that Slow Food USA was eager to promote (see Figure 2).Graph: Figure 2. Fodder for the elitist critique: Slow Food's controversial political alliances.Once these Slow Food–friendly standards went into effect, news (and social) media began to feature anecdotal reports of children refusing to eat these presumably unpalatable lunches and skyrocketing food waste ([28]), with some critics characterizing the program as ""gastro-fascism"" ([71]). The elitist charge became integral to this cultural (and political) backlash:Michelle Obama thinks she knows what your children should eat. She's adamant about promoting her nutrition policies for kids, even the new and disastrous school meal standards implementing the ""Healthy, Hunger-Free Kids Act.""... But attending Ivy-League schools doesn't magically make someone better parent material than an individual who attended a public university, or, dare it be said, someone who didn't attend college. ([ 4])Like other market exemplars of conscious capitalism, Slow Food's aesthetic and experiential arguments have become strongly associated with an unwarranted moralism (i.e., the authentic + inauthentic classification; primary relation of contrariety). Slow Food advocates frequently argue that fast food is a debased cuisine that deprives humanity of meaningful and rewarding experiences of eating and sociability ([73]; [83]). For the many consumers who have warm memories of enjoyable fast-food meals with friends and family (and maybe look forward to such treats), Slow Food's moralizing pronouncements seem to emanate from an elitist taste bubble that is disconnected from everyday pleasures and real-world practicality. Similarly, Slow Food's veneration of locally sourced ingredients, heirloom vegetables and grains, artisan-crafted foods, and seasonal cuisine also seems to assert an unwarranted claim to moral virtue. Rather than sacrificing for a greater societal good, such rarefied culinary objects seem more attuned to signaling that one possesses the discretionary resources of time and money to treat food and cooking as a self-actualizing identity practice. Accordingly, Slow Food is easily, via the elitist critique, decried as an aggrandized form of cultural snobbery (e.g., [53]). Authenticating Strategies in the Slow Food MarketFigure 3 represents the correspondences between Slow Food's contextualized authenticity contradictions, the disauthenticating association that ensues from each contradiction, and the strategies through which Slow Food advocates seek to negate these authenticity challenges. In this representation, indexical authenticity (authentic ↔ not inauthentic) is the contested ideal that our Slow Food advocates are seeking to defend.Graph: Figure 3. Authenticity contradictions and authenticating strategies in the Slow Food market.Our Slow Food consumers place the most emphasis on the reflexive strategy, which they use to counter the authentic + inauthentic contradiction (primary relation of contrariety) and its disauthenticating association of virtue signaling and moral pretentiousness. Rather than rejecting the elitist critique outright, Slow Food advocates interpret it as a warning sign that the Slow Food market has become a gentrified facsimile of the movement's origins in the everyday cuisines of rural Italians (i.e., a disparaging version of iconic authenticity). Accordingly, our participants revere practices that seem to resurrect Slow Food's agrarian values and democratizing goals.The humanistic rebel strategy redresses the not inauthentic + not authentic contradiction (secondary relation of contrariety) and its disauthenticating association of social exclusion. In the context of the elitist critique, this contradiction holds that individuals whose lives have been shaped by class privilege may be blithely unaware of their own internalized elitist predispositions. From this standpoint, Slow Food advocates may have a genuine interest in making the world a better place (i.e., they are not consciously ""faking it""; rather, they are being ""not inauthentic""). However, they are largely oblivious to how their viewpoint on these problems and solutions has been shaped by a life of class privilege and their habituated, middle-class (bourgeoisie) sensibilities. This disauthenticating association renders Slow Food consumers as being somewhat akin to the proverbial fish in water. Rather than not realizing they are wet, however, the analogical implication is that they cannot comprehend that other terrestrial animals lack the requisite resources to enjoy life in the water, as they do.To negate this authenticity challenge, our participants drew from humanistic rationales, such as the idea that certain kinds of experiences and social connections have magical and transformative qualities that transcend social differences ([ 2]). Importantly, this strategy combines a humanistic ethos with the idea of rebelling against a deleterious marketing and cultural status quo and, thereby, creates a distinction to the complicit, part-of-the-problem connotations of liberal elitism (see [92]).The perfective strategy corresponds to the inauthentic + not authentic contradiction (relation of complementarity). This strategy is most relevant to those positioned on the entrepreneurial/production side of this market system. It aims to negate the disauthenticating association of commercialism (i.e., conscious capitalist enterprises are profit-seeking marketing ploys). In response, our Slow Food producers and entrepreneurs draw from the bohemian ideal of the artist who refuses to compromise their artistic vision, despite market incentives to ""sell out"" (i.e., betraying one's artistic integrity in return for financial reward) ([10]; [87]). Accordingly, they present themselves as being intrinsically committed to perfecting their Slow Food craft and pursuing conscious capitalist values and ideals, rather than doing it for the money. The Reflexive StrategySlow Food advocates use the reflexive strategy to negate the authenticity challenge of moral pretentiousness. The implication is that Slow Food assigns an unwarranted degree of moral virtue to those who have the economic wherewithal to buy rarefied ingredients, spend time on complex meal preparations, and dine at expensive farm-to-table restaurants while casting those who lack such resources as less virtuous consumers. In response, our participants interpret Slow Food's cultural associations with affluent foodies and elite taste practices as a regrettable, but correctible, market distortion of the movement's authentic values and practices.While acknowledging that market upscaling has imbued Slow Food with an elitist aura, our participants reiterate that expensive, epicurean cuisine need not be and, indeed should not be, regarded as the quintessential expressions of Slow Food:I think one of the things is this perception that if you shop at farmers' markets or at the co-op, it's a lot more expensive. And there is a little bit of this Slow Food bent into cooking elaborate meals, and I think some people perceive that as being elitist because it's sort of this educated way of thinking about food. I don't think of it as being elitist because a lot of times, recipes can be super expensive to buy all the ingredients for, but they don't have to be. I don't think that enjoying your food should be something that is thought of as elitist.... Like, I buy what's not super expensive at the co-op and I cook pretty simply.... What I really like about Slow Food in particular is the aspect of enjoyment and that good food is for all—what good, fair, clean food means for the farm worker to the people who are consuming the food. (Erin)Erin does not summarily dismiss the elitist charge. Rather, she takes a more ambivalent stance by first conceding that Slow Food values and ideals are often enacted in ways that can be read as elitist, such as cooking elaborate meals using expensive ingredients. In her authenticating interpretation, she counters that Slow Food values are better expressed through fundamentally simple meals that do not require extensive preparation time or costly ingredients. Her emphasis on there being many affordable options at her food co-op (which is, of course, a relative judgment) and on ""cooking simply"" (another relative assessment) convey that she is staying true to Slow Food's core principles rather than trying to place superficial, foodie predilections on a higher moral plane.Erin further counters this aspect of the elitist critique by incorporating the economic interests of farmers into her inclusive interpretation of Slow Food stakeholders. This interpretation creates a rhetorical contrast between Slow Food's foundational discourse of economic populism (emphasizing fair wages for agricultural workers) ([73]) and the elitist condemnation that higher prices are merely a means for affluent consumers to mark status distinctions.Paula's narrative exhibits a similar authenticating logic to that expressed by Erin:Slow Food has often come under fire for being elitist. I don't actually think that's true.... The beginnings of Slow Food were about people eating good food, and those were not necessarily rich people. We are talking about people who might have had very little money.... When most people think about amazing Italian cuisine, they were eating very basic foods. So, the whole idea of eating good food to me doesn't seem elitist at all.... Slow Food in the United States, yes, we do certain things that might be seen as elitist—the restaurant dinners and stuff like that. But again, you are still educating people. You are still getting more people involved. And the more people who know about local farming, sustainable farming, eating seasonally, making sure that farm workers are protected and paid properly, that spreads out. And we do projects with a variety of different populations, and we are trying to do more of that.... Slow Food does a lot of work in all its chapters to help with urban gardens or school gardens.... In the long run, our goal is that all people have access to this kind of food.... We are working toward passing that power on to more people. So, I don't think wanting children and families in need to have high-quality food is elitist. (Paula)In this vignette, Paula first differentiates Slow Food practices from elitist pretentions by invoking its historical connections to rustic Italian foodways. She asserts that Slow Food enjoins a pleasurable, resourceful, and fundamental relationship to food that should be accessible to people from all walks of life, rather than being an exclusive province of affluent consumers. However, Paula also recognizes that her inclusive rendering of Slow Food is contradicted by the realities of socioeconomic stratification. From Paula's viewpoint, Slow Food's community outreach efforts can play a pivotal role in democratizing these forms of culinary cultural capital so that consumers from less privileged backgrounds can acquire the skills and knowledge needed to incorporate Slow Food practices and ideals into their culinary routines.When utilizing this reflexive strategy, Slow Food advocates routinely assert that cultural capital ([ 9]), rather than a lack of economic resources per se, is the primary barrier that keeps consumers from integrating Slow Food ideals and practices into their everyday lives. Christina echoes this rationale when discussing how low-income consumers could enact Slow Food practices if they had more knowledge about utilizing the fresh produce and bulk goods that often go to waste in the local food pantry where she volunteers:Some people who are in Slow Food are foodies. However, it does not cost a lot of money to eat right. There are food pantries who throw away produce because people who come to the pantry don't know what to do with it and they don't take it.... Fresh produce going to waste.... No one wants it because they don't know what to do with it. It's really unfortunate. So, people have more access than they think. There are bulk aisles at grocery stores that you can get food for less money. It is actually a lot less expensive to buy bulk rice or bulk oats or whatever else, than to buy the bagged, boxed stuff that's like creamy preprocessed. I think the real lack of resource is education, not so much money. (Christina)Slow Food advocates often draw an authenticating contrast between foodie-ism—which fetishizes highly aestheticized meals prepared with exotic (and typically expensive) ingredients (see [52])—and the Slow Food ethos of preserving traditional foodways and skills ([75]). This distinction is quite salient to Slow Food chapter leader Kevin. He posits that Slow Food's culinary values and ideals have become misconstrued in their translation to the consumerist, status-conscious culture of the United States. Kevin's goal is to reclaim Slow Food's original ethos from its commercial appropriation by high-end retailers and restaurants:To buy imported cheese, organic wine, and all these kinds of things, I don't think those are meant to be the most obvious expressions of Slow Food values. And I think this is where the cultural translation from Italy to the United States went wrong, is that it got tied up with those folks [i.e., affluent foodies]. In Italy, it's much more about cooking at home. It's much more about preserving grandma's recipes. It's much more about celebrating the seasons and the tradition and preserving home ways of life than it is about eating in restaurants that do everything right. And you know, like anything else, capitalism wants to subsume this revolution.... That's a schism that I am personally trying to address and maybe lead by example. I don't think we should be cooking like a Michelin-starred restaurant at home. I think we should be cooking like our grandparents and great-grandparents. And I think we can learn a lot from traditional cultures and indigenous people—to the extent that there still are any indigenous people—how to eat well, and you know, a lot of those foods have become an affectation in restaurants. They'll have poutine, but it's made with truffles, and confit duck and elaborate things. I have realized that we are all attracted to the comfort foods and the simple foods, like tacos, and they are easy and fun to make. (Kevin)For Kevin, Slow Food should be accessible, basic, fun, and easy—characteristics that diverge from associations with rarity, cosmopolitan sophistication, aesthetic refinement, and technical proficiency that mark elite tastes ([44]). If read in a more critical light, Kevin's narrative reiterates the nostalgic glossing of preindustrial foodways that critics of the Slow Food movement assail for ignoring the harsh realities of scarcity and subsistence endured by those who had to survive on ""traditional diets"" ([56]).While romanticizing images of a bucolic culinary past have considerable appeal to our Slow Food advocates, the idea of rekindling a premodern utopia is not that central to the reflexive strategy's authenticating function. Rather, these homages to a bygone era, when people lived close to the land and prepared food in traditional ways, symbolically link Slow Food practices to agrarian and/or rural lifestyles far removed from elite pretensions:I did an internship through Worldwide Working on Organic Farms.... I went to Italy and I milked sheep and goats for a couple of months. And it was very rural. It was very low-tech. We milked in buckets by hand, sheep and goats, and we kind of went out with big sticks and sheepdogs and herded them.... Slow Food originated in Bra, Italy, and that was only like an hour and a half away from the farm. So, I think that's kind of how Marco [the farmer] was involved in Slow Food. He made cheese that was very well regarded, and he went to cheese festivals and stuff. But I mean the whole day was slow. Like wake up kind of late, drink your espresso, milk leisurely, walk the mount, you know. Dinner took a really long time, but that was kind of okay. And just kind of do the same things over and over again. So, we all cooked together. They also did some kind of agro-tourism. They'd have people from the city come out and we would cook with them the food we either grew or found.... That was fun. (Leslie)Leslie's narrative validates a nexus of Slow Food ideals regarding small-scale production and the slower pace of agrarian life. While this rural setting has some trappings of a staged performance—most notably Marco's side business in agro-tourism—it places Slow Food in a symbolic sphere far removed from the Whole Foods brandscape, expensive farm-to-table restaurants, and other consumption domains invocative of foodie affectations (see [52]). For Leslie, her story of interning on a rustic Italian farm affirms that she has actually lived the conscious capitalist values she endorses through her Slow Food advocacy (and thus is not a hypocritical moralizer). More generally, consumer narratives that link Slow Food practices to traditional modes of food production, down-home family meals, and simpler ways of living—rather than exorbitantly priced gourmet dishes—express a rhetorical parry to the elitist charge of moral pretentiousness. The Humanistic Rebel StrategyOur Slow Food advocates use the humanistic rebel strategy to redress the authenticity challenge posed by the elitist critique's connotation of social exclusion and the related sociological argument that consumers' social class backgrounds structurally predetermine their taste affinities ([44]). From this critical viewpoint, Slow Food advocates may not consciously intend to be elitists, but their preferences for goods that convey meanings of sustainability, locavorism, and artisanship betray a host of class advantages that distinguish them from consumers whose lives are marked by conditions of necessity ([47]). While Slow Food advocates may be well intentioned (i.e., they are being ""not inauthentic""), they are also complicit in a system of institutionalized class-based inequities.To illustrate this tension, let us reassess Leslie's preceding vignette in relation to this association with social exclusion (rather than moral pretentiousness). On the one hand, working for room and board on a small, rural farm clearly diverges from conventional notions of status posturing. However, a sociological counterpoint is that Leslie—as a college-educated young adult engaging in an exploratory experience—is building a reservoir of life stories and cultural capital that can afford career and status advantages later in life (see [91]). Seen in this sociological light, Leslie is enacting her class privilege by having the economic and social latitude to intern on a rustic, Italian farm before transitioning into more conventional middle-class occupational pursuits, such as attending graduate school.As a chapter leader, Kevin has oriented his local chapter's activities toward the goal of making Slow Food a more inclusive organization that does not merely cater to the interests of middle-class consumers. When implementing these outreach projects, however, Kevin recognizes that many Slow Food practices are simply incompatible with the situational demands that lower-income consumers have to negotiate on a daily basis:I have amazing privilege,... like being an American to a middle-aged white guy who has very marketable skills.... But I realize that that is a privilege, and this is the biggest thing for us in Slow Food to grapple with—that a single mother who has three jobs and two kids doesn't have the luxury of deciding, ""I think I would like to work less and spend more time in my garden"".... So, you have to be very careful and sensitive about it. (Kevin)Kevin believes that this class chasm can be bridged if his team of middle-class volunteers presents Slow Food's approach to food provisioning, cooking, and eating in a manner that is ""sensitive"" (i.e., adapted) to the lifestyle constraints faced by less well-resourced consumers. His optimistic viewpoint is based on the authenticating assumption that Slow Food's experiential and social benefits have an inherent appeal to consumers, regardless of their class position, because they tap into fundamental human desires and needs.The humanistic rebel strategy takes this inclusive rationale further by suggesting that a confluence of technological and commercial forces have locked individuals into an accelerating pace of life. Consequently, experiences of social connection, spontaneity, and everyday small pleasures are sacrificed to demands for efficiency, convenience, and the seductive (and ultimately alienating) effects of social media and digital communications.Aaron echoes this humanistic rebel mantra in his commentary on the inherent importance of eating and cooking with others:You have the social component of Slow Food as cooking and eating together and taking time to reflect and to connect and to develop community. That's something that we lose when we have things like drive-through or microwave dinners, which aren't to be destroyed or demonized altogether. I take advantage of these services of society. But for them to be the baseline means that we are losing what ... enriches our social systems a lot more than people eating alone and interacting through screens.... So, I think food, when it's jointly cooked and eaten, serves as a very natural medium for connection and idea generation and creativity. (Aaron)Aaron characterizes Slow Food as a needed corrective to societal transformations in the practices of cooking and eating that have resulted in a loss of sociability, communal bonding, and creative interactions. By interpreting Slow Food as a medium for enjoining meaningful social interactions, Aaron expands its cultural province well beyond the predilections of affluent consumers. Aaron's caveat that he has partaken in the conveniences afforded by fast food and microwave meals is another rhetorical means for distancing his Slow Food preferences from elitist connotations. His narrative signals a conscious disavowal of snobbery by acknowledging that there is a legitimate time and place for ""fast food."" Aaron then specifies the problem as the cultural ubiquity of fast food, which, in turn, he links to dehumanizing consequences.When expressing the humanistic rebel strategy, our participants often couched the communal experiences of preparing or eating food as magical moments that affirm Slow Food's class-transcendent qualities:I ran a cheese-making class, and that was really fun because I've always been interested in cheese-making, just for the fun of it. There were seven or eight people there. And one of the members still is making cheese today. And it was really fun to be able to share that magic with people. That this is how this cheese actually comes into being, and it's totally doable by yourself at home. So, that's really exciting, that he got so inspired. It's fun to see somebody get really interested in something. (Amanda)Amanda emphasizes the magic of cheese-making and the inspiration and rewarding personal experiences of self-sufficiency it can enjoin (i.e., ""totally doable by yourself at home""). Her narrative expresses a cosmological view of nature as a magical, life-transforming force ([ 2]) whose authenticating properties are not inherently tied to class-shaped tastes. In keeping with this formulation, Amanda interprets the sharing of her Slow Food skills as a means to help others experience these inspiring connections to nature, which, in turn, implies a revelatory contrast to the alienated experiences of industrialized fast food.Maggie similarly interprets her self-taught Slow Food skills as a means to help people create a sense of communal togetherness and to experience new sensory pleasures and magical connections to the land:I think of Slow Food as taking your time to respect the ingredients and preparing them from scratch and enjoying food. And that, to me, resonates. And bringing back the social aspect of eating. Like, you take time to prepare this meal, you sit down, you share it with people who care about the same things that you do. And it's also, creating another community of people who value these things whether they're growing, or cooking, or eating; having that kind of common thread, I think is really satisfying. (Maggie)Maggie venerates Slow Food as being inherently conducive to experiences of sociability and community and as a way to recognize the meaningfulness of food (a normative orientation that reads as a general human value, rather than a class-interested practice). However, Maggie is a meat producer who, like other Slow Food entrepreneurs, faces an additional task of negating contradictions that derive from the ""inauthentic + not authentic"" category. The Perfective StrategyThe perfective strategy seeks to negate Slow Food's disauthenticating association with commercialism. This aspect of the elitist critique casts Slow Food producers and entrepreneurs as disingenuous actors who are enrolling consumers into an inauthentic market relationship (akin to a gimmick or a confidence game) to serve their own economic interests. In response, our Slow Food entrepreneurs strive to authenticate their actions by signaling that they would never compromise their Slow Food ideals for the sake of profit, such as by recounting the copious amounts of time and energy they invest into perfecting their Slow Food enterprisesIn this spirit, Tom, a farm-to-table restauranteur, views his business as a way to enact his passionate commitment to producing food in a more meaningful and socially beneficial way:When you go to a fast-food restaurant, you have no idea of who actually made that food and the process of where it came from is not known to you. The taste and flavor are mostly engineered to play off the cheap sensory sensations. So, it's fatty and salty and sweet and so, yeah, on a certain level, it might be gratifying, but it's a cheap way to do it that is less meaningful. Slow Food is like, ""We're going to do things in a way that is process oriented!"" I talk about process a lot.... We [Tom and his restaurant staff] were really structured around learning, and so it was a process where we feel like we've excelled and learned a lot and we'll keep pursuing that.... [With Slow Food,] you have this process where people are eating and making something and understanding where it came from and how it works. Eating is such an important part of our lives, and it can have a really important impact on our community and environment. So, the more you understand about it, hopefully you'll make better decisions. The basic motto of Slow Food is clean, fair, and good food. I can totally get behind those values. (Tom)During his interview, Tom extensively discussed ""the process"" aspects of his restaurant and how he views it less as a business than a means to cultivate and diffuse knowledge about the complex interrelationships among food, cooking, sensory enjoyment, ecology, and societal well-being. His quote also reiterates Slow Food's argument that the experiences of these simple culinary pleasures can mobilize consumers to resist the industrialization of the food system (thus echoing the humanistic rebel strategy). For Slow Food entrepreneurs, however, it serves the additional function of associating their enterprises with civic goals that stand distinct from conventional commercial aims.Returning to Maggie, she raises pasture-fed rabbits for sale to farm-to-table restaurants and consumers. In developing her production techniques, Maggie has constantly experimented with different procedures and equipment designs. Through this long trial-and-error process, Maggie believes she has developed an innovative method that better simulates the lives her rabbits would enjoy outside of captivity:Maggie: Daniel Salatin is the son of Joel Salatin, who is the owner of Polyface Farms, and he is the person who is raising rabbits in this system that he has devised and calls the Hare Pen system. So essentially, you still have your does in cages.... You put them in a glorified cage that you then put on grass.... I've copied their system exactly, and I was very unsatisfied with the results that I got. [Maggie then provides an extensive description of her alternative and labor-intensive system and how she developed it]... I don't know why I kept doing it. But I finally have a system that is really effective.... It just was a lot of observation of the rabbits on pasture, making so many mistakes and then incorporating what I had learned.Interviewer: Did you have any economic incentives?Maggie: No! It has to be a personal belief that there might be a better way to do things.... It's kind of like what makes an artist a good artist. If they all hold the brush the same way and they are using the same colors, but they create vastly different things, and one appeals to you, and one doesn't appeal to you. So, what makes that one piece of art recognized by the vast majority of people as superior?... I have this wonderful platform to invest energy and creativity, and it's nice. And so, I feel in some ways really lucky.Invoking the image of the passionate artist, Maggie distinguishes her efforts to perfect an ecologically appropriate system for raising rabbits from crass commercial and economic interests. Maggie's closing sentiment expresses her authenticating belief that such actions can make things better, rather than being driven by instrumental aims. Through storytelling, and by showing how her system works to customers who visit her farm, Maggie deploys narrative and material resources to negate disauthenticating concerns that her Slow Food affinities are merely an instrumental means to charge higher prices. Her personal investment in learning about rabbits' natural habitats/behaviors and inventing a complex ecosystem for raising them further signals that she is not likely to compromise her Slow Food principles in the interest of commercial expediency. Discussion Theoretical ImplicationsPrior research has treated authenticity as a perceptual value or quality that consumers attribute to a brand ([11]; [55]), person-brand ([87]), product ([57]), or performance ([ 5]; [39]). In contrast, we have reconceptualized authenticity as an ongoing process through which consumers and marketers negotiate a contextualized system of cultural contradictions and ambiguous classifications. We suggest that our semiotic framework can better analyze the authenticity contestations that arise in a given market or sociocultural context than conventional theories that assume authenticity perceptions operate on a continuum or selectively draw from an essential set of defining attributes.The conceptualization of authenticity as a relative point along an authentic-to-inauthentic continuum ([22]; [65]) can depict a zone of ambiguity where the authenticity or inauthenticity of a market actor is perceived as being uncertain and, thus, debatable. However, this conceptualization does not offer a means to specifically analyze the cultural meanings (and the interrelationships among them) that generate these ambiguous perceptions. Accordingly, it offers limited theoretical discrimination and managerial guidance.For example, [65] argue that quality commitment, heritage, and sincerity are the primary perceptual cues of authenticity. They then propose that brands should differentially leverage these cues depending on whether consumers perceive them as having low, moderate, or high levels of authenticity. In their normative framework, brands with low perceived authenticity should emphasize sincerity, brands with moderate perceived authenticity should emphasize quality and heritage, and brands with a high level of perceived authenticity should emphasize all three authenticity cues.Such recommendations presume that brands falling into the lower and middle sectors of this proposed continuum have a shortfall of perceived quality commitment, sincerity, or heritage that is rectifiable through compensatory signaling. However, such contested brands are often plagued by contradictory meanings that undermine their promoted claims to authenticity ([37]; [86]). Furthermore, more complex, disauthenticating narratives, such as the elitist critique, can cast doubt on the very credibility of such authenticating cues when used by a contested brand or actors in a market system.Turning to combinatory definitions, [67] have offered a comprehensive theorization of authenticity (as understood from the consumer's perspective) that warrants comparison to our approach. They identify six subdimensions of authenticity (accuracy, connectedness, integrity, legitimacy, originality, and proficiency) and then trace out the relative impact of those dimensions across different market categories and on consumers' behavioral intentions. Rather than a continuum, Nunes, Ordanini, and Giambastiani argue for a family resemblance explanation in which ""a concept (authenticity, in this case) may be qualified by different subsets of its dimensions across different contexts, and not always by all of them in the same way"" (p. 16).Like a continuum, [67] family resemblance logic is limited to the explanation that authenticity is a multidimensional construct whose subcomponents may be more or less important in a given market or consumption context. In contrast, a semiotic framework shifts attention from correlational premises (e.g., this authenticity subdimension seems more important for hedonic products than utilitarian ones) to the cultural meanings, and underlying structural contradictions, relevant to a particular judgment regarding the authenticity of a given product, brand, or market action. For example, the authentic ↔ inauthentic tension elevates the importance of authenticity's moral dimensions in ways that traverse product category distinctions, such as hedonic or utilitarian.To illustrate, a hamburger would typically be classified as a hedonic good. [67], pp. 3–4) find that judgments of ""legitimacy""—which they define as ""the extent to which a product or service adheres to shared norms, standards, rules, or traditions present in the market ... appear to matter for utilitarian but not hedonic products."" However, if we examine this consumer choice in the context of the Slow Food market, then legitimacy becomes a far more important issue. From this standpoint, an ""authentic burger"" would need to exhibit fidelity to various aesthetic and moral norms—grass-fed beef, local sourcing, traditional preparation techniques, and so on—and, its perceived authenticity would be understood and legitimated through a contrast to fast-food burgers. That authenticating contrast (the fast-food burger vs. a Slow Food burger) could then become subject to the elitist critique, which, in turn, would provide motivation for Slow Food advocates to negate these disauthenticating associations.In summary, we have argued that authenticity is culturally constructed (and contested) in a network of structural relations (rather than being a discrete set of essential properties attributed to a brand, person-brand, market performance, or market relationship). Consumers and marketers alike covet indexical authenticity (i.e., the abstract ideal of authenticity) because it can confer cultural legitimacy ([51]), moral authority ([59]), and identity validation ([ 7]; [86]) all of which, can be converted into micro-celebrity status ([79]) and a branding asset ([31]; [45]). However, this authenticity ideal is structurally linked to contradictory meanings and ambiguous classifications. When consumers' or marketers' authenticity claims are challenged by these cultural contradictions, they have pressing incentives to distinguish their actions and identities from the invoked disauthenticating associations. In the following subsection, we discuss how this authenticating goal can be enacted by negating associations that flow along the contradictory path of deception and promoting those that follow the contradictory path of redemption. Two Managerial Paths to Authenticating a BrandAs [49] have argued, marketing managers often find it difficult to redress brand image problems because they are unable to effectively decipher the cultural meanings contributing to those dilemmas. Our semiotic framework can help redress this managerial shortfall. It offers a tangible means for marketing managers to systematically analyze the cultural contradictions of authenticity that emerge in a given market and then to identify strategies for authenticating their brands in the face of these challenges.As a general heuristic, we propose that marketers can be successful in authenticating their brands and/or other strategic assets when they are able to accomplish two complementary goals. The first is to leverage cultural meanings that negate the disauthenticating associations that flow along the contradictory of deception path (authentic → not authentic; see Figure 1). When consumers follow this perceptual path, they experience a glaring contradiction between a prevailing ideal of authenticity and its market manifestation in a brand or marketing practice (authentic ↔ not authentic), which then leads to an association of inauthenticity via the complementary relation of not authentic → inauthentic. In response, marketers should try to provide consumers with compelling and emotionally resonant meanings and rationales that discount the credibility, relevance, or importance of the disauthenticating associations that have gained cultural currency in their respective market.As one illustration, Patagonia confronted a path of deception authenticity challenge soon after it began campaigning against the Trump administration's executive order to reduce the size of Utah's Bears Ears National Monument by two million acres. On December 4, 2017, Patagonia featured this message on the front page of its website: ""The President Stole Your Land: In an illegal move, the president just reduced the size of Bears Ears and Grand Staircase-Escalante National Monuments. This is the largest elimination of protected land in American history."" This web page then directed consumers to various information sources and encouraged consumers to contact their elected officials and to also take the protest to social media, using the hashtag #MonumentalMistakes (see [ 1]).However, defenders of the administration's policy change were quick to denigrate Patagonia's activism as a deceptive marketing ploy. Interior Secretary Ryan Zinke condemned Patagonia as a dishonest ""special interest"" and proclaimed it was ""shameful and appalling that they would blatantly lie in order to put money in their coffers."" Utah Representative Bob Bishop, then chairman of the House Natural Resources Committee, also evoked the elitist critique in his tweet proclaiming that ""Patagonia is Lying to You... A corporate giant hijacking our public lands debate to sell more products to wealthy elitist urban dwellers from New York to San Francisco"" (quoted in [35]).In terms of our model, the Trump administration's response challenged the authenticity of Patagonia's mobilizing campaign by impugning its motivations, thereby reframing an ostensibly authentic (conscious capitalist) action as a disingenuous public relations stunt designed to extract more profits from elite consumers (authentic → inauthentic), which, in turn, triggers the complementary association to inauthenticity. In response, Patagonia joined as a coplaintiff with five Native American tribes and several nonprofit groups in a lawsuit aiming to halt the policy change ([35]; see also https://www.patagonia.com/stories/hey-hows-that-lawsuit-against-the-president-going/story-72248.html). Patagonia also continued to be a vocal critic of the Trump administration's environmental policies and, in a politically and ideologically related vein, donated all its tax savings from the Trump-backed corporate tax cut to environmental groups while condemning the new corporate tax rates as being irresponsible ([63]). Through these responses, Patagonia signaled a deeper commitment to its conscious capitalist values and gave consumers reasons to doubt or dismiss the disauthenticating associations of greed and deception that were being cast on it. In response to Patagonia's uncompromising stance, Inc. offered the following commentary on its 2018 Company of the Year finalist:For Patagonia and its fans, that purpose is doing whatever they can to try to save the planet. In 2018, Patagonia proved that it will not only preach that mission, it will do so with a much louder voice than most other companies. And—so far, anyway—it's only further burnished the Patagonia brand. ([ 8])The second marketing goal is to create conditions in which consumers interpret a brand or business along the contradictory of redemption path (inauthentic → not inauthentic) (see Figure 1). This redemptive chain of associations begins with the widespread cultural view of marketers as inauthentic and, thus, untrustworthy actors ([45]; [66]; [67]). Accordingly, we can assume that consumers will typically harbor varying degrees of skepticism and suspicion toward the authenticity of marketing and branding claims. Redemptive meanings encourage consumers to believe that a given brand or business is operating in ways that favorably diverge from the marketing status quo (i.e., not inauthentic) which, in turn, leads to the complementary relation of not inauthentic → authentic.Volkswagen's (VW's) ""Hello Light"" advertisement, which launched its new line of electric vehicles (circa 2019), takes viewers on a journey that follows a path of redemption arc (see https://www.youtube.com/watch?v=qEvNL6oEr0U). The ad begins with a silhouetted figure entering a dark and seemingly abandoned production facility, while a news report about VW's emission scandal, or ""Dieselgate,"" blares in the background. In a seemingly counterproductive marketing communications move, the ad explicitly reminds its viewers of all the inauthentic associations (VW as liar, deceiver) that arose from those ""dark"" days. The protagonist is revealed to be a despondent engineer struggling to design a new VW model, against the musical backdrop of Simon and Garfunkel's 1960s anthem, ""The Sound of Silence."" Desperate for inspiration, our engineer scours the company archives and finds his creative muse—an image of the iconic VW Van (aka the ""Love Bus"").Through the choice of song and reference to this totem of the 1960s counterculture, the ad recalls VW's countercultural legacy as an authentic symbol of antimaterialist values and a rebuke to status consciousness and marketing hype ([46]). The ad's message is that VW, despite having lost its way, still possesses a latent essential ""goodness"" that is ""not inauthentic."" As ""The Sound of Silence"" reaches its crescendo, lights go on, puncturing the darkness. We observe the production facility come to life and give metaphorical rebirth to the VW brand in the form of an electric van (which also places ""The Sound of Silence"" on a different, ecofriendly cultural register). Thus, the ad's narrative follows the redemptive path of ""inauthentic"" (VW's Dieselgate) to ""not inauthentic"" (VW's 1960's countercultural heyday) to ""authentic"" (signifying that VW has rekindled its socially conscious roots).Our discussion of the three authenticating strategies used by Slow Food consumers and entrepreneurs has emphasized their function as a defensive means to disavow or negate the disauthenticating associations that flow along the contradictory of deception path. However, these same strategies also promote affirmative meanings and associations that operate along the contradictory of redemption path. For example, the perfective strategy does more than negate the disauthenticating association with commercialism. It also magnifies authenticating differences to fast food or industrialized food production and thereby encourages consumers to interpret Slow Food enterprises in a manner compatible with the contradictory of redemption path (even though they may be aware of some disauthenticating associations). This redemptive associative chain takes the following form: Slow Food entrepreneurs are suspected to be ""inauthentic"" due to their commercial motivations → Slow Food entrepreneurs are seen as being ""not inauthentic"" because their deep commitment to artisan ideals and noncommercial values differentiates them from conventional fast-food establishments and industrialized modes of commercial food production → the signification of ""not inauthentic"" supports a broader conclusion that the Slow Food entrepreneur is an authentic actor. Implications for Managers of Conscious Capitalist BrandsGiven their shared ideological affinities, the authenticating strategies used by Slow Food advocates should also have a high degree of applicability to brands espousing conscious capitalist goals and ideals. Of the three authenticating strategies, we find numerous examples of conscious capitalist brands that have enacted some version of the perfective strategy, which aims to negate associations with commercial opportunism and foster interpretations compatible with the contradictory of redemption path. While less commonplace, we can also find branding campaigns that align with the reflexive and humanistic rebel strategies. In the following discussion, we use these various exemplars to illustrate how these authenticating strategies can be implemented by conscious capitalist brands and the authenticity contradictions they potentially redress. Perfective strategyBrands using the perfective strategy engage in unconventional actions that demonstrate a deep commitment to activist causes that supersede profit motives. Over the years, Patagonia has made frequent use of this authenticating strategy to signal that proenvironmental values were central to its corporate mission, even when such acts could mean sacrificing sales, such as its iconic ""Don't Buy"" promotion ([49]) or their ""Give a Damn"" holiday messaging ([72]). REI has also enacted a perfective strategy in its #OptOutside campaign, whereby the retailer closes its stores on Black Friday and encourages consumers to engage in a range of proenvironmental, outdoor activities. Like Patagonia, REI's campaign builds on (and authenticates) the brand's history of supporting environmental causes and promoting a heightened concern for habitat protection and environmental conservation. Last but not least, Clif Bar illustrated a fairly novel implementation of the perfective strategy when its founder and chief executive officer, Gary Erickson, published an ""advertorial"" in the New York Times, offering to donate ten tons of organic ingredients to his main competitor Kind Bars. This advertorial further promised to share his company's knowledge about organic sourcing and production so that the two companies could collectively ""lay the foundation for a healthier, more just and sustainable food system"" ([26]).Owing to their status as commercial enterprises, whose existence depends on profitability, the perfective strategies of conscious capitalist brands can always be reframed as yet another kind of commercial deception. However, such brands can lessen the cultural viability of such recursive challenges by further signaling that their passionate commitment to the supported causes takes precedence over profit motives. Though addressing a different context, [20] offer evidence that supports this strategic approach. They find that customers attribute the quality of authenticity to third-place establishments (e.g., cafes, coffee shops, restaurants) when they believe the respective proprietors are aiming to create meaningful social connections rather than merely trying to make a profit. As they write, ""The authenticity perceived in treasured commercial places is based on exchanges that go beyond mere commercial aspects.... Although being business operators, proprietors invite the consumer to engage in activities that are not undertaken purely for profit"" ([20], p. 913).Accordingly, we propose that conscious capitalist brands are more likely to be perceived as authentic when they provide tangible means for consumers to participate in their social change mission but do so in ways that are not dependent on purchases. From this standpoint, The Body Shop's repositioning of its stores as activist hubs ([77])—where consumers can listen to speakers discuss environmental and social justice issues, sign petitions, and join activist organizations—is an enactment of the perfective strategy and a culturally viable means to reestablish the authenticity of its conscious capitalist branding claims. Reflexive strategyThis strategy aims to negate the charge that a market actor is evincing a ""holier than thou"" stance for actions that are either hypocritical (e.g., ""do as I say, not as I do"") or overstate the positive impact of the self-proclaimed act of conscious capitalist rectitude. For conscious capitalist brands, this authenticating logic most readily translates into a reformist agenda. As one prominent example, Chipotle's ""Back to the Start"" campaign (circa 2012) rallied a diverse assemblage of activist groups that shared a commitment to transforming the corporate-controlled system of food production and who saw the fast-food sector as exemplifying its presumed ills (see [48]). The two-minute short film, which ran across multiple media platforms, shows an increasingly disenchanted farmer witnessing the steady industrialization of his enterprise, replete with enclosed animals, the heavy use of antibiotics, and food being transformed into nondescript goo-like substances. Against the backdrop of Willie Nelson's plaintive version of Coldplay's ""The Scientist,"" the farmer triumphantly decides to go ""back to the start"" by raising free-range animals, using traditional farming techniques, and selling his preindustrial goods to Chipotle.Some relevant insights into this campaign and its authenticating effects can be gleaned from a Fast Company interview with Jesse Coulter, co–chief creative officer of Creative Artists Agency Marketing, which worked with Chipotle's management team in developing this campaign:We were tasked to find new ways to tell Chipotle's Food with Integrity story.... The first issue Chipotle wanted to address was industrial farming.... Chipotle shared many stories of family farmers who have turned their farms into factory farms and have subsequently grown to regret it.... It was provocative because it took a stab at Big Agriculture. Chipotle is a bold company, who has the courage to really stand up for what they believe in.... At the end of the film, a title card appears letting people know that they can download the song on iTunes, and the proceeds benefit the Chipotle Cultivate Foundation, which is dedicated to creating a sustainable, healthy, and equitable food future. People responded and the song reached number one on the iTunes Country chart. ([15])When interpreted through the lens of the reflexive strategy, Chipotle's ""bold"" move was to accept and amplify activist groups' criticisms of the fast-food industry's sourcing and production practices, rather than attempting to deny, rationalize, or obscure these problems through conventional images of consumers enjoying their fast-food meals. The campaign thereby draws an ideological distinction between Chipotle (as a reformist enterprise returning to more humane and sustainable agricultural practices) and the broader fast-food industry that is portrayed as having debased the time-honored practice of farming in the name of speed, efficiency, and cost reduction. In this way, Chipotle aligned itself not with the interests of the fast-food industry at large, but with activist groups who are seeking to reform the broader system of industrialized food production. Chipotle further reinforced the credibility of their reflexive strategy by investing additional resources to support the broader cause (i.e., creating a synergy between the reflexive and the perfective strategy). Humanistic rebel strategyThis strategy promotes the brand as a means for reconstituting meaningful social connections and breaking down societal boundaries that artificially separate people. To avoid being just another nostalgic marketing ode, this strategy should take a critical stance toward selected status quo consumption and marketing practices. The intended message is that the conscious capitalist brand is enabling consumers to resist or escape the dehumanizing and/or isolating influences of materialism, status consciousness, and upward-ratcheting lifestyle competitions.IKEA has run numerous campaigns that align with the humanistic rebel strategy. These campaigns embed its conscious capitalist commitments to sustainability and support of social justice issues, such as gender equity and LGBTQ rights, in a home-as-haven brand narrative. In these ads, the IKEA-furnished home represents a therapeutic space where people can, at least temporarily, unplug from the stresses and distractions of the ""networked life"" ([88], p. 17) and experience meaningful human connections and emotional fulfillment.More than just a haven, however, IKEA often portrays the home as an active force that keeps at bay the outside forces that would interfere with the pleasures of slow living. In an ad titled ""Home Is a Haven,"" we see a father and daughter running to their house during a rainstorm. As they approach the front door, the child's teddy bears spring to life as human-sized entities (whose muscular physiques resemble bouncers at a club). The bears rearrange the house into an open play area and protect the dad from intrusive calls and other outside distractions. We watch as father and daughter play dress-up and numerous other games, eventually falling asleep after their fully engaged bonding time (https://www.youtube.com/watch?v=wGgcYNlH02g).IKEA's ""Let's Relax"" commercial presents a pointedly critical take on the performative, competitive affectations of Instagram micro-influencers, for whom everyday social activities are treated as an instrumental means to garner likes and followers. In the ad, we first observe an eighteenth-century family about to begin formal dinner in a very well-appointed dining room. Suddenly, the father halts the proceedings so that an artist can paint a portrait of the meal, which is immediately transported across the town in a horse-drawn carriage so that affirmative thumbs up gestures from the populace can be tabulated. The scene then suddenly shifts to a modern-day kitchen table, where the same father meticulously photographs the family meal, while his wife and children begrudgingly wait for this documenting ritual to end. The dad sheepishly retires his camera, and the family begins their more enjoyable and authentic social interactions, all framed by the closing caption: ""Relax: It's a meal, not a competition"" (https://www.youtube.com/watch?v=2BXRGzjo1%5fQ).Drawing on our semiotic framework, we anticipate that conscious capitalist brands would gain the most authenticating benefit from the humanistic rebel strategy when they present their brands as ideological allies ([48]; [49]) of consumers who are sensitized to the psychological and social costs of careerism, exclusionary status hierarchies, and the calculated practices of social media self-presentations. In this way, the humanistic rebel strategy undercuts the elitist critique by suggesting that a conscious capitalist brand enables consumers to tap into more basic and rewarding emotional and sensory experiences. It further emphasizes that helping people, from all walks of life, feel genuinely connected to each other is an important and accessible way to make the world a better place. ConclusionDrawing from structural semiotics ([40]), we have developed a conceptual framework that can be used to analyze the cultural contradictions of authenticity, as they emerge in a given market context, and then to identify strategies for combatting their disauthenticating associations. Our analytic approach recognizes that perceptions of authenticity are constructed and contested in a dynamic cultural system. When negotiating such dynamism, marketing managers need to identify strategically significant patterns in the flux of cultural change and to adroitly react to cultural flash points, competitive shifts, and other exogenous shocks that could undermine the credibility of their existing authenticity claims. Whether undertaken in the context of conscious capitalist brands, status-marketing luxury goods, price-driven big-box retailers, or sharing-economy enterprises such as Uber or Airbnb, marketing managers can use our semiotic approach to more effectively negotiate the sociocultural complexity inherent to the process of authenticating their strategic assets.  "
1,"Analyzing the Cultural Contradictions of Authenticity: Theoretical and Managerial Insights from the Market Logic of Conscious Capitalism This research analyzes the cultural contradictions of authenticity as they pertain to the actions of consumers and marketers. The authors' conceptualization diverges from the conventional assumption that the ambiguity manifest in the concept of authenticity can be resolved by identifying an essential set of defining attributes or by conceptualizing it as a continuum. Using a semiotic approach, the authors identify a general system of structural relationships and ambiguous classifications that organize the meanings through which authenticity is understood and contested in a given market context. They demonstrate the contextually adaptable nature of this framework by analyzing the authenticity contradictions generated by the cultural tensions between ""conscious capitalism""—a market logic that encompasses both global brands and small independent businesses, such as a farm-to-table restaurant or an organic food co-op—and the elitist critique. The Slow Food movement provides a case study for analyzing how consumers, producers, and entrepreneurs who identify with conscious capitalist ideals understand these disauthenticating, elitist associations and the strategies they use to counter them. The authors conclude by discussing implications of the analysis for theories of authenticity and for managing the authenticity challenges facing conscious capitalist brands.Keywords: authenticity; brand image; conscious capitalism; ethical consumerism; consumer identity; market logics; semioticsConsumers crave authenticity—so much so that their quest for authenticity is considered ""one of the cornerstones of contemporary marketing"" ([11], p. 21). This has created an enormous challenge for the field, considering that marketing itself is typically considered inherently inauthentic. —[67], p. 1)In the field of marketing, little doubt exits that ""authenticity"" is highly desired by consumers and thereby is a crucially important strategic resource for marketing management. Consumers are more likely to form stronger emotional attachments to a brand, business, or tourist site they perceive as being authentic ([20]; [30]; [57]; [86]) and to incorporate these market resources into their identities ([ 7]; [11]; [45]). On the managerial side, [31], p. 610) conclude that authenticity is ""the most rare and coveted asset in the contemporary branding landscape."" Their assertion is supported by an array of studies indicating that authenticity is integral to the enhancement of brand equity ([60]), effective brand extensions ([82]), persuasive marketing communications ([ 5]), success in relationship marketing ([22]), and emotionally engaging person and celebrity brands ([31]; [87]).Although there is a clear consensus that authenticity profoundly matters to both consumers and marketers, the marketing literature also presents a recurrent concern that authenticity is a nebulous concept that has eluded precise definition ([ 5]). [67], p. 2) proclaim that this conceptual ambiguity poses a significant barrier to creating ""a coherent theory of authenticity."" Accordingly, they aim to redress this dilemma by presenting a general definition of authenticity based on six key perceptual components. In contrast, [81], p. 3) proposes that ""authenticity is a polysemous and multilayered concept"" and thus ""it might not [emphasis added] be helpful to compress the wealth of disparate meanings associated with the concept into a single definition.""As Södergren further notes in his meta-analysis, ""the majority of the research [on authenticity] has focused on characteristics that distinguish the 'real thing' from the fake"" (p. 11). To further elaborate on this conceptual tendency, marketers' efforts to define authenticity almost invariably invoke some variant of genuineness, such as brands (via their management teams) staying true to ideals of timeless tradition, heritage, craftsmanship, and quality (see also [ 6]; [82]). In this spirit, [55], p. 30) propose that the authenticity of a brand's social media communications hinges on perceptions of honesty, sincerity, and being ""real."" [ 7] similarly contend that the core cultural meanings of authenticity are truth, genuineness, and reality. [67] comprehensive definition of authenticity also incorporates a series of veracity-oriented constructs, such as originality (i.e., not being a copy), accuracy (i.e., being true to others), and integrity (i.e., being true to oneself).While the analytic goal of distinguishing the authentic from the inauthentic makes intuitive sense, it is a Sisyphean undertaking that attempts to specify an ambiguous cultural category by referring to other semantic terms whose meanings are also contextually contingent and malleable (i.e., honesty, sincerity, originality, genuineness, and truthfulness). Furthermore, informing marketing managers that their brand lacks authenticity because consumers see it as being unoriginal, insincere, or dishonest offers little guidance on how to resolve the deeper cultural tensions that drive these unfavorable perceptions. Rather than a checklist of definitional attributes, we argue that marketing managers need an analytic approach that can enable them to answer questions such as ( 1) why is their brand or business susceptible to certain kinds of authenticity challenges?, ( 2) what cultural meanings and contradictions underlie those challenges?, and ( 3) what responses could they take to mitigate the disauthenticating associations that ensue from these tensions?Returning to our opening vignette, we can reframe [67], p. 1) statement that marketers face an ""enormous challenge"" because their profession is ""typically seen as inauthentic"" (see also [ 5]) as a realization that marketing, as a business practice, also occupies an ambiguous cultural position. On the one hand, marketing aims to advocate for the needs (and voices) of customers ([41]) and, yet, it is also means for companies to enhance their profits and market share. This tension readily gives rise to concerns that short-term (and potentially exploitive) profitability goals might take priority over serving customers' best interests. Accordingly, consumers are inundated with cultural narratives (ranging from journalistic reports about deceptive marketing tactics to portrayals of unscrupulous marketers by entertainment media) that encourage cynicism and distrust toward marketers' branding claims and persuasive communications ([32]; [45]; [66]).However, the specific cultural meanings and associations that lead to perceptions of authenticity or inauthenticity vary across brands and markets. For example, consumers are likely to deploy different configurations of meanings, beliefs, and evaluative norms when judging the authenticity of a high-fashion retailer ([23]), a café owner who promotes their establishment as a home-away-from-home ([20]) or a global brand that positions itself as an advocate for environmental justice (e.g., Patagonia; [49]).In this article, we explain and demonstrate how the semiotic square ([40]) can be used to systematically analyze such culturally heterogeneous authenticity contradictions and to develop contextually appropriate responses to the specific authenticity challenges that arise in a given market. The semiotic square is an analytic tool that has often been used to delineate cultural meanings and semantic contradictions that are manifest in both consumer perceptions and marketing strategies ([29]; [36]; [50]; [51]; [54]; [68]; [69]). From a semiotic perspective, the cultural categories of the authentic and the inauthentic are not just contrasting or oppositional terms. Rather, they are anchor points in a broader network of relationships through which the authenticity of a given brand, business, brand ambassador, social media influencer, and the like is culturally constructed and potentially contested.Our market context is conscious capitalism, which refers to a ""way of thinking about capitalism and business that better reflects where we are in the human journey, the state of our world today, and the innate potential of business to make a positive impact on the world"" ([62], p. 273). Conscious capitalism is particularly vulnerable to the broader authenticity–inauthenticity tension that all marketers confront to varying degrees. Therefore, it serves as a very relevant and informative context for our analysis.Conscious capitalism's key premise is that capitalism's societal purpose has, historically, been defined too narrowly (i.e., maximizing shareholder wealth and optimizing consumers' market choices) and, accordingly, its society-enhancing potential remains greatly underutilized. Rather than grafting a social mission onto a traditional profit-maximization model, as per conventional corporate social responsibility approaches, proponents of conscious capitalism contend that businesses should place value-driven goals and social consciousness at the core of their institutional missions ([62]).By aiming to redefine the nature and function of capitalism, conscious capitalism can be analyzed as a market logic that transcends its iconic brands (e.g., Patagonia, Starbucks, TOMS, Whole Foods) or socially conscious businesses (e.g., a cooperatively owned, fair trade, local coffee shop). As discussed by [27], pp. 40–42), a market logic is an integrated network of meanings, values, and norms that provide ( 1) principles that can guide thoughts, actions, and preferences; ( 2) vocabularies of motivation and justification; and ( 3) material and symbolic resources for constructing an identity (such as being an ethical consumer or a purpose-driven business owner).Conscious capitalism organizes a constellation of ideologically aligned brands and an even larger network of businesses that have different scales of operation and serve different roles in the supply chain. Thus, consumers who support this array of brands and enterprises have access to a set of normative principles to guide their purchase choices (e.g., locally sourced materials are preferred over imported ones, plastic product packaging should be avoided); they learn an intricate system of terms and codes (e.g., ""postconsumer recycled content,"" third-party certification labels such as the Rainforest Alliance or Certified Carbon Neutral); and they can express their socially conscious sensibilities through an array of consumption practices—wearing a Patagonia fleece, driving an electric car, shopping at a farmers' market, brandishing a reusable Whole Foods' canvas tote bag, buying fair trade chocolate, or supporting a farm-to-table restaurant.In the general public discourse, however, the authenticity of conscious capitalist brands and businesses, and their consumer supporters, is frequently called into question. These authenticity challenges are sufficiently problematic that leading proponents of conscious capitalism feel compelled to address them:There is a growing network of people building their companies based on the idea that business is about more than making a profit. It's about higher purpose ... and the innate potential of business to make a positive impact on the world.... But one of the most predictable responses we get from people when we mention the idea of conscious capitalism is, ""That's an oxymoron!"" ([61])Conscious capitalism's authenticity challenges hinge on a cultural tension between the profit-maximizing ethos of capitalism and the ennobling idea that capitalist enterprises can serve higher societal and moral purposes that supersede commercial interests ([ 3]; [33]). In this vein, critics often suggest that conscious capitalism deploys the language of sustainability and other socially beneficial goals for the instrumental purpose of catering to higher-income consumers who will pay a premium to imbue their consumption practices with an aura of moral virtue:Thus, the rise of social enterprises [i.e., conscious capitalist enterprises] has been met with hostility, particularly toward its authenticity and its sustainable impact. If their goods and services continue to be priced as they are, is the sustainable movement only for the demographic that can afford it? ([14])[70], p. 73) similarly argue that conscious capitalism is more promotional hyperbole than a viable business reality and, further, add this reservation: ""It is important to note that the firms associated with the Conscious Capitalism movement are far from a random sample of American businesses: In fact, a great many sell relatively expensive products to relatively affluent, socially- or health-conscious consumers.""This incredulous and, at times, adversarial public response to conscious capitalism has not arisen ex nihilo. Rather, it draws from a cultural narrative that we characterize as the ""elitist critique."" As historian [34] elaborates, the political charge of elitism has evolved from its classic populist roots, which railed against the undue power wielded by the captains of industry and affluent political insiders, to an antipathy toward the intellectual class (who may not be unduly wealthy or politically powerful). Through this shift, the charge of elitism was distanced from its origins in economic conflicts between the working class and the owners of capital (and their management intermediaries) and became repositioned in a culture war rift whereby ""the 'elite' could be identified by its liberal ideas, coastal real estate, and highbrow consumer preferences"" ([34], emphasis added).We investigate how the specific authenticity challenges posed by the elitist critique of conscious capitalism are negotiated by consumers and producers in the context of the Slow Food movement ([89]). Slow Food encompasses an array of ideologically aligned brands, enterprises (farm-to-table restaurants, artisan producers, and organic and free-range farmers), consumption practices (e.g., shopping at a farmers' market or a local co-op), and goods and services (e.g., an heirloom tomato, grass-fed beef, a class in fermentation techniques). Slow Food's signature issues and social change goals are grounded in the market logic of conscious capitalism, including local sourcing, fair wages for workers, sustainable modes of production, environmental awareness and habitat protection, and a broader project of redressing societal ills through the coordinated actions of socially conscious businesses and consumers (see [73], [74]). The elitist critique has also become part and parcel of Slow Food's brand image, and it poses salient authenticity challenges for Slow Food's producers, entrepreneurs, and consumers.In the following sections, we first discuss the key analytic premises of the semiotic square. Next, we develop a semiotic conceptualization of authenticity that maps out its structural contradictions (and ambiguous classifications). We use this analytic framework to explicate the ways in which the elitist critique gives a particular cultural form to the authenticity contradictions plaguing the market logic of conscious capitalism. We then profile the authenticating strategies that Slow Food advocates (consumers, producers, and restauranteurs) use to counter these disauthenticating elitist associations. We conclude by discussing the implications of this analysis for theories of authenticity and for managing the authenticity challenges facing conscious capitalist enterprises. The Semiotics of Authenticity The Semiotic Square as an Analytic ToolFrom a semiotic perspective ([40]), the meaning and categorical boundaries of a given concept are defined through relations to what it is not. For example, the cultural meanings of masculinity have been historically established through contrasts to those that have defined femininity and the related nexus of ever-changing ideals, values, and practices through which this binary contrast has been culturally articulated and transformed over time ([50]). These structural relations give rise to ambiguous categories whose associated cultural meanings can become points of contestation and debate, such as in the cases of ""metrosexuals"" ([78]), stay-at-home dads ([19]), or the ongoing controversies sparked by the category of transgender athletes ([12]).The binary opposition between authenticity and inauthenticity presents a similar arrangement of contradictions and ambiguous classifications. Consequently, we propose that authenticity is not a set of discrete properties that distinguish the genuine from the fake—but, rather, an ongoing process of managing a network of contingent relationships. In some markets, for some brands and enterprises, these contingencies may be more stable, whereas in others, they may become more culturally contested and, thus, unstable. We suggest that conscious capitalist brands and businesses, owing to the elitist critique, exemplify this latter and more managerially challenging case.Figure 1 presents a semiotic square representation of authenticity. In this article, we use the ""contradictions of authenticity"" as an integrative term that encompasses the structural relations among the semiotic 3Cs (contrariety, complementarity, and contradictory relations).Graph: Figure 1. A semiotic model of the authenticity–inauthenticity opposition.The horizontal arrows represent contrariety relations. These relations are roughly analogous to the standard binary oppositions that anchor semantic differential scales. However, relations of contrariety further indicate that the meaning of a term is defined through a relationship to its binary contrast (e.g., good is understood relative to evil). Accordingly, the meaning of authenticity is always contingent on the operative meaning of inauthenticity, and vice versa. We refer to the authentic ↔ inauthentic contrariety as the primary contrariety relation because it represents the dominant tension that, in turn, sets the complementary terms for the secondary contrariety relation (i.e., not inauthentic ↔ not authentic).The vertical arrows represent complementarity relations. Such conceptual pairings are compatible and noncontradictory (but are not necessarily synonymous or interchangeable). For example, ""not inauthentic"" is congruent with the dominant term, authentic. However, this classification also harbors other connotations and, thus, ambiguous meanings. For example, imagine a painting created by a famous artist, say Picasso, who at the time was a fledgling beginner, imitating the style of another painter. Because the painting does not evince Picasso's quintessential artistic motifs, its authenticity becomes ambiguous (and debatable)—that is, at what point in his career does a painting by Picasso truly become a ""Picasso""? The term ""not inauthentic"" conveys this type of ambiguity.The diagonal arrows represent contradictory relations. These relations indicate that any entity or action deemed to be authentic (or inauthentic) will harbor some qualities that can be judged as contradicting such an assessment. As an illustration, let us again consider the idea of artistic authenticity. From a conventional standpoint, the authenticity of an artist, even a renowned one, can always be challenged on the grounds that their creations exhibit properties that are derivative of other genres, styles, or artistic predecessors (authentic ↔ not authentic). Conversely, the art world's postmodern movement disavows the idea of artistic originality and, instead, celebrates that all artistic productions are, in some sense, a reworking of something prior. As exemplified by Andy Warhol's replications of iconic cultural images (Coca-Cola bottles, Campbell Soup cans, the face of Marilyn Monroe), postmodern art is also heralded for its capacity to surprise and inspire revelatory aesthetic experiences through its creative (and often ironic) uses of repetition, collage, assemblage, montage, and bricolage (inauthentic ↔ not inauthentic) ([43]).[ 4] A Semiotic Conceptualization of AuthenticityIn Figure 1, the cloud-like drawings represent the specific cultural meanings that give contextual form to the contradictions of authenticity. For purposes of our analysis, the relevant meaning systems are the market logic of conscious capitalism and the elitist critique. This system of semiotic relationships gives rise to four emergent (and ambiguous) classifications, each harboring latent contradictions. In discussing these ambiguous categories, we first illustrate them in more general terms and then address their manifestations in the context of conscious capitalism and the elitist critique. Authentic + not inauthenticThis complementarity relation corresponds to what [39] discuss as indexical authenticity. In this usage, an index refers to a given object or behavior—for example, the actions of a whitewater raft guide, handprints in front of Grauman's Chinese Theater, a painting, or a branded good. Indexes are classified as authentic when they are believed to possess a factual and spatiotemporal connection to some validating condition. For example, consumers will judge the actions of a whitewater raft guide as authentic if they are believed to reflect an inner passion for the outdoors (rather than being a calculated performance done for remunerative purposes; [ 2]). Similarly, consumers will typically deem a branded good to be authentic when they believe its design, production, and quality certification has proceeded under the auspices of those who own or manage the brand of note.As these examples suggest, perceptions of indexical authenticity can be more or less certain. As an example of higher certainty, Prada certifies the genuineness of its handbags by assigning each a unique and traceable serial number that is documented on an authenticity card. On the less certain side, customers have to infer the indexical authenticity of a whitewater raft guide's passion for the outdoors or a retail associate's expressions of friendliness and interpersonal concern. In these cases, consumers' judgements about the authenticity (or inauthenticity) of a marketer's actions (or the actions of other consumers) depend on their inferences about underlying motivations and intent.The elitist critique provides a constellation of culturally shared meanings and rationales that support disconfirming suppositions about the indexical authenticity of conscious capitalist enterprises and their consumer followers. These disauthenticating associations directly correspond to the ambiguous categories emerging from the primary contrariety (authentic ↔ inauthentic), the secondary contrariety (not inauthentic ↔ not authentic), and the complementarity relation of inauthentic ↔ not authentic relations. Authentic + inauthenticThis ambiguous classification corresponds to seemingly oxymoronic constructions such as authentic reproductions—or, in semiotic vernacular, ""iconic authenticity"" ([39]). In this usage, the icon is an object that is a known facsimile of an original referent and that is appreciated for its mimetic properties, as in the case of a comedian doing an uncanny impression of a celebrity. For the category of iconic authenticity, the ensuing goal is to present a compelling sense of verisimilitude through a meticulous recreation of the original referents' characteristics. Iconic authenticity is pursued by, among others, members of the cosplay community ([80]) and consumers who perform in historical recreations, such as Civil War reenactments ([17]). In a different market application, iconic authenticity would also be highly relevant to a budget-conscious consumer who wants to buy a convincing counterfeit version of an expensive designer brand.When situated in the context of the elitist critique, the ""authentic + inauthentic"" category assumes less favorable meanings of moral pretentiousness and hypocrisy. In this disauthenticating cultural frame, affluent (and typically left-leaning) consumers use conscious capitalist brands and goods to distinguish themselves from the price-conscious mainstream and their socioeconomic peers who display affluence through more ostentatious lifestyle choices ([24]). By claiming the mantle of moral virtue, such consumers can pursue social distinction in an otherwise orthodox manner—that is, through material displays of refined tastes ([ 9]; [44])—while appearing to disavow materialism and status consciousness.For example, during its heyday as a cultural icon, the Toyota Prius inspired oppositional brand communities (Muñiz and O'Guinn 2001) who referred to the vehicle (and its drivers) as ""the pious."" This epithet suggested that Prius drivers evinced a self-aggrandizing ""holier than thou"" stance that amplified the moral merits of their automotive preferences relative to those who made different choices ([59]). In a similar cultural vein, [13] note that ecofriendly consumers who purchase organic foods, drive electric luxury cars, and use natural cleaning products typically lead lifestyles that carry a much higher carbon footprint than lower-income consumers who live in smaller housing units, rely on public transport, and seldom fly. Seen in this critical light, such ecoconscious (affluent) consumers are virtue signaling ([24]; [42]; [90]), but their pretense of moral superiority is not warranted by these symbolic acts. Not inauthentic + not authenticThis ambiguous classification highlights that perceptions of genuineness (often taken as the sine qua non of authenticity) are a necessary but not insufficient condition for ascribing this honorific appellation to an object or action. That is, an entity or action may be deemed as genuine (i.e., not fake) but lack the perceived aesthetic or moral virtues needed to be classified as ""authentic,"" or, conversely, to have its authenticity challenged. Thus, we can have marketplace conditions where authenticity, in its full moral and aestheticized sense, is not a relevant cultural category.To illustrate, barring extenuating circumstances, consumers seldom venerate conventional mass-produced goods (e.g., a Big Mac, a Gillette disposable razor) for their authenticity because they lack potent associations with rarefied aesthetic ideals. Conversely, such items are not typically classified as inauthentic either (assuming that they are not knock-off products), with companies often promoting the standardized nature of their branded offerings—and the resulting performance consistency—as value-added benefits.In the context of the elitist critique, the ""not inauthentic + not authentic"" classification suggests that middle-class consumers who support conscious capitalist brands and enterprises are, owing to their class privileges, inherently ""not authentic."" This disauthenticating implication hinges not on conscious intent but on the systemic advantages afforded by their relatively privileged socioeconomic position. Rather than being hypocritical per se (i.e., authentic + inauthentic), the implication is that such consumers may genuinely believe that conscious capitalism is a viable means to create a more equitable and just society. However, their genuine belief is an ideological one, steeped in their internalized class interests. Middle-class consumers' ideological affinity for the market logic of conscious capitalism allows them to lead a materially privileged lifestyle in a guilt-free manner ([92]). By purchasing brands and goods that signify a heightened social consciousness (e.g., fair trade coffee; TOMS shoes; organic, locally sourced foods), they can feel symbolically absolved from culpability in the perpetuation of socioeconomic inequalities. Consequently, their habituated class predilections also create an ideological blind spot toward the exclusionary signals that conscious capitalist ideals and values convey to those who lack the economic and cultural resources needed to fully participate in a middle-class lifestyle ([47]). Inauthentic + not authenticThis ambiguous classification suggests that conscious capitalists' products, services, and brands are, to use[66] term, ""gimmicks"" that always promise more than they can deliver. [21], p. 668) further discuss issues relevant to this classification in their typology of transactions. Among their designations of deceitful transactions (i.e., scams), they list ""fraud"" and ""confidence games."" In the former condition, a disingenuous party misrepresents their intentions to an unsuspecting partner; in the latter condition, the scammer actively enrolls their target in the ruse, such as in catfishing and pyramid schemes.The inauthentic + not authentic classification implies a manipulative opportunism whereby an unethical agent feigns genuineness to extract ill-gotten gains from another. In the context of conscious capitalism, this disauthenticating association is most germane to the marketer side of the exchange. As one well-known example, the business ethics journalist Jon Entine accused the pioneering conscious capitalist brand The Body Shop, and its founder Anita Riddick, of fraudulent misrepresentation. According to [25], Riddick stole the brand concept from a local entrepreneur and fabricated an authenticating origin story about traveling the world searching for natural skin care and hair treatments. His exposé further contended that The Body Shop significantly overstated the percentage of profits that it donated to philanthropic causes. Though Riddick formally denied these charges, the authenticity challenges posed by these accusations, as well as others that subsequently followed, continued to plague the brand ([76]). After years of underperformance, relative to the brand's prescandal pinnacle, The Body Shop undertook a revitalizing strategy that its management characterized as an activist revamp ([77]).The ""inauthentic + not authentic"" classification can also cast more nuanced doubts on the authenticity of conscious capitalist entrepreneurs. Although such conscious capitalist entrepreneurs would not be committing overt acts of fraud (i.e., they are not lying about their business practices per se), the disauthenticating implication is that they are cynically espousing higher-order civic ideals to serve commercial ends, such as charging a premium to their consumers or driving higher stock valuations. This disauthenticating association can arise, for example, when the founder/chief executive of a privately owned conscious capitalist brand sells its rights to a larger corporate entity. Such a backlash arose when Gene Kahn—the founder of Cascadian Farms—sold his business to General Mills. Many leading voices in the organic food community lambasted Kahn's integrity, condemning him as a Boomer sellout and warning that the brand's corporate ownership would not stay true to the higher-order values that originally galvanized the organic food movement (see [75]). Research ProceduresTo investigate how Slow Food advocates negotiate the elitist critique of conscious capitalism and its disauthenticating connotations, we recruited informants from a Slow Food chapter located in a metropolitan area of the Midwestern United States using informational flyers, contacts made at local chapter meetings, and snowballing referrals. We conducted interviews at public locations such as coffee shops or at Slow Food–sponsored events, with exception of two that respectively occurred in these participants' domestic residence and private work office. Interviewees were paid $20 in appreciation for their time. Interviews were audiotaped and ranged from one to four hours in duration, yielding 830 double-spaced pages of verbatim text. All participant names are pseudonyms.Of our 19 interviews, 8 were conducted with chapter organizers, 5 with Slow Food producers and entrepreneurs, and 6 with Slow Food advocates who had volunteered their time to different outreach activities (for our participants' profiles, see Table 1). Most of our Slow Food organizers and consumer advocates are college graduates employed in professional occupations and hail from middle- and upper-middle-class families. Among the entrepreneurs, Dave, Leslie, Maggie, and Tom are also college graduates. This demographic profile matches the membership ranks of Slow Food USA, which skews toward middle-class professionals ([16]).GraphTable 1. Participant Profiles. PseudonymGenderAge (Years)EducationOccupationSlow Food RoleAaronM28Ph.D.Research scientistMiddle-class advocateAlexM28Some collegeArtisan cheese makerEntrepreneurAmandaF31MBASmall business ownerChapter organizerBenM61M.S.Pursuing Ph.D.Middle-class advocateBrendaF44B.A.Former organic farmer, at-home momMiddle-class advocateCarolineF56B.A.Government employeeChapter organizerChrisM47B.ASoftware designerChapter organizerChristinaF44M.S.Nurse practitionerChapter organizerDaveM30B.A.Organic farmerEntrepreneurErinF29M.S.E-commerce food entrepreneurChapter organizerHankM59M.S.Interior designer, former chefMiddle-class advocateHeidiF58B.A.Professional chefMiddle-class advocateJaneF68Associate's degreeHolistic medicine practitionerMiddle-class advocateKevinM42B.S.Software designerChapter organizerLeslieF23B.SOrganic farmerEntrepreneurMaggieF34B.S.Free range farmerEntrepreneurPaulaF49M.S.Retail buyer and jewelry makerChapter organizerRichardM34B.S.Computer systems analystChapter organizerTomM35M.A.Farm-to-table restauranteurEntrepreneur 1 Notes: M = male; F = female; B.A. = bachelor of arts; B.S. = bachelor of science; M.A. = master of arts; MBA = master of business administration; M.S. = master of science; Ph.D. = doctor of philosophy.Following the conventions of in-depth phenomenological interviewing ([85]), our participants largely determined the course of the dialogue. The interviewer relied on follow-up probes to elicit more detailed accounts of the informants' experiences and viewpoints and to ensure that various aspects of food production, distribution, and consumption were covered. Procedurally, our interpretation developed through an iterative process of creating, challenging, and reworking provisional understandings by tacking back and forth between individual transcripts and the broader data set ([84]). We then pivoted to another level of hermeneutic tacking that entailed iterations between these emic narratives and theoretical concepts, which led us to the application of the semiotic square and our resulting focus on the elitist critique and corresponding strategies for countering the authenticity challenges posed by the cultural contradictions manifest in this market system. Contextual Background The Slow Food MovementAs an institutional entity, Slow Food is a transnational organization encompassing 1,500 local chapters plus numerous subsidiary organizations. Beyond its formal institutional boundaries, Slow Food's culinary practices, values, and activist goals organize ideological and economic alliances among a globally diffused network of food writers (such as Mark Bittman and Michael Pollan), consumers, producers, merchants, and restauranteurs (including celebrity chefs Alice Waters and Jamie Oliver). As [18], p. 131) writes, ""The phrase 'slow food' strikes a chord among the public not because it is the name of an organization but because it reflects a series of desires, interests and concerns.""Slow Food discourses valorize meals that are traditionally prepared with fresh ingredients as unique sources of pleasurable experiences that can mobilize consumers to resist the industrialized system of food production. Over the years, the Slow Food movement has embraced a broader conscious capitalist agenda that advocates for sustainable production, environmental protection, and social justice (i.e., fighting hunger, advocating for living wages for agricultural workers; see [16]; [89]). The Elitist Critique of the Slow Food MovementLike other conscious capitalist exemplars, Slow Food has also been plagued by charges of elitism from its inception in 1986 when its founder, Carlo Petrini, organized a series of public protests over the opening of a McDonald's in the heart of Rome (see [89]). This ignominious view of Slow Food finds ready expression in both academic analyses (e.g., Guthman 2007; [56]) as well as journalistic accounts, such as [38], who states that ""none of the aggressive, judgmental pitches of the movement have ever been proven. The power of its association with the economic elite has.""From this skeptical standpoint, Slow Food's exalted rhetoric of sustainable diets, biodiversity, and socially conscious eating (see https://www.slowfood.com/about-us/our-philosophy/) is a guise for privileging upper-middle-class tastes over the dietary practices of less affluent (and lower-cultural-capital) consumer segments (see [53]; [56]). Even Slow Food's ardent proponents, such as food writer Annie Levy, concede that a tacit elitism has hindered the cultural diffusion of its core principles:—The revered Alice Waters once said, ""when we eat food that is fast, cheap, and easy, we digest those very values."" What are the judgments contained in this kind of statement? She intends, I believe, to critique the values of a food system that doesn't care about its conditions or effects on people and the environment. But the words suggest that if you eat fast, cheap, and easy you become fast, cheap, and easy—language many women might recognize as shaming. Isn't this how it really sounds to someone who enjoys such food, or is caught in situations in which it might seem the best available option? ([58])Slow Food encourages consumers to shift their culinary tastes away from fast food and industrialized fare (including the oft-demonized category of junk food) ([16]; [83]). Such admonitions can imply a moralistic hectoring and an invidious comparison with those whose food tastes and practices are more orthodox. These elitist associations often cross into other sociocultural spheres, such as the controversy sparked by former First Lady Michelle Obama's school lunch initiative, which was institutionalized through the Healthy, Hunger-Free Kids Act of 2010. By explicitly disavowing fast food and processed foods, the revised school lunch guidelines dovetailed with Slow Food's mobilizing agenda—an alliance that Slow Food USA was eager to promote (see Figure 2).Graph: Figure 2. Fodder for the elitist critique: Slow Food's controversial political alliances.Once these Slow Food–friendly standards went into effect, news (and social) media began to feature anecdotal reports of children refusing to eat these presumably unpalatable lunches and skyrocketing food waste ([28]), with some critics characterizing the program as ""gastro-fascism"" ([71]). The elitist charge became integral to this cultural (and political) backlash:Michelle Obama thinks she knows what your children should eat. She's adamant about promoting her nutrition policies for kids, even the new and disastrous school meal standards implementing the ""Healthy, Hunger-Free Kids Act.""... But attending Ivy-League schools doesn't magically make someone better parent material than an individual who attended a public university, or, dare it be said, someone who didn't attend college. ([ 4])Like other market exemplars of conscious capitalism, Slow Food's aesthetic and experiential arguments have become strongly associated with an unwarranted moralism (i.e., the authentic + inauthentic classification; primary relation of contrariety). Slow Food advocates frequently argue that fast food is a debased cuisine that deprives humanity of meaningful and rewarding experiences of eating and sociability ([73]; [83]). For the many consumers who have warm memories of enjoyable fast-food meals with friends and family (and maybe look forward to such treats), Slow Food's moralizing pronouncements seem to emanate from an elitist taste bubble that is disconnected from everyday pleasures and real-world practicality. Similarly, Slow Food's veneration of locally sourced ingredients, heirloom vegetables and grains, artisan-crafted foods, and seasonal cuisine also seems to assert an unwarranted claim to moral virtue. Rather than sacrificing for a greater societal good, such rarefied culinary objects seem more attuned to signaling that one possesses the discretionary resources of time and money to treat food and cooking as a self-actualizing identity practice. Accordingly, Slow Food is easily, via the elitist critique, decried as an aggrandized form of cultural snobbery (e.g., [53]). Authenticating Strategies in the Slow Food MarketFigure 3 represents the correspondences between Slow Food's contextualized authenticity contradictions, the disauthenticating association that ensues from each contradiction, and the strategies through which Slow Food advocates seek to negate these authenticity challenges. In this representation, indexical authenticity (authentic ↔ not inauthentic) is the contested ideal that our Slow Food advocates are seeking to defend.Graph: Figure 3. Authenticity contradictions and authenticating strategies in the Slow Food market.Our Slow Food consumers place the most emphasis on the reflexive strategy, which they use to counter the authentic + inauthentic contradiction (primary relation of contrariety) and its disauthenticating association of virtue signaling and moral pretentiousness. Rather than rejecting the elitist critique outright, Slow Food advocates interpret it as a warning sign that the Slow Food market has become a gentrified facsimile of the movement's origins in the everyday cuisines of rural Italians (i.e., a disparaging version of iconic authenticity). Accordingly, our participants revere practices that seem to resurrect Slow Food's agrarian values and democratizing goals.The humanistic rebel strategy redresses the not inauthentic + not authentic contradiction (secondary relation of contrariety) and its disauthenticating association of social exclusion. In the context of the elitist critique, this contradiction holds that individuals whose lives have been shaped by class privilege may be blithely unaware of their own internalized elitist predispositions. From this standpoint, Slow Food advocates may have a genuine interest in making the world a better place (i.e., they are not consciously ""faking it""; rather, they are being ""not inauthentic""). However, they are largely oblivious to how their viewpoint on these problems and solutions has been shaped by a life of class privilege and their habituated, middle-class (bourgeoisie) sensibilities. This disauthenticating association renders Slow Food consumers as being somewhat akin to the proverbial fish in water. Rather than not realizing they are wet, however, the analogical implication is that they cannot comprehend that other terrestrial animals lack the requisite resources to enjoy life in the water, as they do.To negate this authenticity challenge, our participants drew from humanistic rationales, such as the idea that certain kinds of experiences and social connections have magical and transformative qualities that transcend social differences ([ 2]). Importantly, this strategy combines a humanistic ethos with the idea of rebelling against a deleterious marketing and cultural status quo and, thereby, creates a distinction to the complicit, part-of-the-problem connotations of liberal elitism (see [92]).The perfective strategy corresponds to the inauthentic + not authentic contradiction (relation of complementarity). This strategy is most relevant to those positioned on the entrepreneurial/production side of this market system. It aims to negate the disauthenticating association of commercialism (i.e., conscious capitalist enterprises are profit-seeking marketing ploys). In response, our Slow Food producers and entrepreneurs draw from the bohemian ideal of the artist who refuses to compromise their artistic vision, despite market incentives to ""sell out"" (i.e., betraying one's artistic integrity in return for financial reward) ([10]; [87]). Accordingly, they present themselves as being intrinsically committed to perfecting their Slow Food craft and pursuing conscious capitalist values and ideals, rather than doing it for the money. The Reflexive StrategySlow Food advocates use the reflexive strategy to negate the authenticity challenge of moral pretentiousness. The implication is that Slow Food assigns an unwarranted degree of moral virtue to those who have the economic wherewithal to buy rarefied ingredients, spend time on complex meal preparations, and dine at expensive farm-to-table restaurants while casting those who lack such resources as less virtuous consumers. In response, our participants interpret Slow Food's cultural associations with affluent foodies and elite taste practices as a regrettable, but correctible, market distortion of the movement's authentic values and practices.While acknowledging that market upscaling has imbued Slow Food with an elitist aura, our participants reiterate that expensive, epicurean cuisine need not be and, indeed should not be, regarded as the quintessential expressions of Slow Food:I think one of the things is this perception that if you shop at farmers' markets or at the co-op, it's a lot more expensive. And there is a little bit of this Slow Food bent into cooking elaborate meals, and I think some people perceive that as being elitist because it's sort of this educated way of thinking about food. I don't think of it as being elitist because a lot of times, recipes can be super expensive to buy all the ingredients for, but they don't have to be. I don't think that enjoying your food should be something that is thought of as elitist.... Like, I buy what's not super expensive at the co-op and I cook pretty simply.... What I really like about Slow Food in particular is the aspect of enjoyment and that good food is for all—what good, fair, clean food means for the farm worker to the people who are consuming the food. (Erin)Erin does not summarily dismiss the elitist charge. Rather, she takes a more ambivalent stance by first conceding that Slow Food values and ideals are often enacted in ways that can be read as elitist, such as cooking elaborate meals using expensive ingredients. In her authenticating interpretation, she counters that Slow Food values are better expressed through fundamentally simple meals that do not require extensive preparation time or costly ingredients. Her emphasis on there being many affordable options at her food co-op (which is, of course, a relative judgment) and on ""cooking simply"" (another relative assessment) convey that she is staying true to Slow Food's core principles rather than trying to place superficial, foodie predilections on a higher moral plane.Erin further counters this aspect of the elitist critique by incorporating the economic interests of farmers into her inclusive interpretation of Slow Food stakeholders. This interpretation creates a rhetorical contrast between Slow Food's foundational discourse of economic populism (emphasizing fair wages for agricultural workers) ([73]) and the elitist condemnation that higher prices are merely a means for affluent consumers to mark status distinctions.Paula's narrative exhibits a similar authenticating logic to that expressed by Erin:Slow Food has often come under fire for being elitist. I don't actually think that's true.... The beginnings of Slow Food were about people eating good food, and those were not necessarily rich people. We are talking about people who might have had very little money.... When most people think about amazing Italian cuisine, they were eating very basic foods. So, the whole idea of eating good food to me doesn't seem elitist at all.... Slow Food in the United States, yes, we do certain things that might be seen as elitist—the restaurant dinners and stuff like that. But again, you are still educating people. You are still getting more people involved. And the more people who know about local farming, sustainable farming, eating seasonally, making sure that farm workers are protected and paid properly, that spreads out. And we do projects with a variety of different populations, and we are trying to do more of that.... Slow Food does a lot of work in all its chapters to help with urban gardens or school gardens.... In the long run, our goal is that all people have access to this kind of food.... We are working toward passing that power on to more people. So, I don't think wanting children and families in need to have high-quality food is elitist. (Paula)In this vignette, Paula first differentiates Slow Food practices from elitist pretentions by invoking its historical connections to rustic Italian foodways. She asserts that Slow Food enjoins a pleasurable, resourceful, and fundamental relationship to food that should be accessible to people from all walks of life, rather than being an exclusive province of affluent consumers. However, Paula also recognizes that her inclusive rendering of Slow Food is contradicted by the realities of socioeconomic stratification. From Paula's viewpoint, Slow Food's community outreach efforts can play a pivotal role in democratizing these forms of culinary cultural capital so that consumers from less privileged backgrounds can acquire the skills and knowledge needed to incorporate Slow Food practices and ideals into their culinary routines.When utilizing this reflexive strategy, Slow Food advocates routinely assert that cultural capital ([ 9]), rather than a lack of economic resources per se, is the primary barrier that keeps consumers from integrating Slow Food ideals and practices into their everyday lives. Christina echoes this rationale when discussing how low-income consumers could enact Slow Food practices if they had more knowledge about utilizing the fresh produce and bulk goods that often go to waste in the local food pantry where she volunteers:Some people who are in Slow Food are foodies. However, it does not cost a lot of money to eat right. There are food pantries who throw away produce because people who come to the pantry don't know what to do with it and they don't take it.... Fresh produce going to waste.... No one wants it because they don't know what to do with it. It's really unfortunate. So, people have more access than they think. There are bulk aisles at grocery stores that you can get food for less money. It is actually a lot less expensive to buy bulk rice or bulk oats or whatever else, than to buy the bagged, boxed stuff that's like creamy preprocessed. I think the real lack of resource is education, not so much money. (Christina)Slow Food advocates often draw an authenticating contrast between foodie-ism—which fetishizes highly aestheticized meals prepared with exotic (and typically expensive) ingredients (see [52])—and the Slow Food ethos of preserving traditional foodways and skills ([75]). This distinction is quite salient to Slow Food chapter leader Kevin. He posits that Slow Food's culinary values and ideals have become misconstrued in their translation to the consumerist, status-conscious culture of the United States. Kevin's goal is to reclaim Slow Food's original ethos from its commercial appropriation by high-end retailers and restaurants:To buy imported cheese, organic wine, and all these kinds of things, I don't think those are meant to be the most obvious expressions of Slow Food values. And I think this is where the cultural translation from Italy to the United States went wrong, is that it got tied up with those folks [i.e., affluent foodies]. In Italy, it's much more about cooking at home. It's much more about preserving grandma's recipes. It's much more about celebrating the seasons and the tradition and preserving home ways of life than it is about eating in restaurants that do everything right. And you know, like anything else, capitalism wants to subsume this revolution.... That's a schism that I am personally trying to address and maybe lead by example. I don't think we should be cooking like a Michelin-starred restaurant at home. I think we should be cooking like our grandparents and great-grandparents. And I think we can learn a lot from traditional cultures and indigenous people—to the extent that there still are any indigenous people—how to eat well, and you know, a lot of those foods have become an affectation in restaurants. They'll have poutine, but it's made with truffles, and confit duck and elaborate things. I have realized that we are all attracted to the comfort foods and the simple foods, like tacos, and they are easy and fun to make. (Kevin)For Kevin, Slow Food should be accessible, basic, fun, and easy—characteristics that diverge from associations with rarity, cosmopolitan sophistication, aesthetic refinement, and technical proficiency that mark elite tastes ([44]). If read in a more critical light, Kevin's narrative reiterates the nostalgic glossing of preindustrial foodways that critics of the Slow Food movement assail for ignoring the harsh realities of scarcity and subsistence endured by those who had to survive on ""traditional diets"" ([56]).While romanticizing images of a bucolic culinary past have considerable appeal to our Slow Food advocates, the idea of rekindling a premodern utopia is not that central to the reflexive strategy's authenticating function. Rather, these homages to a bygone era, when people lived close to the land and prepared food in traditional ways, symbolically link Slow Food practices to agrarian and/or rural lifestyles far removed from elite pretensions:I did an internship through Worldwide Working on Organic Farms.... I went to Italy and I milked sheep and goats for a couple of months. And it was very rural. It was very low-tech. We milked in buckets by hand, sheep and goats, and we kind of went out with big sticks and sheepdogs and herded them.... Slow Food originated in Bra, Italy, and that was only like an hour and a half away from the farm. So, I think that's kind of how Marco [the farmer] was involved in Slow Food. He made cheese that was very well regarded, and he went to cheese festivals and stuff. But I mean the whole day was slow. Like wake up kind of late, drink your espresso, milk leisurely, walk the mount, you know. Dinner took a really long time, but that was kind of okay. And just kind of do the same things over and over again. So, we all cooked together. They also did some kind of agro-tourism. They'd have people from the city come out and we would cook with them the food we either grew or found.... That was fun. (Leslie)Leslie's narrative validates a nexus of Slow Food ideals regarding small-scale production and the slower pace of agrarian life. While this rural setting has some trappings of a staged performance—most notably Marco's side business in agro-tourism—it places Slow Food in a symbolic sphere far removed from the Whole Foods brandscape, expensive farm-to-table restaurants, and other consumption domains invocative of foodie affectations (see [52]). For Leslie, her story of interning on a rustic Italian farm affirms that she has actually lived the conscious capitalist values she endorses through her Slow Food advocacy (and thus is not a hypocritical moralizer). More generally, consumer narratives that link Slow Food practices to traditional modes of food production, down-home family meals, and simpler ways of living—rather than exorbitantly priced gourmet dishes—express a rhetorical parry to the elitist charge of moral pretentiousness. The Humanistic Rebel StrategyOur Slow Food advocates use the humanistic rebel strategy to redress the authenticity challenge posed by the elitist critique's connotation of social exclusion and the related sociological argument that consumers' social class backgrounds structurally predetermine their taste affinities ([44]). From this critical viewpoint, Slow Food advocates may not consciously intend to be elitists, but their preferences for goods that convey meanings of sustainability, locavorism, and artisanship betray a host of class advantages that distinguish them from consumers whose lives are marked by conditions of necessity ([47]). While Slow Food advocates may be well intentioned (i.e., they are being ""not inauthentic""), they are also complicit in a system of institutionalized class-based inequities.To illustrate this tension, let us reassess Leslie's preceding vignette in relation to this association with social exclusion (rather than moral pretentiousness). On the one hand, working for room and board on a small, rural farm clearly diverges from conventional notions of status posturing. However, a sociological counterpoint is that Leslie—as a college-educated young adult engaging in an exploratory experience—is building a reservoir of life stories and cultural capital that can afford career and status advantages later in life (see [91]). Seen in this sociological light, Leslie is enacting her class privilege by having the economic and social latitude to intern on a rustic, Italian farm before transitioning into more conventional middle-class occupational pursuits, such as attending graduate school.As a chapter leader, Kevin has oriented his local chapter's activities toward the goal of making Slow Food a more inclusive organization that does not merely cater to the interests of middle-class consumers. When implementing these outreach projects, however, Kevin recognizes that many Slow Food practices are simply incompatible with the situational demands that lower-income consumers have to negotiate on a daily basis:I have amazing privilege,... like being an American to a middle-aged white guy who has very marketable skills.... But I realize that that is a privilege, and this is the biggest thing for us in Slow Food to grapple with—that a single mother who has three jobs and two kids doesn't have the luxury of deciding, ""I think I would like to work less and spend more time in my garden"".... So, you have to be very careful and sensitive about it. (Kevin)Kevin believes that this class chasm can be bridged if his team of middle-class volunteers presents Slow Food's approach to food provisioning, cooking, and eating in a manner that is ""sensitive"" (i.e., adapted) to the lifestyle constraints faced by less well-resourced consumers. His optimistic viewpoint is based on the authenticating assumption that Slow Food's experiential and social benefits have an inherent appeal to consumers, regardless of their class position, because they tap into fundamental human desires and needs.The humanistic rebel strategy takes this inclusive rationale further by suggesting that a confluence of technological and commercial forces have locked individuals into an accelerating pace of life. Consequently, experiences of social connection, spontaneity, and everyday small pleasures are sacrificed to demands for efficiency, convenience, and the seductive (and ultimately alienating) effects of social media and digital communications.Aaron echoes this humanistic rebel mantra in his commentary on the inherent importance of eating and cooking with others:You have the social component of Slow Food as cooking and eating together and taking time to reflect and to connect and to develop community. That's something that we lose when we have things like drive-through or microwave dinners, which aren't to be destroyed or demonized altogether. I take advantage of these services of society. But for them to be the baseline means that we are losing what ... enriches our social systems a lot more than people eating alone and interacting through screens.... So, I think food, when it's jointly cooked and eaten, serves as a very natural medium for connection and idea generation and creativity. (Aaron)Aaron characterizes Slow Food as a needed corrective to societal transformations in the practices of cooking and eating that have resulted in a loss of sociability, communal bonding, and creative interactions. By interpreting Slow Food as a medium for enjoining meaningful social interactions, Aaron expands its cultural province well beyond the predilections of affluent consumers. Aaron's caveat that he has partaken in the conveniences afforded by fast food and microwave meals is another rhetorical means for distancing his Slow Food preferences from elitist connotations. His narrative signals a conscious disavowal of snobbery by acknowledging that there is a legitimate time and place for ""fast food."" Aaron then specifies the problem as the cultural ubiquity of fast food, which, in turn, he links to dehumanizing consequences.When expressing the humanistic rebel strategy, our participants often couched the communal experiences of preparing or eating food as magical moments that affirm Slow Food's class-transcendent qualities:I ran a cheese-making class, and that was really fun because I've always been interested in cheese-making, just for the fun of it. There were seven or eight people there. And one of the members still is making cheese today. And it was really fun to be able to share that magic with people. That this is how this cheese actually comes into being, and it's totally doable by yourself at home. So, that's really exciting, that he got so inspired. It's fun to see somebody get really interested in something. (Amanda)Amanda emphasizes the magic of cheese-making and the inspiration and rewarding personal experiences of self-sufficiency it can enjoin (i.e., ""totally doable by yourself at home""). Her narrative expresses a cosmological view of nature as a magical, life-transforming force ([ 2]) whose authenticating properties are not inherently tied to class-shaped tastes. In keeping with this formulation, Amanda interprets the sharing of her Slow Food skills as a means to help others experience these inspiring connections to nature, which, in turn, implies a revelatory contrast to the alienated experiences of industrialized fast food.Maggie similarly interprets her self-taught Slow Food skills as a means to help people create a sense of communal togetherness and to experience new sensory pleasures and magical connections to the land:I think of Slow Food as taking your time to respect the ingredients and preparing them from scratch and enjoying food. And that, to me, resonates. And bringing back the social aspect of eating. Like, you take time to prepare this meal, you sit down, you share it with people who care about the same things that you do. And it's also, creating another community of people who value these things whether they're growing, or cooking, or eating; having that kind of common thread, I think is really satisfying. (Maggie)Maggie venerates Slow Food as being inherently conducive to experiences of sociability and community and as a way to recognize the meaningfulness of food (a normative orientation that reads as a general human value, rather than a class-interested practice). However, Maggie is a meat producer who, like other Slow Food entrepreneurs, faces an additional task of negating contradictions that derive from the ""inauthentic + not authentic"" category. The Perfective StrategyThe perfective strategy seeks to negate Slow Food's disauthenticating association with commercialism. This aspect of the elitist critique casts Slow Food producers and entrepreneurs as disingenuous actors who are enrolling consumers into an inauthentic market relationship (akin to a gimmick or a confidence game) to serve their own economic interests. In response, our Slow Food entrepreneurs strive to authenticate their actions by signaling that they would never compromise their Slow Food ideals for the sake of profit, such as by recounting the copious amounts of time and energy they invest into perfecting their Slow Food enterprisesIn this spirit, Tom, a farm-to-table restauranteur, views his business as a way to enact his passionate commitment to producing food in a more meaningful and socially beneficial way:When you go to a fast-food restaurant, you have no idea of who actually made that food and the process of where it came from is not known to you. The taste and flavor are mostly engineered to play off the cheap sensory sensations. So, it's fatty and salty and sweet and so, yeah, on a certain level, it might be gratifying, but it's a cheap way to do it that is less meaningful. Slow Food is like, ""We're going to do things in a way that is process oriented!"" I talk about process a lot.... We [Tom and his restaurant staff] were really structured around learning, and so it was a process where we feel like we've excelled and learned a lot and we'll keep pursuing that.... [With Slow Food,] you have this process where people are eating and making something and understanding where it came from and how it works. Eating is such an important part of our lives, and it can have a really important impact on our community and environment. So, the more you understand about it, hopefully you'll make better decisions. The basic motto of Slow Food is clean, fair, and good food. I can totally get behind those values. (Tom)During his interview, Tom extensively discussed ""the process"" aspects of his restaurant and how he views it less as a business than a means to cultivate and diffuse knowledge about the complex interrelationships among food, cooking, sensory enjoyment, ecology, and societal well-being. His quote also reiterates Slow Food's argument that the experiences of these simple culinary pleasures can mobilize consumers to resist the industrialization of the food system (thus echoing the humanistic rebel strategy). For Slow Food entrepreneurs, however, it serves the additional function of associating their enterprises with civic goals that stand distinct from conventional commercial aims.Returning to Maggie, she raises pasture-fed rabbits for sale to farm-to-table restaurants and consumers. In developing her production techniques, Maggie has constantly experimented with different procedures and equipment designs. Through this long trial-and-error process, Maggie believes she has developed an innovative method that better simulates the lives her rabbits would enjoy outside of captivity:Maggie: Daniel Salatin is the son of Joel Salatin, who is the owner of Polyface Farms, and he is the person who is raising rabbits in this system that he has devised and calls the Hare Pen system. So essentially, you still have your does in cages.... You put them in a glorified cage that you then put on grass.... I've copied their system exactly, and I was very unsatisfied with the results that I got. [Maggie then provides an extensive description of her alternative and labor-intensive system and how she developed it]... I don't know why I kept doing it. But I finally have a system that is really effective.... It just was a lot of observation of the rabbits on pasture, making so many mistakes and then incorporating what I had learned.Interviewer: Did you have any economic incentives?Maggie: No! It has to be a personal belief that there might be a better way to do things.... It's kind of like what makes an artist a good artist. If they all hold the brush the same way and they are using the same colors, but they create vastly different things, and one appeals to you, and one doesn't appeal to you. So, what makes that one piece of art recognized by the vast majority of people as superior?... I have this wonderful platform to invest energy and creativity, and it's nice. And so, I feel in some ways really lucky.Invoking the image of the passionate artist, Maggie distinguishes her efforts to perfect an ecologically appropriate system for raising rabbits from crass commercial and economic interests. Maggie's closing sentiment expresses her authenticating belief that such actions can make things better, rather than being driven by instrumental aims. Through storytelling, and by showing how her system works to customers who visit her farm, Maggie deploys narrative and material resources to negate disauthenticating concerns that her Slow Food affinities are merely an instrumental means to charge higher prices. Her personal investment in learning about rabbits' natural habitats/behaviors and inventing a complex ecosystem for raising them further signals that she is not likely to compromise her Slow Food principles in the interest of commercial expediency. Discussion Theoretical ImplicationsPrior research has treated authenticity as a perceptual value or quality that consumers attribute to a brand ([11]; [55]), person-brand ([87]), product ([57]), or performance ([ 5]; [39]). In contrast, we have reconceptualized authenticity as an ongoing process through which consumers and marketers negotiate a contextualized system of cultural contradictions and ambiguous classifications. We suggest that our semiotic framework can better analyze the authenticity contestations that arise in a given market or sociocultural context than conventional theories that assume authenticity perceptions operate on a continuum or selectively draw from an essential set of defining attributes.The conceptualization of authenticity as a relative point along an authentic-to-inauthentic continuum ([22]; [65]) can depict a zone of ambiguity where the authenticity or inauthenticity of a market actor is perceived as being uncertain and, thus, debatable. However, this conceptualization does not offer a means to specifically analyze the cultural meanings (and the interrelationships among them) that generate these ambiguous perceptions. Accordingly, it offers limited theoretical discrimination and managerial guidance.For example, [65] argue that quality commitment, heritage, and sincerity are the primary perceptual cues of authenticity. They then propose that brands should differentially leverage these cues depending on whether consumers perceive them as having low, moderate, or high levels of authenticity. In their normative framework, brands with low perceived authenticity should emphasize sincerity, brands with moderate perceived authenticity should emphasize quality and heritage, and brands with a high level of perceived authenticity should emphasize all three authenticity cues.Such recommendations presume that brands falling into the lower and middle sectors of this proposed continuum have a shortfall of perceived quality commitment, sincerity, or heritage that is rectifiable through compensatory signaling. However, such contested brands are often plagued by contradictory meanings that undermine their promoted claims to authenticity ([37]; [86]). Furthermore, more complex, disauthenticating narratives, such as the elitist critique, can cast doubt on the very credibility of such authenticating cues when used by a contested brand or actors in a market system.Turning to combinatory definitions, [67] have offered a comprehensive theorization of authenticity (as understood from the consumer's perspective) that warrants comparison to our approach. They identify six subdimensions of authenticity (accuracy, connectedness, integrity, legitimacy, originality, and proficiency) and then trace out the relative impact of those dimensions across different market categories and on consumers' behavioral intentions. Rather than a continuum, Nunes, Ordanini, and Giambastiani argue for a family resemblance explanation in which ""a concept (authenticity, in this case) may be qualified by different subsets of its dimensions across different contexts, and not always by all of them in the same way"" (p. 16).Like a continuum, [67] family resemblance logic is limited to the explanation that authenticity is a multidimensional construct whose subcomponents may be more or less important in a given market or consumption context. In contrast, a semiotic framework shifts attention from correlational premises (e.g., this authenticity subdimension seems more important for hedonic products than utilitarian ones) to the cultural meanings, and underlying structural contradictions, relevant to a particular judgment regarding the authenticity of a given product, brand, or market action. For example, the authentic ↔ inauthentic tension elevates the importance of authenticity's moral dimensions in ways that traverse product category distinctions, such as hedonic or utilitarian.To illustrate, a hamburger would typically be classified as a hedonic good. [67], pp. 3–4) find that judgments of ""legitimacy""—which they define as ""the extent to which a product or service adheres to shared norms, standards, rules, or traditions present in the market ... appear to matter for utilitarian but not hedonic products."" However, if we examine this consumer choice in the context of the Slow Food market, then legitimacy becomes a far more important issue. From this standpoint, an ""authentic burger"" would need to exhibit fidelity to various aesthetic and moral norms—grass-fed beef, local sourcing, traditional preparation techniques, and so on—and, its perceived authenticity would be understood and legitimated through a contrast to fast-food burgers. That authenticating contrast (the fast-food burger vs. a Slow Food burger) could then become subject to the elitist critique, which, in turn, would provide motivation for Slow Food advocates to negate these disauthenticating associations.In summary, we have argued that authenticity is culturally constructed (and contested) in a network of structural relations (rather than being a discrete set of essential properties attributed to a brand, person-brand, market performance, or market relationship). Consumers and marketers alike covet indexical authenticity (i.e., the abstract ideal of authenticity) because it can confer cultural legitimacy ([51]), moral authority ([59]), and identity validation ([ 7]; [86]) all of which, can be converted into micro-celebrity status ([79]) and a branding asset ([31]; [45]). However, this authenticity ideal is structurally linked to contradictory meanings and ambiguous classifications. When consumers' or marketers' authenticity claims are challenged by these cultural contradictions, they have pressing incentives to distinguish their actions and identities from the invoked disauthenticating associations. In the following subsection, we discuss how this authenticating goal can be enacted by negating associations that flow along the contradictory path of deception and promoting those that follow the contradictory path of redemption. Two Managerial Paths to Authenticating a BrandAs [49] have argued, marketing managers often find it difficult to redress brand image problems because they are unable to effectively decipher the cultural meanings contributing to those dilemmas. Our semiotic framework can help redress this managerial shortfall. It offers a tangible means for marketing managers to systematically analyze the cultural contradictions of authenticity that emerge in a given market and then to identify strategies for authenticating their brands in the face of these challenges.As a general heuristic, we propose that marketers can be successful in authenticating their brands and/or other strategic assets when they are able to accomplish two complementary goals. The first is to leverage cultural meanings that negate the disauthenticating associations that flow along the contradictory of deception path (authentic → not authentic; see Figure 1). When consumers follow this perceptual path, they experience a glaring contradiction between a prevailing ideal of authenticity and its market manifestation in a brand or marketing practice (authentic ↔ not authentic), which then leads to an association of inauthenticity via the complementary relation of not authentic → inauthentic. In response, marketers should try to provide consumers with compelling and emotionally resonant meanings and rationales that discount the credibility, relevance, or importance of the disauthenticating associations that have gained cultural currency in their respective market.As one illustration, Patagonia confronted a path of deception authenticity challenge soon after it began campaigning against the Trump administration's executive order to reduce the size of Utah's Bears Ears National Monument by two million acres. On December 4, 2017, Patagonia featured this message on the front page of its website: ""The President Stole Your Land: In an illegal move, the president just reduced the size of Bears Ears and Grand Staircase-Escalante National Monuments. This is the largest elimination of protected land in American history."" This web page then directed consumers to various information sources and encouraged consumers to contact their elected officials and to also take the protest to social media, using the hashtag #MonumentalMistakes (see [ 1]).However, defenders of the administration's policy change were quick to denigrate Patagonia's activism as a deceptive marketing ploy. Interior Secretary Ryan Zinke condemned Patagonia as a dishonest ""special interest"" and proclaimed it was ""shameful and appalling that they would blatantly lie in order to put money in their coffers."" Utah Representative Bob Bishop, then chairman of the House Natural Resources Committee, also evoked the elitist critique in his tweet proclaiming that ""Patagonia is Lying to You... A corporate giant hijacking our public lands debate to sell more products to wealthy elitist urban dwellers from New York to San Francisco"" (quoted in [35]).In terms of our model, the Trump administration's response challenged the authenticity of Patagonia's mobilizing campaign by impugning its motivations, thereby reframing an ostensibly authentic (conscious capitalist) action as a disingenuous public relations stunt designed to extract more profits from elite consumers (authentic → inauthentic), which, in turn, triggers the complementary association to inauthenticity. In response, Patagonia joined as a coplaintiff with five Native American tribes and several nonprofit groups in a lawsuit aiming to halt the policy change ([35]; see also https://www.patagonia.com/stories/hey-hows-that-lawsuit-against-the-president-going/story-72248.html). Patagonia also continued to be a vocal critic of the Trump administration's environmental policies and, in a politically and ideologically related vein, donated all its tax savings from the Trump-backed corporate tax cut to environmental groups while condemning the new corporate tax rates as being irresponsible ([63]). Through these responses, Patagonia signaled a deeper commitment to its conscious capitalist values and gave consumers reasons to doubt or dismiss the disauthenticating associations of greed and deception that were being cast on it. In response to Patagonia's uncompromising stance, Inc. offered the following commentary on its 2018 Company of the Year finalist:For Patagonia and its fans, that purpose is doing whatever they can to try to save the planet. In 2018, Patagonia proved that it will not only preach that mission, it will do so with a much louder voice than most other companies. And—so far, anyway—it's only further burnished the Patagonia brand. ([ 8])The second marketing goal is to create conditions in which consumers interpret a brand or business along the contradictory of redemption path (inauthentic → not inauthentic) (see Figure 1). This redemptive chain of associations begins with the widespread cultural view of marketers as inauthentic and, thus, untrustworthy actors ([45]; [66]; [67]). Accordingly, we can assume that consumers will typically harbor varying degrees of skepticism and suspicion toward the authenticity of marketing and branding claims. Redemptive meanings encourage consumers to believe that a given brand or business is operating in ways that favorably diverge from the marketing status quo (i.e., not inauthentic) which, in turn, leads to the complementary relation of not inauthentic → authentic.Volkswagen's (VW's) ""Hello Light"" advertisement, which launched its new line of electric vehicles (circa 2019), takes viewers on a journey that follows a path of redemption arc (see https://www.youtube.com/watch?v=qEvNL6oEr0U). The ad begins with a silhouetted figure entering a dark and seemingly abandoned production facility, while a news report about VW's emission scandal, or ""Dieselgate,"" blares in the background. In a seemingly counterproductive marketing communications move, the ad explicitly reminds its viewers of all the inauthentic associations (VW as liar, deceiver) that arose from those ""dark"" days. The protagonist is revealed to be a despondent engineer struggling to design a new VW model, against the musical backdrop of Simon and Garfunkel's 1960s anthem, ""The Sound of Silence."" Desperate for inspiration, our engineer scours the company archives and finds his creative muse—an image of the iconic VW Van (aka the ""Love Bus"").Through the choice of song and reference to this totem of the 1960s counterculture, the ad recalls VW's countercultural legacy as an authentic symbol of antimaterialist values and a rebuke to status consciousness and marketing hype ([46]). The ad's message is that VW, despite having lost its way, still possesses a latent essential ""goodness"" that is ""not inauthentic."" As ""The Sound of Silence"" reaches its crescendo, lights go on, puncturing the darkness. We observe the production facility come to life and give metaphorical rebirth to the VW brand in the form of an electric van (which also places ""The Sound of Silence"" on a different, ecofriendly cultural register). Thus, the ad's narrative follows the redemptive path of ""inauthentic"" (VW's Dieselgate) to ""not inauthentic"" (VW's 1960's countercultural heyday) to ""authentic"" (signifying that VW has rekindled its socially conscious roots).Our discussion of the three authenticating strategies used by Slow Food consumers and entrepreneurs has emphasized their function as a defensive means to disavow or negate the disauthenticating associations that flow along the contradictory of deception path. However, these same strategies also promote affirmative meanings and associations that operate along the contradictory of redemption path. For example, the perfective strategy does more than negate the disauthenticating association with commercialism. It also magnifies authenticating differences to fast food or industrialized food production and thereby encourages consumers to interpret Slow Food enterprises in a manner compatible with the contradictory of redemption path (even though they may be aware of some disauthenticating associations). This redemptive associative chain takes the following form: Slow Food entrepreneurs are suspected to be ""inauthentic"" due to their commercial motivations → Slow Food entrepreneurs are seen as being ""not inauthentic"" because their deep commitment to artisan ideals and noncommercial values differentiates them from conventional fast-food establishments and industrialized modes of commercial food production → the signification of ""not inauthentic"" supports a broader conclusion that the Slow Food entrepreneur is an authentic actor. Implications for Managers of Conscious Capitalist BrandsGiven their shared ideological affinities, the authenticating strategies used by Slow Food advocates should also have a high degree of applicability to brands espousing conscious capitalist goals and ideals. Of the three authenticating strategies, we find numerous examples of conscious capitalist brands that have enacted some version of the perfective strategy, which aims to negate associations with commercial opportunism and foster interpretations compatible with the contradictory of redemption path. While less commonplace, we can also find branding campaigns that align with the reflexive and humanistic rebel strategies. In the following discussion, we use these various exemplars to illustrate how these authenticating strategies can be implemented by conscious capitalist brands and the authenticity contradictions they potentially redress. Perfective strategyBrands using the perfective strategy engage in unconventional actions that demonstrate a deep commitment to activist causes that supersede profit motives. Over the years, Patagonia has made frequent use of this authenticating strategy to signal that proenvironmental values were central to its corporate mission, even when such acts could mean sacrificing sales, such as its iconic ""Don't Buy"" promotion ([49]) or their ""Give a Damn"" holiday messaging ([72]). REI has also enacted a perfective strategy in its #OptOutside campaign, whereby the retailer closes its stores on Black Friday and encourages consumers to engage in a range of proenvironmental, outdoor activities. Like Patagonia, REI's campaign builds on (and authenticates) the brand's history of supporting environmental causes and promoting a heightened concern for habitat protection and environmental conservation. Last but not least, Clif Bar illustrated a fairly novel implementation of the perfective strategy when its founder and chief executive officer, Gary Erickson, published an ""advertorial"" in the New York Times, offering to donate ten tons of organic ingredients to his main competitor Kind Bars. This advertorial further promised to share his company's knowledge about organic sourcing and production so that the two companies could collectively ""lay the foundation for a healthier, more just and sustainable food system"" ([26]).Owing to their status as commercial enterprises, whose existence depends on profitability, the perfective strategies of conscious capitalist brands can always be reframed as yet another kind of commercial deception. However, such brands can lessen the cultural viability of such recursive challenges by further signaling that their passionate commitment to the supported causes takes precedence over profit motives. Though addressing a different context, [20] offer evidence that supports this strategic approach. They find that customers attribute the quality of authenticity to third-place establishments (e.g., cafes, coffee shops, restaurants) when they believe the respective proprietors are aiming to create meaningful social connections rather than merely trying to make a profit. As they write, ""The authenticity perceived in treasured commercial places is based on exchanges that go beyond mere commercial aspects.... Although being business operators, proprietors invite the consumer to engage in activities that are not undertaken purely for profit"" ([20], p. 913).Accordingly, we propose that conscious capitalist brands are more likely to be perceived as authentic when they provide tangible means for consumers to participate in their social change mission but do so in ways that are not dependent on purchases. From this standpoint, The Body Shop's repositioning of its stores as activist hubs ([77])—where consumers can listen to speakers discuss environmental and social justice issues, sign petitions, and join activist organizations—is an enactment of the perfective strategy and a culturally viable means to reestablish the authenticity of its conscious capitalist branding claims. Reflexive strategyThis strategy aims to negate the charge that a market actor is evincing a ""holier than thou"" stance for actions that are either hypocritical (e.g., ""do as I say, not as I do"") or overstate the positive impact of the self-proclaimed act of conscious capitalist rectitude. For conscious capitalist brands, this authenticating logic most readily translates into a reformist agenda. As one prominent example, Chipotle's ""Back to the Start"" campaign (circa 2012) rallied a diverse assemblage of activist groups that shared a commitment to transforming the corporate-controlled system of food production and who saw the fast-food sector as exemplifying its presumed ills (see [48]). The two-minute short film, which ran across multiple media platforms, shows an increasingly disenchanted farmer witnessing the steady industrialization of his enterprise, replete with enclosed animals, the heavy use of antibiotics, and food being transformed into nondescript goo-like substances. Against the backdrop of Willie Nelson's plaintive version of Coldplay's ""The Scientist,"" the farmer triumphantly decides to go ""back to the start"" by raising free-range animals, using traditional farming techniques, and selling his preindustrial goods to Chipotle.Some relevant insights into this campaign and its authenticating effects can be gleaned from a Fast Company interview with Jesse Coulter, co–chief creative officer of Creative Artists Agency Marketing, which worked with Chipotle's management team in developing this campaign:We were tasked to find new ways to tell Chipotle's Food with Integrity story.... The first issue Chipotle wanted to address was industrial farming.... Chipotle shared many stories of family farmers who have turned their farms into factory farms and have subsequently grown to regret it.... It was provocative because it took a stab at Big Agriculture. Chipotle is a bold company, who has the courage to really stand up for what they believe in.... At the end of the film, a title card appears letting people know that they can download the song on iTunes, and the proceeds benefit the Chipotle Cultivate Foundation, which is dedicated to creating a sustainable, healthy, and equitable food future. People responded and the song reached number one on the iTunes Country chart. ([15])When interpreted through the lens of the reflexive strategy, Chipotle's ""bold"" move was to accept and amplify activist groups' criticisms of the fast-food industry's sourcing and production practices, rather than attempting to deny, rationalize, or obscure these problems through conventional images of consumers enjoying their fast-food meals. The campaign thereby draws an ideological distinction between Chipotle (as a reformist enterprise returning to more humane and sustainable agricultural practices) and the broader fast-food industry that is portrayed as having debased the time-honored practice of farming in the name of speed, efficiency, and cost reduction. In this way, Chipotle aligned itself not with the interests of the fast-food industry at large, but with activist groups who are seeking to reform the broader system of industrialized food production. Chipotle further reinforced the credibility of their reflexive strategy by investing additional resources to support the broader cause (i.e., creating a synergy between the reflexive and the perfective strategy). Humanistic rebel strategyThis strategy promotes the brand as a means for reconstituting meaningful social connections and breaking down societal boundaries that artificially separate people. To avoid being just another nostalgic marketing ode, this strategy should take a critical stance toward selected status quo consumption and marketing practices. The intended message is that the conscious capitalist brand is enabling consumers to resist or escape the dehumanizing and/or isolating influences of materialism, status consciousness, and upward-ratcheting lifestyle competitions.IKEA has run numerous campaigns that align with the humanistic rebel strategy. These campaigns embed its conscious capitalist commitments to sustainability and support of social justice issues, such as gender equity and LGBTQ rights, in a home-as-haven brand narrative. In these ads, the IKEA-furnished home represents a therapeutic space where people can, at least temporarily, unplug from the stresses and distractions of the ""networked life"" ([88], p. 17) and experience meaningful human connections and emotional fulfillment.More than just a haven, however, IKEA often portrays the home as an active force that keeps at bay the outside forces that would interfere with the pleasures of slow living. In an ad titled ""Home Is a Haven,"" we see a father and daughter running to their house during a rainstorm. As they approach the front door, the child's teddy bears spring to life as human-sized entities (whose muscular physiques resemble bouncers at a club). The bears rearrange the house into an open play area and protect the dad from intrusive calls and other outside distractions. We watch as father and daughter play dress-up and numerous other games, eventually falling asleep after their fully engaged bonding time (https://www.youtube.com/watch?v=wGgcYNlH02g).IKEA's ""Let's Relax"" commercial presents a pointedly critical take on the performative, competitive affectations of Instagram micro-influencers, for whom everyday social activities are treated as an instrumental means to garner likes and followers. In the ad, we first observe an eighteenth-century family about to begin formal dinner in a very well-appointed dining room. Suddenly, the father halts the proceedings so that an artist can paint a portrait of the meal, which is immediately transported across the town in a horse-drawn carriage so that affirmative thumbs up gestures from the populace can be tabulated. The scene then suddenly shifts to a modern-day kitchen table, where the same father meticulously photographs the family meal, while his wife and children begrudgingly wait for this documenting ritual to end. The dad sheepishly retires his camera, and the family begins their more enjoyable and authentic social interactions, all framed by the closing caption: ""Relax: It's a meal, not a competition"" (https://www.youtube.com/watch?v=2BXRGzjo1%5fQ).Drawing on our semiotic framework, we anticipate that conscious capitalist brands would gain the most authenticating benefit from the humanistic rebel strategy when they present their brands as ideological allies ([48]; [49]) of consumers who are sensitized to the psychological and social costs of careerism, exclusionary status hierarchies, and the calculated practices of social media self-presentations. In this way, the humanistic rebel strategy undercuts the elitist critique by suggesting that a conscious capitalist brand enables consumers to tap into more basic and rewarding emotional and sensory experiences. It further emphasizes that helping people, from all walks of life, feel genuinely connected to each other is an important and accessible way to make the world a better place. ConclusionDrawing from structural semiotics ([40]), we have developed a conceptual framework that can be used to analyze the cultural contradictions of authenticity, as they emerge in a given market context, and then to identify strategies for combatting their disauthenticating associations. Our analytic approach recognizes that perceptions of authenticity are constructed and contested in a dynamic cultural system. When negotiating such dynamism, marketing managers need to identify strategically significant patterns in the flux of cultural change and to adroitly react to cultural flash points, competitive shifts, and other exogenous shocks that could undermine the credibility of their existing authenticity claims. Whether undertaken in the context of conscious capitalist brands, status-marketing luxury goods, price-driven big-box retailers, or sharing-economy enterprises such as Uber or Airbnb, marketing managers can use our semiotic approach to more effectively negotiate the sociocultural complexity inherent to the process of authenticating their strategic assets.  "
2,"Befriending the Enemy: The Effects of Observing Brand-to-Brand Praise on Consumer Evaluations and Choices Consumers have grown increasingly skeptical of brands, leaving managers in a dire search for novel ways to connect. The authors suggest that focusing on one's relationships with competitors is a valuable, albeit unexpected, way for brands to do so. More specifically, the present research demonstrates that praising one's competitor—via ""brand-to-brand praise""—often heightens preference for the praiser more so than other common forms of communication, such as self-promotion or benevolent information. This is because brand-to-brand praise increases perceptions of brand warmth, which leads to enhanced brand evaluations and choice. The authors support this theory with seven studies conducted in the lab, online, and in the field that feature multiple managerially relevant outcomes, including brand attitudes, social media and advertising engagement, brand choice, and purchase behavior, in a variety of product and service contexts. The authors also identify key boundary conditions and rule out alternative explanations, further elucidating the underlying mechanism and important implementation insights. This work contributes to the understanding of brand perception and warmth, providing a novel way for brands to connect to consumers by connecting with each other.Keywords: praise; brand relationships; brand communication; brand evaluations; warmth and competenceIn 2017, a popular video gaming brand, Xbox, openly congratulated its competitor, Nintendo, on the launch of its new Switch gaming system ([53]). A few months later, The New York Times encouraged readers to read other news sources such as The Wall Street Journal ([20]). And, in responding to a playful challenge from Kit Kat, Oreo disarmed the brand by communicating how truly irresistible Kit Kat is ([62]). Conventional wisdom fervently advises that ""even mentioning your competition is a bad idea"" ([51]), so why have these brands not only mentioned their competitors but praised them?In a world where brands are trying hard to connect with consumers, many of whom have grown increasingly skeptical of marketers' intentions ([21]), it may be that praising the competition provides unexpected benefits. Our research explores the consequences of brand-to-brand praise—when a brand communicates positively and publicly about another brand. We argue that consumers who observe a brand praising a competitor will believe that the brand has positive intentions toward others, also known as brand warmth, which heightens consumer evaluations and interest in the brand giving the praise.As an introductory illustration of this idea, we scraped data from the Twitter pages of Nintendo and its fiercest competitors, Xbox and PlayStation, around the time of the Nintendo Switch launch in 2017. We found a greater number of likes and retweets (in fact, over ten times as many), as well as more positive sentiment among consumers' comments, when Xbox and PlayStation praised Nintendo for the launch compared with all other types of messages (Web Appendix A). Such preliminary field data motivate our exploration into whether, when, and why brand-to-brand praise affects consumer reactions to brands.Across seven studies, we examine the effects of brand-to-brand praise on consumer attitudes and behavior versus more common forms of brand messaging (e.g., self-promotion or providing helpful information to consumers) and identify important boundaries. In doing so, this research offers several contributions.First, we contribute to the brand perception literature by demonstrating how consumers' perceptions of brands are affected by a brand's interactions with other brands. Prior work has focused on how brand-to-consumer interactions affect consumer perceptions (e.g., [42]) but has not yet explored how observing brand-to-brand interactions do so. Furthermore, we contribute to research on the fundamental dimensions by which people judge other people and brands: warmth and competence (e.g., [ 1]). We identify brand-to-brand praise as a novel antecedent that often leads consumers to perceive brands that praise their competitors as warmer. We show that praising a competitor is viewed as a costly action that does not obviously benefit the praiser, thus making it a credible signal. In doing so, we demonstrate the role of costliness in signaling warmth that effectively combats consumer skepticism, a major barrier to warmth identified in prior research ([18]). We also introduce two moderators—organization type and individual differences in skepticism—to identify when costly displays of warmth are most important.Second, we further contribute to the literature on warmth and competence by introducing a context in which brand-to-brand praise increases warmth without damaging perceptions of competence. Prior work suggests that warmth and competence are often negatively correlated, particularly in contexts in which people are considering two or more entities ([32]). Leveraging consumers' lay theories about the characteristics of brands that would be willing to praise their competitors, brand-to-brand praise provides an opportunity for brands to communicate warmth while maintaining perceptions of competence.Third, while prior work has examined brand communication in which the competitive brand is ultimately shown to be inferior, such as in comparative advertising and two-sided messaging campaigns (e.g., [ 4]; [17]; [40]), our research identifies how directing attention away from one's own brand and toward the competition in a purely positive light affects brand evaluations and choice.Finally, we add to the current literature on praise by identifying brand-to-brand communication as a viable and distinct form of praise, noting that observers respond more favorably to praise in a brand-to-brand context than is typically observed in traditional person-to-person contexts. In doing so, we also highlight the understudied effects of praise in competitive relationships. In what follows, we review literature on brand relationships, praise, and brand warmth and present seven studies that test our hypotheses. Building Consumer–Brand RelationshipsA great deal of research has investigated how brands establish and build relationships with consumers, identifying influential factors such as the personalities, motives, and communication styles of both consumers and brands (for a review, see MacInnis, Park, and Priester [2009]). Surprisingly, researchers interested in brand relationships have not yet explored how brands' public communication with other brands affects their relationships with consumers. Of course, researchers have explored how strategic partnerships, such as brand collaborations and alliances, affect brand outcomes (e.g., [38]; [57]). However, these brand interactions occur largely behind the scenes and reflect formal business arrangements as opposed to informal, public communication that makes consumers privy to how a brand treats its competitors. We suggest that consumers will infer important information about a brand based on the way it communicates with other brands. Specifically, we posit that consumers who observe a brand praising its competition will perceive the praiser brand as having positive intentions toward others, known as brand warmth. Subsequently, consumers will develop more positive evaluations of and interest in the praiser brand. Brand-to-Brand Praise and Warmth PerceptionsPrior research has established that people judge others—individuals, groups, cultures, countries—using two fundamental dimensions, often referred to as warmth and competence ([25]; [32]).[ 5] Warmth is the degree to which one has positive intentions toward others and includes perceptions of thoughtfulness, kindness, honesty, and trustworthiness ([ 1]; [25]). Evolutionarily speaking, it allowed our ancestors to quickly distinguish friend from foe and prepare to fight or flee. Warmth judgments are therefore formed more quickly and generally have the greatest impact on attitudes toward individuals ([64]). Conversely, competence reflects the degree to which one is able to enact one's intentions.Given that people relate to brands similarly to how they relate to people in many ways ([26]), warmth and competence are important traits for firms to consider ([33]). Surprisingly, scant literature has explored the drivers of warmth and competence for brands ([ 1]). The research that does exist identifies factors such as a firm's profit focus ([ 2]), social responsibility ([ 8]), racial dynamics ([ 6]), and expression/communication style ([61]; [65]) as affecting consumers' perceptions of warmth and competence. Significantly more work has explored the consequences of warmth and competence. As examples, research has shown that warmth and competence perceptions affect consumer emotions ([ 1]), product evaluations and interest ([10]; [14]; [34]; [36]; [39]; [58]), and word of mouth ([ 7]; [54]). Importantly, while the precise contribution of warmth versus competence to different downstream consequences varies (e.g., [37]), brands generally aspire to be strong on both dimensions and occupy the coveted ""golden quadrant"" ([ 1]).In this research, we suggest that brand-to-brand praise affects consumers' reactions primarily through perceptions of warmth. Prior research has theorized that warmth is established through signals of cooperation (vs. competition) and actions that appear to serve others as opposed to the self (i.e., actions that are ""other-profitable""; [16]; [52]). We suggest that offering praise to a competitor provides a strong illustration of such cooperative, ""other-profitable"" activity. Consumers will therefore perceive a brand that praises competitive brands as warmer (relative to a brand that engages in other types of common brand messaging). This then leads to positive downstream consequences, such as increased purchases. Costliness as an Antidote to SkepticismIn hypothesizing the positive effect of brand-to-brand praise on warmth, it is important to note that high warmth is more difficult to establish and maintain than high competence, as people are often skeptical of others' motives. That is, warm behavior is often discounted, considered to be driven by ulterior motives and easier to fake than competence ([18]; [56]). Such skepticism has also been identified as an issue in research on praise more specifically. For example, in many brand-to-consumer exchanges, such as salesclerk-to-consumer interactions, praise generates suspicion from the consumer; consumers often suspect ulterior motives behind a salesclerk's compliment and perceive the salesclerk to be insincere when the praise occurs before a purchase ([11]; [44]). Further, observers who witness praise happening among others tend to be more skeptical of the praiser and do not react as positively as recipients of the praise ([13]; [29]; [60]). This then leads to the question: When and why might brand-to-brand praise surmount the skepticism associated with praise and other displays of warmth?We suggest that brand-to-brand praise operates uniquely from the aforementioned person-to-person and brand-to-consumer displays of warmth, particularly when the praise is directed toward competitors, due to its costliness. Consumers assume that complimenting a competitor is a costly action that does not directly benefit the complimenting brand. Research across disciplines suggests that the costliness of an act is the key component of whether the act is perceived as a meaningful signal of an underlying trait rather than an uninformative act motivated by devious or ulterior motives, also known as ""cheap talk"" (e.g., [59]; [67]). A common example from the natural world is the male peacock's tail. The costliness of this large and colorful tail, such as how it can handicap the bird's ability to escape from predators, is what makes it a credible signal of fitness to potential mates. Only those who truly possess the focal underlying trait would or could incur such costs. Linking this principle to the present context, only brands that are truly warm would incur the real or potential costs of praising the competition. Consumers should therefore not be as suspicious of brands that praise competitors as compared with brands that engage in less costly messaging. This costliness is why brand-to-brand praise, when directed toward competitors, is a strong signal of warmth, differentiating it from other common types of praise (e.g., salesclerk-to-consumer), surmounting consumer skepticism, and driving its positive influence on consumer attitudes and reactions. Effect on Competence PerceptionsAlthough the focus thus far has been on perceptions of warmth, one might wonder whether brands sacrifice competence when enhancing perceptions of warmth via brand-to-brand praise. This is a reasonable concern, as prior research suggests that in comparative contexts, for people and brands alike, when warmth (competence) is relatively high for a given entity, perceptions of that entity's competence (warmth) suffer (e.g., [ 2]; [28]; [36]; [54]). We suggest that praising a competitor offers brands unique advantages that allow them to maintain perceptions of competence in the face of increasing warmth. In particular, we suggest that consumers hold a lay intuition that a brand that is willing to praise its competitors must be fairly confident in its own abilities, which then allows consumers to maintain, if not increase, positive perceptions of its competence. This is reminiscent of the lay belief—confirmed by academic research—that people who are secure in their identity are the most willing to be kind and compliment others (e.g., [ 9]; [22]; [41]; [63]). Still, while competence may play a role in driving the effects of brand-to-brand praise, we do not expect it to be the primary driver of increased interest in the praiser brand, as perceptions of competence should be most strongly driven by cues of status versus the cues of cooperation that are focal in brand-to-brand praise ([24]). Summary of the Current ResearchIn summary, we argue that brand-to-brand praise often promotes positive brand evaluations and choice of the praiser. Specifically, we predict that consumers will evaluate a brand more favorably and show more interest in the brand when observing brand-to-brand praise compared with observing traditional self-focused messages or even other benevolent messages (H1). We theorize that this is because such praise increases perceptions of the praiser brand's warmth (H2). Moderators/Boundary Conditions CostlinessWe expect that this effect exists when the praise is costly (H3), such as when the brand is praising a competitor, so as to provide a meaningful signal of warmth to the consumer. We reason that the costly signal of warmth indicates to the consumer that the brand truly has positive intentions toward others, even when it is not in the best interest of the brand. As prior work suggests, such warmth leads consumers to be more interested in identifying with, using, and sharing the brand ([ 7]; [37]). For-profit versus nonprofit organizationsAs additional evidence for the role of warmth, we expect the effect of brand-to-brand praise to be stronger for brands that are typically associated with lower levels of warmth than those already endowed with high levels of warmth; brands with high levels of warmth, such as nonprofits, have little to gain from increasing warmth even further (and thus less to gain from brand-to-brand praise). This suggests that for-profit brands (which are lower in warmth than nonprofit brands; [ 2]) will benefit more from brand-to-brand praise (H4). Chronic consumer skepticismFinally, we posit that brand-to-brand praise allows brands to manage consumer skepticism often associated with displays of warmth. We introduce an important moderator to illustrate this point: individual differences in consumers' skepticism toward brand messaging. We predict that the effect of brand-to-brand communication will be strongest among individuals who are highly skeptical of brands. This is because the costliness associated with praising a competitor minimizes the extent to which persuasion knowledge concerns are activated among this group of consumers as compared with more traditional brand communication (H5). We summarize these predictions in the conceptual model in Figure 1 and discuss the boundary predictions further in the study introductions relevant to each.Graph: Figure 1. Conceptual model. Overview of StudiesWe first demonstrate the positive effects of brand-to-brand praise (vs. more traditional messages) across three field and lab studies involving real, consequential behaviors (Studies 1a, 1b, and 2; H1). Next, we show that this effect is specific to praise that is aimed toward a brand's competitor and thus deemed costly (Study 3; H3). We then explore warmth as the key mechanism, demonstrating that it mediates the effect of brand-to-brand praise (Study 4; H2) and that the effect is strongest for for-profit brands that have greater need to enhance perceptions of warmth (Study 5; H4). Finally, we demonstrate that brand-to-brand praise has the largest effect on individuals with high levels of skepticism toward traditional brand communication (Study 6; H5). Throughout these studies, we also test for alternative explanations, including that praise confers benefits due to novelty or authenticity, or that the effect is driven by negative reactions to self-promotion (i.e., bragging) rather than positive reactions to praise. Finally, we also examine the role of competence in several studies (Studies 4, 5, and 6). Evidence suggests that, while perceptions of brand warmth are the primary mechanism underlying this effect, brand-to-brand praise does not harm and can even boost perceptions of competence, which can influence desired outcomes.Together, these studies provide insight into when and why brand-to-brand praise is beneficial for brands. Notably, however, we demonstrate (in Studies 3, 5, and 6) and explain (in the ""General Discussion"" section) when and among whom brand-to-brand praise might not be as beneficial or might even be less beneficial. Study 1: The Effect of Brand-to-Brand PraiseStudies 1a and 1b provide initial causal evidence for our hypothesis by testing the effects of various types of brand messaging on advertisement click-through rate and brand choice in a field and lab study, respectively. We compare the effects of brand-to-brand praise with a traditional self-promotion message from the brand. In addition, in Study 1a, we utilize another type of control message in the form of an endorsement from another organization in the industry to show that praising a competitor enhances evaluations more than receiving an endorsement from others. Further, the endorsement condition serves as a separate point of comparison, ensuring that the hypothesized difference between the brand-to-brand praise condition and the self-promotion condition can be interpreted as a boost from brand-to-brand praise and not merely a negative reaction to self-promotion. Study 1a: Facebook Advertisement Click-Through Rate MethodParticipants and design. This preregistered study utilized a three-cell between-subjects design: Facebook users saw an advertisement on Facebook featuring self-promotion, an external endorsement, or brand-to-brand praise.[ 6]Procedure. We created a fictitious car wash brand called Precision Car Wash and launched three advertisements for the brand on Facebook. The self-promotion message stated, ""Precision Car Wash is proud to receive the Industry Best 2020 Award."" The external endorsement consisted of a message from a fictitious organization, The Industry Best 2020 Award Committee, announcing Precision Car Wash as the year's award recipient. Finally, the brand-to-brand praise ad consisted of a message from Precision Car Wash congratulating another fictitious car wash business, LikeNew Car Wash, on winning the Industry Best 2020 Award (for study stimuli, see Web Appendix B). Facebook users who saw the ad could click on the message to be taken to the Facebook page of Precision Car Wash for more information. We measured the number of impressions and clicks, which allowed us to compare click-through rates (clicks as a percent of impressions; CTRs), for each ad. We ran the ads over the course of five days and used Facebook's recommended advertising algorithm to reach 80% power in testing the ads for a final sample of 13,719 impressions.We also conducted two separate supplemental tests of these stimuli. First, we conducted a manipulation check for the ads, assigning participants to one of the three ad conditions and asking them to indicate the extent to which the brand was praising its competition (1 = ""definitely NOT praising the competition,"" and 7 = ""definitely praising the competition""; N = 105). Second, given variations in the text used to communicate the focal messages (e.g., number of words, fonts), we conducted a posttest to assess the ""graphic design"" (three items: quality, clarity, graphic appeal) of the assigned ad (1 = ""not done very well,"" and 7 = ""done very well""; α = .87; N = 105). These items allowed us to be sure that our focal praise condition was not (unintentionally) more aesthetically appealing and more likely to be clicked for that reason. ResultsManipulation check and aesthetic appeal. The separate manipulation check showed a significant difference across conditions (F( 2, 101) = 21.47, p < .001,  ηp2   = .30). Those who viewed the brand-to-brand praise message perceived it to praise the competition (M = 4.82) more than the self-promotion (M = 1.96; p < .001) and external endorsement (M = 2.43; p < .001) ads. The self-promotion and external endorsement conditions did not differ (p = .30). This manipulation check was conducted for and confirmed the manipulations in each of the remaining studies (see Web Appendix C).In addition, the separate test designed to assess the aesthetic appeal of the different messages indicated that the focal praise ad was not advantaged aesthetically. We find a significant difference across conditions (F( 2, 102) = 5.50, p = .005,  ηp2   = .10) such that those who viewed the brand-to-brand praise ad perceived it to be lower in aesthetic appeal (M = 3.83) than the self-promotion ad (M = 5.03; p = .001) and not statistically different from the external endorsement ad (M = 4.42; p = .11). The self-promotion and external endorsement conditions did not significantly differ (p = .10). However, because the praise ad was rated to be lower in aesthetic appeal, any positive benefits of the praise ad should not be a result of viewers liking the appearance of the praise ad more.CTR. A chi-squared analysis revealed that the CTR differed across conditions (χ2( 2, N = 13,719) = 91.59, p < .001). The percentage of those who clicked on the ad was greater for the brand-to-brand praise condition (5.4% of 4,392 impressions) compared with the self-promotion (3.3% of 4,075 impressions; χ2( 1, N = 8,467) = 21.42, p < .001) and external endorsement (1.8% of 5,252 impressions; χ2( 1, N = 9,644) = 90.45, p < .001) conditions. Study 1b: Lab Brand Choice MethodParticipants and design. One hundred fifty-four members of the local community (general public, students, and staff) were recruited through a business school's behavioral lab. Participants were randomly assigned to one of two conditions: self-promotion or praise.Procedure. First, participants learned that they would give their opinions about snack brands that the business school was considering for its café and vending machines. Next, they were informed that they would be able to choose a sample snack to take home after the study.Participants then saw two advertisements from real local popcorn shops in the Raleigh-Durham, North Carolina, area. The first ad was from the recipient brand, Carolina Popcorn Shoppe, and stated, ""Come check out our FIVE newest flavors! In-store or online."" Everyone saw this ad. The second ad was from the focal brand of the study, The Mad Popper, and differed by condition. The praise condition ad read, ""We love good popcorn. Big shout-out to Carolina Popcorn Shoppe on their FIVE new flavors!"" The self-promotion ad read, ""We love good popcorn. Come explore our FIVE brand new flavors! In-store or online"" (Web Appendix D).Next, participants were reminded that both brands were being considered for use at the business school. They were then asked to choose Carolina Popcorn Shoppe or The Mad Popper as the brand they would prefer to sample, which they received at the conclusion of the study.[ 7] Finally, participants responded to a stimuli believability measure, assessing their willingness to suspend disbelief and assume that the experimental stimuli were real (""How believable was this advertisement by The Mad Popper?""; 1 = ""not at all,"" and 7 = ""very""). We asked this believability question consistently across experiments (except Study 1a, where we could not) because of the low prominence of brand-to-brand praise currently in the market. Although brand-to-brand praise is not yet commonly practiced in the real world and thus may not yet be highly believable (i.e., seem real) for some consumers, we are examining what could happen if consumers witnessed brand-to-brand praise, meaning controlling for variation in whether the stimuli were believed to be real.[ 8] We use believability as a covariate across each subsequent experiment to enhance experimental power ([45]). Details on this measure appear in Web Appendix E, which includes a summary of key results with and without the covariate.[ 9] ResultsBrand choice. We conducted a binary logistic regression with condition (1 = praise, 0 = self-promotion) as the key predictor and believability as a continuous covariate on brand choice (1 = The Mad Popper, 0 = Carolina Popcorn Shoppe). Conceptually replicating the results of Study 1a, those in the praise condition were significantly more likely to choose the focal brand, The Mad Popper, compared with those in the self-promotion condition (β = .91, SE = .43, Wald χ2( 1, N = 154) = 4.47, p = .03). In percentages, 27.63% chose the focal brand in the self-promotion condition, and 34.62% did so in the praise condition.[10] We also find that believability does not act as a moderator (β = −.02, SE = .25, Wald χ2( 1, N = 154) = −.08, p = .93); thus, we use it as a covariate in our remaining studies (for moderation by believability results for the remaining studies, see Web Appendix E). DiscussionStudies 1a and 1b provide initial experimental evidence that brand-to-brand praise can have positive consequences for the praiser on real behavior, including advertisement CTR and brand choice. The comparison of brand-to-brand praise to an external endorsement in Study 1a suggests that the key effect is not because consumers dislike the other control condition—the self-promotion message—but is instead driven by a boost from observing brand-to-brand praise. Furthermore, by presenting participants with a forced choice between two brands in Study 1b, this data begins to suggest that praise benefits the praiser more than the praised. We further explore this in Study 2. Study 2: How Are Real Purchases Affected for the Praising and Praised Brands?In Study 2, we build on the findings of Studies 1a and 1b in assessing consumers' real, behavioral reactions to brand-to-brand praise, but we do so with some important changes. First, we assess a longer-term reaction to brand-to-brand praise by investigating purchase behavior for popular national brands (Kit Kat and Twix) 11 days after people were exposed to the brand messaging. Second, unlike Study 1b (but similar to Study 1a), we only expose participants to the competitor brand in the brand-to-brand praise condition. This enables us to further assess the potential costs of making competitive brands more top-of-mind (via praise) than they might otherwise be. This study also allows us to assess behavioral reactions to both the focal and competitive brands by gauging consumers' purchase behavior toward both, as opposed to forcing a choice between them (Study 1b) or tracking reactions to only the focal brand (Study 1a). Finally, this design offers an opportunity to see the proposed effect of praise on behaviors of greater financial consequence for the consumer and the brand: purchases. Methods Participants and designThis preregistered study had a two-cell between-subjects design (control, praise) and was conducted in two stages on Prolific.[11] First-stage procedureIn the first stage of this study, participants recruited from Prolific (N = 1,502; 49.6% female) viewed an image of Kit Kat's Twitter page. In the control condition, participants read a tweet that said, ""Start your day off with a tasty treat!"" In the praise condition, participants read a tweet that said, ""@twix, Competitor or not, congrats on your 54 years in business! Even we can admit—Twix are delicious"" (Web Appendix F). Unlike Study 1b (but similar to Study 1a), participants did not see a separate introduction to the nonfocal brand, Twix. Thus, Twix would presumably not be as top-of-mind unless they read the praise tweet. Afterward, we measured participants' attitudes toward both Kit Kat and Twix using the attitude measure from [46] (""negative/positive,"" ""dislikeable/likeable,"" and ""unfavorable/favorable"" on seven-point scales; α = .95; for details on this measure, see Web Appendix F). Second-stage procedureFrom the initial sample of 1,502 participants, we excluded those who indicated that they have not bought chocolate candy for themselves within the past six months, those with dietary restrictions that prevent them from buying chocolate candy, and those who indicated that they were not interested in completing a follow-up survey, leaving us with a sample of 1,298 potential participants for the second stage. Approximately 11 days after participants completed the first stage, we sent a follow-up survey to the 1,298 participants. Of these participants, 772 participants completed the second-stage survey. Attrition rates were similar across conditions (control = 40.64%, praise = 40.40%, χ2( 1, N = 1,298) = .008, p = .93).In the second-stage survey, participants indicated whether they had purchased any Kit Kats (yes/no) or Twix (yes/no) since they took the first portion of the survey. Finally, we asked participants in an open-ended question what they could recall about the tweet they saw in the first stage of the study, bringing the tweet to their attention so that we could measure the believability of the tweet using an adapted version of the measure in the prior study. ResultsWe conducted a binary logistic regression with condition (1 = praise, 0 = control) as the key predictor and believability as a continuous covariate on purchase behavior (1 = purchased Kit Kat, 0 = did not purchase Kit Kat). Those in the praise condition were significantly more likely to purchase Kit Kat compared with those in the control condition (β = .34, SE = .16, Wald χ2( 1, N = 772) = 4.17, p = .04). In raw percentages, 23.77% in the control condition purchased Kit Kat versus 31.95% in the praise condition. We conducted the same analysis for purchase behavior for Twix and find that there was no difference between the praise and control conditions (β = −.25, SE = .19, Wald χ2( 1, N = 772) = 1.68, p = .20) DiscussionStudy 2 extends our prior findings, demonstrating the effect of brand-to-brand praise on actual purchase behavior over a longer time horizon and relative to competitors' purchase outcomes. In doing so, we also show that even if the competitor brand becomes more salient than it would have normally been as a result of brand-to-brand praise, such praise still primarily provides positive benefits to the focal brand.Notably, in a supplemental study, we replicate the current findings using a paradigm with Subway and Jimmy John's (Web Appendix G). We find that when Subway brings Jimmy John's into the consideration set through brand-to-brand praise, Subway gains a boost in brand attitude compared with the self-promotion condition; Jimmy John's, however, does not gain a boost from receiving the praise.In the remaining studies, we identify when the beneficial effects of praise are mitigated and the underlying mechanism of the effect. In doing so, we offer additional understanding of the psychological processes driving consumer responses to praise as well as practical insights for choosing the appropriate recipients of and circumstances for praise. Study 3: The Effects of Brand-to-Brand Praise Toward Competitors Versus NoncompetitorsIn Study 3, we expand on the findings of the prior studies by examining the effects of brand-to-brand praise toward both a direct competitor and a noncompetitor. We expect that when a brand compliments a relevant direct competitor, which we define as a brand that competes in the same category as the focal brand, consumers view that compliment as relatively costly or risky because a brand has more to lose when bringing positive attention to such competition. It is such costliness that credibly signals that the brand must truly have warm intentions. We do not expect our effects to hold when a brand compliments an irrelevant noncompetitor, which we define as a brand that does not compete in the same category as the focal brand. In this scenario, the brand incurs lower cost by bringing attention to the irrelevant noncompetitor because there are less obvious repercussions, and thus the compliment is a less credible signal of a brand's warmth. This study also aims to demonstrate that the effects of brand-to-brand praise are not driven solely by the perceived novelty of the message or by negative reactions to self-promotional messages. Method Participants and designThree hundred ninety-nine participants (50.4% female) recruited from Amazon Mechanical Turk took part in this four-cell between-subjects design (helpful control, self-promotion, costly praise, noncostly praise). ProcedureWe created two eyeglasses brands for this study, Franklin's Frames and Lazlo's Lenses, and introduced them to participants as competitors. Participants then viewed an ostensibly recent series of three tweets from the focal brand, Franklin's Frames—one manipulated tweet that varied by condition and two filler tweets. The manipulated tweet in the praise condition read, ""@lazloslenses, Wow! Your new frames are looking good!"" with an image of a pair of glasses. In the self-promotion condition, the manipulated tweet read, ""Wow! Our new frames are looking good!"" with an image of a pair of glasses identical to the praise condition. In the helpful control condition, the manipulated tweet read, ""How to clean your frames:"" with a screen shot from a video showing how to clean eyeglasses. Participants in the noncostly praise condition saw a burger brand that is clearly not a direct competitor to Franklin's Frames, called Ben's Burgers. The focal tweet for those in the noncostly praise condition said, ""@bensburgers, Wow! Your new burgers are looking good!"" with an image of a burger attached (Web Appendix H).After reading the scenario, participants completed the measures of brand attitude (α = .95) noted in Study 2. In addition, we measured the perceived costliness of the tweets with four items on seven-point scales (α = .75; Web Appendix C). Finally, participants answered the same believability question as in prior studies as well as a measure of perceived novelty (""How novel was this tweet by Franklin's Frames?""; 1 = ""not novel,"" and 7 = ""very novel"") of the focal tweets. Results CostlinessA one-way analysis of covariance (ANCOVA) showed significant differences across conditions on perceived costliness of the focal tweet (F( 3, 394) = 37.66, p < .001,  ηp2   = .22).[12] As we expected, contrasts revealed that perceived costliness was significantly greater for the costly praise condition (M = 3.82) than the self-promotion (M = 2.21, p < .001), control (M = 2.37, p < .001), and noncostly praise (M = 2.52, p < .001) conditions. Perceived costliness for the noncostly praise condition was greater than the self-promotion condition (p = .05) but not significantly different from the control condition (p = .34). Lastly, the self-promotion and control conditions did not differ (p = .32). Notably, we measure perceived costliness of the stimuli in all of our studies and find the same pattern of results; praise toward a direct competitor is perceived as more costly than other types of messages (Web Appendix C). Brand attitudeA one-way ANCOVA revealed an effect of condition on brand attitude toward the focal brand, Franklin's Frames (F( 3, 394) = 4.83, p = .003,  ηp2   = .04). Contrasts revealed that brand attitude was significantly greater for the costly praise condition (M = 6.05) than the control (M = 5.43, p < .001) and self-promotion (M = 5.46, p < .001) conditions. Brand attitudes were also significantly greater for costly praise than noncostly praise (M = 5.65, p = .02). Brand attitude for those in the control, self-promotion, and noncostly praise conditions were not significantly different (all ps > .18), suggesting that the results are driven by a boost from the praise message and not a dislike of the self-promotion message. NoveltyUnsurprisingly, a one-way ANCOVA revealed significant differences across conditions on how novel the tweet seemed (F( 3, 394) = 8.80, p < .001,  ηp2   = .06). Contrasts revealed that both the noncostly praise (M = 4.30) and the costly praise (M = 4.56) conditions were perceived to be more novel than the control (M = 3.48) and self-promotion (M = 3.43, all ps ≤ .001) conditions. Crucially, however, there was no difference in perceived novelty between the noncostly praise and the costly praise conditions (p = .31), suggesting that the differences between these conditions on brand attitude were not driven purely by differences in novelty.[13] DiscussionIn Study 3, we replicate the findings of prior studies, demonstrating that brand-to-brand praise between competitors boosts brand evaluations compared with other messages, including self-promotion and a helpful control message. Furthermore, we find that noncostly praise (i.e., praise toward irrelevant noncompetitors) did not give the same boost. This result suggests that simply speaking positively about another brand is not enough; the consumer must view the compliment as costly to the brand for it to have positive effects. Although we find costliness to be necessary to show the benefits of praise, it is not the underlying driver of the effects (mediation comparing the self-promotion and costly praise conditions: ab = .06, 95% confidence interval [CI] = [−.045,.191]; PROCESS Model 4, 5,000 bootstrap samples; [31]). This study also begins to cast doubt on novelty as a primary driver of the effect, given that the noncostly praise was perceived to be as novel (but not evaluated as positively) as the costly praise. We further test the role of novelty in subsequent studies. In addition, we again find that the effects on brand attitude do not occur because consumers dislike self-promotion messages but are instead driven by the boost from seeing the costly praise message, replicating the findings of Studies 1a and 2. Study 4: Testing Warmth as the MechanismTo begin to understand why brand-to-brand praise increases brand evaluations and choice, we conducted a qualitative pretest gauging people's natural reactions (i.e., inferences, attributions) to observing a brand complimenting a competitor. Participants (N = 150) on Prolific saw a tweet from PlayStation congratulating Nintendo on its Nintendo Switch launch and then listed five adjectives to describe PlayStation. We find that observers more frequently attribute warmth-related words (e.g., friendly, supportive, kind; 55.73% of the adjectives), rather than competence-related words (e.g., confident, successful, intelligent; 17.07% of the adjectives), or any other kind of perception, to the brand, implicating warmth as the most top-of-mind inference following brand-to-brand praise. For additional insight, we asked participants to take the perspective of a manager and indicate why they would or would not post a message praising the competition. We find that most participants would be willing to praise a competitor (69.33%). Again, the majority of the responses (61.5%) indicated warmth to be the primary reason for the action (e.g., ""I want to show others that I act in selfless ways""). Moreover, some responses also positively noted competence-related reasons (e.g., ""have confidence in my own brand and its qualities""; ""we appear to be positive and not insecure about our place in the market"") either exclusively or in combination with warmth (25%), suggesting that while warmth-related reasons are more top-of-mind, perceptions of competence are unlikely to be harmed (for pretest details, see Web Appendix I).In light of these initial qualitative insights, we empirically test warmth as the driver of the benefits of brand-to-brand praise and compare it with competence in Study 4. We predict that brand-to-brand praise enhances perceptions of brand warmth, which predominantly drives the boost in brand evaluations, rather than competence. Ultimately, we expect the heightened brand evaluations to drive consumer action, which is measured in this study as their willingness to sacrifice their own time (for free) on behalf of the brand. Methods Participants and designTwo hundred participants (57% female) from Prolific took part in this two-cell between-subjects study (control, praise). ProcedureParticipants were introduced to a real tea brand, Treecup Tea, from Kickstarter. They were told that Treecup Tea focuses on making high-quality tea blends and competes with other tea companies on Kickstarter for funds. Participants read that Treecup Tea's competitors include Teafir, Shisso Tea, and Phat Tea. Next, participants were introduced to Treecup Tea's Twitter page. For the control condition, participants saw only the brand's Twitter header. We use this control condition, different from the self-promotion control in prior studies, to show again that the boost for the brand results from people feeling positive about the praise message instead of negative toward self-promotion. For the praise condition, participants saw a Twitter page that consisted of three praise messages toward the three competitors interspersed throughout other tweets on the page (e.g., ""@ShissoTea Your tea is fresh and sustainable! That's amazing!""; Web Appendix J).Participants then completed the same measure of brand attitude as in prior studies. Participants were also asked how much time (scale from 0 to 4 minutes, in 30-second increments) they were willing to volunteer to answer some market research questions for Treecup Tea after the main study, without additional pay. We chose this dependent variable as an action of consequence to the consumer, given that volunteering time is a costly consumer behavior. Finally, participants completed measures for warmth (""warm, friendly"" on a seven-point scale; r = .81) and competence (""competent, capable"" on a seven-point scale; r = .90; [ 1]). The order of the warmth and competence measures was randomized to ensure that one did not explain the effect more than the other simply due to order. We again measured believability of the stimuli as a covariate in our analyses. Results Brand attitudeReplicating prior results, a one-way ANCOVA revealed a significant effect of message (F( 1, 197) = 17.61, p < .001,  ηp2   = .08), whereby praise (M = 5.86) led to greater brand attitude compared with the control (M = 5.33). Volunteer timeThe data were skewed because 34.1% of the sample indicated that they would not volunteer any time,[14] so we conducted a binary split on volunteer time (1 = those who would volunteer any amount of time, 0 = those who would not volunteer). A binary logistic regression with condition (1 = praise, 0 = self-promotion) and believability as the predictors on willingness to volunteer revealed that those in the praise condition were significantly more likely to volunteer time compared with those in the self-promotion condition (β = .59, SE = .30, Wald χ2( 1, N = 200) = 3.76, p = .05).[15] Warmth and competenceA one-way ANCOVA revealed a significant effect of message on warmth (F( 1, 197) = 22.54, p < .001,  ηp2   = .10): praise (M = 5.92) led to greater perceptions of warmth compared with the control (M = 5.20). A one-way ANCOVA also revealed a smaller but significant effect of message on competence (F( 1, 197) = 5.59, p = .02,  ηp2   = .03): praise (M = 5.49) led to greater perceptions of competence compared with the control (M = 5.17). MediationNext, we tested the effect of praise on willingness to volunteer through warmth and brand attitude as serial mediators. We find that the praise message leads to increased warmth perceptions, which leads to improved brand attitude and, subsequently, greater willingness to volunteer (indirect = .08, 95% CI = [.008,.228]). However, a serial mediation model replacing warmth with competence is not significant (indirect = .05, 95% CI = [−.005,.169]). DiscussionIn Study 4, we replicate prior findings and also demonstrate a novel downstream consequence of praise whereby observers are more likely to give up some of their time, without additional compensation, to help the praiser brand after viewing a praise message. We find warmth to be the most direct driver of the effects of brand-to-brand praise. However, given that competence perceptions also benefit from a praise message, it is worth considering how brand-to-brand praise may usher brands into the ""golden quadrant"" ([32]) of warmth and competence dimensions. Lastly, the stimuli used for the praise message in this study consisted of three compliments to three different competitors on a single Twitter page. The fact that we still see a boost in brand attitude suggests that giving praise repeatedly, at least to some extent, may not harm the brand in this context (an idea we revisit in the ""General Discussion"" section). Study 5: Moderated Mediation by Organization TypeIn Study 5, we again test for warmth as the mechanism underlying the effect of brand-to-brand praise on evaluations, and we do so with process by moderation, directly manipulating (and again also measuring) brand warmth. Compared with brands that are lower in warmth, we predict that brand-to-brand praise will not increase brand evaluations as much for brands that are higher in warmth. We theorize that this is because brands that are already high in perceived warmth will not have as much need or space to grow in that aspect, thus rendering praise less influential. In contrast, brands with lower perceived warmth at baseline will benefit more from the boost given by brand-to-brand praise. Based on prior research demonstrating that nonprofit organizations are high in perceived warmth ([ 2]), we compare the effects of praise on nonprofit and for-profit organizations. Beyond its relation to our theory, comparing the effect of praise in for-profit versus nonprofit organizations is of practical importance, as it will help managers from these clearly identifiable sectors better assess how effective brand-to-brand praise may be for their brands. Lastly, we measure perceptions of the brand's arrogance as an alternative explanation, given that self-promotional messages may be perceived as a form of bragging. We also measure perceptions of authenticity and novelty of the message as other potential explanations for the effect. Method Participants and designSix hundred one participants (62.2% female) from Prolific took part in this 2 (message: control, praise) × 2 (organization type: nonprofit, for-profit) between-subjects experimental study that was preregistered.[16] ProcedureParticipants were introduced to a fictitious internet service organization, Tech Dev. In the nonprofit condition, participants were told that Tech Dev was a nonprofit organization that had a goal of helping the community. In the for-profit condition, participants read that Tech Dev was a for-profit organization with a goal of increasing profits. Participants were also told that Tech Dev competed with another organization, Networks.org or Networks.com, for either donations or sales (depending on its nonprofit or for-profit status, respectively). Next, participants were shown the Twitter page of Tech Dev in which they read either a control message (""Want to know more about the quality of our services? Click here: techdev[.com/.org]/internet"") or a message praising Networks (""Networks[.com/.org], we are impressed by the quality of your services—competitor or not!""; Web Appendix K).Participants then indicated their interest in Tech Dev using two measures (""How likely are you to seek out more information about Tech Dev?"" and ""How willing are you to talk to a customer representative to learn more about Tech Dev?""; r = .79).[17] We also measured warmth, competence, and believability of the stimuli. As in the prior study, the order of warmth and competence was randomized. In addition, we measured the extent to which participants' assigned condition was perceived as arrogant (braggy, conceited, arrogant; α = .94), authentic (authentic, self-aware; r = .65), and novel (single-item measure) as alternative explanations,. Results Brand interestUsing a two-way ANCOVA, we find a main effect of message (F( 1, 596) = 24.51, p < .001,  ηp2   = .04), a main effect of organization type (F( 1, 596) = 35.82, p < .001,  ηp2   = .06), and a significant interaction (F( 1, 596) = 4.79, p = .03,  ηp2   = .008). As predicted, in the for-profit conditions, we replicate prior results in that praise (M = 3.74) led to greater brand attitude than the control did (M = 2.78; p < .001). In the nonprofit conditions, praise (M = 4.25) also led to greater brand evaluations than the control did (M = 3.86; p = .04), but to a diminished extent. WarmthWe find a similar pattern for warmth. A two-way ANCOVA revealed a main effect of message (F( 1, 596) = 98.85, p < .001,  ηp2   = .14), a main effect of organization type (F( 1, 596) = 124.43, p < .001,  ηp2   = .17), and a significant interaction (F( 1, 596) = 8.44, p = .004,  ηp2   = .01). In the for-profit conditions, we again replicate prior results where praise (M = 4.78) led to significantly greater warmth compared with the control (M = 3.37; p < .001). In the nonprofit conditions, we find that praise (M = 5.67) led to weaker, though still significant, effects compared with the control (M = 4.88; p < .001). CompetenceWe also find a similar pattern for competence. A two-way ANCOVA revealed a main effect of message (F( 1, 596) = 26.90, p < .001,  ηp2   = .04), a main effect of organization type (F( 1, 596) = 23.33, p < .001,  ηp2   = .04), and a significant interaction (F( 1, 596) = 4.49, p = .03,  ηp2   = .007). In the for-profit conditions, we find that praise (M = 5.22) led to greater competence compared with the control (M = 4.53; p < .001). In the nonprofit conditions, we find that praise (M = 5.48) led to weaker, though still significant, effects compared with the control (M = 5.18; p = .02). AlternativesWe do not find the same pattern of results for bragging. A two-way ANCOVA revealed only a main effect of organization (F( 1, 596) = 55.77, p < .001,  ηp2   = .09) such that the nonprofit (M = 2.08) was perceived as less arrogant than the for-profit (M = 2.93).For authenticity, we find a similar pattern to that of warmth in which there was a significant interaction (F( 1, 596) = 16.33, p < .001,  ηp2   = .03) where the boost from praise was stronger in the for-profit conditions. We find similar patterns for novelty (interaction F( 1, 596) = 6.58, p = .01,  ηp2   = .01). Moderated mediationWe first tested the predicted model. Specifically, we tested for moderated mediation (PROCESS Model 7, 5000 bootstrap samples, [31]) for the effect of praise on brand attitude through warmth, moderated by organization type, controlling for believability. As expected, we find that organization type significantly moderated the mediation through warmth (index of moderated mediation = .36, 95% CI = [.119,.609]), and the mediation effect is stronger in the for-profit conditions (ab = .82, 95% CI = [.624, 1.039]) compared with the nonprofit conditions (ab = .46, 95% CI = [.299,.632]).Then, to explore the role of alternative explanations, we entered all the potential mediators (warmth, competence, bragging, authenticity, and novelty) in parallel into the moderated mediation model. We find that warmth remains a significant mediator in the model (index of moderated mediation = .23, 95% CI = [.079,.408]), suggesting that none of these alternatives ""swamp"" warmth in explaining this effect. Next, we find that the indices of moderated mediation for competence (index of moderated mediation = .08, 95% CI = [.005,.180]), authenticity (index of moderated mediation = .13, 95% CI = [.022,.271]), and novelty (index of moderated mediation = .12, 95% CI = [.025,.231]) were also significant. However, the index for bragging was not significant (index of moderated mediation = −.02, 95% CI = [−.083,.038]). Although competence, authenticity, and novelty were also significant mediators in the model in addition to warmth, warmth remains significant with the largest index of moderated mediation.Because warmth is most frequently compared with competence, we conducted a final analysis statistically comparing the sizes of their indirect effects in a parallel mediation model. We do this within the for-profit conditions in which the effects were more prominent (and because we are unable to statistically compare the full moderated mediation models). We find that the indirect effect for warmth as a mediator was significantly greater than that of competence (p < .001, 95% CI = [.205,.718]). Thus, though other factors may play a role, our results point to warmth as the primary driver of the effects. DiscussionStudy 5 sheds further light on warmth as the primary mechanism underlying brand-to-brand praise. We demonstrate that brand-to-brand praise increases perceived warmth, which enhances brand interest, but this effect is attenuated when the brand is already high in perceived warmth (e.g., nonprofits), as there is less room and need for growth in warmth. Thus, the benefits of praise can be most clearly seen among brands that are perceived as less warm (e.g., for-profits) at baseline. In addition, while perceptions of competence, authenticity, and novelty may play a role in driving brand interest, we show warmth to be the more consistent, primary driver of the effect. Finally, we rule out bragging as an alternative explanation. Study 6: Moderation by SkepticismIn Study 6, we look to another context in which the effects of brand-to-brand praise may be attenuated. Here, we examine skepticism toward advertising as an individual difference that may moderate the effects of praise. Advertising skepticism is an individual difference that has been defined as consumers' chronic doubt or mistrust in a marketer's message ([47]). Individuals who are more skeptical are generally less persuaded by marketing messages and are less trusting of brands. As brands are confronting an ""age of cynicism"" where skepticism is at an all-time high ([21]; [49]) and 71% of consumers report having little faith in brands ([30]), it is very important to understand its effects.As seen in the previous study, the effects of brand-to-brand praise become more prominent when there is room for increasing perceptions of brand warmth. Consumers who are more skeptical of advertising distrust that brands have positive intentions, or warmth, and thus inherently provide brands with greater room to improve in warmth than their nonskeptical counterparts who are already trusting. Thus, brand-to-brand communication may be most effective among skeptics, as it can bypass their cynicism and boost perceptions of warmth. Moreover, such skeptics are generally more persuaded by nonadvertising sources of information and emotional appeals ([48]), which allow marketers to circumvent consumer resistance by decreasing the activation of persuasion knowledge ([27]). Because brand-to-brand praise does not explicitly aim to promote one's brand and instead relies on the more emotional signal that the brand is high in warmth, it is less likely to activate persuasion knowledge and more likely to be accepted. Thus, we predict that consumers high in skepticism will be the most affected by brand-to-brand praise. Study 6 tests this idea utilizing two well-known competitors, Lyft and Uber. Method Participants and designSix hundred participants were recruited from Prolific for this 2 (message: self-promotion, praise) × measured (advertising skepticism) preregistered study.[18] ProcedureParticipants first completed the skepticism toward advertising scale ([47]; 1 = ""strongly disagree,"" and 5 = ""strongly agree"") and then completed filler items. Next, participants saw Lyft's Twitter page, where they read either a self-promotion tweet (""Congratulations to us on all our achievements this past year!"") or a praise tweet toward Uber (""@Uber Congratulations on all your achievements this past year!""; Web Appendix L). Then, participants completed the same measure of brand attitude, warmth, and competence as in previous studies. We also measured perceptions of bragging, authenticity, and novelty using the same measures as in Study 5 as potential alternative explanations, as well as believability of the stimuli as a covariate. Results Brand attitudeReplicating prior results, an ANCOVA controlling for believability revealed a main effect of message on brand attitude (F( 1, 597) = 22.12, p < .001,  ηp2   = .04), where praise (M = 5.22) outperformed self-promotion (M = 4.74). Next, we find a significant interaction between message and skepticism (skepticism scale reverse coded; β = .21, SE = .10, p = .03; Figure 2) on brand attitude, such that when skepticism toward advertising was higher, the praise message led to a greater increase in brand attitude (Johnson–Neyman point = 1.48, 75.67% of participants; β = .23, SE = .12, p = .05). The effect of message was attenuated at lower levels of skepticism below the Johnson–Neyman point.Graph: Figure 2. Study 6: Moderation by skepticism. MediationWe find that praise significantly boosted perceptions of warmth (Mpraise = 5.36 vs. Mself = 4.99; F( 1, 597) = 13.76, p < .001,  ηp2   = .02), competence (Mpraise = 5.52 vs. Mself = 5.32; F( 1, 597) = 4.88, p = .03,  ηp2   = .008), authenticity (Mpraise = 4.90 vs. Mself = 4.56; F( 1, 597) = 9.46, p = .002,  ηp2   = .02), and novelty (Mpraise = 4.58 vs. Mself = 4.28; F( 1, 597) = 6.59, p = .01,  ηp2   = .01) compared with self-promotion, controlling for believability. Self-promotion was seen as more arrogant than praise (Mpraise = 3.18 vs. Mself = 4.16; F( 1, 597) = 54.23, p < .001,  ηp2   = .08).We then tested for the predicted moderated mediation (PROCESS Model 7, 5000 bootstrap samples, [31]) for the effect of praise on brand attitude through warmth, moderated by skepticism, controlling for believability. While the index of moderated mediation was not significant (index = .10, 95% CI = [−.046,.247]), we find that the indirect effect patterns were in line with our predictions, such that the indirect effect is stronger and significant for those higher in skepticism (+1 SD = 3.17, ab = .31, 95% CI = [.084,.550]) and nonsignificant for those lower in skepticism (−1 SD = 1.30, ab = .12, 95% CI = [−.026,.274]).Next, we test for mediation with all of the potential alternative explanations in parallel (PROCESS Model 4, 5,000 bootstrap samples, [31]). Warmth (ab = .14, 95% CI = [.064,.230]), competence (ab = .04, 95% CI = [.004,.091]), bragging (ab = .09, 95% CI = [.049,.146]), authenticity (ab = .06, 95% CI = [.021,.116]), and novelty (ab = .03, 95% CI = [.004,.054]) mediate the effect of message on brand attitude. However, we again find that the indirect effect for warmth was the largest, pointing to its primary role in causing this effect.Finally, we statistically compared the indirect effects of warmth and competence in the model and find the indirect effect of warmth to be significantly greater than that of competence (p = .001, 95% CI = [.085,.349]), again suggesting that warmth plays the primary role in driving the effect of brand-to-brand praise on brand attitudes. DiscussionIn Study 6 (and in a behavioral replication in Web Appendix M),[19] we show that brand-to-brand praise can actually operate more effectively for people who are generally more skeptical of advertising, as brand-to-brand praise bypasses their suspicions and creates more favorable consequences for the brand. Further, while we find that competence, bragging, authenticity, and novelty play a role in driving the effects of brand-to-brand praise on brand evaluations, we still identify warmth as the primary, more consistent underlying driver. General DiscussionWe investigate how observing brand-to-brand praise affects consumers' brand evaluations and choices. Across a variety of different modes of communication (social media, print advertising, digital advertising), study methods (web scraping; field, lab, and online studies), contexts (consumer products and services), outcomes (brand attitudes, social media and advertising engagement, brand choice, purchase behavior), and praise content, we show that consumers who witness brand-to-brand praise between competitors form more favorable evaluations of the praiser brand than consumers who witness other forms of communication, including typical self-promotion messages, helpful messages, basic brand information, and even outside-industry praise (for a summary of contexts, conditions, and findings across studies, see Web Appendix N). In addition to showing robustness to a variety of praise messages in the presented studies, a preregistered supplemental study demonstrates that the effect is further robust to both general and specific praise (Web Appendix O). Furthermore, the varied study stimuli suggest that this effect is robust to brands in a wide range of industries—including car care, snacks, candy, eyewear, beverages, technology, and transportation—with competitive relationships varying in intensity. In other words, brand-to-brand praise seems to benefit the praiser in less competitively intense relationships (e.g., mom-and-pop popcorn brands) as well as more competitively intense relationships (e.g., PlayStation and Nintendo, Uber and Lyft) in a variety of industries. We trace this effect primarily to the notion that brand-to-brand praise signals a brand's warmth, which leads to improved brand evaluations and affects consumer choices.In addition, we show that this effect only exists when the praise is deemed to be associated with significant cost or risk (Study 3). Importantly, we find that the effects of brand-to-brand praise are diminished in some situations or among some consumers, such as when the brand is already high in warmth (Study 5) or among consumers who are already trusting of brand intent (Study 6). These boundaries provide further evidence for the crucial role that perceptions of warmth play in driving the benefits of praise. While other mechanisms may also play a role, as consumer behaviors are generally multiply determined ([35]; [55]), we find warmth to be the most consistent, primary driver of the effects of brand-to-brand praise. Theoretical ContributionsOur research makes several theoretical contributions to literature streams on brand perceptions and relationships, brand communication, and praise. First, we contribute to the brand perception literature by showing that consumers' perceptions of brands can be affected by viewing a brand's interactions with other brands. We demonstrate that observing brand-to-brand praise can positively affect perceptions of a brand's warmth and influence subsequent brand evaluations. Second, we add to the warmth and competence literature by introducing a novel context in which brand-to-brand praise increases warmth without harming perceptions of competence. Third, our research demonstrates that directing positive attention toward the competition instead of toward one's own brand brings about benefits for the praiser brand, unlike what brands would typically do in comparative advertising and two-sided messaging campaigns (e.g., [ 4]; [17]; [40]). Finally, we contribute to the literature on praise by identifying brand-to-brand communication between competitors as a feasible form of praise that is less likely to induce suspicion compared with praise in traditional person-to-person contexts. Marketing Implications and Future ResearchAs we have noted, our findings may be surprising to practitioners who have been regularly and reasonably advised to avoid bringing positive attention to their competitors. However, our studies show that in some circumstances, praise is a method of brand-to-brand interaction that can result in beneficial consequences for the praising brand. Managers might consider offering compliments to competitors to boost their own brand evaluations. In other words, brands can expand from solely focusing on brand-to-consumer relationships to also focusing on their brand-to-brand relationships. This is akin to the positive reactions that politicians sometimes receive when positively acknowledging their opponent ([12]). We suggest a new context in which positive acknowledgment can benefit brands. With the rise of the digital age, brands can easily ""speak"" with each other and be observed by consumers. While it is not uncommon for brands to speak via ""feuds"" on social media, such as when Wendy's teases McDonald's for using frozen beef ([19]), we show in a supplemental study (Web Appendix P) that positive communication provides unique advantages. Negative or snarky communication, directed at a competitor or even directed at the self (as in the case of two-sided messaging), does not provide the same increase in brand evaluations. Although there will likely be variation depending on the exact content and cleverness of snarky communication (a ripe area for future research), marketers would be wise to consider opportunities for brand-to-brand praise instead, perhaps utilizing social media as a platform, to foster a warmer image.Brand-to-brand praise may also be a valuable way to respond to competitors' actions. While the norm for companies responding to a competitor's new product release is to avoid saying anything that would bring attention to that competitor, our studies suggest that responding positively can increase purchases for the praiser brand without boosting the competitor to the same degree. Importantly, while prior work has shown that a brand's positioning as an underdog or market leader has important implications ([50]), brand-to-brand praise may be appropriate regardless of the competitor's market status. In a preregistered supplemental study (Web Appendix Q), we find that the favorable effects of brand-to-brand praise on brand evaluations are robust to the market leadership status of the brand. Future research could further explore how characteristics about the firm such as market leadership or firm size may factor into the effect of brand-to-brand praise.Similarly, while the variety of brands leveraged in our studies suggest that brand-to-brand praise is likely beneficial when directed toward both less intense (e.g., local popcorn brands) and more intense (e.g., PlayStation and Nintendo, Uber and Lyft) competitors, future research should more systematically explore the role of competitive intensity. Such research might explore brand-to-brand praise with indirect versus direct competitors, which would likely vary in levels of perceived costliness. Indirect competitors may include other brands that are in a similar industry but do not compete directly in the same product category, such as in the case of soda and water in the drinks industry. In Study 3, we find that praise toward a direct competitor is deemed to be costly and results in better brand evaluations compared with praise toward noncompetitors. However, would praise toward an indirect competitor be perceived to be costly enough to increase evaluations? Outside of direct competition, what else determines whether praise is deemed to be costly or not? How would consumers react if complementary brands, such as soda and popcorn brands, praised each other? While we suggest that praise should benefit the praiser as long as it is perceived as costly, understanding what kind of brand-to-brand praise is considered costly may be beneficial to marketers.In addition, future research could also explore how brand-to-brand praise compares to other types of messages that convey warmth. For example, brands often communicate prosocial messages, such as a food brand showing active support for a local food bank. Brands might also post self-deprecating messages that recognize their own room for improvement. While these types of messages may signal warmth, they may not benefit the messenger brand in the same way as brand-to-brand praise does because of the unique aspect of costliness associated with praising a direct competitor. Future research could compare these various types of brand messaging to find ones that benefit brands the most.Future work might also examine whether certain brand personalities benefit more from praise than others. Our for-profit versus nonprofit results in Study 5 hint that brands with personalities that are inherently associated with warmth (e.g., sincere brands) may benefit less. Relatedly, are there circumstances under which snarky interactions or backhanded compliments are better suited for some brands, such as Wendy's ([15])? We noted that negativity did not fare as well as positivity (Web Appendix P), but under what conditions might this be different? Cultural differences, such as collectivism and individualism, may also play an important role. Collectivists, known for valuing community and kinship, may be particularly likely to value the warmth associated with brand-to-brand praise ([ 3]). Furthermore, future work could also explore when the effects of brand-to-brand praise are affected by gender. While women may value warmth more in some circumstances ([66]), we do not find consistent moderation by gender in our studies (Web Appendix R), which is consistent with prior work showing that gender does not always moderate brand warmth perceptions ([ 5]). Further research is needed to better understand the possible role that factors such as brand personality, consumer culture, and gender play in affecting brand-to-brand praise responses.Another direction for future research involves investigating the optimal frequency of brand-to-brand praise. Although we demonstrate that some repetition may be acceptable (Study 4), one limitation of this research is the focus on a single instance of praise in a short span of time. Praise when repeated over time may become less effective or even backfire. Excessive praise may trigger suspicion in observers (e.g., [23]), and previous research has shown that suspicious praise can lead to the praiser being perceived as less sincere ([11]; [44]). Thus, future research might explore the optimal levels or repetition of praise and the consequences of excessive praise to prevent such a backfiring effect.To further prevent backfiring effects, future research might also explore what happens if the praised competitor leverages the praise ""against"" the praiser? For instance, what if a praised brand publicly suggests that they are so great that even their competition praises them? It is possible that, if used by the competitor in this manner, the praise could damage the praiser brand. However, it is also possible that this kind of act could be perceived as arrogant or manipulative, damaging the praised brand and eliciting sympathy for the praiser.Overall, this work raises the question: Are brands missing an opportunity to build positive relationships with consumers by not (publicly) building positive relationships with competitors? We suggest that marketers would be wise to explore the intriguing benefits of brand-to-brand praise. Supplemental Materialsj-pdf-1-jmx-10.1177_00222429211053002 - Supplemental material for Befriending the Enemy: The Effects of Observing Brand-to-Brand Praise on Consumer Evaluations and ChoicesSupplemental material, sj-pdf-1-jmx-10.1177_00222429211053002 for Befriending the Enemy: The Effects of Observing Brand-to-Brand Praise on Consumer Evaluations and Choices by Lingrui Zhou, Katherine M. Du and Keisha M. Cutright in Journal of Marketing  "
2,"Befriending the Enemy: The Effects of Observing Brand-to-Brand Praise on Consumer Evaluations and Choices Consumers have grown increasingly skeptical of brands, leaving managers in a dire search for novel ways to connect. The authors suggest that focusing on one's relationships with competitors is a valuable, albeit unexpected, way for brands to do so. More specifically, the present research demonstrates that praising one's competitor—via ""brand-to-brand praise""—often heightens preference for the praiser more so than other common forms of communication, such as self-promotion or benevolent information. This is because brand-to-brand praise increases perceptions of brand warmth, which leads to enhanced brand evaluations and choice. The authors support this theory with seven studies conducted in the lab, online, and in the field that feature multiple managerially relevant outcomes, including brand attitudes, social media and advertising engagement, brand choice, and purchase behavior, in a variety of product and service contexts. The authors also identify key boundary conditions and rule out alternative explanations, further elucidating the underlying mechanism and important implementation insights. This work contributes to the understanding of brand perception and warmth, providing a novel way for brands to connect to consumers by connecting with each other.Keywords: praise; brand relationships; brand communication; brand evaluations; warmth and competenceIn 2017, a popular video gaming brand, Xbox, openly congratulated its competitor, Nintendo, on the launch of its new Switch gaming system ([53]). A few months later, The New York Times encouraged readers to read other news sources such as The Wall Street Journal ([20]). And, in responding to a playful challenge from Kit Kat, Oreo disarmed the brand by communicating how truly irresistible Kit Kat is ([62]). Conventional wisdom fervently advises that ""even mentioning your competition is a bad idea"" ([51]), so why have these brands not only mentioned their competitors but praised them?In a world where brands are trying hard to connect with consumers, many of whom have grown increasingly skeptical of marketers' intentions ([21]), it may be that praising the competition provides unexpected benefits. Our research explores the consequences of brand-to-brand praise—when a brand communicates positively and publicly about another brand. We argue that consumers who observe a brand praising a competitor will believe that the brand has positive intentions toward others, also known as brand warmth, which heightens consumer evaluations and interest in the brand giving the praise.As an introductory illustration of this idea, we scraped data from the Twitter pages of Nintendo and its fiercest competitors, Xbox and PlayStation, around the time of the Nintendo Switch launch in 2017. We found a greater number of likes and retweets (in fact, over ten times as many), as well as more positive sentiment among consumers' comments, when Xbox and PlayStation praised Nintendo for the launch compared with all other types of messages (Web Appendix A). Such preliminary field data motivate our exploration into whether, when, and why brand-to-brand praise affects consumer reactions to brands.Across seven studies, we examine the effects of brand-to-brand praise on consumer attitudes and behavior versus more common forms of brand messaging (e.g., self-promotion or providing helpful information to consumers) and identify important boundaries. In doing so, this research offers several contributions.First, we contribute to the brand perception literature by demonstrating how consumers' perceptions of brands are affected by a brand's interactions with other brands. Prior work has focused on how brand-to-consumer interactions affect consumer perceptions (e.g., [42]) but has not yet explored how observing brand-to-brand interactions do so. Furthermore, we contribute to research on the fundamental dimensions by which people judge other people and brands: warmth and competence (e.g., [ 1]). We identify brand-to-brand praise as a novel antecedent that often leads consumers to perceive brands that praise their competitors as warmer. We show that praising a competitor is viewed as a costly action that does not obviously benefit the praiser, thus making it a credible signal. In doing so, we demonstrate the role of costliness in signaling warmth that effectively combats consumer skepticism, a major barrier to warmth identified in prior research ([18]). We also introduce two moderators—organization type and individual differences in skepticism—to identify when costly displays of warmth are most important.Second, we further contribute to the literature on warmth and competence by introducing a context in which brand-to-brand praise increases warmth without damaging perceptions of competence. Prior work suggests that warmth and competence are often negatively correlated, particularly in contexts in which people are considering two or more entities ([32]). Leveraging consumers' lay theories about the characteristics of brands that would be willing to praise their competitors, brand-to-brand praise provides an opportunity for brands to communicate warmth while maintaining perceptions of competence.Third, while prior work has examined brand communication in which the competitive brand is ultimately shown to be inferior, such as in comparative advertising and two-sided messaging campaigns (e.g., [ 4]; [17]; [40]), our research identifies how directing attention away from one's own brand and toward the competition in a purely positive light affects brand evaluations and choice.Finally, we add to the current literature on praise by identifying brand-to-brand communication as a viable and distinct form of praise, noting that observers respond more favorably to praise in a brand-to-brand context than is typically observed in traditional person-to-person contexts. In doing so, we also highlight the understudied effects of praise in competitive relationships. In what follows, we review literature on brand relationships, praise, and brand warmth and present seven studies that test our hypotheses. Building Consumer–Brand RelationshipsA great deal of research has investigated how brands establish and build relationships with consumers, identifying influential factors such as the personalities, motives, and communication styles of both consumers and brands (for a review, see MacInnis, Park, and Priester [2009]). Surprisingly, researchers interested in brand relationships have not yet explored how brands' public communication with other brands affects their relationships with consumers. Of course, researchers have explored how strategic partnerships, such as brand collaborations and alliances, affect brand outcomes (e.g., [38]; [57]). However, these brand interactions occur largely behind the scenes and reflect formal business arrangements as opposed to informal, public communication that makes consumers privy to how a brand treats its competitors. We suggest that consumers will infer important information about a brand based on the way it communicates with other brands. Specifically, we posit that consumers who observe a brand praising its competition will perceive the praiser brand as having positive intentions toward others, known as brand warmth. Subsequently, consumers will develop more positive evaluations of and interest in the praiser brand. Brand-to-Brand Praise and Warmth PerceptionsPrior research has established that people judge others—individuals, groups, cultures, countries—using two fundamental dimensions, often referred to as warmth and competence ([25]; [32]).[ 5] Warmth is the degree to which one has positive intentions toward others and includes perceptions of thoughtfulness, kindness, honesty, and trustworthiness ([ 1]; [25]). Evolutionarily speaking, it allowed our ancestors to quickly distinguish friend from foe and prepare to fight or flee. Warmth judgments are therefore formed more quickly and generally have the greatest impact on attitudes toward individuals ([64]). Conversely, competence reflects the degree to which one is able to enact one's intentions.Given that people relate to brands similarly to how they relate to people in many ways ([26]), warmth and competence are important traits for firms to consider ([33]). Surprisingly, scant literature has explored the drivers of warmth and competence for brands ([ 1]). The research that does exist identifies factors such as a firm's profit focus ([ 2]), social responsibility ([ 8]), racial dynamics ([ 6]), and expression/communication style ([61]; [65]) as affecting consumers' perceptions of warmth and competence. Significantly more work has explored the consequences of warmth and competence. As examples, research has shown that warmth and competence perceptions affect consumer emotions ([ 1]), product evaluations and interest ([10]; [14]; [34]; [36]; [39]; [58]), and word of mouth ([ 7]; [54]). Importantly, while the precise contribution of warmth versus competence to different downstream consequences varies (e.g., [37]), brands generally aspire to be strong on both dimensions and occupy the coveted ""golden quadrant"" ([ 1]).In this research, we suggest that brand-to-brand praise affects consumers' reactions primarily through perceptions of warmth. Prior research has theorized that warmth is established through signals of cooperation (vs. competition) and actions that appear to serve others as opposed to the self (i.e., actions that are ""other-profitable""; [16]; [52]). We suggest that offering praise to a competitor provides a strong illustration of such cooperative, ""other-profitable"" activity. Consumers will therefore perceive a brand that praises competitive brands as warmer (relative to a brand that engages in other types of common brand messaging). This then leads to positive downstream consequences, such as increased purchases. Costliness as an Antidote to SkepticismIn hypothesizing the positive effect of brand-to-brand praise on warmth, it is important to note that high warmth is more difficult to establish and maintain than high competence, as people are often skeptical of others' motives. That is, warm behavior is often discounted, considered to be driven by ulterior motives and easier to fake than competence ([18]; [56]). Such skepticism has also been identified as an issue in research on praise more specifically. For example, in many brand-to-consumer exchanges, such as salesclerk-to-consumer interactions, praise generates suspicion from the consumer; consumers often suspect ulterior motives behind a salesclerk's compliment and perceive the salesclerk to be insincere when the praise occurs before a purchase ([11]; [44]). Further, observers who witness praise happening among others tend to be more skeptical of the praiser and do not react as positively as recipients of the praise ([13]; [29]; [60]). This then leads to the question: When and why might brand-to-brand praise surmount the skepticism associated with praise and other displays of warmth?We suggest that brand-to-brand praise operates uniquely from the aforementioned person-to-person and brand-to-consumer displays of warmth, particularly when the praise is directed toward competitors, due to its costliness. Consumers assume that complimenting a competitor is a costly action that does not directly benefit the complimenting brand. Research across disciplines suggests that the costliness of an act is the key component of whether the act is perceived as a meaningful signal of an underlying trait rather than an uninformative act motivated by devious or ulterior motives, also known as ""cheap talk"" (e.g., [59]; [67]). A common example from the natural world is the male peacock's tail. The costliness of this large and colorful tail, such as how it can handicap the bird's ability to escape from predators, is what makes it a credible signal of fitness to potential mates. Only those who truly possess the focal underlying trait would or could incur such costs. Linking this principle to the present context, only brands that are truly warm would incur the real or potential costs of praising the competition. Consumers should therefore not be as suspicious of brands that praise competitors as compared with brands that engage in less costly messaging. This costliness is why brand-to-brand praise, when directed toward competitors, is a strong signal of warmth, differentiating it from other common types of praise (e.g., salesclerk-to-consumer), surmounting consumer skepticism, and driving its positive influence on consumer attitudes and reactions. Effect on Competence PerceptionsAlthough the focus thus far has been on perceptions of warmth, one might wonder whether brands sacrifice competence when enhancing perceptions of warmth via brand-to-brand praise. This is a reasonable concern, as prior research suggests that in comparative contexts, for people and brands alike, when warmth (competence) is relatively high for a given entity, perceptions of that entity's competence (warmth) suffer (e.g., [ 2]; [28]; [36]; [54]). We suggest that praising a competitor offers brands unique advantages that allow them to maintain perceptions of competence in the face of increasing warmth. In particular, we suggest that consumers hold a lay intuition that a brand that is willing to praise its competitors must be fairly confident in its own abilities, which then allows consumers to maintain, if not increase, positive perceptions of its competence. This is reminiscent of the lay belief—confirmed by academic research—that people who are secure in their identity are the most willing to be kind and compliment others (e.g., [ 9]; [22]; [41]; [63]). Still, while competence may play a role in driving the effects of brand-to-brand praise, we do not expect it to be the primary driver of increased interest in the praiser brand, as perceptions of competence should be most strongly driven by cues of status versus the cues of cooperation that are focal in brand-to-brand praise ([24]). Summary of the Current ResearchIn summary, we argue that brand-to-brand praise often promotes positive brand evaluations and choice of the praiser. Specifically, we predict that consumers will evaluate a brand more favorably and show more interest in the brand when observing brand-to-brand praise compared with observing traditional self-focused messages or even other benevolent messages (H1). We theorize that this is because such praise increases perceptions of the praiser brand's warmth (H2). Moderators/Boundary Conditions CostlinessWe expect that this effect exists when the praise is costly (H3), such as when the brand is praising a competitor, so as to provide a meaningful signal of warmth to the consumer. We reason that the costly signal of warmth indicates to the consumer that the brand truly has positive intentions toward others, even when it is not in the best interest of the brand. As prior work suggests, such warmth leads consumers to be more interested in identifying with, using, and sharing the brand ([ 7]; [37]). For-profit versus nonprofit organizationsAs additional evidence for the role of warmth, we expect the effect of brand-to-brand praise to be stronger for brands that are typically associated with lower levels of warmth than those already endowed with high levels of warmth; brands with high levels of warmth, such as nonprofits, have little to gain from increasing warmth even further (and thus less to gain from brand-to-brand praise). This suggests that for-profit brands (which are lower in warmth than nonprofit brands; [ 2]) will benefit more from brand-to-brand praise (H4). Chronic consumer skepticismFinally, we posit that brand-to-brand praise allows brands to manage consumer skepticism often associated with displays of warmth. We introduce an important moderator to illustrate this point: individual differences in consumers' skepticism toward brand messaging. We predict that the effect of brand-to-brand communication will be strongest among individuals who are highly skeptical of brands. This is because the costliness associated with praising a competitor minimizes the extent to which persuasion knowledge concerns are activated among this group of consumers as compared with more traditional brand communication (H5). We summarize these predictions in the conceptual model in Figure 1 and discuss the boundary predictions further in the study introductions relevant to each.Graph: Figure 1. Conceptual model. Overview of StudiesWe first demonstrate the positive effects of brand-to-brand praise (vs. more traditional messages) across three field and lab studies involving real, consequential behaviors (Studies 1a, 1b, and 2; H1). Next, we show that this effect is specific to praise that is aimed toward a brand's competitor and thus deemed costly (Study 3; H3). We then explore warmth as the key mechanism, demonstrating that it mediates the effect of brand-to-brand praise (Study 4; H2) and that the effect is strongest for for-profit brands that have greater need to enhance perceptions of warmth (Study 5; H4). Finally, we demonstrate that brand-to-brand praise has the largest effect on individuals with high levels of skepticism toward traditional brand communication (Study 6; H5). Throughout these studies, we also test for alternative explanations, including that praise confers benefits due to novelty or authenticity, or that the effect is driven by negative reactions to self-promotion (i.e., bragging) rather than positive reactions to praise. Finally, we also examine the role of competence in several studies (Studies 4, 5, and 6). Evidence suggests that, while perceptions of brand warmth are the primary mechanism underlying this effect, brand-to-brand praise does not harm and can even boost perceptions of competence, which can influence desired outcomes.Together, these studies provide insight into when and why brand-to-brand praise is beneficial for brands. Notably, however, we demonstrate (in Studies 3, 5, and 6) and explain (in the ""General Discussion"" section) when and among whom brand-to-brand praise might not be as beneficial or might even be less beneficial. Study 1: The Effect of Brand-to-Brand PraiseStudies 1a and 1b provide initial causal evidence for our hypothesis by testing the effects of various types of brand messaging on advertisement click-through rate and brand choice in a field and lab study, respectively. We compare the effects of brand-to-brand praise with a traditional self-promotion message from the brand. In addition, in Study 1a, we utilize another type of control message in the form of an endorsement from another organization in the industry to show that praising a competitor enhances evaluations more than receiving an endorsement from others. Further, the endorsement condition serves as a separate point of comparison, ensuring that the hypothesized difference between the brand-to-brand praise condition and the self-promotion condition can be interpreted as a boost from brand-to-brand praise and not merely a negative reaction to self-promotion. Study 1a: Facebook Advertisement Click-Through Rate MethodParticipants and design. This preregistered study utilized a three-cell between-subjects design: Facebook users saw an advertisement on Facebook featuring self-promotion, an external endorsement, or brand-to-brand praise.[ 6]Procedure. We created a fictitious car wash brand called Precision Car Wash and launched three advertisements for the brand on Facebook. The self-promotion message stated, ""Precision Car Wash is proud to receive the Industry Best 2020 Award."" The external endorsement consisted of a message from a fictitious organization, The Industry Best 2020 Award Committee, announcing Precision Car Wash as the year's award recipient. Finally, the brand-to-brand praise ad consisted of a message from Precision Car Wash congratulating another fictitious car wash business, LikeNew Car Wash, on winning the Industry Best 2020 Award (for study stimuli, see Web Appendix B). Facebook users who saw the ad could click on the message to be taken to the Facebook page of Precision Car Wash for more information. We measured the number of impressions and clicks, which allowed us to compare click-through rates (clicks as a percent of impressions; CTRs), for each ad. We ran the ads over the course of five days and used Facebook's recommended advertising algorithm to reach 80% power in testing the ads for a final sample of 13,719 impressions.We also conducted two separate supplemental tests of these stimuli. First, we conducted a manipulation check for the ads, assigning participants to one of the three ad conditions and asking them to indicate the extent to which the brand was praising its competition (1 = ""definitely NOT praising the competition,"" and 7 = ""definitely praising the competition""; N = 105). Second, given variations in the text used to communicate the focal messages (e.g., number of words, fonts), we conducted a posttest to assess the ""graphic design"" (three items: quality, clarity, graphic appeal) of the assigned ad (1 = ""not done very well,"" and 7 = ""done very well""; α = .87; N = 105). These items allowed us to be sure that our focal praise condition was not (unintentionally) more aesthetically appealing and more likely to be clicked for that reason. ResultsManipulation check and aesthetic appeal. The separate manipulation check showed a significant difference across conditions (F( 2, 101) = 21.47, p < .001,  ηp2   = .30). Those who viewed the brand-to-brand praise message perceived it to praise the competition (M = 4.82) more than the self-promotion (M = 1.96; p < .001) and external endorsement (M = 2.43; p < .001) ads. The self-promotion and external endorsement conditions did not differ (p = .30). This manipulation check was conducted for and confirmed the manipulations in each of the remaining studies (see Web Appendix C).In addition, the separate test designed to assess the aesthetic appeal of the different messages indicated that the focal praise ad was not advantaged aesthetically. We find a significant difference across conditions (F( 2, 102) = 5.50, p = .005,  ηp2   = .10) such that those who viewed the brand-to-brand praise ad perceived it to be lower in aesthetic appeal (M = 3.83) than the self-promotion ad (M = 5.03; p = .001) and not statistically different from the external endorsement ad (M = 4.42; p = .11). The self-promotion and external endorsement conditions did not significantly differ (p = .10). However, because the praise ad was rated to be lower in aesthetic appeal, any positive benefits of the praise ad should not be a result of viewers liking the appearance of the praise ad more.CTR. A chi-squared analysis revealed that the CTR differed across conditions (χ2( 2, N = 13,719) = 91.59, p < .001). The percentage of those who clicked on the ad was greater for the brand-to-brand praise condition (5.4% of 4,392 impressions) compared with the self-promotion (3.3% of 4,075 impressions; χ2( 1, N = 8,467) = 21.42, p < .001) and external endorsement (1.8% of 5,252 impressions; χ2( 1, N = 9,644) = 90.45, p < .001) conditions. Study 1b: Lab Brand Choice MethodParticipants and design. One hundred fifty-four members of the local community (general public, students, and staff) were recruited through a business school's behavioral lab. Participants were randomly assigned to one of two conditions: self-promotion or praise.Procedure. First, participants learned that they would give their opinions about snack brands that the business school was considering for its café and vending machines. Next, they were informed that they would be able to choose a sample snack to take home after the study.Participants then saw two advertisements from real local popcorn shops in the Raleigh-Durham, North Carolina, area. The first ad was from the recipient brand, Carolina Popcorn Shoppe, and stated, ""Come check out our FIVE newest flavors! In-store or online."" Everyone saw this ad. The second ad was from the focal brand of the study, The Mad Popper, and differed by condition. The praise condition ad read, ""We love good popcorn. Big shout-out to Carolina Popcorn Shoppe on their FIVE new flavors!"" The self-promotion ad read, ""We love good popcorn. Come explore our FIVE brand new flavors! In-store or online"" (Web Appendix D).Next, participants were reminded that both brands were being considered for use at the business school. They were then asked to choose Carolina Popcorn Shoppe or The Mad Popper as the brand they would prefer to sample, which they received at the conclusion of the study.[ 7] Finally, participants responded to a stimuli believability measure, assessing their willingness to suspend disbelief and assume that the experimental stimuli were real (""How believable was this advertisement by The Mad Popper?""; 1 = ""not at all,"" and 7 = ""very""). We asked this believability question consistently across experiments (except Study 1a, where we could not) because of the low prominence of brand-to-brand praise currently in the market. Although brand-to-brand praise is not yet commonly practiced in the real world and thus may not yet be highly believable (i.e., seem real) for some consumers, we are examining what could happen if consumers witnessed brand-to-brand praise, meaning controlling for variation in whether the stimuli were believed to be real.[ 8] We use believability as a covariate across each subsequent experiment to enhance experimental power ([45]). Details on this measure appear in Web Appendix E, which includes a summary of key results with and without the covariate.[ 9] ResultsBrand choice. We conducted a binary logistic regression with condition (1 = praise, 0 = self-promotion) as the key predictor and believability as a continuous covariate on brand choice (1 = The Mad Popper, 0 = Carolina Popcorn Shoppe). Conceptually replicating the results of Study 1a, those in the praise condition were significantly more likely to choose the focal brand, The Mad Popper, compared with those in the self-promotion condition (β = .91, SE = .43, Wald χ2( 1, N = 154) = 4.47, p = .03). In percentages, 27.63% chose the focal brand in the self-promotion condition, and 34.62% did so in the praise condition.[10] We also find that believability does not act as a moderator (β = −.02, SE = .25, Wald χ2( 1, N = 154) = −.08, p = .93); thus, we use it as a covariate in our remaining studies (for moderation by believability results for the remaining studies, see Web Appendix E). DiscussionStudies 1a and 1b provide initial experimental evidence that brand-to-brand praise can have positive consequences for the praiser on real behavior, including advertisement CTR and brand choice. The comparison of brand-to-brand praise to an external endorsement in Study 1a suggests that the key effect is not because consumers dislike the other control condition—the self-promotion message—but is instead driven by a boost from observing brand-to-brand praise. Furthermore, by presenting participants with a forced choice between two brands in Study 1b, this data begins to suggest that praise benefits the praiser more than the praised. We further explore this in Study 2. Study 2: How Are Real Purchases Affected for the Praising and Praised Brands?In Study 2, we build on the findings of Studies 1a and 1b in assessing consumers' real, behavioral reactions to brand-to-brand praise, but we do so with some important changes. First, we assess a longer-term reaction to brand-to-brand praise by investigating purchase behavior for popular national brands (Kit Kat and Twix) 11 days after people were exposed to the brand messaging. Second, unlike Study 1b (but similar to Study 1a), we only expose participants to the competitor brand in the brand-to-brand praise condition. This enables us to further assess the potential costs of making competitive brands more top-of-mind (via praise) than they might otherwise be. This study also allows us to assess behavioral reactions to both the focal and competitive brands by gauging consumers' purchase behavior toward both, as opposed to forcing a choice between them (Study 1b) or tracking reactions to only the focal brand (Study 1a). Finally, this design offers an opportunity to see the proposed effect of praise on behaviors of greater financial consequence for the consumer and the brand: purchases. Methods Participants and designThis preregistered study had a two-cell between-subjects design (control, praise) and was conducted in two stages on Prolific.[11] First-stage procedureIn the first stage of this study, participants recruited from Prolific (N = 1,502; 49.6% female) viewed an image of Kit Kat's Twitter page. In the control condition, participants read a tweet that said, ""Start your day off with a tasty treat!"" In the praise condition, participants read a tweet that said, ""@twix, Competitor or not, congrats on your 54 years in business! Even we can admit—Twix are delicious"" (Web Appendix F). Unlike Study 1b (but similar to Study 1a), participants did not see a separate introduction to the nonfocal brand, Twix. Thus, Twix would presumably not be as top-of-mind unless they read the praise tweet. Afterward, we measured participants' attitudes toward both Kit Kat and Twix using the attitude measure from [46] (""negative/positive,"" ""dislikeable/likeable,"" and ""unfavorable/favorable"" on seven-point scales; α = .95; for details on this measure, see Web Appendix F). Second-stage procedureFrom the initial sample of 1,502 participants, we excluded those who indicated that they have not bought chocolate candy for themselves within the past six months, those with dietary restrictions that prevent them from buying chocolate candy, and those who indicated that they were not interested in completing a follow-up survey, leaving us with a sample of 1,298 potential participants for the second stage. Approximately 11 days after participants completed the first stage, we sent a follow-up survey to the 1,298 participants. Of these participants, 772 participants completed the second-stage survey. Attrition rates were similar across conditions (control = 40.64%, praise = 40.40%, χ2( 1, N = 1,298) = .008, p = .93).In the second-stage survey, participants indicated whether they had purchased any Kit Kats (yes/no) or Twix (yes/no) since they took the first portion of the survey. Finally, we asked participants in an open-ended question what they could recall about the tweet they saw in the first stage of the study, bringing the tweet to their attention so that we could measure the believability of the tweet using an adapted version of the measure in the prior study. ResultsWe conducted a binary logistic regression with condition (1 = praise, 0 = control) as the key predictor and believability as a continuous covariate on purchase behavior (1 = purchased Kit Kat, 0 = did not purchase Kit Kat). Those in the praise condition were significantly more likely to purchase Kit Kat compared with those in the control condition (β = .34, SE = .16, Wald χ2( 1, N = 772) = 4.17, p = .04). In raw percentages, 23.77% in the control condition purchased Kit Kat versus 31.95% in the praise condition. We conducted the same analysis for purchase behavior for Twix and find that there was no difference between the praise and control conditions (β = −.25, SE = .19, Wald χ2( 1, N = 772) = 1.68, p = .20) DiscussionStudy 2 extends our prior findings, demonstrating the effect of brand-to-brand praise on actual purchase behavior over a longer time horizon and relative to competitors' purchase outcomes. In doing so, we also show that even if the competitor brand becomes more salient than it would have normally been as a result of brand-to-brand praise, such praise still primarily provides positive benefits to the focal brand.Notably, in a supplemental study, we replicate the current findings using a paradigm with Subway and Jimmy John's (Web Appendix G). We find that when Subway brings Jimmy John's into the consideration set through brand-to-brand praise, Subway gains a boost in brand attitude compared with the self-promotion condition; Jimmy John's, however, does not gain a boost from receiving the praise.In the remaining studies, we identify when the beneficial effects of praise are mitigated and the underlying mechanism of the effect. In doing so, we offer additional understanding of the psychological processes driving consumer responses to praise as well as practical insights for choosing the appropriate recipients of and circumstances for praise. Study 3: The Effects of Brand-to-Brand Praise Toward Competitors Versus NoncompetitorsIn Study 3, we expand on the findings of the prior studies by examining the effects of brand-to-brand praise toward both a direct competitor and a noncompetitor. We expect that when a brand compliments a relevant direct competitor, which we define as a brand that competes in the same category as the focal brand, consumers view that compliment as relatively costly or risky because a brand has more to lose when bringing positive attention to such competition. It is such costliness that credibly signals that the brand must truly have warm intentions. We do not expect our effects to hold when a brand compliments an irrelevant noncompetitor, which we define as a brand that does not compete in the same category as the focal brand. In this scenario, the brand incurs lower cost by bringing attention to the irrelevant noncompetitor because there are less obvious repercussions, and thus the compliment is a less credible signal of a brand's warmth. This study also aims to demonstrate that the effects of brand-to-brand praise are not driven solely by the perceived novelty of the message or by negative reactions to self-promotional messages. Method Participants and designThree hundred ninety-nine participants (50.4% female) recruited from Amazon Mechanical Turk took part in this four-cell between-subjects design (helpful control, self-promotion, costly praise, noncostly praise). ProcedureWe created two eyeglasses brands for this study, Franklin's Frames and Lazlo's Lenses, and introduced them to participants as competitors. Participants then viewed an ostensibly recent series of three tweets from the focal brand, Franklin's Frames—one manipulated tweet that varied by condition and two filler tweets. The manipulated tweet in the praise condition read, ""@lazloslenses, Wow! Your new frames are looking good!"" with an image of a pair of glasses. In the self-promotion condition, the manipulated tweet read, ""Wow! Our new frames are looking good!"" with an image of a pair of glasses identical to the praise condition. In the helpful control condition, the manipulated tweet read, ""How to clean your frames:"" with a screen shot from a video showing how to clean eyeglasses. Participants in the noncostly praise condition saw a burger brand that is clearly not a direct competitor to Franklin's Frames, called Ben's Burgers. The focal tweet for those in the noncostly praise condition said, ""@bensburgers, Wow! Your new burgers are looking good!"" with an image of a burger attached (Web Appendix H).After reading the scenario, participants completed the measures of brand attitude (α = .95) noted in Study 2. In addition, we measured the perceived costliness of the tweets with four items on seven-point scales (α = .75; Web Appendix C). Finally, participants answered the same believability question as in prior studies as well as a measure of perceived novelty (""How novel was this tweet by Franklin's Frames?""; 1 = ""not novel,"" and 7 = ""very novel"") of the focal tweets. Results CostlinessA one-way analysis of covariance (ANCOVA) showed significant differences across conditions on perceived costliness of the focal tweet (F( 3, 394) = 37.66, p < .001,  ηp2   = .22).[12] As we expected, contrasts revealed that perceived costliness was significantly greater for the costly praise condition (M = 3.82) than the self-promotion (M = 2.21, p < .001), control (M = 2.37, p < .001), and noncostly praise (M = 2.52, p < .001) conditions. Perceived costliness for the noncostly praise condition was greater than the self-promotion condition (p = .05) but not significantly different from the control condition (p = .34). Lastly, the self-promotion and control conditions did not differ (p = .32). Notably, we measure perceived costliness of the stimuli in all of our studies and find the same pattern of results; praise toward a direct competitor is perceived as more costly than other types of messages (Web Appendix C). Brand attitudeA one-way ANCOVA revealed an effect of condition on brand attitude toward the focal brand, Franklin's Frames (F( 3, 394) = 4.83, p = .003,  ηp2   = .04). Contrasts revealed that brand attitude was significantly greater for the costly praise condition (M = 6.05) than the control (M = 5.43, p < .001) and self-promotion (M = 5.46, p < .001) conditions. Brand attitudes were also significantly greater for costly praise than noncostly praise (M = 5.65, p = .02). Brand attitude for those in the control, self-promotion, and noncostly praise conditions were not significantly different (all ps > .18), suggesting that the results are driven by a boost from the praise message and not a dislike of the self-promotion message. NoveltyUnsurprisingly, a one-way ANCOVA revealed significant differences across conditions on how novel the tweet seemed (F( 3, 394) = 8.80, p < .001,  ηp2   = .06). Contrasts revealed that both the noncostly praise (M = 4.30) and the costly praise (M = 4.56) conditions were perceived to be more novel than the control (M = 3.48) and self-promotion (M = 3.43, all ps ≤ .001) conditions. Crucially, however, there was no difference in perceived novelty between the noncostly praise and the costly praise conditions (p = .31), suggesting that the differences between these conditions on brand attitude were not driven purely by differences in novelty.[13] DiscussionIn Study 3, we replicate the findings of prior studies, demonstrating that brand-to-brand praise between competitors boosts brand evaluations compared with other messages, including self-promotion and a helpful control message. Furthermore, we find that noncostly praise (i.e., praise toward irrelevant noncompetitors) did not give the same boost. This result suggests that simply speaking positively about another brand is not enough; the consumer must view the compliment as costly to the brand for it to have positive effects. Although we find costliness to be necessary to show the benefits of praise, it is not the underlying driver of the effects (mediation comparing the self-promotion and costly praise conditions: ab = .06, 95% confidence interval [CI] = [−.045,.191]; PROCESS Model 4, 5,000 bootstrap samples; [31]). This study also begins to cast doubt on novelty as a primary driver of the effect, given that the noncostly praise was perceived to be as novel (but not evaluated as positively) as the costly praise. We further test the role of novelty in subsequent studies. In addition, we again find that the effects on brand attitude do not occur because consumers dislike self-promotion messages but are instead driven by the boost from seeing the costly praise message, replicating the findings of Studies 1a and 2. Study 4: Testing Warmth as the MechanismTo begin to understand why brand-to-brand praise increases brand evaluations and choice, we conducted a qualitative pretest gauging people's natural reactions (i.e., inferences, attributions) to observing a brand complimenting a competitor. Participants (N = 150) on Prolific saw a tweet from PlayStation congratulating Nintendo on its Nintendo Switch launch and then listed five adjectives to describe PlayStation. We find that observers more frequently attribute warmth-related words (e.g., friendly, supportive, kind; 55.73% of the adjectives), rather than competence-related words (e.g., confident, successful, intelligent; 17.07% of the adjectives), or any other kind of perception, to the brand, implicating warmth as the most top-of-mind inference following brand-to-brand praise. For additional insight, we asked participants to take the perspective of a manager and indicate why they would or would not post a message praising the competition. We find that most participants would be willing to praise a competitor (69.33%). Again, the majority of the responses (61.5%) indicated warmth to be the primary reason for the action (e.g., ""I want to show others that I act in selfless ways""). Moreover, some responses also positively noted competence-related reasons (e.g., ""have confidence in my own brand and its qualities""; ""we appear to be positive and not insecure about our place in the market"") either exclusively or in combination with warmth (25%), suggesting that while warmth-related reasons are more top-of-mind, perceptions of competence are unlikely to be harmed (for pretest details, see Web Appendix I).In light of these initial qualitative insights, we empirically test warmth as the driver of the benefits of brand-to-brand praise and compare it with competence in Study 4. We predict that brand-to-brand praise enhances perceptions of brand warmth, which predominantly drives the boost in brand evaluations, rather than competence. Ultimately, we expect the heightened brand evaluations to drive consumer action, which is measured in this study as their willingness to sacrifice their own time (for free) on behalf of the brand. Methods Participants and designTwo hundred participants (57% female) from Prolific took part in this two-cell between-subjects study (control, praise). ProcedureParticipants were introduced to a real tea brand, Treecup Tea, from Kickstarter. They were told that Treecup Tea focuses on making high-quality tea blends and competes with other tea companies on Kickstarter for funds. Participants read that Treecup Tea's competitors include Teafir, Shisso Tea, and Phat Tea. Next, participants were introduced to Treecup Tea's Twitter page. For the control condition, participants saw only the brand's Twitter header. We use this control condition, different from the self-promotion control in prior studies, to show again that the boost for the brand results from people feeling positive about the praise message instead of negative toward self-promotion. For the praise condition, participants saw a Twitter page that consisted of three praise messages toward the three competitors interspersed throughout other tweets on the page (e.g., ""@ShissoTea Your tea is fresh and sustainable! That's amazing!""; Web Appendix J).Participants then completed the same measure of brand attitude as in prior studies. Participants were also asked how much time (scale from 0 to 4 minutes, in 30-second increments) they were willing to volunteer to answer some market research questions for Treecup Tea after the main study, without additional pay. We chose this dependent variable as an action of consequence to the consumer, given that volunteering time is a costly consumer behavior. Finally, participants completed measures for warmth (""warm, friendly"" on a seven-point scale; r = .81) and competence (""competent, capable"" on a seven-point scale; r = .90; [ 1]). The order of the warmth and competence measures was randomized to ensure that one did not explain the effect more than the other simply due to order. We again measured believability of the stimuli as a covariate in our analyses. Results Brand attitudeReplicating prior results, a one-way ANCOVA revealed a significant effect of message (F( 1, 197) = 17.61, p < .001,  ηp2   = .08), whereby praise (M = 5.86) led to greater brand attitude compared with the control (M = 5.33). Volunteer timeThe data were skewed because 34.1% of the sample indicated that they would not volunteer any time,[14] so we conducted a binary split on volunteer time (1 = those who would volunteer any amount of time, 0 = those who would not volunteer). A binary logistic regression with condition (1 = praise, 0 = self-promotion) and believability as the predictors on willingness to volunteer revealed that those in the praise condition were significantly more likely to volunteer time compared with those in the self-promotion condition (β = .59, SE = .30, Wald χ2( 1, N = 200) = 3.76, p = .05).[15] Warmth and competenceA one-way ANCOVA revealed a significant effect of message on warmth (F( 1, 197) = 22.54, p < .001,  ηp2   = .10): praise (M = 5.92) led to greater perceptions of warmth compared with the control (M = 5.20). A one-way ANCOVA also revealed a smaller but significant effect of message on competence (F( 1, 197) = 5.59, p = .02,  ηp2   = .03): praise (M = 5.49) led to greater perceptions of competence compared with the control (M = 5.17). MediationNext, we tested the effect of praise on willingness to volunteer through warmth and brand attitude as serial mediators. We find that the praise message leads to increased warmth perceptions, which leads to improved brand attitude and, subsequently, greater willingness to volunteer (indirect = .08, 95% CI = [.008,.228]). However, a serial mediation model replacing warmth with competence is not significant (indirect = .05, 95% CI = [−.005,.169]). DiscussionIn Study 4, we replicate prior findings and also demonstrate a novel downstream consequence of praise whereby observers are more likely to give up some of their time, without additional compensation, to help the praiser brand after viewing a praise message. We find warmth to be the most direct driver of the effects of brand-to-brand praise. However, given that competence perceptions also benefit from a praise message, it is worth considering how brand-to-brand praise may usher brands into the ""golden quadrant"" ([32]) of warmth and competence dimensions. Lastly, the stimuli used for the praise message in this study consisted of three compliments to three different competitors on a single Twitter page. The fact that we still see a boost in brand attitude suggests that giving praise repeatedly, at least to some extent, may not harm the brand in this context (an idea we revisit in the ""General Discussion"" section). Study 5: Moderated Mediation by Organization TypeIn Study 5, we again test for warmth as the mechanism underlying the effect of brand-to-brand praise on evaluations, and we do so with process by moderation, directly manipulating (and again also measuring) brand warmth. Compared with brands that are lower in warmth, we predict that brand-to-brand praise will not increase brand evaluations as much for brands that are higher in warmth. We theorize that this is because brands that are already high in perceived warmth will not have as much need or space to grow in that aspect, thus rendering praise less influential. In contrast, brands with lower perceived warmth at baseline will benefit more from the boost given by brand-to-brand praise. Based on prior research demonstrating that nonprofit organizations are high in perceived warmth ([ 2]), we compare the effects of praise on nonprofit and for-profit organizations. Beyond its relation to our theory, comparing the effect of praise in for-profit versus nonprofit organizations is of practical importance, as it will help managers from these clearly identifiable sectors better assess how effective brand-to-brand praise may be for their brands. Lastly, we measure perceptions of the brand's arrogance as an alternative explanation, given that self-promotional messages may be perceived as a form of bragging. We also measure perceptions of authenticity and novelty of the message as other potential explanations for the effect. Method Participants and designSix hundred one participants (62.2% female) from Prolific took part in this 2 (message: control, praise) × 2 (organization type: nonprofit, for-profit) between-subjects experimental study that was preregistered.[16] ProcedureParticipants were introduced to a fictitious internet service organization, Tech Dev. In the nonprofit condition, participants were told that Tech Dev was a nonprofit organization that had a goal of helping the community. In the for-profit condition, participants read that Tech Dev was a for-profit organization with a goal of increasing profits. Participants were also told that Tech Dev competed with another organization, Networks.org or Networks.com, for either donations or sales (depending on its nonprofit or for-profit status, respectively). Next, participants were shown the Twitter page of Tech Dev in which they read either a control message (""Want to know more about the quality of our services? Click here: techdev[.com/.org]/internet"") or a message praising Networks (""Networks[.com/.org], we are impressed by the quality of your services—competitor or not!""; Web Appendix K).Participants then indicated their interest in Tech Dev using two measures (""How likely are you to seek out more information about Tech Dev?"" and ""How willing are you to talk to a customer representative to learn more about Tech Dev?""; r = .79).[17] We also measured warmth, competence, and believability of the stimuli. As in the prior study, the order of warmth and competence was randomized. In addition, we measured the extent to which participants' assigned condition was perceived as arrogant (braggy, conceited, arrogant; α = .94), authentic (authentic, self-aware; r = .65), and novel (single-item measure) as alternative explanations,. Results Brand interestUsing a two-way ANCOVA, we find a main effect of message (F( 1, 596) = 24.51, p < .001,  ηp2   = .04), a main effect of organization type (F( 1, 596) = 35.82, p < .001,  ηp2   = .06), and a significant interaction (F( 1, 596) = 4.79, p = .03,  ηp2   = .008). As predicted, in the for-profit conditions, we replicate prior results in that praise (M = 3.74) led to greater brand attitude than the control did (M = 2.78; p < .001). In the nonprofit conditions, praise (M = 4.25) also led to greater brand evaluations than the control did (M = 3.86; p = .04), but to a diminished extent. WarmthWe find a similar pattern for warmth. A two-way ANCOVA revealed a main effect of message (F( 1, 596) = 98.85, p < .001,  ηp2   = .14), a main effect of organization type (F( 1, 596) = 124.43, p < .001,  ηp2   = .17), and a significant interaction (F( 1, 596) = 8.44, p = .004,  ηp2   = .01). In the for-profit conditions, we again replicate prior results where praise (M = 4.78) led to significantly greater warmth compared with the control (M = 3.37; p < .001). In the nonprofit conditions, we find that praise (M = 5.67) led to weaker, though still significant, effects compared with the control (M = 4.88; p < .001). CompetenceWe also find a similar pattern for competence. A two-way ANCOVA revealed a main effect of message (F( 1, 596) = 26.90, p < .001,  ηp2   = .04), a main effect of organization type (F( 1, 596) = 23.33, p < .001,  ηp2   = .04), and a significant interaction (F( 1, 596) = 4.49, p = .03,  ηp2   = .007). In the for-profit conditions, we find that praise (M = 5.22) led to greater competence compared with the control (M = 4.53; p < .001). In the nonprofit conditions, we find that praise (M = 5.48) led to weaker, though still significant, effects compared with the control (M = 5.18; p = .02). AlternativesWe do not find the same pattern of results for bragging. A two-way ANCOVA revealed only a main effect of organization (F( 1, 596) = 55.77, p < .001,  ηp2   = .09) such that the nonprofit (M = 2.08) was perceived as less arrogant than the for-profit (M = 2.93).For authenticity, we find a similar pattern to that of warmth in which there was a significant interaction (F( 1, 596) = 16.33, p < .001,  ηp2   = .03) where the boost from praise was stronger in the for-profit conditions. We find similar patterns for novelty (interaction F( 1, 596) = 6.58, p = .01,  ηp2   = .01). Moderated mediationWe first tested the predicted model. Specifically, we tested for moderated mediation (PROCESS Model 7, 5000 bootstrap samples, [31]) for the effect of praise on brand attitude through warmth, moderated by organization type, controlling for believability. As expected, we find that organization type significantly moderated the mediation through warmth (index of moderated mediation = .36, 95% CI = [.119,.609]), and the mediation effect is stronger in the for-profit conditions (ab = .82, 95% CI = [.624, 1.039]) compared with the nonprofit conditions (ab = .46, 95% CI = [.299,.632]).Then, to explore the role of alternative explanations, we entered all the potential mediators (warmth, competence, bragging, authenticity, and novelty) in parallel into the moderated mediation model. We find that warmth remains a significant mediator in the model (index of moderated mediation = .23, 95% CI = [.079,.408]), suggesting that none of these alternatives ""swamp"" warmth in explaining this effect. Next, we find that the indices of moderated mediation for competence (index of moderated mediation = .08, 95% CI = [.005,.180]), authenticity (index of moderated mediation = .13, 95% CI = [.022,.271]), and novelty (index of moderated mediation = .12, 95% CI = [.025,.231]) were also significant. However, the index for bragging was not significant (index of moderated mediation = −.02, 95% CI = [−.083,.038]). Although competence, authenticity, and novelty were also significant mediators in the model in addition to warmth, warmth remains significant with the largest index of moderated mediation.Because warmth is most frequently compared with competence, we conducted a final analysis statistically comparing the sizes of their indirect effects in a parallel mediation model. We do this within the for-profit conditions in which the effects were more prominent (and because we are unable to statistically compare the full moderated mediation models). We find that the indirect effect for warmth as a mediator was significantly greater than that of competence (p < .001, 95% CI = [.205,.718]). Thus, though other factors may play a role, our results point to warmth as the primary driver of the effects. DiscussionStudy 5 sheds further light on warmth as the primary mechanism underlying brand-to-brand praise. We demonstrate that brand-to-brand praise increases perceived warmth, which enhances brand interest, but this effect is attenuated when the brand is already high in perceived warmth (e.g., nonprofits), as there is less room and need for growth in warmth. Thus, the benefits of praise can be most clearly seen among brands that are perceived as less warm (e.g., for-profits) at baseline. In addition, while perceptions of competence, authenticity, and novelty may play a role in driving brand interest, we show warmth to be the more consistent, primary driver of the effect. Finally, we rule out bragging as an alternative explanation. Study 6: Moderation by SkepticismIn Study 6, we look to another context in which the effects of brand-to-brand praise may be attenuated. Here, we examine skepticism toward advertising as an individual difference that may moderate the effects of praise. Advertising skepticism is an individual difference that has been defined as consumers' chronic doubt or mistrust in a marketer's message ([47]). Individuals who are more skeptical are generally less persuaded by marketing messages and are less trusting of brands. As brands are confronting an ""age of cynicism"" where skepticism is at an all-time high ([21]; [49]) and 71% of consumers report having little faith in brands ([30]), it is very important to understand its effects.As seen in the previous study, the effects of brand-to-brand praise become more prominent when there is room for increasing perceptions of brand warmth. Consumers who are more skeptical of advertising distrust that brands have positive intentions, or warmth, and thus inherently provide brands with greater room to improve in warmth than their nonskeptical counterparts who are already trusting. Thus, brand-to-brand communication may be most effective among skeptics, as it can bypass their cynicism and boost perceptions of warmth. Moreover, such skeptics are generally more persuaded by nonadvertising sources of information and emotional appeals ([48]), which allow marketers to circumvent consumer resistance by decreasing the activation of persuasion knowledge ([27]). Because brand-to-brand praise does not explicitly aim to promote one's brand and instead relies on the more emotional signal that the brand is high in warmth, it is less likely to activate persuasion knowledge and more likely to be accepted. Thus, we predict that consumers high in skepticism will be the most affected by brand-to-brand praise. Study 6 tests this idea utilizing two well-known competitors, Lyft and Uber. Method Participants and designSix hundred participants were recruited from Prolific for this 2 (message: self-promotion, praise) × measured (advertising skepticism) preregistered study.[18] ProcedureParticipants first completed the skepticism toward advertising scale ([47]; 1 = ""strongly disagree,"" and 5 = ""strongly agree"") and then completed filler items. Next, participants saw Lyft's Twitter page, where they read either a self-promotion tweet (""Congratulations to us on all our achievements this past year!"") or a praise tweet toward Uber (""@Uber Congratulations on all your achievements this past year!""; Web Appendix L). Then, participants completed the same measure of brand attitude, warmth, and competence as in previous studies. We also measured perceptions of bragging, authenticity, and novelty using the same measures as in Study 5 as potential alternative explanations, as well as believability of the stimuli as a covariate. Results Brand attitudeReplicating prior results, an ANCOVA controlling for believability revealed a main effect of message on brand attitude (F( 1, 597) = 22.12, p < .001,  ηp2   = .04), where praise (M = 5.22) outperformed self-promotion (M = 4.74). Next, we find a significant interaction between message and skepticism (skepticism scale reverse coded; β = .21, SE = .10, p = .03; Figure 2) on brand attitude, such that when skepticism toward advertising was higher, the praise message led to a greater increase in brand attitude (Johnson–Neyman point = 1.48, 75.67% of participants; β = .23, SE = .12, p = .05). The effect of message was attenuated at lower levels of skepticism below the Johnson–Neyman point.Graph: Figure 2. Study 6: Moderation by skepticism. MediationWe find that praise significantly boosted perceptions of warmth (Mpraise = 5.36 vs. Mself = 4.99; F( 1, 597) = 13.76, p < .001,  ηp2   = .02), competence (Mpraise = 5.52 vs. Mself = 5.32; F( 1, 597) = 4.88, p = .03,  ηp2   = .008), authenticity (Mpraise = 4.90 vs. Mself = 4.56; F( 1, 597) = 9.46, p = .002,  ηp2   = .02), and novelty (Mpraise = 4.58 vs. Mself = 4.28; F( 1, 597) = 6.59, p = .01,  ηp2   = .01) compared with self-promotion, controlling for believability. Self-promotion was seen as more arrogant than praise (Mpraise = 3.18 vs. Mself = 4.16; F( 1, 597) = 54.23, p < .001,  ηp2   = .08).We then tested for the predicted moderated mediation (PROCESS Model 7, 5000 bootstrap samples, [31]) for the effect of praise on brand attitude through warmth, moderated by skepticism, controlling for believability. While the index of moderated mediation was not significant (index = .10, 95% CI = [−.046,.247]), we find that the indirect effect patterns were in line with our predictions, such that the indirect effect is stronger and significant for those higher in skepticism (+1 SD = 3.17, ab = .31, 95% CI = [.084,.550]) and nonsignificant for those lower in skepticism (−1 SD = 1.30, ab = .12, 95% CI = [−.026,.274]).Next, we test for mediation with all of the potential alternative explanations in parallel (PROCESS Model 4, 5,000 bootstrap samples, [31]). Warmth (ab = .14, 95% CI = [.064,.230]), competence (ab = .04, 95% CI = [.004,.091]), bragging (ab = .09, 95% CI = [.049,.146]), authenticity (ab = .06, 95% CI = [.021,.116]), and novelty (ab = .03, 95% CI = [.004,.054]) mediate the effect of message on brand attitude. However, we again find that the indirect effect for warmth was the largest, pointing to its primary role in causing this effect.Finally, we statistically compared the indirect effects of warmth and competence in the model and find the indirect effect of warmth to be significantly greater than that of competence (p = .001, 95% CI = [.085,.349]), again suggesting that warmth plays the primary role in driving the effect of brand-to-brand praise on brand attitudes. DiscussionIn Study 6 (and in a behavioral replication in Web Appendix M),[19] we show that brand-to-brand praise can actually operate more effectively for people who are generally more skeptical of advertising, as brand-to-brand praise bypasses their suspicions and creates more favorable consequences for the brand. Further, while we find that competence, bragging, authenticity, and novelty play a role in driving the effects of brand-to-brand praise on brand evaluations, we still identify warmth as the primary, more consistent underlying driver. General DiscussionWe investigate how observing brand-to-brand praise affects consumers' brand evaluations and choices. Across a variety of different modes of communication (social media, print advertising, digital advertising), study methods (web scraping; field, lab, and online studies), contexts (consumer products and services), outcomes (brand attitudes, social media and advertising engagement, brand choice, purchase behavior), and praise content, we show that consumers who witness brand-to-brand praise between competitors form more favorable evaluations of the praiser brand than consumers who witness other forms of communication, including typical self-promotion messages, helpful messages, basic brand information, and even outside-industry praise (for a summary of contexts, conditions, and findings across studies, see Web Appendix N). In addition to showing robustness to a variety of praise messages in the presented studies, a preregistered supplemental study demonstrates that the effect is further robust to both general and specific praise (Web Appendix O). Furthermore, the varied study stimuli suggest that this effect is robust to brands in a wide range of industries—including car care, snacks, candy, eyewear, beverages, technology, and transportation—with competitive relationships varying in intensity. In other words, brand-to-brand praise seems to benefit the praiser in less competitively intense relationships (e.g., mom-and-pop popcorn brands) as well as more competitively intense relationships (e.g., PlayStation and Nintendo, Uber and Lyft) in a variety of industries. We trace this effect primarily to the notion that brand-to-brand praise signals a brand's warmth, which leads to improved brand evaluations and affects consumer choices.In addition, we show that this effect only exists when the praise is deemed to be associated with significant cost or risk (Study 3). Importantly, we find that the effects of brand-to-brand praise are diminished in some situations or among some consumers, such as when the brand is already high in warmth (Study 5) or among consumers who are already trusting of brand intent (Study 6). These boundaries provide further evidence for the crucial role that perceptions of warmth play in driving the benefits of praise. While other mechanisms may also play a role, as consumer behaviors are generally multiply determined ([35]; [55]), we find warmth to be the most consistent, primary driver of the effects of brand-to-brand praise. Theoretical ContributionsOur research makes several theoretical contributions to literature streams on brand perceptions and relationships, brand communication, and praise. First, we contribute to the brand perception literature by showing that consumers' perceptions of brands can be affected by viewing a brand's interactions with other brands. We demonstrate that observing brand-to-brand praise can positively affect perceptions of a brand's warmth and influence subsequent brand evaluations. Second, we add to the warmth and competence literature by introducing a novel context in which brand-to-brand praise increases warmth without harming perceptions of competence. Third, our research demonstrates that directing positive attention toward the competition instead of toward one's own brand brings about benefits for the praiser brand, unlike what brands would typically do in comparative advertising and two-sided messaging campaigns (e.g., [ 4]; [17]; [40]). Finally, we contribute to the literature on praise by identifying brand-to-brand communication between competitors as a feasible form of praise that is less likely to induce suspicion compared with praise in traditional person-to-person contexts. Marketing Implications and Future ResearchAs we have noted, our findings may be surprising to practitioners who have been regularly and reasonably advised to avoid bringing positive attention to their competitors. However, our studies show that in some circumstances, praise is a method of brand-to-brand interaction that can result in beneficial consequences for the praising brand. Managers might consider offering compliments to competitors to boost their own brand evaluations. In other words, brands can expand from solely focusing on brand-to-consumer relationships to also focusing on their brand-to-brand relationships. This is akin to the positive reactions that politicians sometimes receive when positively acknowledging their opponent ([12]). We suggest a new context in which positive acknowledgment can benefit brands. With the rise of the digital age, brands can easily ""speak"" with each other and be observed by consumers. While it is not uncommon for brands to speak via ""feuds"" on social media, such as when Wendy's teases McDonald's for using frozen beef ([19]), we show in a supplemental study (Web Appendix P) that positive communication provides unique advantages. Negative or snarky communication, directed at a competitor or even directed at the self (as in the case of two-sided messaging), does not provide the same increase in brand evaluations. Although there will likely be variation depending on the exact content and cleverness of snarky communication (a ripe area for future research), marketers would be wise to consider opportunities for brand-to-brand praise instead, perhaps utilizing social media as a platform, to foster a warmer image.Brand-to-brand praise may also be a valuable way to respond to competitors' actions. While the norm for companies responding to a competitor's new product release is to avoid saying anything that would bring attention to that competitor, our studies suggest that responding positively can increase purchases for the praiser brand without boosting the competitor to the same degree. Importantly, while prior work has shown that a brand's positioning as an underdog or market leader has important implications ([50]), brand-to-brand praise may be appropriate regardless of the competitor's market status. In a preregistered supplemental study (Web Appendix Q), we find that the favorable effects of brand-to-brand praise on brand evaluations are robust to the market leadership status of the brand. Future research could further explore how characteristics about the firm such as market leadership or firm size may factor into the effect of brand-to-brand praise.Similarly, while the variety of brands leveraged in our studies suggest that brand-to-brand praise is likely beneficial when directed toward both less intense (e.g., local popcorn brands) and more intense (e.g., PlayStation and Nintendo, Uber and Lyft) competitors, future research should more systematically explore the role of competitive intensity. Such research might explore brand-to-brand praise with indirect versus direct competitors, which would likely vary in levels of perceived costliness. Indirect competitors may include other brands that are in a similar industry but do not compete directly in the same product category, such as in the case of soda and water in the drinks industry. In Study 3, we find that praise toward a direct competitor is deemed to be costly and results in better brand evaluations compared with praise toward noncompetitors. However, would praise toward an indirect competitor be perceived to be costly enough to increase evaluations? Outside of direct competition, what else determines whether praise is deemed to be costly or not? How would consumers react if complementary brands, such as soda and popcorn brands, praised each other? While we suggest that praise should benefit the praiser as long as it is perceived as costly, understanding what kind of brand-to-brand praise is considered costly may be beneficial to marketers.In addition, future research could also explore how brand-to-brand praise compares to other types of messages that convey warmth. For example, brands often communicate prosocial messages, such as a food brand showing active support for a local food bank. Brands might also post self-deprecating messages that recognize their own room for improvement. While these types of messages may signal warmth, they may not benefit the messenger brand in the same way as brand-to-brand praise does because of the unique aspect of costliness associated with praising a direct competitor. Future research could compare these various types of brand messaging to find ones that benefit brands the most.Future work might also examine whether certain brand personalities benefit more from praise than others. Our for-profit versus nonprofit results in Study 5 hint that brands with personalities that are inherently associated with warmth (e.g., sincere brands) may benefit less. Relatedly, are there circumstances under which snarky interactions or backhanded compliments are better suited for some brands, such as Wendy's ([15])? We noted that negativity did not fare as well as positivity (Web Appendix P), but under what conditions might this be different? Cultural differences, such as collectivism and individualism, may also play an important role. Collectivists, known for valuing community and kinship, may be particularly likely to value the warmth associated with brand-to-brand praise ([ 3]). Furthermore, future work could also explore when the effects of brand-to-brand praise are affected by gender. While women may value warmth more in some circumstances ([66]), we do not find consistent moderation by gender in our studies (Web Appendix R), which is consistent with prior work showing that gender does not always moderate brand warmth perceptions ([ 5]). Further research is needed to better understand the possible role that factors such as brand personality, consumer culture, and gender play in affecting brand-to-brand praise responses.Another direction for future research involves investigating the optimal frequency of brand-to-brand praise. Although we demonstrate that some repetition may be acceptable (Study 4), one limitation of this research is the focus on a single instance of praise in a short span of time. Praise when repeated over time may become less effective or even backfire. Excessive praise may trigger suspicion in observers (e.g., [23]), and previous research has shown that suspicious praise can lead to the praiser being perceived as less sincere ([11]; [44]). Thus, future research might explore the optimal levels or repetition of praise and the consequences of excessive praise to prevent such a backfiring effect.To further prevent backfiring effects, future research might also explore what happens if the praised competitor leverages the praise ""against"" the praiser? For instance, what if a praised brand publicly suggests that they are so great that even their competition praises them? It is possible that, if used by the competitor in this manner, the praise could damage the praiser brand. However, it is also possible that this kind of act could be perceived as arrogant or manipulative, damaging the praised brand and eliciting sympathy for the praiser.Overall, this work raises the question: Are brands missing an opportunity to build positive relationships with consumers by not (publicly) building positive relationships with competitors? We suggest that marketers would be wise to explore the intriguing benefits of brand-to-brand praise. Supplemental Materialsj-pdf-1-jmx-10.1177_00222429211053002 - Supplemental material for Befriending the Enemy: The Effects of Observing Brand-to-Brand Praise on Consumer Evaluations and ChoicesSupplemental material, sj-pdf-1-jmx-10.1177_00222429211053002 for Befriending the Enemy: The Effects of Observing Brand-to-Brand Praise on Consumer Evaluations and Choices by Lingrui Zhou, Katherine M. Du and Keisha M. Cutright in Journal of Marketing  "
3,"Communication in the Gig Economy: Buying and Selling in Online Freelance Marketplaces The proliferating gig economy relies on online freelance marketplaces, which support relatively anonymous interactions through text-based messages. Informational asymmetries thus arise that can lead to exchange uncertainties between buyers and freelancers. Conventional marketing thought recommends reducing such uncertainty. However, uncertainty reduction and uncertainty management theories indicate that buyers and freelancers might benefit more from balancing—rather than reducing—uncertainty, such as by strategically adhering to or deviating from common communication principles. With dyadic analyses of calls for bids and bids from a leading online freelance marketplace, this study reveals that buyers attract more bids from freelancers when they provide moderate degrees of task information and concreteness, avoid sharing personal information, and limit the affective intensity of their communication. Freelancers' bid success and price premiums increase when they mimic the degree of task information and affective intensity exhibited by buyers. However, mimicking a lack of personal information and concreteness reduces freelancers' success, so freelancers should always be more concrete and offer more personal information than buyers. These contingent perspectives offer insights into buyer–seller communication in two-sided online marketplaces. They clarify that despite, or sometimes due to, communication uncertainty, both sides can achieve success in the online gig economy.Keywords: business-to-business exchange; gig economy; multisided platforms; online freelance marketplaces; text analysis; uncertainty managementOnline freelance marketplaces, such as Upwork, Fiverr, and PeoplePerHour, have prompted massive transformations in business-to-business (B2B) markets ([13]; [82]). In particular, they allow buyers to post gigs, or short-term service projects, which initiate reverse auctions whereby interested freelance workers submit bids to offer their services ([39]). In these digital environments, buyers and freelancers often devote rather limited time and attention to detailed assessments and instead make choices on the basis of rational value expectations or prices ([ 2]). In addition, online freelance marketplaces suffer from information asymmetries because they rely on text-based messages, which can create uncertainty and hinder the exchange ([34]; [77]). Imagine a buyer wants to hire a freelancer to optimize their pet website's search rankings, so they post a call for bids, requesting ""someone for an SEO job."" In response, Freelancer A might vaguely promise, ""I have plenty of experience writing content that users find interesting to improve the quality and quantity of your traffic,"" whereas Freelancer B more concretely states, ""I have four years of experience writing articles and blogs that engage users and are SEO-friendly. For example, I could focus on interest pieces like the everyday lives of pets."" The communication of both the buyer and freelancer create different degrees of uncertainty, likely impacting who applies and who gets hired.Uncertainty regarding communication can lead to various negative outcomes on both sides, including high rates (more than 50%) of service gigs that go unfulfilled ([36]), diminished bid success, and less-than-optimal pricing for freelancers ([ 2]). However, parties in B2B exchanges can also strategically leverage uncertainty in their communication to achieve more effective outcomes, such as when negotiators conceal information ([69]) or when ambiguous contracts help reduce litigation concerns and increase cooperation ([81]). Buyers and freelancers on online freelance marketplaces engage in a form of B2B exchange, so we propose that they similarly might balance their communication efforts by strategically reducing and increasing uncertainty to maximize their exchange success. In our previous example, by staying vague and without any specific direction from the buyer, Freelancer A might be trying to keep multiple options open and avoid overpromising outcomes.In addition to fundamental questions regarding how to manage uncertainty in B2B exchanges ([50]; [63]), we seek to address the role of communication in such exchanges ([ 7]; Rajdeep et al. 2015; see also Web Appendices A and B). We integrate uncertainty reduction theory ([ 6]) and uncertainty management theory ([ 9]) to predict that, in online freelance marketplaces, various strategies for reducing and increasing the ability of message recipients to anticipate message senders' meaning and actions can benefit the exchange ([ 8]). Using Grice's ([27]) communication principles, we argue that greater provision of task and personal information might reduce uncertainty in service exchanges ([54]) but could also lead to information overload or disagreements ([18]; [40]). Presenting information in a more concrete (cf. abstract) manner or with greater affective intensity also can reduce uncertainty ([27]; [28]; [61]). But again, too much concreteness or affective intensity might lead to restrictive communication that hinders exchanges ([18]; [37]).We apply this theoretical reasoning to exchanges in online freelance marketplaces, in which buyers post calls for bids to attract as many freelancers as possible to apply ([36]). These buyers face a trade-off between reducing uncertainty for freelancers (e.g., providing more information, using less ambiguity) and still efficiently granting them sufficient interpretative freedom. Theorists concur that principles for using relevant information or less ambiguity often get deliberately flouted in conversation, such as when an individual is attempting to save face ([23]) or please a counterpart ([44]). If different communication strategies might entice more freelancers to bid, buyers could establish optimal designs for calls for bids.In response to those calls for bids, freelancers write and submit their bids. In doing so, these freelancers also must manage uncertainty. Thus, they might benefit from matching or mimicking the communication approach adopted by the prospective buyer that issued the call ([78]). Communicative mimicry can evoke similarity perceptions, which tend to increase receivers' sense of rapport and reduce their uncertainty ([75]). Research on adaptive selling recommends matching the buyer's communication (e.g., [57]; [72]). However, in some situations, deviations also may be beneficial ([ 1]), so we consider a more nuanced distinction related to the level at which the similarity occurs. Furthermore, if freelancers compete on price, they may become enmeshed in a self-defeating value trap ([34]; [76]) in which they win more bids but earn less revenue. Strategically mimicking or deviating from a buyer's communication might provide a viable means to winning more gigs without being trapped. We accordingly suggest how freelancers should calibrate their bid formulations to improve their bid success and achieve a price premium.Using a unique, large-scale data set of calls for bids and bids, obtained from a leading online freelance marketplace, along with a series of multilevel models that account for endogeneity, we establish three main contributions. First, we determine the effects of buyers' strategic communications in two-sided online marketplaces ([ 7]). Rather than uncritically recommending that communication should always be informative and unambiguous, we specify the diminishing, even adverse consequences that can result if buyers relay too much task or personal information in a very concrete, intense manner. Second, in an extension of research into adaptive selling ([57]; [78]), we reveal how freelancers' dyadic communicative mimicry affects bid success. Mimicry effects are contingent on the communicative aspect and the buyer's relative uses of each aspect. As we show, mimicking buyers in terms of the provision of task information and use of affective intensity increases bid success. In contrast, we find that freelancers should always offer more personal information and be more concrete in their bid formulations than buyers' calls for bids. Third, we offer insights into how freelancers can avoid predatory pricing ([13]) and escape a value trap ([76]). By strategically managing uncertainty according to the information communicated, and by managing the manner in which they do so, freelancers can earn price premiums. Online Freelance Marketplace ExchangesOnline freelance marketplaces that feature reverse auctions rely on a three-stage process ([34]; [39]). First, in seeking a suitable freelancer, a buyer describes a gig or short-term service project in a call for bids. Second, multiple freelancers apply by formulating and submitting bids that describe themselves, the service offering, and the price requested. Third, the buyer compares the bids and selects a freelancer to complete the project. The outcome of each stage defines exchange success. That is, buyers' success results from a large pool of viable freelance offers. A higher number of bids increases the chances of finding a suitable freelancer for the gig ([35], [36]). Freelancers' success depends on whether their bids are chosen, preferably at a price premium ([13]; [32]). In this context, a price premium is the monetary amount in excess of the buyer's original payment offer (i.e., expected price; [26]; [73]). Buyers might pay a premium beyond their original payment offer for various reasons, including their willingness or ""need to compensate the seller for reducing transaction risks"" ([ 2], p. 248). In competitive online marketplaces, freelancers also might encounter value traps in which they wind up selling more of their services at a lower price ([ 2]; [76]). In this sense, freelancers' success depends on winning the bid but also earning price premiums (or avoiding discounts). Unlike traditional B2B exchanges, buyers' and freelancers' success hinges on textual communication ([13]; [36]). Comparing theories on uncertainty and the role of communication in producing or reducing it, we delineate how both buyers and freelancers may best strike a balance between providing more information and reducing ambiguity versus preserving some uncertainty and maintaining interpretative flexibility. Conceptual BackgroundUncertainty reduction theory ([ 6]) and uncertainty management theory ([ 9]) draw on a central tenet of information theory ([71])—namely, that communication, information, and uncertainty are inextricably linked. Thus, uncertainty is inherent to any interaction. [22] suggests uncertainty depends on the ability to draw inferences from provided information content and the manner in which it is provided. Whereas uncertainty reduction theory predicts how communication can reduce uncertainty, uncertainty management theory examines how people cope with uncertainty, which may include efforts to increase uncertainty to attain beneficial outcomes ([ 8]). Our conceptual development relies on these fundamental principles. Communication Principles in Online Freelance MarketplacesIn online freelance marketplaces, buyers and freelancers depend on one another; all else being equal, they want their mutual exchange to succeed. In such interactions, [27] suggests that four generalized cooperative communication principles (or maxims) apply. Three principles refer to what should be said: the quantity of information (""give as much information as is required and no more than is required""), its quality (""do not say what is false or that for which you lack adequate evidence""), and its relevance. The fourth principle, manner (be clear and avoid ambiguity), pertains to ""how what is said is to be said"" ([27], p. 46). In our study context, neither a buyer nor a freelancer can know upfront whether the other party might be lying, so truthfulness would have to be assumed prior to the exchange. We also highlight that information does not have to be ""correct"" to influence uncertainty perceptions ([ 9]). Therefore, among the four maxims, we focus on the quantity of relevant information that buyers and freelancers offer and the manner in which they present it. Uncertainty Implications of Communication PrinciplesCommunication outcomes are fundamentally uncertain ([ 6]). When people vary their use of communication principles ([27]), they create conversational implications such that message recipients must infer what speakers are trying to imply with their wording. Accordingly, the (un)certainty that buyers and freelancers encounter while making inferences should depend on the degree to which calls for bids and bids provide relevant information in an unambiguous manner, though the meaning of relevant information varies by context. In line with prior research (e.g., [ 7]), we define this degree as the proportion of specific lexical terms used relative to the total number of words in a message.More information reduces uncertainty ([ 6]) and increases receivers' perceptions of the information's value ([79]). In service exchanges, the parties seek information about the task and the person who will complete it ([54]). A greater degree of task information should reduce uncertainty about functional service aspects ([54]). By self-disclosing greater degrees of personal information, a sender also provides a receiver with more information about the self ([15]). In line with the quantity principle ([27]), sparse provision of relevant task and personal information would make it difficult for the receiver to anticipate outcomes or distinguish among options, thus creating uncertainty ([19]).Regarding the principle of manner ([27]), greater degrees of concreteness and affective intensity should reduce ambiguity and enhance clarity. Concrete terms describe something in a perceptible, precise, specific, or clear manner ([11]; [49]; [61]). A greater degree of concreteness reduces ambiguity because it makes it easier for receivers to perceive or recognize what the message sender is implying ([11]; [28]; [61]). Affective intensity reflects the proportion of affective terms included in a message. More affective terms as a proportion of the total word count produce a greater degree of affective intensity, which increases receivers' ability to make evaluative judgments ([28]; [37]).[ 5] We provide examples of these principles in Table 1.GraphTable 1. Communication Elements, Links to Uncertainty, and Examples. Communication ElementDefinitionLink to UncertaintyExampleTask informationA content element of communication. In service exchanges, it is conveyed through functional, duty terms (Ma and Dubé 2011). The proportion of task terms to the total number of words in a message defines the degree of task information.Greater (lesser) degrees of task information decrease (increase) uncertainty.Sparse degree of task information: ""I saw your project description and I would like to work for you. I have plenty experience in different settings where I have written content which users find interesting.""Dense degree of task information: ""I saw your project description and would like to write the content for your website. I have experience in writing articles, blogs & E-books which is user engaging and SEO friendly as well.""Personal informationA content element of communication that is conveyed through self-disclosing terms (Derlega, Harris, and Chaikin 1973). The proportion of self-disclosing terms to the total number of words in a message defines the degree of personal information.Greater (lesser) degrees of personal information decrease (increase) uncertainty.Sparse degree of personal information: ""Saw your project description and would like to write the content for your site. I have experience in writing articles, blogs & E-books which is user engaging and SEO friendly as well.""Dense degree of personal information: ""I saw your project description and I would like to write the content for your site. I have 12 years of work experience in copy writing for articles, blogs & E-books. I have a Master's in Journalism and have worked fulltime for companies like Adobe.""ConcretenessA manner element of communication conveyed by terms that are perceptible, precise, or specific (Brysbaert, Warriner, and Kuperman 2014; Packard and Berger 2020). The proportion of concrete terms to the total number of words in a message defines the degree of concreteness.Greater (lesser) degrees of concreteness decrease (increase) uncertainty.Sparse degree of concreteness: ""I noticed your project description and I would like to do work on it. I have plenty of experience in scripting text, which is engaging, compelling, and SEO friendly.""Dense degree of concreteness: ""I saw your posted project description on Upwork, and I would like to write the contents for your website. I have a lot of experience in article and weblog writing in an SEO friendly fashion.""Affective intensityA manner element of communication that is conveyed through affective terms (Hamilton and Hunter 1998). The proportion of affective terms to the total number of words in a message defines the degree of affective intensity.Greater (lesser) degrees of intensity decrease (increase) uncertainty.Sparse degree of affective intensity: ""I saw your project description and I can write the required content for your site. I have plenty of experience in writing articles, blogs & E-books which is user engaging and SEO friendly as well.""Dense degree of affective intensity: ""I liked your project description and would be happy to write the content for your site. I have great experience in writing articles, blogs & E-books which is user engaging and SEO friendly as well.""  Reducing and Maintaining Uncertainty in Communication ExchangesCross-disciplinary research provides ample evidence that conversational partners generally prefer to reduce uncertainty ([ 5]). In B2B relationships, reducing uncertainty increases exchange effectiveness ([29]; [50]; [63]). In Web Appendix A, we offer an overview of some key empirical marketing studies on B2B communication aspects. Specifically in online freelance marketplaces, which are relatively anonymous, the required coordination and dependence between rational buyers and freelancers may increase their need for information and clarity ([13]; [34]). Thus, for example, reputation cues commonly appear in online freelance marketplaces as a way to reduce uncertainty and facilitate exchanges ([34]). More broadly, reducing uncertainty by adhering to [27] principles in dyadic buyer–freelancer communications may boost exchange success.However, people experience uncertainty differently and do not always prefer to reduce it ([ 8]). Instead, according to uncertainty management theory ([ 9]), strategic communication choices that might not minimize uncertainty, and even cultivate it, can be effective and lead to better outcomes for consumers ([38]), organizations ([18]; [31]), and interorganizational governance ([81]). For example, [38] find that a lack of concreteness aids consumers' initial online searches because such vague queries return a greater variety of search results. In collective bargaining settings, seasoned negotiators use concealment and ambiguity to enhance the likelihood of agreement ([69]). In B2B exchanges, parties can use less information and more ambiguity strategically to accomplish specific goals ([ 3]; [81]). Even if such efforts are not universally favored, uncertainty-cultivating communication provides benefits by allowing different receivers to perceive multiple different meanings simultaneously ([18]). Moreover, communication theorists concur that people sometimes deliberately flout or violate [27] conversation principles, such as when they intentionally maintain uncertainty to save face ([23]) or please a counterpart ([44]). Subverting the principles is not necessarily less cooperative, and furthermore, the purpose of communication is not always to be as informative and clear as possible. Arguably, cooperative principles encourage reasonable adherence, not compulsion. Thus, strategically allowing recipients to develop a broader range of possible interpretations by maintaining some level of uncertainty might facilitate buyer–freelancer exchanges. Research Propositions Managing Freelancers' Uncertainty in Calls for BidsFreelancers choose whether to offer their services in response to a buyer's call for bids. The number of freelancers who choose to do so is consequential for the buyer, as more bids implies a greater likelihood of finding a suitable service provider ([36]). Managing freelancers' uncertainty through relevant information provision and the manner of communication in the calls for bids should influence freelancers' decisions to apply. Relevant informationIn calls for bids, buyers can vary the degree of task and personal information included in the description of the gig. If freelancers evaluate this information favorably, they develop more positive dispositions and are more likely to apply ([72]). As prior research establishes, more information enhances communication outcomes in business settings by reducing uncertainty. For example, studying web forums, [79] indicate that the breadth of information provided by a sender affects receivers' objective judgments of the value of that information. [49] find that greater degrees of monetary information increase peer-to-peer lending, and [41] shows that more task information increases the time and commitment sellers allocate to a buyer. Greater degrees of personal information also reduce uncertainty, increase trust ([55]), and enhance performance on crowdsourcing platforms ([68]). Such self-disclosure can strengthen ongoing buyer–seller relations as well ([14]). In contrast, a greater proportion of nonrelevant information (i.e., a lesser degree of relevant information) increases uncertainty ([ 9]). Because greater degrees of task and personal information in calls for bids help reduce freelancers' uncertainty, freelancers who believe they qualify for the gig should be more willing to submit bids.However, excessive relevant information may be ineffective, even if it reduces freelancers' uncertainty. That is, if buyers provide excessive details about the task, the gig may appear too restrictive or prescriptive ([18]), which might not appeal to freelancers. For example, leaving detailed information out of contracts ([21]) or negotiations ([69]) represents a tactic for improving exchange performance. In a downsizing context, a greater degree of information provision can increase uncertainty and negative reactions ([31]). For freelancers, excessive information can feel overwhelming and can limit their motivation, opportunity, or ability to process the information and submit bids ([40]). A buyer that self-discloses a high amount of personal information might also appear less attractive as a prospective business contact ([12]). Because extensive self-disclosures are unusual in initial B2B online exchanges ([46]), such disclosures might be perceived as inappropriate ([59]).In summary, we argue that moderate degrees of task and personal information in calls for bids relate to more freelancer bids. Buyers who provide greater degrees of task and personal information should attract more bids, but beyond a moderate degree (i.e., a very dense provision of relevant information), providing still greater degrees of task and personal information may decrease the number of bids. We thus propose a curvilinear relationship: P1:  Extremely sparse and extremely dense degrees of (a) task and (b) personal information in calls for bids yield fewer freelance bids than moderate degrees. Communication mannerIn calls for bids, buyers can vary the concreteness and affective intensity with which they describe the gig. Researchers disagree about whether more or less ambiguous communication leads to more efficacious speech ([ 8]; [18]; [37]), but in an online freelance marketplace, we posit that buyers must reduce ambiguity to at least some extent by being more concrete and intense. Greater concreteness and affective intensity can be more efficient because recipients can process the information with less time and effort ([37]; [61]). These approaches also tend to result in communication that is more persuasive, memorable, and accessible than communication that uses predominantly abstract or unemotional wording ([28]; [37]). In other settings, greater concreteness increases consumer satisfaction with employee interactions and purchase likelihood ([61]). Greater degrees of intensity achieved through proportionally more affective words provide accessible, diagnostic signals to customers ([52]). They can also sway business partners' decisions when used as inspirational appeals ([72]). Finally, greater concreteness and affective intensity provide heuristic cues that allow freelancers to take mental shortcuts, which makes them more likely to bid ([37]).However, if the calls for bids appear too concrete or too intense, the task might appear narrow, which reduces the appeal of performing the gig ([37]). [80] finds that greater vagueness (i.e., less concreteness) can enhance judgments of a speaker's character, message acceptance, and recall. Moreover, some research asserts that reducing uncertainty with more concrete formulations is ineffective ([ 9]; [18]), so managers instead should embrace strategic ambiguity to allow for interpretative freedom ([43]). In contracts, unexpected specificity even increases ex ante costs ([58]). Contrastingly, greater task ambiguity can lower costs as well as reduce the risk of litigation and enhance cooperation in B2B exchanges ([81]). Greater degrees of concrete terms in communications with investors also can have adverse effects ([64]), and excessive degrees of positive affective words diminish the impact of customer reviews ([52]). Thus, we predict a stylistic trade-off: Overly ambiguous calls for bids, lacking any concreteness or affective intensity, may undercut buyers' success in attracting freelancers, but some degree of ambiguity (i.e., avoiding overly concrete, affectively intense communication) can allow for divergent interpretations to coexist. Thus, moderate degrees of concreteness and affective intensity may be most effective in encouraging freelancers to bid. P2:  Extremely sparse and extremely dense degrees of (a) concreteness and (b) affective intensity in calls for bids yield fewer freelance bids than moderate degrees. Managing Buyers' Uncertainty in BidsBuyers also face uncertainty when deciding whom to hire and how much to pay ([ 2]; [13]). By managing these uncertainties through their bids, freelancers can affect their chances of winning bids and their price premiums. To establish relevant predictions, we integrate [27] communication principles with uncertainty research such that we anticipate a greater provision of relevant information communicated with greater concreteness and affective intensity allows buyers to draw inferences from freelancers' bids with more certainty. Beyond these communication principles, [ 6] suggest that perceived similarity to a message sender reduces receivers' uncertainty. Thus, both purchase likelihood and buyers' willingness to pay a price premium might be influenced by freelancers' adherence to certain communication principles, as well as by their communicative similarity to the buyer. Winning bidsIn other exchange contexts, research has established that when service employees relay greater degrees of service or personal information ([51] 2015; [62]), it improves customers' intentions to purchase. Willingness to purchase also increases if employees use greater concreteness in online service chats ([61]) or greater degrees of affective words in their emails ([72]).However, the dense provision of relevant information in a bid risks information overload ([40]), and a freelancer being overly concrete or intense might signal a restrictive, narrow approach to the gig ([37]). Our reasoning here parallels that for buyers' formulations of calls for bids. We thus similarly predict that moderate degrees of task and personal information provided in a moderately unambiguous manner (i.e., moderate degrees of concreteness and affective intensity) enhance freelancers' chances of winning the gig.Yet preferences for uncertainty also might be situational and dispositional ([ 9]), as reflected in buyers' own communicative choices ([30]). Specifically, calls for bids can reveal buyers' expectations and preferences for communication behaviors. For example, buyers might like to get to know freelancers, or they may prefer to keep their business relationships impersonal. The extent to which they disclose their own personal information in calls for bids should signal these preferences. An ambiguous bid offered in response to an ambiguous call for bids might lead the buyer to conclude that the freelancer is tactful, sensitive, and noncoercive ([10]). Adaptive communications also raise perceptions of credibility, common social identity, approval, and trust ([52]; [75]), as well as similarity perceptions, all of which in turn reduce uncertainty ([ 6]). Crafting responses that mimic the buyer's communication is a common personal selling recommendation ([78]). As [72] show, when sellers mimic buyers' communicative manner, it increases buyers' attention. Accordingly, freelancers who mimic a buyer's communication content and manner might improve their exchange success.In some situations, though, deviating from buyers' communications may be more beneficial ([ 1]). Even in studies that note the performance benefits of adaption, researchers highlight the importance of the degree of adaptivity (e.g., the degree to which salesperson behaviors adjust for each customer during interactions; [78]). Similarly, studies of communication accommodation investigate the degree of accommodation used ([75]). Extending these insights, the outcomes of adaptation likely depend on communication levels (e.g., very informative vs. not informative). In keeping with uncertainty reduction theory, we expect that buyers are less likely to hire freelancers whose bids offer sparse information and are very ambiguous, even if the call for bids has these characteristics. P3:  When the degrees of (a) task and (b) personal information, (c) concreteness, and (d) affective intensity provided by the buyer are at least moderate (sparse), freelancers can increase (decrease) their chances of bid success by mimicking buyers' communications. Achieving price premiumsBuyers' uncertainty about a freelancer should influence their willingness to pay a price premium ([ 2]). Although there are many reasons for price variations ([26]) in online freelance marketplaces, buyers compensate (penalize) freelancers for reducing (increasing) their transaction uncertainty by deciding to accept a price above (below) their original payment offer ([ 2]). In line with [70], freelancers' greater provision of relevant task and personal information in a more concrete and intense manner in bids likely reduces buyers' information asymmetry and exchange-specific risks. Therefore, buyers who want to transact with high certainty may render a price premium for such bids ([51] 2015).The degree to which freelancers mimic buyers' communication also may influence the price premium. For example, [60] find that adaptive approaches for different customers help salespeople increase those customers' willingness to pay a price premium. However, in line with our arguments regarding bid success, we expect that the positive influence of a freelancer's communicative mimicry depends on the specific degree to which the buyer uses a specific communicative element. This reasoning aligns conceptually with the communication principles ([27]), the recommendation that uncertainty should be carefully managed ([ 8]), and the benefits of mimicry identified in studies of communication accommodation ([75]) and adaptive selling ([78]). However, we know of no studies that consider price premium implications of communicative trade-offs between reducing buyers' uncertainty and adapting to buyers' communication. In addition, we are not aware of any research that considers the possible negative effects when sellers mimic buyers who provide less task and personal information, are less concrete, or sparsely use affective intensity.Buyers who want to transact with high certainty might render a price premium to freelancers who reduce uncertainty by providing greater degrees of relevant information in a more concrete and intense manner. But if buyers perceive that the provision of relevant information, degree of concreteness, and level of intensity surpasses their own reasonable level, they might feel overloaded or restricted and thus unwilling to pay a premium. We therefore predict that buyers offer a price premium to freelancers who provide degrees of relevant information, concreteness, and affective intensity at a level similar (but never too sparse) to their own communication, as only these bids help reduce buyers' exchange risks. P4:  When the degrees of (a) task and (b) personal information, (c) concreteness, and (d) affective intensity provided by the buyer are at least moderate (sparse), freelancers can increase (decrease) their chances of earning a price premium by mimicking the communication of the buyer.Graph: Figure 1. Effect of buyers' communication on call for bids success. Field Study of an Online Freelance Marketplace Setting and SampleWe conducted a large-scale field study with a proprietary data set of calls for bids and corresponding bids posted on a leading, global online freelance marketplace. The marketplace hosts seven freelance service submarkets: ( 1) design; ( 2) writing and translation; ( 3) video, photo, and audio; ( 4) business support; ( 5) social media, sales, and marketing; ( 6) software and mobile development; and ( 7) web development. The bidding process follows a sequential, sealed-bid reverse auction format, and it concludes when the buyer chooses one winning bid ([34]; [39]). As with recent marketing research that investigates large scales of communication (see Web Appendix B for an illustrative overview), this process depends on and is captured in text data. We used ( 1) text data from 343,796 calls for bids issued by 49,081 buyers (restricted to those who posted at least two gigs) to predict buyers' call for bids success, ( 2) 2,327,216 bids submitted by 34,851 freelancers (restricted to those who submitted at least two bids) to predict freelancers' bid success, and ( 3) 148,158 bids submitted by 30,851 freelancers (restricted to those who won and for which the payment was disclosed) to predict freelancers' price premium. Our multilevel approach required more than one observation (call for bid or bid) in each Level 2 unit (buyer or freelancer); otherwise, Level 2 and Level 1 variance might have been confounded ([74]). Web Appendix C summarizes the definitions and operationalizations, and Web Appendix J provides the descriptive statistics and correlations. Measurement of ConstructsThe number of freelancers who submit bids to offer their services provided the measure of success of buyers' call for bids. More submitted bids increases the probability that buyers can find an appropriate freelancer, whereas failing to find a suitable match is time consuming and costly because it requires further searches and delays the project ([35], [36]). We measured freelancers' bid success as a binary indicator of whether ( 1) or not (0) the freelancer was chosen by the buyer and won the bid ([32]). For freelancers' price premium, we gauged the percentage by which the accepted bid price for the project exceeded (or fell short of) the buyer's original payment offered (i.e., benchmark price; [20]). This operationalization accounted for the difference between the final price a buyer paid and the original price they offered (i.e., what the buyer expected to pay) ([73]).To capture the independent communication variables, we mined the text of each call for bids and each bid. For the preprocessing and extraction steps, we used the R package Quanteda ([ 4]), as well as a combination of newly developed and prevalidated text mining dictionaries. For the degree of task information in each text, we inductively sourced a list of context-specific task descriptor words. To start, we acquired all 34,851 freelancers' service skill tags ([ 7]; for an illustration, see Web Appendix D), which freelancers list in their profiles to describe the service tasks they offer (e.g., ""developer,"" ""illustrator""). After removing stop words and duplicates, two coders reviewed the remaining word list, deleted any misspelled words, and removed terms that did not describe a service (e.g., ""great,"" ""reliable""). Using Quanteda ([ 4]), we stemmed the remaining words, leaving 1,912 unique word stems that describe service tasks. We mined each call for bids and bid, then summed word occurrences reflecting the new task dictionary. By dividing this sum by total words, we obtained a measure of the degree (ratio) of task information in each text. When people self-disclose personal information, they use singular, first-person pronouns. In line with previous research (e.g., [67]), we measured the degree of personal information as the ratio of first-person singular pronoun words (e.g., ""I,"" ""me"") to the total words in each text. To determine the degree of communication concreteness, we mined each text for [11] list of generally known English lemmas that indicate whether a concept denoted by a term refers to a perceptible entity. Following their operationalization, we included all terms that received a rating of 3 or greater on their bipolar, five-point abstract-to-concrete rating scale.[ 6] That is, terms that score 3 or higher refer to relatively more specific objects, materials, people, processes, or relationships. We again divided the sum of the concrete terms by the total words in each text. Finally, the ratio of emotion-laden words (e.g., ""problematic,"" ""easy""; [28]; [37]) determined affective intensity. Using the Linguistic Inquiry and Word Count (LIWC) affect dictionary, we obtained a list of affect words, which we then summed for each text ([66]) and divided each by the corresponding total word count to obtain the degree of affective intensity. Pilot Studies Validity of text-mined measuresTo ensure the validity of our text-mined communication measures, we asked two coders to classify the texts of a random subsample of 100 calls for bids (Mlength = 129 words) and 100 bids (Mlength = 102 words). The coders indicated whether considerable task information, personal information, concreteness, and affective intensity were present in each text ( 1) or not (0). Comparing the coders' classifications with our text-mined classification revealed substantial agreement for both calls for bids (.73 to.94) and bids (.66 to.88) ([47]). The average F1 measure was sufficiently high for both bids (.79 to.95) and calls for bids (.80 to.95), as we detail in Web Appendix F. Experimental evidence of uncertainty reductionTo establish the internal validity of the chosen communication aspects on receivers' uncertainty perceptions, we conducted a series of experimental pilot studies. We used single-factor, within-subject designs for ( 1) task information, ( 2) personal information, ( 3) concreteness, and ( 4) affective intensity. For each pilot study, we recruited between 50 and 53 U.S. consumers with a mean age of 37.6 years (50% women) from Amazon Mechanical Turk (for details, see Web Appendix G). In line with previous research (e.g., [28]; [49]; [55]; [61]), we find that greater use of all four communication aspects in bids significantly reduces buyers' uncertainty perceptions and affects their hiring intentions. Predicting the Success of Buyers' Calls for Bids Model-free evidenceIn Web Appendix H, we summarize the model-free findings. The mean-level comparison indicates that calls for bids with significantly greater degrees of task information and concreteness, as well as significantly lower degrees of personal information and affective intensity, receive more freelance bids than an average call for bids (M = 5). Econometric model and identificationThe success of calls for bids reflects a count variable. Noting the overdispersion in the data (p < .001), we used a negative binomial model instead of a Poisson model. Furthermore, calls for bids are nested within buyers, and thus, the call for bids and number of freelancers who offer their service might be interdependent. The significant between-group variance (p < .001) and ICC( 1) of.27 suggests a multilevel structure. We therefore specified a multilevel model with a random intercept to control for time-invariant unobserved differences between buyers (e.g., education, country, gender) that could relate to differences in their success, using the following base equation: CALSUCij=y00+y01BTASKij+y02BPERSij+y03BCONCij+y04BINTEij+y05BTASK_SQij+y06BPERS_SQij+y07BCONC_SQij+y08BINTE_SQij+μ0j+εij, Graph( 1)where  CALSUCij  is the success of a call for bids i (i = 1, ..., 343,796) issued by buyer j (j = 1, ..., 49,081),  BTASKij  is buyer task information,  BPERSij  indicates buyer personal information,  BCONCij  is buyer concreteness, and  BINTEij  refers to buyer affective intensity in the call for bids. In turn,  BTASK_SQij  is buyer task information squared,  BPERS_SQij  is buyer personal information squared,  BCONC_SQij  is buyer concreteness squared, and  BINTE_SQij  is buyer affective intensity squared. Finally,  μ0j  is the random intercept and  εij  is the error term.Some empirical challenges inhibited a robust model identification, which we addressed in several ways. To account for observed heterogeneity, we incorporated covariates that might influence how many freelancers respond to a particular call for bids. First, in line with extant text mining studies ([ 7]), we controlled for the word count in each call for bids. Second, as a reputation cue, we measured buyer experience as the number of projects a buyer had commissioned previously on the platform prior to posting the focal call for bids ([32]). Third, a higher payment offer may attract more freelancers ([36]), so we determined the payment offered by the buyer in U.S. dollars, multiplied by an undisclosed index for anonymity. We used a dummy for nondisclosed payments, but we replaced missing values with a grand mean to retain the observations. Fourth, we measured project duration, as longer projects attract more freelancers ([36]). A dummy variable indicated whether the project was slated to last more ( 1) or less than a month (0). Fifth, more buyers demanding freelance services at the same time creates a relative shortage of freelancers ([36]). To account for an excess supply of freelancers, we calculated the sum of all active freelancers in the specific submarket of the call for bids and divided by the sum of all calls for bids posted around the same time (±31 days) in the same submarket. Sixth, the marketplace grew over time, so we included fixed effects for the year of the call for bids. Seventh, we included fixed effects for the seven submarkets, since submarkets that feature more complex projects have fewer qualified freelancers.Beyond these observed covariates, buyers' bid formulations might have varied by project characteristics unobservable to us. To the extent that these unobserved project characteristics influenced both the buyers' communication strategies and buyer outcomes, the estimated parameters might be biased. Therefore, we concatenated all service skill tags from the service profile of each freelancer who submitted a bid in response to a specific call. Then, to uncover the latent mixture of project types, we applied a latent Dirichlet allocation model to the project-specific skill tags (e.g., [ 7]; see Web Appendix I). We included the resulting 12 latent project characteristics as fixed effects to account for unobserved heterogeneity.Buyers also strategically make their communication decisions in learned anticipation of a larger number of bids or other factors, which were potentially unobservable to us. This strategic behavior could make communication approaches endogenous ([42]). Because our data did not contain valid, strong instruments for buyers' communications, we adopted [65] approach and used Gaussian copulas to model the correlation between each buyer communication  BCOMij1-4  and the error term. We added regressors to Equation 1, such that BCOMij1−4~=Φ−1[H(BCOMij1−4)], Graph( 2)where  Φ−1  is the inverse of the normal cumulative distribution function and  [H(BCOMij1−4)]  represents the empirical distribution functions of the four buyer communication approaches. The endogenous regressors must be nonnormally distributed for identification ([65]), and we confirmed this was true using Shapiro–Wilks tests (all p < .001). The updated equation to predict buyers' call for bids success, after correcting for endogeneity, was thus CALSUCij=y00+y01BTASKij+y02BPERSijqquad+y03BCONCij+y04BINTEijqquad+y05BTASK_SQij+y06BPERS_SQijqquad+y07BCONC_SQij+y08BINTE_SQijqquad+y09−14CONij1−6+y15−20YEARij1−6qquad+y21−26SUBMij1−6+y27−37PROJij1−11qquad+y38−41BCOM~ij1−4+μ0j+εij, Graph( 3)where  CONij1−5  is the vector of control variables,  YEARij1−6  are year effects,  SUBMij1−6  are submarket effects,  PROJij1−11  are latent project clusters, and  BCOM~ij1−4  are Gaussian copulas. We used a robust estimator to account for correlated and clustered standard errors. Results and discussionThe maximum variance inflation factor is 2.11, indicating no potential threat of multicollinearity. Table 2 contains the results of a main effects model and the full model, and Figure 1, Panels A–D, display the curvilinear effects from the full model. We have proposed that extremely sparse and extremely dense degrees of relevant information, concreteness, and affective intensity in calls for bids yield fewer freelance bids than moderate degrees of these communication elements. In line with our expectations, we find a positive linear effect (.152, p < .01) and negative squared effect for task information (−.026, p < .01), as displayed in Figure 1, Panel A. Moderate levels of the use of task information (50%:.222, p < .01) yield better results than sparse (10%: −.426, p < .01) and dense (90%: −.495, p < .01) uses. Furthermore, we find a positive linear effect (.052, p < .01) and negative squared effect for concreteness (−.080, p < .01) (Figure 1, Panel C). Moderate use (50%:.078, p < .01) yields better results than sparse use (10%: −.092, p < .01) or dense use (90%: −.251, p < .01) of concreteness. Contrary to our expectations, we find a negative linear effect (−.190, p < .01) and a positive squared effect (.032, p < .01) of personal information (Figure 1, Panel B). We also find a negative linear effect (−.084, p < .01) and a nonsignificant squared effect (.001, ns) of affective intensity (Figure 1, Panel D). Thus, it appears that any provision of personal information or greater use of affective intensity by the buyer is always ineffective. As a possible explanation, we note that in B2B online conversations, self-disclosure and emotions may be valued only after business relations have been established, not at the moment they form ([46]). Most of the exchanges in our data were between strangers, rather than being repeat exchanges, so it may be more appropriate for buyers to avoid personal details and appear rational rather than emotive.Graph: Figure 2. Response surfaces for bid success and price premium.GraphTable 2. Predicting the Success of Buyers' Calls for Bids. Model 1:Main EffectsModel 2:Full ModelβSE95% CIβSE95% CIBuyer Communication Task information.123**.003.117,.128.152**.003.146,.159 Personal information−.149**.004−.157, −.141−.190**.004−.199, −.181 Concreteness.040**.003.035,.045.052**.003.046,.057 Affective intensity−.098**.007−.112, −.085−.084**.008−.100, −.068Buyer Communication Squared Task information squared−.026**.001−.028, −.024 Personal information squared.032**.002.029,.035 Concreteness squared−.008**.001−.011, −.007 Affective intensity squared.001.001−.001,.004Controls Word count−.025**.003−.031, −.019−.027**.003−.033, −.021 Buyer experience−.033**.007−.046, −.020−.033**.007−.046, −.020 Project payment.170**.005.159,.181.168**.005.158,.179 Payment not disclosed.073**.005.063,.083.071**.005.061,.081 Project duration.116**.003.110,.122.117**.003.111,.123 Excess supply of freelancers.614**.004.606,.622.606**.004.598,.613Fixed Effects Yearsincludedincluded SubmarketsincludedincludedUnobserved Heterogeneity Project characteristicsincludedincludedEndogeneity Corrections Gaussian copulasincludedincludedBuyers49,081Call for bids343,796 1 Notes: Standardized results. Significance is based on two-tailed tests. The dependent variable is the count of all bids received. The sample included all projects listed by buyers with at least two projects to which at least one freelancer submitted a bid. Effects for years, submarkets, project characteristics, and Gaussian copulas are detailed in Web Appendix Q.2 *p < .05.3 **p < .01.To entice more freelancers to bid, buyers should keep their calls for bids brief (−.027, p < .01 for word count), which emphasizes the need for careful formulations. Higher payment offers (.168, p < .01), longer project durations (.117, p < .01), and an excess supply of freelancers (.606, p < .01) all increase the number of bids. Notably, the number of projects a buyer has previously commissioned relates negatively to the number of freelancers who bid (−.033, p < .01). These experienced buyers might have established relationships with specific freelancers, which reduces other freelancers' chances and causes them to refrain from bidding ([48]). Predicting Freelancers' Bid Success Model-free evidenceBids that offer less personal information and greater task information, concreteness, and affective intensity are more successful in winning projects. Among bids that won, the mean-level comparisons indicate nonlinear effects of mimicry. That is, successful freelancers mimic buyers' use of task information, personal information, and concreteness closely. If a buyer uses very sparse or very dense degrees of these communication aspects, the winning freelancers deviate more, indicating a nonlinear impact of mimicry. We do not find evidence of this mimicry relationship for affective intensity (see Web Appendix H). Measurement of similarityPrevious studies often operationalize communication similarity as the absolute difference between two measures (e.g., [52]; [75]), but this approach suffers some implicit constraints ([17]). In particular, difference scores suggest that one party's communication increases at the same magnitude as the other's decreases. They also ignore the degree at which the relative mimicry occurs. As a preferable alternative, we use polynomial regression, which allows for simultaneous testing of similarity and dissimilarity effects on bid success, at different levels of freelancers' and buyers' uses of the four communication aspects. In their study of positive and negative emotional tone convergence, [24] also use polynomial regression to explore the nuanced effects of convergence in leader–follower relationships on leader–member exchange quality. A simple regression model that captures absolute deviation cannot simultaneously assess the degree of task information by the buyer and the potential nonlinear effects of task information mimicry by the freelancer. So, we performed a polynomial regression with response surface analyses for each communication aspect to capture the extent to which freelancers mimicked a prospective buyer's provision of relevant information and communication manner. We detail this polynomial modeling approach that led to Equation 4 and the calculation of all polynomial terms, using task information as an example, in Web Appendix E. Econometric model and identificationWe tested freelancers' trade-off between adding more uncertainty-reducing communication versus mimicking the buyer's communication in a polynomial regression model that included linear terms, quadratic terms, and interactions. In the multilevel base equation to predict freelancers' bid success (ICC( 1) = .09, p < .001), BIDSUCkl=y00+y01−04FCOMkl1−4+y05−08BCOMkl1−4+y09−12FCOM_SQkl1−4+y13−16(FCOMkl1−4×BCOMkl1−4)+y17−20BCOM_SQkl1−4+μ0l+εkl, Graph( 4) BIDSUCkl  is the success of bid k (k = 1, ..., 2,327,216) by freelancer l (l = 1, ..., 34,851),  FCOMkl1−4  are the four freelancer communication aspects,  BCOMkl1−4  indicate the four buyer communication aspects,  FCOM_SQkl1−4  are freelancer communication aspects squared,  (FCOMkl1−4×BCOMkl1−4)  are interactions of freelancer and buyer communication aspects,  BCOM_SQkl1−4  are buyer communication squared,  μ0l  is the random intercept, and  εkl  is the error term.We incorporated several covariates that might influence freelancers' bid success. As in the buyer model, we controlled for word count, project payment, project duration, and excess supply of active freelancers. We also included fixed effects for years, submarkets, and latent project characteristics. We accounted for the number of projects the freelancer completed prior to submitting the focal bid as a reputation cue that might determine bid success ([32]). Freelancer rating is an average five-point satisfaction rating that a freelancer has received for all completed projects. To retain observations of unrated freelancers, we included a dummy for observations without star ratings and replaced the missing values with a grand mean rating.Several additional controls relate to whether a bid is successful. First, following prior research, we assessed linguistic style matching, or the similarity between each bid and the respective call for bids, across nine function word categories ([52]). Second, we accounted for any previous relationship in which the freelancer had completed at least one project for the same buyer prior to the specific call for bids ([32]). Third, freelancers submit a bid price that may differ from the payment offered by the buyer, and a higher bid price may reduce the likelihood of bid success ([32]). In light of this, we measured each bid price as a ratio between the asking price and the average indexed bid price requested by all competing freelancers for the same call for bids. Fourth, the longer it takes freelancers to submit a bid, the lower their chances of success ([34]). So, we measured time-to-bid as the number of days between the posting of the call for bids and the bid submission. A dummy variable also indicates whether the bid was submitted late ( 1) or on time (0). Fifth, competition for a specific call for bid should impact each bid's success chances, so we controlled for the number of bids for the same call ([34]).Similar to buyers, freelancers make communication decisions strategically in anticipation of higher bid success or other, unobservable factors. Thus, freelancer communication is potentially endogenous, so we again used Gaussian copulas (Shapiro–Wilk tests: all p < .001). The updated equation to predict freelancers' bid success is as follows: BIDSUCkl=y00+y01−04FCOMkl1−4+y05−08BCOMkl1−4+y09−12FCOM_SQkl1−4+y13−16(FCOMkl1−4×BCOMkl1−4)+y17−20BCOM_SQkl1−4+y21−33CONkl1−13+y34−39YEARkl1−6+y40−45SUBMkl1−6+y46−56PROJkl1−11+y57−60FCOM~kl1−4+y61−64BCOM~kl1−4+μ0l+εkl, Graph( 5)where  CONkl1−14  is the vector of control variables,  YEARkl1−6  are year effects,  SUBMkl1−6  are submarket effects,  PROJkl1−11  are latent project clusters,  FCOM~kl1−4  are Gaussian copulas for bid text, and  BCOM~kl1−4  are Gaussian copulas for calls for bids text. Results and discussionThe maximum variance inflation factor is 3.86, indicating no threat of multicollinearity. Table 3 contains the results of the freelancer bid success models, Web Appendix K summarizes the response surface coefficients, and Figure 2 displays these coefficients on three-dimensional surfaces, reflecting relationships among freelancer communication, buyer communication, and bid success. We also highlight the misfit line used to explore the trade-off between exceeding and falling short of buyers' communication levels.GraphTable 3. Predicting Freelancers' Bid Success. Model 3:Freelancer CommunicationModel 4:Full ModelβSE95% CIβSE95% CIFreelancer Communication y01: Task information.014**.001.013,.015.015**.001.014,.016 y02: Personal information.018**.001.016,.019.017**.001.016,.017 y03: Concreteness.030**.001.029,.031.031**.001.030,.032 y04: Affective intensity.001.001−.001,.003.000.001−.002,.001Buyer Communication y05: Task information−.009**.000−.009, −.008 y06: Personal information−.017**.000−.018, −.017 y07: Concreteness−.008**.000−.009, −.008 y08: Affective intensity.001**.000.001,.002Freelancer Communication Squared y09: Task information squared−.006**.000−.006, −.005−.006**.000−.007, −.006 y10: Personal information squared−.005**.000−.005, −.004−.005**.000−.005, −.004 y11: Concreteness squared−.006**.000−.007, −.006−.007**.000−.007, −.007 y12: Affective intensity squared.000**.000.000,.001.000**.000.000,.001Freelancer–Buyer Interactions y13: Task information interaction.015**.000.015,.016 y14: Personal information interaction−.002**.000−.002, −.001 y15: Concreteness interaction.005**.000.004,.005 y16: Affective intensity interaction.020**.000.020,.021Buyer Communication Squared y17: Task information squared.001**.000.001,.002 y18: Personal information squared−.004**.000−.005, −.004 y19: Concreteness squared.001**.000.001,.001 y20: Affective intensity squared.000*.000.000,.000Controls Word count−.022**.001−.024, −.021−.021**.001−.022, −.020 Linguistic style matching.051**.001.048,.053.051**.001.048,.053 Freelancer experience.002**.001.001,.003.002**.001.001,.003 Freelancer rating.010**.001.009,.010.010**.001.009,.010 Project payment−.001**.000−.001, −.001−.001**.000−.001, −.001 Payment not disclosed−.028**.000−.029, −.028−.029**.000−.030, −.028 Previous relationship.078**.001.076,.081.078**.001.075,.080 Bid price−.006**.000−.006, −.006−.006**.000−.007, −.006 Time-to-bid.001.000.000,.001.001.000.000,.001 Late submission−.005**.000−.005, −.004−.004**.000−.005, −.004 Competition−.251**.007−.265, −.238−.251**.007−.264, −.238 Excess supply of freelancers−.044**.000−.045, −.043−.042**.000−.043, −.042Fixed Effects Yearsincludedincluded SubmarketsincludedincludedUnobserved Heterogeneity Project characteristicsincludedincludedEndogeneity Corrections Gaussian copulasincludedincludedFreelancers34,851Bids2,327,216 4 Notes: Standardized results. Significance is based on two-tailed tests. The dependent variable is whether the freelancer was chosen and won the bidding process. The sample included all bids by freelancers with at least one winning and at least one losing bid. Effects for years, submarkets, project characteristics, and Gaussian copulas are detailed in Web Appendix Q.5 *p < .05.6 **p < .01.We have proposed that when the degree of relevant information, concreteness, and affective intensity provided by the buyer is at least moderately dense (sparse), freelancers can increase (decrease) their chances of bid success by mimicking the buyer's communication. The surface-level tests along the plotted misfit line (Web Appendix K) display negative curvatures for task information (−.020, p < .01), personal information (−.007, p < .01), concreteness (−.011, p < .01), and affective intensity (−.020, p < .01). These results indicate that mimicking the buyer's communication increases bid success (see Web Appendix L for further clarification).In line with our proposition, we qualify this effect for sparse degrees of task and personal information, concreteness, and affective intensity provided by the buyer in Web Appendix M. If we were to find positive slope coefficients at lower levels, it would suggest that freelancers can increase their chances of bid success by exceeding, rather than mimicking, the buyer's communication. This prediction holds for personal information (.020, p < .01) and concreteness (.024, p < .01), according to the slopes at low levels of buyer communication. However, contrary to our expectations, we find negative effects for the slopes of task information (−.008, p < .01) and affective intensity (−.030, p < .01) at low levels of buyer communication. Therefore, freelancers should always mimic the degree of task information and affective intensity provided by the buyer. For these two communication aspects, the tenets of communication accommodation theory ([75]) and adaptive selling ([78]) hold: mimicking the buyer is always better. To increase their chances of bid success further, freelancers also must keep their bids concise (−.021, p < .01 for word count). Reputation cues (experience:.002, p < .01; rating:.010, p<.01) increase freelancers' chances of bid success, as do linguistic style matching (.051, p < .01), previous business relations with the buyer (.078, p < .01), lower bid prices (−.006, p < .01), timely (cf. late) bid submissions (−.004, p<.01), lack of competition (−.251, p < .01), and reduced supply of freelancers (−.042, p < .01). Predicting Freelancers' Price Premium Model-free evidenceBids with significantly less affective intensity and significantly more task information, personal information, and concreteness achieve greater price premiums than an average bid (M = 14% discount). Moreover, 96% of freelancers completed projects without any price premium, indicating the prevalence of value traps. The bids that achieved price premiums mimicked those buyers that made moderate use of task information, concreteness, and affective intensity closely, yet they deviated from buyers that made very sparse or very dense use of them. For personal information, we find a distinctive, positive, linear relationship for mimicry. Successful freelancers mimicked buyers that supplied a lot of personal details but deviated if buyers supplied very little or moderate degrees of personal information (Web Appendix H). Econometric model and identificationThe price premium analysis is restricted to bids that win and buyers that disclose their payment offer upfront. Thus, our estimates may be biased by buyers' self-selection, in terms of which bid they chose and whether they disclosed payments. Therefore, we employed a two-stage selection model. In the first stage, we estimated a choice model, with the availability of the necessary data as a binary dependent variable (i.e., bid was won and payment was disclosed). Using this model, we computed the inverse Mills ratio to account for the potential selection bias (probit model in Web Appendix N) and included this correction term in the final model estimation. To identify second-stage parameters, there needed to be one term in the first-stage equation that was unrelated to the error term in the freelance price premium equation. We thus included the dummy that indicates if the bid was submitted late only in the first-stage equation because this term explained buyers' choice of the bid, but we did not expect it to be conceptually related with the eventual price premium. Thus, this term satisfied both relevance and exogeneity requirements. The updated equation of our multilevel model (ICC( 1) = .13, p < .001) is as follows: PREMIUMkl=y00+y01−04FCOMkl1−4+y05−08BCOMkl1−4+y09−12FCOM_SQkl1−4+y13−16(FCOMkl1−4×BCOMkl1−4)+y17−20BCOM_SQkl1−4+y21−31CONkl1−11+y32−37YEARkl1−6+y39−43SUBMkl1−6+y44−54PROJkl1−11+y55−58FCOM~kl1−4+y59−62BCOM~kl1−4+y63IMRkl+μ0l+εkl, Graph( 6)where  PREMIUMkl  is the price premium of bid k (k = 1, ..., 148,158) offered by freelancer l (l = 1, ..., 30,851), and  IMRkl  is the correction term. Results and discussionThe maximum variance inflation factor is 2.74, indicating no threat of multicollinearity. Table 4 contains the results of the freelancer price premium models, Web Appendix K details the response surface coefficients, and Figure 2 displays the surfaces.GraphTable 4. Predicting Freelancers' Price Premium. Model 5:Freelancer CommunicationModel 6:Full ModelβSE95% CIβSE95% CIFreelancer Communication y01: Task information.023**.002.020,.026.022**.002.019,.025 y02: Personal information.021**.002.017,.025.021**.002.017,.026 y03: Concreteness.006**.001.004,.007.005**.001.003,.007 y04: Affective intensity.004.003−.002,.010.003.003−.003,.009Buyer Communication y05: Task information.003*.001.001,.005 y06: Personal information−.016**.002−.019, −.012 y07: Concreteness−.004**.001−.006, −.002 y08: Affective intensity−.001.002−.005,.003Freelancer Communication Squared y09: Task information squared−.001.001−.002,.000−.001.001−.002,.001 y10: Personal information squared−.004**.001−.006, −.002−.004**.001−.006, −.002 y11: Concreteness squared−.003**.001−.004, −.002−.003**.001−.005, −.002 y12: Affective intensity squared.000.000−.001,.000.000.000−.001,.000Freelancer–Buyer Interactions y13: Task information interaction.025**.003.024,.027 y14: Personal information interaction−.004**.001−.005, −.002 y15: Concreteness interaction.002**.000.001,.002 y16: Affective intensity interaction.010**.003.008,.012Buyer Communication Squared, y17: Task information squared.003**.001.001,.004 y18: Personal information squared.003**.001.002,.005 y19: Concreteness squared−.002**.001−.004, −.001 y20: Affective intensity squared.002**.001.000,.003Controls Word count−.014**.002−.018, −.011−.014**.002−.018, −.011 Linguistic style matching.022**.004.013,.030.023**.004.015,.032 Freelancer experience−.001.001−.004,.001−.001.001−.004,.001 Freelancer rating−.001.001−.003,.001−.001.001−.003,.001 Project payment−.029**.010−.049, −.009−.029**.010−.049, −.009 Previous relationship.057**.001.054,.059.056**.001.054,.059 Time-to-bid.009**.001.007,.012.009**.001.006,.011 Competition−.015**.002−.019, −.011−.015**.002−.019, −.011 Excess supply of freelancers−.001.001−.002,.001−.001.001−.002,.001Fixed Effects Yearsincludedincluded SubmarketsincludedincludedUnobserved Heterogeneity Project characteristicsincludedincludedEndogeneity Corrections Gaussian copulasincludedincludedSample-Selection Correction Inverse Mills ratio−.016**.002−.020, −.012−.016**.002−.020, −.012Freelancers30,851Bids148,158 7 Notes: Standardized results. Significance is based on two-tailed tests. The dependent variable is price premium for the chosen bid. The sample includes all winning bids for which the payment was disclosed. Effects for years, submarkets, project characteristics, and Gaussian copulas are detailed in Web Appendix Q.8 *p < .05.9 **p < .01.We proposed that when the degree of relevant information and communication manner provided by the buyer is at least moderately high (low), freelancers increase (decrease) their chances of earning a price premium by mimicking this communication. Web Appendix O displays the misfit lines on two-dimensional planes. In line with our expectations, the surface-level tests along the plotted misfit line show a negative curvature for task information (−.023, p < .01), concreteness (−.007, p < .01), and affective intensity (−.008, p < .01), such that mimicking the buyer's communication increases bid success. However, for personal information, we find a positive curvature (.003, p < .05), which implies freelancers should always offer more personal information than the buyer. For these B2B services, the provider and the service are inseparable, which may lead buyers to place more value on personal information about freelancers, even if their own provision of personal details in the calls for bids is sparse.If a buyer provides little relevant information or is less concrete (Web Appendix P), a positive slope would suggest that freelancers can increase their chances of earning a price premium by exceeding rather than mimicking the buyer. We find support for this prediction in the slope of personal information (.027, p < .01) at low levels of buyer personal information. However, negative effects emerge from the slopes of task information (−.016, p < .01) and affective intensity (−.012, p < .01), and we find a nonsignificant effect for concreteness (.002, n.s.). Mimicking the buyer's task information and affective intensity is always better, which is in line with accommodation theory and adaptive selling ([75]; [78]).Freelancers also increase their price premiums by avoiding lengthy bids (−.014, p < .01). Although platform reputation cues (experience and rating) can boost freelancers' chances of bid success, they do not determine the final price buyers pay. The skew in the ratings toward very high scores may limit their ability to help prospective buyers determine an appropriate price ([45]. Linguistic style matching (.023, p < .01), a previous relationship with the prospective buyer (.056, p < .01), submitting early in the bid process (.009, p < .01), and reduced competition (−.015, p < .01) all increase buyers' acceptance of a price premium. General DiscussionAcross disciplines, substantial research has identified various success determinants in online freelance marketplaces (e.g., [36]; [77]). For example, studies of B2B exchanges and two-sided marketplaces emphasize communication (see Web Appendix A). But at the specific word level, we lack insights into the optimal information or manner of communication ([ 7]). With this initial investigation of how buyers' and freelancers' success might be enhanced by appropriately managing the other party's uncertainty, we postulate, in line with uncertainty reduction ([ 6]) and uncertainty management ([ 9]) theories, that communication that is not completely informative and clear may still be effective. Accordingly, we investigate how buyers' communication can attract freelance bids and how freelancers' communication can determine their bid success and price most effectively, and the results offer both theoretical and practical implications. Theoretical ContributionsFirst, we advance research on how buyers' communication determines their ability to attract freelancers. Drawing on prior communication research, we identify communication principles that critically relate to receivers' uncertainty, such as relevant task and personal information and the relative concreteness and affective intensity with which this information is communicated ([ 8]; [27]). To entice more freelancers to bid, buyers should carefully formulate their calls for bids to keep them brief. Freelancers' information processing motivation, time, skills, and proficiency likely are limited, so buyers must choose their wording carefully and select from various effective communicative aspects. They can attract a larger pool of bids if they provide moderate degrees of task information in a moderately concrete manner. Offering too little of these features leaves freelancers with too much uncertainty, and dense information provision or being very concrete is too restrictive. If buyers provide greater degrees of personal information or express greater affective intensity in their calls for bids, it reduces the number of service offers they receive. This finding contrasts with uncertainty reduction theory ([ 6]) and B2B research that suggests self-disclosure strengthens buyer–seller cooperativeness ([41]). However, instead of ongoing B2B relationships, our study refers mostly to initial interactions between strangers (in 98% of cases, the freelancer had never worked for the prospective buyer). Evidence obtained from buyer–seller online chats similarly suggests that self-disclosure and emotive expressions are valued only in existing B2B relationships, not in new ones ([46]). Overall, we offer empirical support for communication theorists' suggestions that common communication principles can be purposefully flouted to achieve better conversation outcomes ([23]).Second, freelancers must keep their bids concise. They too face a trade-off between reducing the buyer's uncertainty and offering overly dense information. In line with research on communication accommodation ([75]) and adaptive selling ([78]), we show that freelancers can improve their bid success by mimicking the prospective buyer's communication. Adding to these research streams, we introduce a contingency perspective that reveals the efficacy of mimicry depends on the degree to which buyers use specific communication elements. In line with accommodation theory and adaptive selling, bid success always improves when freelancers mimic buyers' provision of task information and use of affective intensity. However, in line with uncertainty reduction theory ([ 6]) and expectancy violations ([ 1]) when buyers supply little personal information and are less concrete, freelancers can increase their chances of bid success by diverging and providing more personal information and concreteness.Third, freelancers often struggle to avoid value traps in which they sell more of their services for less ([76]). Rational buyer expectations should allow high-quality freelancers to charge price premiums ([70]), but the quality of freelance services is unobservable prior to purchase, and rational buyers might refuse to pay any price premium if they feel uncertain and suspect the freelancer may be hiding information ([16]). Therefore, to achieve premiums, freelancers should offer short, appropriately formulated bids. Buyers are more willing to pay a premium to freelancers who mimic their provision of task information, concreteness, and affective intensity, which is in line with communication accommodation theory ([75]) and adaptive selling research ([78]). However, similar to the findings for bid success, freelancers should offer more personal information than buyers, rather than mimicking buyers' provision of such information. In most service settings, a ""bad"" seller might provide a great product by chance; however, almost by definition, a bad freelancer produces bad service ([36]). This tight coupling between the freelancer and service quality represents a conceptual distinction in our study, which accordingly shows that buyers' willingness to pay a premium increases with more personal information issued by the freelancer. Practical ImplicationsOur findings offer actionable insights for the millions of buyers and freelancers utilizing online freelance marketplaces, the collective value of which is predicted to reach $2.7 trillion by 2025 ([56]). In detail, being informative and unambiguous may be a common assumption, but it is not an imperative, nor does it always lead to success. Implications for buyersAlthough 59% of U.S. companies use a flexible workforce to some degree, more than one-third of contracted projects are never completed ([33]). To attract freelancers, buyers should keep their calls for bids succinct. Beyond that recommendation, we offer several tips for formulating calls for bids in Table 5. In particular, a task description with a moderate amount of information helps freelancers anticipate the task without overloading them with details. Due to the relative anonymity of online freelance marketplaces, buyers might assume that freelancers will need to know who they are, but instead, we find that the less buyers describe themselves (to focus on describing the task), the better the outcomes. Relatable and imaginable (rather than abstract) descriptions of the project help freelancers grasp the requirements. However, being excessively concrete becomes prescriptive, which deters freelancers. Using emotion words makes the content of a call for bids relatively more intense. Such intensity can remove ambiguity and make opinions quickly accessible, but we find that calls for bids are more effective if they are formulated relatively impassively. Enthusiastic project descriptions seemingly might raise freelancers' suspicion that the project is too good to be true. Also, offering higher payment might attract a larger pool of freelance bids, as do long- rather than short-term gigs. Finally, more freelancers bid when there are fewer calls for bids in the subsector.GraphTable 5. Buying and Selling Services in Online Freelance Marketplaces. How to Formulate Calls for Bids to Attract FreelancersBad Practice ExcerptGood Practice ExcerptLift in BidsSpecify tasks and skills""I need a website to showcase the full range of my fitness workouts.""""I need a website designer who can design a WordPress website using a WordPress premium theme.""An increase in task terms from 18% to 29%, resulting in 5% more bids.Avoid personal information""I have been creating my own classes for almost 10 years now...clients tend to especially love my classes on strength and flexibility. Now I need help setting up my website.""""I am a Fitness Trainer and need help with building my website to showcase my mixed services and home workouts.""A decrease in personal terms from 9% to 4%, resulting in 4% more bids.Be moderately concrete""I require a professional who is savvy in configuring a stylish website employing a premium theme.""""You should have got very good creative skills but know how to design for web and also know how to include calls to actions within a good design.""An increase in concrete terms from 21% to 26%, resulting in 1% more bids.Avoid being affectively intense""I have created a fantastic theme but you should be confident and eager about WordPress and help optimize.""""The theme and examples will be provided, but you should also know about WordPress and optimize.""A decrease in affective terms from 11% to 4%, resulting in 4% more bids.How to Formulate Successful Bids and Achieve Price Premiums Bad Practice ExcerptGood Practice ExcerptLift in Bid SuccessMimic task description""Dear Sir, would love to work for you...""""Hi Gary, I am happy to help you with your fitness website development and design...""An increase in task terms from 16% to 25%, resulting in 7% higher bid success and 8% higher price premium.Exceed buyers who supply little personal information""I am an enthusiastic designer and expert in Web development...""""I am a WordPress Freelancer with 15 years of work experience...""An increase in personal terms from 6% to 8%, resulting in 3% higher bid success and 4% higher price premium.Exceed buyers who are not concrete""I have great skills and plenty of fantastic experience in creating relevant websites...""""I have worked on several similar projects, designing websites, also using WordPress, including premium themes and I can deliver to a tight schedule...""An increase in concrete terms from 24% to 30%, resulting in 7% higher bid success (but no effect on price premium).Mimic the buyer's affective intensity""The content will be creative and fun, attractive, and thoughtful...""""Website content that I produce will be creative and include original designs...""A decrease in affective terms from 18% to 6%, resulting in 11% higher bid success and 7% higher price premium. 10 Notes: Web Appendix S provides the full call for bids and bid examples we used for calculating the degrees of each communicative principle and the corresponding expected lift success. We used the ""good practice"" call for bids example to devise the bad and good examples for the corresponding freelance bid. Implications for freelancersFreelancers are not necessarily natural marketers, but their bid formulations determine their marketability. Existing online reputation systems provide some assistance, but they also create entry barriers to new freelancers who first must earn good overall ratings ([13]). Fortunately, winning gigs and achieving price premiums also depends on freelancers' communication. Table 5 includes advice to help freelancers formulate more successful bids and avoid the value trap. In line with the mantra of adaptive selling, the call for bids provides a starting point in which mimicking the buyer's task information and affective intensity increases freelancers' success—even if they provide few task details or seem very impassive. But freelancers should always offer personal information and be concrete. Even if a buyer does not provide personal information or the call is relatively abstract, freelancers' chances of success and obtaining price premiums increase if their bids contain more personal information and are at least somewhat concrete. The strongest predictor of bid success is a preexisting buyer relationship, so more broadly, freelancers should grow their buyer relations. Limitations and Directions for Further ResearchIn examining theoretically grounded communicative aspects, we offer novel insights into how to manage uncertainty in buyer–freelancer exchanges. Intriguingly, we find that communication approaches that do not aim to minimize uncertainty can be effective. Continued research should investigate this notion further and develop additional insights into the exchange implications of linguistic choices in B2B but also B2C and C2C communication on multisided platforms ([53]). For example, affiliative ([66]) or collaborative terms might affect uncertainty and influence exchanges as well. Arguably, the personal characteristics of buyers and freelancers (e.g., gender, education, experience), channel choices ([50]), different sources of uncertainty ([29]), perceived risks ([25]), and spatial distances between buyers and freelancers also might moderate the efficacy of communication aspects, so additional research should specify their influences. For example, if buyers lack the expertise to specify what they want, they might benefit from more ambiguous calls for bids ([38]). Perhaps buyers' communication or alternative factors that we cannot account for (e.g., underestimation of the amount of work required to fulfill the task) influence the final price they pay, too. Efforts to specify these additional effects also might address some of our more controversial findings, such as the evidence that the number of previously commissioned projects by a buyer relates negatively to the number of freelancers who bid. We posit that experienced buyers might prefer freelancers whom they have hired in the past ([48]). Buyers also might have incurred switching costs or supplier dependencies ([29]). Methodologically, we estimated all the models sequentially, as buyers' calls for bids and their success occur prior to freelancers' bids and their success. But an equilibrium approach that estimates these models simultaneously at the bid level could reflect an alternative way to think about the data structure. The concreteness word list we used ([11]) may also require further refinement to differentiate specific concreteness levels among the word set. Finally, the anonymity and speed of exchanges in online freelance marketplaces may make communication particularly important in this context. A comparative analysis of the influence of uncertainty management efforts across different B2B contexts beyond these marketplaces could offer interesting insights, especially if uncertainty avoidance is a central goal.  "
3,"Communication in the Gig Economy: Buying and Selling in Online Freelance Marketplaces The proliferating gig economy relies on online freelance marketplaces, which support relatively anonymous interactions through text-based messages. Informational asymmetries thus arise that can lead to exchange uncertainties between buyers and freelancers. Conventional marketing thought recommends reducing such uncertainty. However, uncertainty reduction and uncertainty management theories indicate that buyers and freelancers might benefit more from balancing—rather than reducing—uncertainty, such as by strategically adhering to or deviating from common communication principles. With dyadic analyses of calls for bids and bids from a leading online freelance marketplace, this study reveals that buyers attract more bids from freelancers when they provide moderate degrees of task information and concreteness, avoid sharing personal information, and limit the affective intensity of their communication. Freelancers' bid success and price premiums increase when they mimic the degree of task information and affective intensity exhibited by buyers. However, mimicking a lack of personal information and concreteness reduces freelancers' success, so freelancers should always be more concrete and offer more personal information than buyers. These contingent perspectives offer insights into buyer–seller communication in two-sided online marketplaces. They clarify that despite, or sometimes due to, communication uncertainty, both sides can achieve success in the online gig economy.Keywords: business-to-business exchange; gig economy; multisided platforms; online freelance marketplaces; text analysis; uncertainty managementOnline freelance marketplaces, such as Upwork, Fiverr, and PeoplePerHour, have prompted massive transformations in business-to-business (B2B) markets ([13]; [82]). In particular, they allow buyers to post gigs, or short-term service projects, which initiate reverse auctions whereby interested freelance workers submit bids to offer their services ([39]). In these digital environments, buyers and freelancers often devote rather limited time and attention to detailed assessments and instead make choices on the basis of rational value expectations or prices ([ 2]). In addition, online freelance marketplaces suffer from information asymmetries because they rely on text-based messages, which can create uncertainty and hinder the exchange ([34]; [77]). Imagine a buyer wants to hire a freelancer to optimize their pet website's search rankings, so they post a call for bids, requesting ""someone for an SEO job."" In response, Freelancer A might vaguely promise, ""I have plenty of experience writing content that users find interesting to improve the quality and quantity of your traffic,"" whereas Freelancer B more concretely states, ""I have four years of experience writing articles and blogs that engage users and are SEO-friendly. For example, I could focus on interest pieces like the everyday lives of pets."" The communication of both the buyer and freelancer create different degrees of uncertainty, likely impacting who applies and who gets hired.Uncertainty regarding communication can lead to various negative outcomes on both sides, including high rates (more than 50%) of service gigs that go unfulfilled ([36]), diminished bid success, and less-than-optimal pricing for freelancers ([ 2]). However, parties in B2B exchanges can also strategically leverage uncertainty in their communication to achieve more effective outcomes, such as when negotiators conceal information ([69]) or when ambiguous contracts help reduce litigation concerns and increase cooperation ([81]). Buyers and freelancers on online freelance marketplaces engage in a form of B2B exchange, so we propose that they similarly might balance their communication efforts by strategically reducing and increasing uncertainty to maximize their exchange success. In our previous example, by staying vague and without any specific direction from the buyer, Freelancer A might be trying to keep multiple options open and avoid overpromising outcomes.In addition to fundamental questions regarding how to manage uncertainty in B2B exchanges ([50]; [63]), we seek to address the role of communication in such exchanges ([ 7]; Rajdeep et al. 2015; see also Web Appendices A and B). We integrate uncertainty reduction theory ([ 6]) and uncertainty management theory ([ 9]) to predict that, in online freelance marketplaces, various strategies for reducing and increasing the ability of message recipients to anticipate message senders' meaning and actions can benefit the exchange ([ 8]). Using Grice's ([27]) communication principles, we argue that greater provision of task and personal information might reduce uncertainty in service exchanges ([54]) but could also lead to information overload or disagreements ([18]; [40]). Presenting information in a more concrete (cf. abstract) manner or with greater affective intensity also can reduce uncertainty ([27]; [28]; [61]). But again, too much concreteness or affective intensity might lead to restrictive communication that hinders exchanges ([18]; [37]).We apply this theoretical reasoning to exchanges in online freelance marketplaces, in which buyers post calls for bids to attract as many freelancers as possible to apply ([36]). These buyers face a trade-off between reducing uncertainty for freelancers (e.g., providing more information, using less ambiguity) and still efficiently granting them sufficient interpretative freedom. Theorists concur that principles for using relevant information or less ambiguity often get deliberately flouted in conversation, such as when an individual is attempting to save face ([23]) or please a counterpart ([44]). If different communication strategies might entice more freelancers to bid, buyers could establish optimal designs for calls for bids.In response to those calls for bids, freelancers write and submit their bids. In doing so, these freelancers also must manage uncertainty. Thus, they might benefit from matching or mimicking the communication approach adopted by the prospective buyer that issued the call ([78]). Communicative mimicry can evoke similarity perceptions, which tend to increase receivers' sense of rapport and reduce their uncertainty ([75]). Research on adaptive selling recommends matching the buyer's communication (e.g., [57]; [72]). However, in some situations, deviations also may be beneficial ([ 1]), so we consider a more nuanced distinction related to the level at which the similarity occurs. Furthermore, if freelancers compete on price, they may become enmeshed in a self-defeating value trap ([34]; [76]) in which they win more bids but earn less revenue. Strategically mimicking or deviating from a buyer's communication might provide a viable means to winning more gigs without being trapped. We accordingly suggest how freelancers should calibrate their bid formulations to improve their bid success and achieve a price premium.Using a unique, large-scale data set of calls for bids and bids, obtained from a leading online freelance marketplace, along with a series of multilevel models that account for endogeneity, we establish three main contributions. First, we determine the effects of buyers' strategic communications in two-sided online marketplaces ([ 7]). Rather than uncritically recommending that communication should always be informative and unambiguous, we specify the diminishing, even adverse consequences that can result if buyers relay too much task or personal information in a very concrete, intense manner. Second, in an extension of research into adaptive selling ([57]; [78]), we reveal how freelancers' dyadic communicative mimicry affects bid success. Mimicry effects are contingent on the communicative aspect and the buyer's relative uses of each aspect. As we show, mimicking buyers in terms of the provision of task information and use of affective intensity increases bid success. In contrast, we find that freelancers should always offer more personal information and be more concrete in their bid formulations than buyers' calls for bids. Third, we offer insights into how freelancers can avoid predatory pricing ([13]) and escape a value trap ([76]). By strategically managing uncertainty according to the information communicated, and by managing the manner in which they do so, freelancers can earn price premiums. Online Freelance Marketplace ExchangesOnline freelance marketplaces that feature reverse auctions rely on a three-stage process ([34]; [39]). First, in seeking a suitable freelancer, a buyer describes a gig or short-term service project in a call for bids. Second, multiple freelancers apply by formulating and submitting bids that describe themselves, the service offering, and the price requested. Third, the buyer compares the bids and selects a freelancer to complete the project. The outcome of each stage defines exchange success. That is, buyers' success results from a large pool of viable freelance offers. A higher number of bids increases the chances of finding a suitable freelancer for the gig ([35], [36]). Freelancers' success depends on whether their bids are chosen, preferably at a price premium ([13]; [32]). In this context, a price premium is the monetary amount in excess of the buyer's original payment offer (i.e., expected price; [26]; [73]). Buyers might pay a premium beyond their original payment offer for various reasons, including their willingness or ""need to compensate the seller for reducing transaction risks"" ([ 2], p. 248). In competitive online marketplaces, freelancers also might encounter value traps in which they wind up selling more of their services at a lower price ([ 2]; [76]). In this sense, freelancers' success depends on winning the bid but also earning price premiums (or avoiding discounts). Unlike traditional B2B exchanges, buyers' and freelancers' success hinges on textual communication ([13]; [36]). Comparing theories on uncertainty and the role of communication in producing or reducing it, we delineate how both buyers and freelancers may best strike a balance between providing more information and reducing ambiguity versus preserving some uncertainty and maintaining interpretative flexibility. Conceptual BackgroundUncertainty reduction theory ([ 6]) and uncertainty management theory ([ 9]) draw on a central tenet of information theory ([71])—namely, that communication, information, and uncertainty are inextricably linked. Thus, uncertainty is inherent to any interaction. [22] suggests uncertainty depends on the ability to draw inferences from provided information content and the manner in which it is provided. Whereas uncertainty reduction theory predicts how communication can reduce uncertainty, uncertainty management theory examines how people cope with uncertainty, which may include efforts to increase uncertainty to attain beneficial outcomes ([ 8]). Our conceptual development relies on these fundamental principles. Communication Principles in Online Freelance MarketplacesIn online freelance marketplaces, buyers and freelancers depend on one another; all else being equal, they want their mutual exchange to succeed. In such interactions, [27] suggests that four generalized cooperative communication principles (or maxims) apply. Three principles refer to what should be said: the quantity of information (""give as much information as is required and no more than is required""), its quality (""do not say what is false or that for which you lack adequate evidence""), and its relevance. The fourth principle, manner (be clear and avoid ambiguity), pertains to ""how what is said is to be said"" ([27], p. 46). In our study context, neither a buyer nor a freelancer can know upfront whether the other party might be lying, so truthfulness would have to be assumed prior to the exchange. We also highlight that information does not have to be ""correct"" to influence uncertainty perceptions ([ 9]). Therefore, among the four maxims, we focus on the quantity of relevant information that buyers and freelancers offer and the manner in which they present it. Uncertainty Implications of Communication PrinciplesCommunication outcomes are fundamentally uncertain ([ 6]). When people vary their use of communication principles ([27]), they create conversational implications such that message recipients must infer what speakers are trying to imply with their wording. Accordingly, the (un)certainty that buyers and freelancers encounter while making inferences should depend on the degree to which calls for bids and bids provide relevant information in an unambiguous manner, though the meaning of relevant information varies by context. In line with prior research (e.g., [ 7]), we define this degree as the proportion of specific lexical terms used relative to the total number of words in a message.More information reduces uncertainty ([ 6]) and increases receivers' perceptions of the information's value ([79]). In service exchanges, the parties seek information about the task and the person who will complete it ([54]). A greater degree of task information should reduce uncertainty about functional service aspects ([54]). By self-disclosing greater degrees of personal information, a sender also provides a receiver with more information about the self ([15]). In line with the quantity principle ([27]), sparse provision of relevant task and personal information would make it difficult for the receiver to anticipate outcomes or distinguish among options, thus creating uncertainty ([19]).Regarding the principle of manner ([27]), greater degrees of concreteness and affective intensity should reduce ambiguity and enhance clarity. Concrete terms describe something in a perceptible, precise, specific, or clear manner ([11]; [49]; [61]). A greater degree of concreteness reduces ambiguity because it makes it easier for receivers to perceive or recognize what the message sender is implying ([11]; [28]; [61]). Affective intensity reflects the proportion of affective terms included in a message. More affective terms as a proportion of the total word count produce a greater degree of affective intensity, which increases receivers' ability to make evaluative judgments ([28]; [37]).[ 5] We provide examples of these principles in Table 1.GraphTable 1. Communication Elements, Links to Uncertainty, and Examples. Communication ElementDefinitionLink to UncertaintyExampleTask informationA content element of communication. In service exchanges, it is conveyed through functional, duty terms (Ma and Dubé 2011). The proportion of task terms to the total number of words in a message defines the degree of task information.Greater (lesser) degrees of task information decrease (increase) uncertainty.Sparse degree of task information: ""I saw your project description and I would like to work for you. I have plenty experience in different settings where I have written content which users find interesting.""Dense degree of task information: ""I saw your project description and would like to write the content for your website. I have experience in writing articles, blogs & E-books which is user engaging and SEO friendly as well.""Personal informationA content element of communication that is conveyed through self-disclosing terms (Derlega, Harris, and Chaikin 1973). The proportion of self-disclosing terms to the total number of words in a message defines the degree of personal information.Greater (lesser) degrees of personal information decrease (increase) uncertainty.Sparse degree of personal information: ""Saw your project description and would like to write the content for your site. I have experience in writing articles, blogs & E-books which is user engaging and SEO friendly as well.""Dense degree of personal information: ""I saw your project description and I would like to write the content for your site. I have 12 years of work experience in copy writing for articles, blogs & E-books. I have a Master's in Journalism and have worked fulltime for companies like Adobe.""ConcretenessA manner element of communication conveyed by terms that are perceptible, precise, or specific (Brysbaert, Warriner, and Kuperman 2014; Packard and Berger 2020). The proportion of concrete terms to the total number of words in a message defines the degree of concreteness.Greater (lesser) degrees of concreteness decrease (increase) uncertainty.Sparse degree of concreteness: ""I noticed your project description and I would like to do work on it. I have plenty of experience in scripting text, which is engaging, compelling, and SEO friendly.""Dense degree of concreteness: ""I saw your posted project description on Upwork, and I would like to write the contents for your website. I have a lot of experience in article and weblog writing in an SEO friendly fashion.""Affective intensityA manner element of communication that is conveyed through affective terms (Hamilton and Hunter 1998). The proportion of affective terms to the total number of words in a message defines the degree of affective intensity.Greater (lesser) degrees of intensity decrease (increase) uncertainty.Sparse degree of affective intensity: ""I saw your project description and I can write the required content for your site. I have plenty of experience in writing articles, blogs & E-books which is user engaging and SEO friendly as well.""Dense degree of affective intensity: ""I liked your project description and would be happy to write the content for your site. I have great experience in writing articles, blogs & E-books which is user engaging and SEO friendly as well.""  Reducing and Maintaining Uncertainty in Communication ExchangesCross-disciplinary research provides ample evidence that conversational partners generally prefer to reduce uncertainty ([ 5]). In B2B relationships, reducing uncertainty increases exchange effectiveness ([29]; [50]; [63]). In Web Appendix A, we offer an overview of some key empirical marketing studies on B2B communication aspects. Specifically in online freelance marketplaces, which are relatively anonymous, the required coordination and dependence between rational buyers and freelancers may increase their need for information and clarity ([13]; [34]). Thus, for example, reputation cues commonly appear in online freelance marketplaces as a way to reduce uncertainty and facilitate exchanges ([34]). More broadly, reducing uncertainty by adhering to [27] principles in dyadic buyer–freelancer communications may boost exchange success.However, people experience uncertainty differently and do not always prefer to reduce it ([ 8]). Instead, according to uncertainty management theory ([ 9]), strategic communication choices that might not minimize uncertainty, and even cultivate it, can be effective and lead to better outcomes for consumers ([38]), organizations ([18]; [31]), and interorganizational governance ([81]). For example, [38] find that a lack of concreteness aids consumers' initial online searches because such vague queries return a greater variety of search results. In collective bargaining settings, seasoned negotiators use concealment and ambiguity to enhance the likelihood of agreement ([69]). In B2B exchanges, parties can use less information and more ambiguity strategically to accomplish specific goals ([ 3]; [81]). Even if such efforts are not universally favored, uncertainty-cultivating communication provides benefits by allowing different receivers to perceive multiple different meanings simultaneously ([18]). Moreover, communication theorists concur that people sometimes deliberately flout or violate [27] conversation principles, such as when they intentionally maintain uncertainty to save face ([23]) or please a counterpart ([44]). Subverting the principles is not necessarily less cooperative, and furthermore, the purpose of communication is not always to be as informative and clear as possible. Arguably, cooperative principles encourage reasonable adherence, not compulsion. Thus, strategically allowing recipients to develop a broader range of possible interpretations by maintaining some level of uncertainty might facilitate buyer–freelancer exchanges. Research Propositions Managing Freelancers' Uncertainty in Calls for BidsFreelancers choose whether to offer their services in response to a buyer's call for bids. The number of freelancers who choose to do so is consequential for the buyer, as more bids implies a greater likelihood of finding a suitable service provider ([36]). Managing freelancers' uncertainty through relevant information provision and the manner of communication in the calls for bids should influence freelancers' decisions to apply. Relevant informationIn calls for bids, buyers can vary the degree of task and personal information included in the description of the gig. If freelancers evaluate this information favorably, they develop more positive dispositions and are more likely to apply ([72]). As prior research establishes, more information enhances communication outcomes in business settings by reducing uncertainty. For example, studying web forums, [79] indicate that the breadth of information provided by a sender affects receivers' objective judgments of the value of that information. [49] find that greater degrees of monetary information increase peer-to-peer lending, and [41] shows that more task information increases the time and commitment sellers allocate to a buyer. Greater degrees of personal information also reduce uncertainty, increase trust ([55]), and enhance performance on crowdsourcing platforms ([68]). Such self-disclosure can strengthen ongoing buyer–seller relations as well ([14]). In contrast, a greater proportion of nonrelevant information (i.e., a lesser degree of relevant information) increases uncertainty ([ 9]). Because greater degrees of task and personal information in calls for bids help reduce freelancers' uncertainty, freelancers who believe they qualify for the gig should be more willing to submit bids.However, excessive relevant information may be ineffective, even if it reduces freelancers' uncertainty. That is, if buyers provide excessive details about the task, the gig may appear too restrictive or prescriptive ([18]), which might not appeal to freelancers. For example, leaving detailed information out of contracts ([21]) or negotiations ([69]) represents a tactic for improving exchange performance. In a downsizing context, a greater degree of information provision can increase uncertainty and negative reactions ([31]). For freelancers, excessive information can feel overwhelming and can limit their motivation, opportunity, or ability to process the information and submit bids ([40]). A buyer that self-discloses a high amount of personal information might also appear less attractive as a prospective business contact ([12]). Because extensive self-disclosures are unusual in initial B2B online exchanges ([46]), such disclosures might be perceived as inappropriate ([59]).In summary, we argue that moderate degrees of task and personal information in calls for bids relate to more freelancer bids. Buyers who provide greater degrees of task and personal information should attract more bids, but beyond a moderate degree (i.e., a very dense provision of relevant information), providing still greater degrees of task and personal information may decrease the number of bids. We thus propose a curvilinear relationship: P1:  Extremely sparse and extremely dense degrees of (a) task and (b) personal information in calls for bids yield fewer freelance bids than moderate degrees. Communication mannerIn calls for bids, buyers can vary the concreteness and affective intensity with which they describe the gig. Researchers disagree about whether more or less ambiguous communication leads to more efficacious speech ([ 8]; [18]; [37]), but in an online freelance marketplace, we posit that buyers must reduce ambiguity to at least some extent by being more concrete and intense. Greater concreteness and affective intensity can be more efficient because recipients can process the information with less time and effort ([37]; [61]). These approaches also tend to result in communication that is more persuasive, memorable, and accessible than communication that uses predominantly abstract or unemotional wording ([28]; [37]). In other settings, greater concreteness increases consumer satisfaction with employee interactions and purchase likelihood ([61]). Greater degrees of intensity achieved through proportionally more affective words provide accessible, diagnostic signals to customers ([52]). They can also sway business partners' decisions when used as inspirational appeals ([72]). Finally, greater concreteness and affective intensity provide heuristic cues that allow freelancers to take mental shortcuts, which makes them more likely to bid ([37]).However, if the calls for bids appear too concrete or too intense, the task might appear narrow, which reduces the appeal of performing the gig ([37]). [80] finds that greater vagueness (i.e., less concreteness) can enhance judgments of a speaker's character, message acceptance, and recall. Moreover, some research asserts that reducing uncertainty with more concrete formulations is ineffective ([ 9]; [18]), so managers instead should embrace strategic ambiguity to allow for interpretative freedom ([43]). In contracts, unexpected specificity even increases ex ante costs ([58]). Contrastingly, greater task ambiguity can lower costs as well as reduce the risk of litigation and enhance cooperation in B2B exchanges ([81]). Greater degrees of concrete terms in communications with investors also can have adverse effects ([64]), and excessive degrees of positive affective words diminish the impact of customer reviews ([52]). Thus, we predict a stylistic trade-off: Overly ambiguous calls for bids, lacking any concreteness or affective intensity, may undercut buyers' success in attracting freelancers, but some degree of ambiguity (i.e., avoiding overly concrete, affectively intense communication) can allow for divergent interpretations to coexist. Thus, moderate degrees of concreteness and affective intensity may be most effective in encouraging freelancers to bid. P2:  Extremely sparse and extremely dense degrees of (a) concreteness and (b) affective intensity in calls for bids yield fewer freelance bids than moderate degrees. Managing Buyers' Uncertainty in BidsBuyers also face uncertainty when deciding whom to hire and how much to pay ([ 2]; [13]). By managing these uncertainties through their bids, freelancers can affect their chances of winning bids and their price premiums. To establish relevant predictions, we integrate [27] communication principles with uncertainty research such that we anticipate a greater provision of relevant information communicated with greater concreteness and affective intensity allows buyers to draw inferences from freelancers' bids with more certainty. Beyond these communication principles, [ 6] suggest that perceived similarity to a message sender reduces receivers' uncertainty. Thus, both purchase likelihood and buyers' willingness to pay a price premium might be influenced by freelancers' adherence to certain communication principles, as well as by their communicative similarity to the buyer. Winning bidsIn other exchange contexts, research has established that when service employees relay greater degrees of service or personal information ([51] 2015; [62]), it improves customers' intentions to purchase. Willingness to purchase also increases if employees use greater concreteness in online service chats ([61]) or greater degrees of affective words in their emails ([72]).However, the dense provision of relevant information in a bid risks information overload ([40]), and a freelancer being overly concrete or intense might signal a restrictive, narrow approach to the gig ([37]). Our reasoning here parallels that for buyers' formulations of calls for bids. We thus similarly predict that moderate degrees of task and personal information provided in a moderately unambiguous manner (i.e., moderate degrees of concreteness and affective intensity) enhance freelancers' chances of winning the gig.Yet preferences for uncertainty also might be situational and dispositional ([ 9]), as reflected in buyers' own communicative choices ([30]). Specifically, calls for bids can reveal buyers' expectations and preferences for communication behaviors. For example, buyers might like to get to know freelancers, or they may prefer to keep their business relationships impersonal. The extent to which they disclose their own personal information in calls for bids should signal these preferences. An ambiguous bid offered in response to an ambiguous call for bids might lead the buyer to conclude that the freelancer is tactful, sensitive, and noncoercive ([10]). Adaptive communications also raise perceptions of credibility, common social identity, approval, and trust ([52]; [75]), as well as similarity perceptions, all of which in turn reduce uncertainty ([ 6]). Crafting responses that mimic the buyer's communication is a common personal selling recommendation ([78]). As [72] show, when sellers mimic buyers' communicative manner, it increases buyers' attention. Accordingly, freelancers who mimic a buyer's communication content and manner might improve their exchange success.In some situations, though, deviating from buyers' communications may be more beneficial ([ 1]). Even in studies that note the performance benefits of adaption, researchers highlight the importance of the degree of adaptivity (e.g., the degree to which salesperson behaviors adjust for each customer during interactions; [78]). Similarly, studies of communication accommodation investigate the degree of accommodation used ([75]). Extending these insights, the outcomes of adaptation likely depend on communication levels (e.g., very informative vs. not informative). In keeping with uncertainty reduction theory, we expect that buyers are less likely to hire freelancers whose bids offer sparse information and are very ambiguous, even if the call for bids has these characteristics. P3:  When the degrees of (a) task and (b) personal information, (c) concreteness, and (d) affective intensity provided by the buyer are at least moderate (sparse), freelancers can increase (decrease) their chances of bid success by mimicking buyers' communications. Achieving price premiumsBuyers' uncertainty about a freelancer should influence their willingness to pay a price premium ([ 2]). Although there are many reasons for price variations ([26]) in online freelance marketplaces, buyers compensate (penalize) freelancers for reducing (increasing) their transaction uncertainty by deciding to accept a price above (below) their original payment offer ([ 2]). In line with [70], freelancers' greater provision of relevant task and personal information in a more concrete and intense manner in bids likely reduces buyers' information asymmetry and exchange-specific risks. Therefore, buyers who want to transact with high certainty may render a price premium for such bids ([51] 2015).The degree to which freelancers mimic buyers' communication also may influence the price premium. For example, [60] find that adaptive approaches for different customers help salespeople increase those customers' willingness to pay a price premium. However, in line with our arguments regarding bid success, we expect that the positive influence of a freelancer's communicative mimicry depends on the specific degree to which the buyer uses a specific communicative element. This reasoning aligns conceptually with the communication principles ([27]), the recommendation that uncertainty should be carefully managed ([ 8]), and the benefits of mimicry identified in studies of communication accommodation ([75]) and adaptive selling ([78]). However, we know of no studies that consider price premium implications of communicative trade-offs between reducing buyers' uncertainty and adapting to buyers' communication. In addition, we are not aware of any research that considers the possible negative effects when sellers mimic buyers who provide less task and personal information, are less concrete, or sparsely use affective intensity.Buyers who want to transact with high certainty might render a price premium to freelancers who reduce uncertainty by providing greater degrees of relevant information in a more concrete and intense manner. But if buyers perceive that the provision of relevant information, degree of concreteness, and level of intensity surpasses their own reasonable level, they might feel overloaded or restricted and thus unwilling to pay a premium. We therefore predict that buyers offer a price premium to freelancers who provide degrees of relevant information, concreteness, and affective intensity at a level similar (but never too sparse) to their own communication, as only these bids help reduce buyers' exchange risks. P4:  When the degrees of (a) task and (b) personal information, (c) concreteness, and (d) affective intensity provided by the buyer are at least moderate (sparse), freelancers can increase (decrease) their chances of earning a price premium by mimicking the communication of the buyer.Graph: Figure 1. Effect of buyers' communication on call for bids success. Field Study of an Online Freelance Marketplace Setting and SampleWe conducted a large-scale field study with a proprietary data set of calls for bids and corresponding bids posted on a leading, global online freelance marketplace. The marketplace hosts seven freelance service submarkets: ( 1) design; ( 2) writing and translation; ( 3) video, photo, and audio; ( 4) business support; ( 5) social media, sales, and marketing; ( 6) software and mobile development; and ( 7) web development. The bidding process follows a sequential, sealed-bid reverse auction format, and it concludes when the buyer chooses one winning bid ([34]; [39]). As with recent marketing research that investigates large scales of communication (see Web Appendix B for an illustrative overview), this process depends on and is captured in text data. We used ( 1) text data from 343,796 calls for bids issued by 49,081 buyers (restricted to those who posted at least two gigs) to predict buyers' call for bids success, ( 2) 2,327,216 bids submitted by 34,851 freelancers (restricted to those who submitted at least two bids) to predict freelancers' bid success, and ( 3) 148,158 bids submitted by 30,851 freelancers (restricted to those who won and for which the payment was disclosed) to predict freelancers' price premium. Our multilevel approach required more than one observation (call for bid or bid) in each Level 2 unit (buyer or freelancer); otherwise, Level 2 and Level 1 variance might have been confounded ([74]). Web Appendix C summarizes the definitions and operationalizations, and Web Appendix J provides the descriptive statistics and correlations. Measurement of ConstructsThe number of freelancers who submit bids to offer their services provided the measure of success of buyers' call for bids. More submitted bids increases the probability that buyers can find an appropriate freelancer, whereas failing to find a suitable match is time consuming and costly because it requires further searches and delays the project ([35], [36]). We measured freelancers' bid success as a binary indicator of whether ( 1) or not (0) the freelancer was chosen by the buyer and won the bid ([32]). For freelancers' price premium, we gauged the percentage by which the accepted bid price for the project exceeded (or fell short of) the buyer's original payment offered (i.e., benchmark price; [20]). This operationalization accounted for the difference between the final price a buyer paid and the original price they offered (i.e., what the buyer expected to pay) ([73]).To capture the independent communication variables, we mined the text of each call for bids and each bid. For the preprocessing and extraction steps, we used the R package Quanteda ([ 4]), as well as a combination of newly developed and prevalidated text mining dictionaries. For the degree of task information in each text, we inductively sourced a list of context-specific task descriptor words. To start, we acquired all 34,851 freelancers' service skill tags ([ 7]; for an illustration, see Web Appendix D), which freelancers list in their profiles to describe the service tasks they offer (e.g., ""developer,"" ""illustrator""). After removing stop words and duplicates, two coders reviewed the remaining word list, deleted any misspelled words, and removed terms that did not describe a service (e.g., ""great,"" ""reliable""). Using Quanteda ([ 4]), we stemmed the remaining words, leaving 1,912 unique word stems that describe service tasks. We mined each call for bids and bid, then summed word occurrences reflecting the new task dictionary. By dividing this sum by total words, we obtained a measure of the degree (ratio) of task information in each text. When people self-disclose personal information, they use singular, first-person pronouns. In line with previous research (e.g., [67]), we measured the degree of personal information as the ratio of first-person singular pronoun words (e.g., ""I,"" ""me"") to the total words in each text. To determine the degree of communication concreteness, we mined each text for [11] list of generally known English lemmas that indicate whether a concept denoted by a term refers to a perceptible entity. Following their operationalization, we included all terms that received a rating of 3 or greater on their bipolar, five-point abstract-to-concrete rating scale.[ 6] That is, terms that score 3 or higher refer to relatively more specific objects, materials, people, processes, or relationships. We again divided the sum of the concrete terms by the total words in each text. Finally, the ratio of emotion-laden words (e.g., ""problematic,"" ""easy""; [28]; [37]) determined affective intensity. Using the Linguistic Inquiry and Word Count (LIWC) affect dictionary, we obtained a list of affect words, which we then summed for each text ([66]) and divided each by the corresponding total word count to obtain the degree of affective intensity. Pilot Studies Validity of text-mined measuresTo ensure the validity of our text-mined communication measures, we asked two coders to classify the texts of a random subsample of 100 calls for bids (Mlength = 129 words) and 100 bids (Mlength = 102 words). The coders indicated whether considerable task information, personal information, concreteness, and affective intensity were present in each text ( 1) or not (0). Comparing the coders' classifications with our text-mined classification revealed substantial agreement for both calls for bids (.73 to.94) and bids (.66 to.88) ([47]). The average F1 measure was sufficiently high for both bids (.79 to.95) and calls for bids (.80 to.95), as we detail in Web Appendix F. Experimental evidence of uncertainty reductionTo establish the internal validity of the chosen communication aspects on receivers' uncertainty perceptions, we conducted a series of experimental pilot studies. We used single-factor, within-subject designs for ( 1) task information, ( 2) personal information, ( 3) concreteness, and ( 4) affective intensity. For each pilot study, we recruited between 50 and 53 U.S. consumers with a mean age of 37.6 years (50% women) from Amazon Mechanical Turk (for details, see Web Appendix G). In line with previous research (e.g., [28]; [49]; [55]; [61]), we find that greater use of all four communication aspects in bids significantly reduces buyers' uncertainty perceptions and affects their hiring intentions. Predicting the Success of Buyers' Calls for Bids Model-free evidenceIn Web Appendix H, we summarize the model-free findings. The mean-level comparison indicates that calls for bids with significantly greater degrees of task information and concreteness, as well as significantly lower degrees of personal information and affective intensity, receive more freelance bids than an average call for bids (M = 5). Econometric model and identificationThe success of calls for bids reflects a count variable. Noting the overdispersion in the data (p < .001), we used a negative binomial model instead of a Poisson model. Furthermore, calls for bids are nested within buyers, and thus, the call for bids and number of freelancers who offer their service might be interdependent. The significant between-group variance (p < .001) and ICC( 1) of.27 suggests a multilevel structure. We therefore specified a multilevel model with a random intercept to control for time-invariant unobserved differences between buyers (e.g., education, country, gender) that could relate to differences in their success, using the following base equation: CALSUCij=y00+y01BTASKij+y02BPERSij+y03BCONCij+y04BINTEij+y05BTASK_SQij+y06BPERS_SQij+y07BCONC_SQij+y08BINTE_SQij+μ0j+εij, Graph( 1)where  CALSUCij  is the success of a call for bids i (i = 1, ..., 343,796) issued by buyer j (j = 1, ..., 49,081),  BTASKij  is buyer task information,  BPERSij  indicates buyer personal information,  BCONCij  is buyer concreteness, and  BINTEij  refers to buyer affective intensity in the call for bids. In turn,  BTASK_SQij  is buyer task information squared,  BPERS_SQij  is buyer personal information squared,  BCONC_SQij  is buyer concreteness squared, and  BINTE_SQij  is buyer affective intensity squared. Finally,  μ0j  is the random intercept and  εij  is the error term.Some empirical challenges inhibited a robust model identification, which we addressed in several ways. To account for observed heterogeneity, we incorporated covariates that might influence how many freelancers respond to a particular call for bids. First, in line with extant text mining studies ([ 7]), we controlled for the word count in each call for bids. Second, as a reputation cue, we measured buyer experience as the number of projects a buyer had commissioned previously on the platform prior to posting the focal call for bids ([32]). Third, a higher payment offer may attract more freelancers ([36]), so we determined the payment offered by the buyer in U.S. dollars, multiplied by an undisclosed index for anonymity. We used a dummy for nondisclosed payments, but we replaced missing values with a grand mean to retain the observations. Fourth, we measured project duration, as longer projects attract more freelancers ([36]). A dummy variable indicated whether the project was slated to last more ( 1) or less than a month (0). Fifth, more buyers demanding freelance services at the same time creates a relative shortage of freelancers ([36]). To account for an excess supply of freelancers, we calculated the sum of all active freelancers in the specific submarket of the call for bids and divided by the sum of all calls for bids posted around the same time (±31 days) in the same submarket. Sixth, the marketplace grew over time, so we included fixed effects for the year of the call for bids. Seventh, we included fixed effects for the seven submarkets, since submarkets that feature more complex projects have fewer qualified freelancers.Beyond these observed covariates, buyers' bid formulations might have varied by project characteristics unobservable to us. To the extent that these unobserved project characteristics influenced both the buyers' communication strategies and buyer outcomes, the estimated parameters might be biased. Therefore, we concatenated all service skill tags from the service profile of each freelancer who submitted a bid in response to a specific call. Then, to uncover the latent mixture of project types, we applied a latent Dirichlet allocation model to the project-specific skill tags (e.g., [ 7]; see Web Appendix I). We included the resulting 12 latent project characteristics as fixed effects to account for unobserved heterogeneity.Buyers also strategically make their communication decisions in learned anticipation of a larger number of bids or other factors, which were potentially unobservable to us. This strategic behavior could make communication approaches endogenous ([42]). Because our data did not contain valid, strong instruments for buyers' communications, we adopted [65] approach and used Gaussian copulas to model the correlation between each buyer communication  BCOMij1-4  and the error term. We added regressors to Equation 1, such that BCOMij1−4~=Φ−1[H(BCOMij1−4)], Graph( 2)where  Φ−1  is the inverse of the normal cumulative distribution function and  [H(BCOMij1−4)]  represents the empirical distribution functions of the four buyer communication approaches. The endogenous regressors must be nonnormally distributed for identification ([65]), and we confirmed this was true using Shapiro–Wilks tests (all p < .001). The updated equation to predict buyers' call for bids success, after correcting for endogeneity, was thus CALSUCij=y00+y01BTASKij+y02BPERSijqquad+y03BCONCij+y04BINTEijqquad+y05BTASK_SQij+y06BPERS_SQijqquad+y07BCONC_SQij+y08BINTE_SQijqquad+y09−14CONij1−6+y15−20YEARij1−6qquad+y21−26SUBMij1−6+y27−37PROJij1−11qquad+y38−41BCOM~ij1−4+μ0j+εij, Graph( 3)where  CONij1−5  is the vector of control variables,  YEARij1−6  are year effects,  SUBMij1−6  are submarket effects,  PROJij1−11  are latent project clusters, and  BCOM~ij1−4  are Gaussian copulas. We used a robust estimator to account for correlated and clustered standard errors. Results and discussionThe maximum variance inflation factor is 2.11, indicating no potential threat of multicollinearity. Table 2 contains the results of a main effects model and the full model, and Figure 1, Panels A–D, display the curvilinear effects from the full model. We have proposed that extremely sparse and extremely dense degrees of relevant information, concreteness, and affective intensity in calls for bids yield fewer freelance bids than moderate degrees of these communication elements. In line with our expectations, we find a positive linear effect (.152, p < .01) and negative squared effect for task information (−.026, p < .01), as displayed in Figure 1, Panel A. Moderate levels of the use of task information (50%:.222, p < .01) yield better results than sparse (10%: −.426, p < .01) and dense (90%: −.495, p < .01) uses. Furthermore, we find a positive linear effect (.052, p < .01) and negative squared effect for concreteness (−.080, p < .01) (Figure 1, Panel C). Moderate use (50%:.078, p < .01) yields better results than sparse use (10%: −.092, p < .01) or dense use (90%: −.251, p < .01) of concreteness. Contrary to our expectations, we find a negative linear effect (−.190, p < .01) and a positive squared effect (.032, p < .01) of personal information (Figure 1, Panel B). We also find a negative linear effect (−.084, p < .01) and a nonsignificant squared effect (.001, ns) of affective intensity (Figure 1, Panel D). Thus, it appears that any provision of personal information or greater use of affective intensity by the buyer is always ineffective. As a possible explanation, we note that in B2B online conversations, self-disclosure and emotions may be valued only after business relations have been established, not at the moment they form ([46]). Most of the exchanges in our data were between strangers, rather than being repeat exchanges, so it may be more appropriate for buyers to avoid personal details and appear rational rather than emotive.Graph: Figure 2. Response surfaces for bid success and price premium.GraphTable 2. Predicting the Success of Buyers' Calls for Bids. Model 1:Main EffectsModel 2:Full ModelβSE95% CIβSE95% CIBuyer Communication Task information.123**.003.117,.128.152**.003.146,.159 Personal information−.149**.004−.157, −.141−.190**.004−.199, −.181 Concreteness.040**.003.035,.045.052**.003.046,.057 Affective intensity−.098**.007−.112, −.085−.084**.008−.100, −.068Buyer Communication Squared Task information squared−.026**.001−.028, −.024 Personal information squared.032**.002.029,.035 Concreteness squared−.008**.001−.011, −.007 Affective intensity squared.001.001−.001,.004Controls Word count−.025**.003−.031, −.019−.027**.003−.033, −.021 Buyer experience−.033**.007−.046, −.020−.033**.007−.046, −.020 Project payment.170**.005.159,.181.168**.005.158,.179 Payment not disclosed.073**.005.063,.083.071**.005.061,.081 Project duration.116**.003.110,.122.117**.003.111,.123 Excess supply of freelancers.614**.004.606,.622.606**.004.598,.613Fixed Effects Yearsincludedincluded SubmarketsincludedincludedUnobserved Heterogeneity Project characteristicsincludedincludedEndogeneity Corrections Gaussian copulasincludedincludedBuyers49,081Call for bids343,796 1 Notes: Standardized results. Significance is based on two-tailed tests. The dependent variable is the count of all bids received. The sample included all projects listed by buyers with at least two projects to which at least one freelancer submitted a bid. Effects for years, submarkets, project characteristics, and Gaussian copulas are detailed in Web Appendix Q.2 *p < .05.3 **p < .01.To entice more freelancers to bid, buyers should keep their calls for bids brief (−.027, p < .01 for word count), which emphasizes the need for careful formulations. Higher payment offers (.168, p < .01), longer project durations (.117, p < .01), and an excess supply of freelancers (.606, p < .01) all increase the number of bids. Notably, the number of projects a buyer has previously commissioned relates negatively to the number of freelancers who bid (−.033, p < .01). These experienced buyers might have established relationships with specific freelancers, which reduces other freelancers' chances and causes them to refrain from bidding ([48]). Predicting Freelancers' Bid Success Model-free evidenceBids that offer less personal information and greater task information, concreteness, and affective intensity are more successful in winning projects. Among bids that won, the mean-level comparisons indicate nonlinear effects of mimicry. That is, successful freelancers mimic buyers' use of task information, personal information, and concreteness closely. If a buyer uses very sparse or very dense degrees of these communication aspects, the winning freelancers deviate more, indicating a nonlinear impact of mimicry. We do not find evidence of this mimicry relationship for affective intensity (see Web Appendix H). Measurement of similarityPrevious studies often operationalize communication similarity as the absolute difference between two measures (e.g., [52]; [75]), but this approach suffers some implicit constraints ([17]). In particular, difference scores suggest that one party's communication increases at the same magnitude as the other's decreases. They also ignore the degree at which the relative mimicry occurs. As a preferable alternative, we use polynomial regression, which allows for simultaneous testing of similarity and dissimilarity effects on bid success, at different levels of freelancers' and buyers' uses of the four communication aspects. In their study of positive and negative emotional tone convergence, [24] also use polynomial regression to explore the nuanced effects of convergence in leader–follower relationships on leader–member exchange quality. A simple regression model that captures absolute deviation cannot simultaneously assess the degree of task information by the buyer and the potential nonlinear effects of task information mimicry by the freelancer. So, we performed a polynomial regression with response surface analyses for each communication aspect to capture the extent to which freelancers mimicked a prospective buyer's provision of relevant information and communication manner. We detail this polynomial modeling approach that led to Equation 4 and the calculation of all polynomial terms, using task information as an example, in Web Appendix E. Econometric model and identificationWe tested freelancers' trade-off between adding more uncertainty-reducing communication versus mimicking the buyer's communication in a polynomial regression model that included linear terms, quadratic terms, and interactions. In the multilevel base equation to predict freelancers' bid success (ICC( 1) = .09, p < .001), BIDSUCkl=y00+y01−04FCOMkl1−4+y05−08BCOMkl1−4+y09−12FCOM_SQkl1−4+y13−16(FCOMkl1−4×BCOMkl1−4)+y17−20BCOM_SQkl1−4+μ0l+εkl, Graph( 4) BIDSUCkl  is the success of bid k (k = 1, ..., 2,327,216) by freelancer l (l = 1, ..., 34,851),  FCOMkl1−4  are the four freelancer communication aspects,  BCOMkl1−4  indicate the four buyer communication aspects,  FCOM_SQkl1−4  are freelancer communication aspects squared,  (FCOMkl1−4×BCOMkl1−4)  are interactions of freelancer and buyer communication aspects,  BCOM_SQkl1−4  are buyer communication squared,  μ0l  is the random intercept, and  εkl  is the error term.We incorporated several covariates that might influence freelancers' bid success. As in the buyer model, we controlled for word count, project payment, project duration, and excess supply of active freelancers. We also included fixed effects for years, submarkets, and latent project characteristics. We accounted for the number of projects the freelancer completed prior to submitting the focal bid as a reputation cue that might determine bid success ([32]). Freelancer rating is an average five-point satisfaction rating that a freelancer has received for all completed projects. To retain observations of unrated freelancers, we included a dummy for observations without star ratings and replaced the missing values with a grand mean rating.Several additional controls relate to whether a bid is successful. First, following prior research, we assessed linguistic style matching, or the similarity between each bid and the respective call for bids, across nine function word categories ([52]). Second, we accounted for any previous relationship in which the freelancer had completed at least one project for the same buyer prior to the specific call for bids ([32]). Third, freelancers submit a bid price that may differ from the payment offered by the buyer, and a higher bid price may reduce the likelihood of bid success ([32]). In light of this, we measured each bid price as a ratio between the asking price and the average indexed bid price requested by all competing freelancers for the same call for bids. Fourth, the longer it takes freelancers to submit a bid, the lower their chances of success ([34]). So, we measured time-to-bid as the number of days between the posting of the call for bids and the bid submission. A dummy variable also indicates whether the bid was submitted late ( 1) or on time (0). Fifth, competition for a specific call for bid should impact each bid's success chances, so we controlled for the number of bids for the same call ([34]).Similar to buyers, freelancers make communication decisions strategically in anticipation of higher bid success or other, unobservable factors. Thus, freelancer communication is potentially endogenous, so we again used Gaussian copulas (Shapiro–Wilk tests: all p < .001). The updated equation to predict freelancers' bid success is as follows: BIDSUCkl=y00+y01−04FCOMkl1−4+y05−08BCOMkl1−4+y09−12FCOM_SQkl1−4+y13−16(FCOMkl1−4×BCOMkl1−4)+y17−20BCOM_SQkl1−4+y21−33CONkl1−13+y34−39YEARkl1−6+y40−45SUBMkl1−6+y46−56PROJkl1−11+y57−60FCOM~kl1−4+y61−64BCOM~kl1−4+μ0l+εkl, Graph( 5)where  CONkl1−14  is the vector of control variables,  YEARkl1−6  are year effects,  SUBMkl1−6  are submarket effects,  PROJkl1−11  are latent project clusters,  FCOM~kl1−4  are Gaussian copulas for bid text, and  BCOM~kl1−4  are Gaussian copulas for calls for bids text. Results and discussionThe maximum variance inflation factor is 3.86, indicating no threat of multicollinearity. Table 3 contains the results of the freelancer bid success models, Web Appendix K summarizes the response surface coefficients, and Figure 2 displays these coefficients on three-dimensional surfaces, reflecting relationships among freelancer communication, buyer communication, and bid success. We also highlight the misfit line used to explore the trade-off between exceeding and falling short of buyers' communication levels.GraphTable 3. Predicting Freelancers' Bid Success. Model 3:Freelancer CommunicationModel 4:Full ModelβSE95% CIβSE95% CIFreelancer Communication y01: Task information.014**.001.013,.015.015**.001.014,.016 y02: Personal information.018**.001.016,.019.017**.001.016,.017 y03: Concreteness.030**.001.029,.031.031**.001.030,.032 y04: Affective intensity.001.001−.001,.003.000.001−.002,.001Buyer Communication y05: Task information−.009**.000−.009, −.008 y06: Personal information−.017**.000−.018, −.017 y07: Concreteness−.008**.000−.009, −.008 y08: Affective intensity.001**.000.001,.002Freelancer Communication Squared y09: Task information squared−.006**.000−.006, −.005−.006**.000−.007, −.006 y10: Personal information squared−.005**.000−.005, −.004−.005**.000−.005, −.004 y11: Concreteness squared−.006**.000−.007, −.006−.007**.000−.007, −.007 y12: Affective intensity squared.000**.000.000,.001.000**.000.000,.001Freelancer–Buyer Interactions y13: Task information interaction.015**.000.015,.016 y14: Personal information interaction−.002**.000−.002, −.001 y15: Concreteness interaction.005**.000.004,.005 y16: Affective intensity interaction.020**.000.020,.021Buyer Communication Squared y17: Task information squared.001**.000.001,.002 y18: Personal information squared−.004**.000−.005, −.004 y19: Concreteness squared.001**.000.001,.001 y20: Affective intensity squared.000*.000.000,.000Controls Word count−.022**.001−.024, −.021−.021**.001−.022, −.020 Linguistic style matching.051**.001.048,.053.051**.001.048,.053 Freelancer experience.002**.001.001,.003.002**.001.001,.003 Freelancer rating.010**.001.009,.010.010**.001.009,.010 Project payment−.001**.000−.001, −.001−.001**.000−.001, −.001 Payment not disclosed−.028**.000−.029, −.028−.029**.000−.030, −.028 Previous relationship.078**.001.076,.081.078**.001.075,.080 Bid price−.006**.000−.006, −.006−.006**.000−.007, −.006 Time-to-bid.001.000.000,.001.001.000.000,.001 Late submission−.005**.000−.005, −.004−.004**.000−.005, −.004 Competition−.251**.007−.265, −.238−.251**.007−.264, −.238 Excess supply of freelancers−.044**.000−.045, −.043−.042**.000−.043, −.042Fixed Effects Yearsincludedincluded SubmarketsincludedincludedUnobserved Heterogeneity Project characteristicsincludedincludedEndogeneity Corrections Gaussian copulasincludedincludedFreelancers34,851Bids2,327,216 4 Notes: Standardized results. Significance is based on two-tailed tests. The dependent variable is whether the freelancer was chosen and won the bidding process. The sample included all bids by freelancers with at least one winning and at least one losing bid. Effects for years, submarkets, project characteristics, and Gaussian copulas are detailed in Web Appendix Q.5 *p < .05.6 **p < .01.We have proposed that when the degree of relevant information, concreteness, and affective intensity provided by the buyer is at least moderately dense (sparse), freelancers can increase (decrease) their chances of bid success by mimicking the buyer's communication. The surface-level tests along the plotted misfit line (Web Appendix K) display negative curvatures for task information (−.020, p < .01), personal information (−.007, p < .01), concreteness (−.011, p < .01), and affective intensity (−.020, p < .01). These results indicate that mimicking the buyer's communication increases bid success (see Web Appendix L for further clarification).In line with our proposition, we qualify this effect for sparse degrees of task and personal information, concreteness, and affective intensity provided by the buyer in Web Appendix M. If we were to find positive slope coefficients at lower levels, it would suggest that freelancers can increase their chances of bid success by exceeding, rather than mimicking, the buyer's communication. This prediction holds for personal information (.020, p < .01) and concreteness (.024, p < .01), according to the slopes at low levels of buyer communication. However, contrary to our expectations, we find negative effects for the slopes of task information (−.008, p < .01) and affective intensity (−.030, p < .01) at low levels of buyer communication. Therefore, freelancers should always mimic the degree of task information and affective intensity provided by the buyer. For these two communication aspects, the tenets of communication accommodation theory ([75]) and adaptive selling ([78]) hold: mimicking the buyer is always better. To increase their chances of bid success further, freelancers also must keep their bids concise (−.021, p < .01 for word count). Reputation cues (experience:.002, p < .01; rating:.010, p<.01) increase freelancers' chances of bid success, as do linguistic style matching (.051, p < .01), previous business relations with the buyer (.078, p < .01), lower bid prices (−.006, p < .01), timely (cf. late) bid submissions (−.004, p<.01), lack of competition (−.251, p < .01), and reduced supply of freelancers (−.042, p < .01). Predicting Freelancers' Price Premium Model-free evidenceBids with significantly less affective intensity and significantly more task information, personal information, and concreteness achieve greater price premiums than an average bid (M = 14% discount). Moreover, 96% of freelancers completed projects without any price premium, indicating the prevalence of value traps. The bids that achieved price premiums mimicked those buyers that made moderate use of task information, concreteness, and affective intensity closely, yet they deviated from buyers that made very sparse or very dense use of them. For personal information, we find a distinctive, positive, linear relationship for mimicry. Successful freelancers mimicked buyers that supplied a lot of personal details but deviated if buyers supplied very little or moderate degrees of personal information (Web Appendix H). Econometric model and identificationThe price premium analysis is restricted to bids that win and buyers that disclose their payment offer upfront. Thus, our estimates may be biased by buyers' self-selection, in terms of which bid they chose and whether they disclosed payments. Therefore, we employed a two-stage selection model. In the first stage, we estimated a choice model, with the availability of the necessary data as a binary dependent variable (i.e., bid was won and payment was disclosed). Using this model, we computed the inverse Mills ratio to account for the potential selection bias (probit model in Web Appendix N) and included this correction term in the final model estimation. To identify second-stage parameters, there needed to be one term in the first-stage equation that was unrelated to the error term in the freelance price premium equation. We thus included the dummy that indicates if the bid was submitted late only in the first-stage equation because this term explained buyers' choice of the bid, but we did not expect it to be conceptually related with the eventual price premium. Thus, this term satisfied both relevance and exogeneity requirements. The updated equation of our multilevel model (ICC( 1) = .13, p < .001) is as follows: PREMIUMkl=y00+y01−04FCOMkl1−4+y05−08BCOMkl1−4+y09−12FCOM_SQkl1−4+y13−16(FCOMkl1−4×BCOMkl1−4)+y17−20BCOM_SQkl1−4+y21−31CONkl1−11+y32−37YEARkl1−6+y39−43SUBMkl1−6+y44−54PROJkl1−11+y55−58FCOM~kl1−4+y59−62BCOM~kl1−4+y63IMRkl+μ0l+εkl, Graph( 6)where  PREMIUMkl  is the price premium of bid k (k = 1, ..., 148,158) offered by freelancer l (l = 1, ..., 30,851), and  IMRkl  is the correction term. Results and discussionThe maximum variance inflation factor is 2.74, indicating no threat of multicollinearity. Table 4 contains the results of the freelancer price premium models, Web Appendix K details the response surface coefficients, and Figure 2 displays the surfaces.GraphTable 4. Predicting Freelancers' Price Premium. Model 5:Freelancer CommunicationModel 6:Full ModelβSE95% CIβSE95% CIFreelancer Communication y01: Task information.023**.002.020,.026.022**.002.019,.025 y02: Personal information.021**.002.017,.025.021**.002.017,.026 y03: Concreteness.006**.001.004,.007.005**.001.003,.007 y04: Affective intensity.004.003−.002,.010.003.003−.003,.009Buyer Communication y05: Task information.003*.001.001,.005 y06: Personal information−.016**.002−.019, −.012 y07: Concreteness−.004**.001−.006, −.002 y08: Affective intensity−.001.002−.005,.003Freelancer Communication Squared y09: Task information squared−.001.001−.002,.000−.001.001−.002,.001 y10: Personal information squared−.004**.001−.006, −.002−.004**.001−.006, −.002 y11: Concreteness squared−.003**.001−.004, −.002−.003**.001−.005, −.002 y12: Affective intensity squared.000.000−.001,.000.000.000−.001,.000Freelancer–Buyer Interactions y13: Task information interaction.025**.003.024,.027 y14: Personal information interaction−.004**.001−.005, −.002 y15: Concreteness interaction.002**.000.001,.002 y16: Affective intensity interaction.010**.003.008,.012Buyer Communication Squared, y17: Task information squared.003**.001.001,.004 y18: Personal information squared.003**.001.002,.005 y19: Concreteness squared−.002**.001−.004, −.001 y20: Affective intensity squared.002**.001.000,.003Controls Word count−.014**.002−.018, −.011−.014**.002−.018, −.011 Linguistic style matching.022**.004.013,.030.023**.004.015,.032 Freelancer experience−.001.001−.004,.001−.001.001−.004,.001 Freelancer rating−.001.001−.003,.001−.001.001−.003,.001 Project payment−.029**.010−.049, −.009−.029**.010−.049, −.009 Previous relationship.057**.001.054,.059.056**.001.054,.059 Time-to-bid.009**.001.007,.012.009**.001.006,.011 Competition−.015**.002−.019, −.011−.015**.002−.019, −.011 Excess supply of freelancers−.001.001−.002,.001−.001.001−.002,.001Fixed Effects Yearsincludedincluded SubmarketsincludedincludedUnobserved Heterogeneity Project characteristicsincludedincludedEndogeneity Corrections Gaussian copulasincludedincludedSample-Selection Correction Inverse Mills ratio−.016**.002−.020, −.012−.016**.002−.020, −.012Freelancers30,851Bids148,158 7 Notes: Standardized results. Significance is based on two-tailed tests. The dependent variable is price premium for the chosen bid. The sample includes all winning bids for which the payment was disclosed. Effects for years, submarkets, project characteristics, and Gaussian copulas are detailed in Web Appendix Q.8 *p < .05.9 **p < .01.We proposed that when the degree of relevant information and communication manner provided by the buyer is at least moderately high (low), freelancers increase (decrease) their chances of earning a price premium by mimicking this communication. Web Appendix O displays the misfit lines on two-dimensional planes. In line with our expectations, the surface-level tests along the plotted misfit line show a negative curvature for task information (−.023, p < .01), concreteness (−.007, p < .01), and affective intensity (−.008, p < .01), such that mimicking the buyer's communication increases bid success. However, for personal information, we find a positive curvature (.003, p < .05), which implies freelancers should always offer more personal information than the buyer. For these B2B services, the provider and the service are inseparable, which may lead buyers to place more value on personal information about freelancers, even if their own provision of personal details in the calls for bids is sparse.If a buyer provides little relevant information or is less concrete (Web Appendix P), a positive slope would suggest that freelancers can increase their chances of earning a price premium by exceeding rather than mimicking the buyer. We find support for this prediction in the slope of personal information (.027, p < .01) at low levels of buyer personal information. However, negative effects emerge from the slopes of task information (−.016, p < .01) and affective intensity (−.012, p < .01), and we find a nonsignificant effect for concreteness (.002, n.s.). Mimicking the buyer's task information and affective intensity is always better, which is in line with accommodation theory and adaptive selling ([75]; [78]).Freelancers also increase their price premiums by avoiding lengthy bids (−.014, p < .01). Although platform reputation cues (experience and rating) can boost freelancers' chances of bid success, they do not determine the final price buyers pay. The skew in the ratings toward very high scores may limit their ability to help prospective buyers determine an appropriate price ([45]. Linguistic style matching (.023, p < .01), a previous relationship with the prospective buyer (.056, p < .01), submitting early in the bid process (.009, p < .01), and reduced competition (−.015, p < .01) all increase buyers' acceptance of a price premium. General DiscussionAcross disciplines, substantial research has identified various success determinants in online freelance marketplaces (e.g., [36]; [77]). For example, studies of B2B exchanges and two-sided marketplaces emphasize communication (see Web Appendix A). But at the specific word level, we lack insights into the optimal information or manner of communication ([ 7]). With this initial investigation of how buyers' and freelancers' success might be enhanced by appropriately managing the other party's uncertainty, we postulate, in line with uncertainty reduction ([ 6]) and uncertainty management ([ 9]) theories, that communication that is not completely informative and clear may still be effective. Accordingly, we investigate how buyers' communication can attract freelance bids and how freelancers' communication can determine their bid success and price most effectively, and the results offer both theoretical and practical implications. Theoretical ContributionsFirst, we advance research on how buyers' communication determines their ability to attract freelancers. Drawing on prior communication research, we identify communication principles that critically relate to receivers' uncertainty, such as relevant task and personal information and the relative concreteness and affective intensity with which this information is communicated ([ 8]; [27]). To entice more freelancers to bid, buyers should carefully formulate their calls for bids to keep them brief. Freelancers' information processing motivation, time, skills, and proficiency likely are limited, so buyers must choose their wording carefully and select from various effective communicative aspects. They can attract a larger pool of bids if they provide moderate degrees of task information in a moderately concrete manner. Offering too little of these features leaves freelancers with too much uncertainty, and dense information provision or being very concrete is too restrictive. If buyers provide greater degrees of personal information or express greater affective intensity in their calls for bids, it reduces the number of service offers they receive. This finding contrasts with uncertainty reduction theory ([ 6]) and B2B research that suggests self-disclosure strengthens buyer–seller cooperativeness ([41]). However, instead of ongoing B2B relationships, our study refers mostly to initial interactions between strangers (in 98% of cases, the freelancer had never worked for the prospective buyer). Evidence obtained from buyer–seller online chats similarly suggests that self-disclosure and emotive expressions are valued only in existing B2B relationships, not in new ones ([46]). Overall, we offer empirical support for communication theorists' suggestions that common communication principles can be purposefully flouted to achieve better conversation outcomes ([23]).Second, freelancers must keep their bids concise. They too face a trade-off between reducing the buyer's uncertainty and offering overly dense information. In line with research on communication accommodation ([75]) and adaptive selling ([78]), we show that freelancers can improve their bid success by mimicking the prospective buyer's communication. Adding to these research streams, we introduce a contingency perspective that reveals the efficacy of mimicry depends on the degree to which buyers use specific communication elements. In line with accommodation theory and adaptive selling, bid success always improves when freelancers mimic buyers' provision of task information and use of affective intensity. However, in line with uncertainty reduction theory ([ 6]) and expectancy violations ([ 1]) when buyers supply little personal information and are less concrete, freelancers can increase their chances of bid success by diverging and providing more personal information and concreteness.Third, freelancers often struggle to avoid value traps in which they sell more of their services for less ([76]). Rational buyer expectations should allow high-quality freelancers to charge price premiums ([70]), but the quality of freelance services is unobservable prior to purchase, and rational buyers might refuse to pay any price premium if they feel uncertain and suspect the freelancer may be hiding information ([16]). Therefore, to achieve premiums, freelancers should offer short, appropriately formulated bids. Buyers are more willing to pay a premium to freelancers who mimic their provision of task information, concreteness, and affective intensity, which is in line with communication accommodation theory ([75]) and adaptive selling research ([78]). However, similar to the findings for bid success, freelancers should offer more personal information than buyers, rather than mimicking buyers' provision of such information. In most service settings, a ""bad"" seller might provide a great product by chance; however, almost by definition, a bad freelancer produces bad service ([36]). This tight coupling between the freelancer and service quality represents a conceptual distinction in our study, which accordingly shows that buyers' willingness to pay a premium increases with more personal information issued by the freelancer. Practical ImplicationsOur findings offer actionable insights for the millions of buyers and freelancers utilizing online freelance marketplaces, the collective value of which is predicted to reach $2.7 trillion by 2025 ([56]). In detail, being informative and unambiguous may be a common assumption, but it is not an imperative, nor does it always lead to success. Implications for buyersAlthough 59% of U.S. companies use a flexible workforce to some degree, more than one-third of contracted projects are never completed ([33]). To attract freelancers, buyers should keep their calls for bids succinct. Beyond that recommendation, we offer several tips for formulating calls for bids in Table 5. In particular, a task description with a moderate amount of information helps freelancers anticipate the task without overloading them with details. Due to the relative anonymity of online freelance marketplaces, buyers might assume that freelancers will need to know who they are, but instead, we find that the less buyers describe themselves (to focus on describing the task), the better the outcomes. Relatable and imaginable (rather than abstract) descriptions of the project help freelancers grasp the requirements. However, being excessively concrete becomes prescriptive, which deters freelancers. Using emotion words makes the content of a call for bids relatively more intense. Such intensity can remove ambiguity and make opinions quickly accessible, but we find that calls for bids are more effective if they are formulated relatively impassively. Enthusiastic project descriptions seemingly might raise freelancers' suspicion that the project is too good to be true. Also, offering higher payment might attract a larger pool of freelance bids, as do long- rather than short-term gigs. Finally, more freelancers bid when there are fewer calls for bids in the subsector.GraphTable 5. Buying and Selling Services in Online Freelance Marketplaces. How to Formulate Calls for Bids to Attract FreelancersBad Practice ExcerptGood Practice ExcerptLift in BidsSpecify tasks and skills""I need a website to showcase the full range of my fitness workouts.""""I need a website designer who can design a WordPress website using a WordPress premium theme.""An increase in task terms from 18% to 29%, resulting in 5% more bids.Avoid personal information""I have been creating my own classes for almost 10 years now...clients tend to especially love my classes on strength and flexibility. Now I need help setting up my website.""""I am a Fitness Trainer and need help with building my website to showcase my mixed services and home workouts.""A decrease in personal terms from 9% to 4%, resulting in 4% more bids.Be moderately concrete""I require a professional who is savvy in configuring a stylish website employing a premium theme.""""You should have got very good creative skills but know how to design for web and also know how to include calls to actions within a good design.""An increase in concrete terms from 21% to 26%, resulting in 1% more bids.Avoid being affectively intense""I have created a fantastic theme but you should be confident and eager about WordPress and help optimize.""""The theme and examples will be provided, but you should also know about WordPress and optimize.""A decrease in affective terms from 11% to 4%, resulting in 4% more bids.How to Formulate Successful Bids and Achieve Price Premiums Bad Practice ExcerptGood Practice ExcerptLift in Bid SuccessMimic task description""Dear Sir, would love to work for you...""""Hi Gary, I am happy to help you with your fitness website development and design...""An increase in task terms from 16% to 25%, resulting in 7% higher bid success and 8% higher price premium.Exceed buyers who supply little personal information""I am an enthusiastic designer and expert in Web development...""""I am a WordPress Freelancer with 15 years of work experience...""An increase in personal terms from 6% to 8%, resulting in 3% higher bid success and 4% higher price premium.Exceed buyers who are not concrete""I have great skills and plenty of fantastic experience in creating relevant websites...""""I have worked on several similar projects, designing websites, also using WordPress, including premium themes and I can deliver to a tight schedule...""An increase in concrete terms from 24% to 30%, resulting in 7% higher bid success (but no effect on price premium).Mimic the buyer's affective intensity""The content will be creative and fun, attractive, and thoughtful...""""Website content that I produce will be creative and include original designs...""A decrease in affective terms from 18% to 6%, resulting in 11% higher bid success and 7% higher price premium. 10 Notes: Web Appendix S provides the full call for bids and bid examples we used for calculating the degrees of each communicative principle and the corresponding expected lift success. We used the ""good practice"" call for bids example to devise the bad and good examples for the corresponding freelance bid. Implications for freelancersFreelancers are not necessarily natural marketers, but their bid formulations determine their marketability. Existing online reputation systems provide some assistance, but they also create entry barriers to new freelancers who first must earn good overall ratings ([13]). Fortunately, winning gigs and achieving price premiums also depends on freelancers' communication. Table 5 includes advice to help freelancers formulate more successful bids and avoid the value trap. In line with the mantra of adaptive selling, the call for bids provides a starting point in which mimicking the buyer's task information and affective intensity increases freelancers' success—even if they provide few task details or seem very impassive. But freelancers should always offer personal information and be concrete. Even if a buyer does not provide personal information or the call is relatively abstract, freelancers' chances of success and obtaining price premiums increase if their bids contain more personal information and are at least somewhat concrete. The strongest predictor of bid success is a preexisting buyer relationship, so more broadly, freelancers should grow their buyer relations. Limitations and Directions for Further ResearchIn examining theoretically grounded communicative aspects, we offer novel insights into how to manage uncertainty in buyer–freelancer exchanges. Intriguingly, we find that communication approaches that do not aim to minimize uncertainty can be effective. Continued research should investigate this notion further and develop additional insights into the exchange implications of linguistic choices in B2B but also B2C and C2C communication on multisided platforms ([53]). For example, affiliative ([66]) or collaborative terms might affect uncertainty and influence exchanges as well. Arguably, the personal characteristics of buyers and freelancers (e.g., gender, education, experience), channel choices ([50]), different sources of uncertainty ([29]), perceived risks ([25]), and spatial distances between buyers and freelancers also might moderate the efficacy of communication aspects, so additional research should specify their influences. For example, if buyers lack the expertise to specify what they want, they might benefit from more ambiguous calls for bids ([38]). Perhaps buyers' communication or alternative factors that we cannot account for (e.g., underestimation of the amount of work required to fulfill the task) influence the final price they pay, too. Efforts to specify these additional effects also might address some of our more controversial findings, such as the evidence that the number of previously commissioned projects by a buyer relates negatively to the number of freelancers who bid. We posit that experienced buyers might prefer freelancers whom they have hired in the past ([48]). Buyers also might have incurred switching costs or supplier dependencies ([29]). Methodologically, we estimated all the models sequentially, as buyers' calls for bids and their success occur prior to freelancers' bids and their success. But an equilibrium approach that estimates these models simultaneously at the bid level could reflect an alternative way to think about the data structure. The concreteness word list we used ([11]) may also require further refinement to differentiate specific concreteness levels among the word set. Finally, the anonymity and speed of exchanges in online freelance marketplaces may make communication particularly important in this context. A comparative analysis of the influence of uncertainty management efforts across different B2B contexts beyond these marketplaces could offer interesting insights, especially if uncertainty avoidance is a central goal.  "
4,"Conducting Research in Marketing with Quasi-Experiments This article aims to broaden the understanding of quasi-experimental methods among marketing scholars and those who read their work by describing the underlying logic and set of actions that make their work convincing. The purpose of quasi-experimental methods is, in the absence of experimental variation, to determine the presence of a causal relationship. First, the authors explore how to identify settings and data where it is interesting to understand whether an action causally affects a marketing outcome. Second, they outline how to structure an empirical strategy to identify a causal empirical relationship. The article details the application of various methods to identify how an action affects an outcome in marketing, including difference-in-differences, regression discontinuity, instrumental variables, propensity score matching, synthetic control, and selection bias correction. The authors emphasize the importance of clearly communicating the identifying assumptions underlying the assertion of causality. Last, they explain how exploring the behavioral mechanism—whether individual, organizational, or market level—can actually reinforce arguments of causality.Keywords: quasi-experiments; marketing methods; econometricsQuasi-experimental methods have been widely applied in marketing to explain changes in consumer behavior, firm behavior, and market-level outcomes. ""Quasi-experiment"" refers to the use of an experimental mode of analysis and interpretation to data sets where the data-generating process is not itself intentionally experimental ([23]). Instead, quasi-experimental research uses variation that occurs without experimental intervention but is nonetheless exogenous to the particular research setting. Work using quasi-experiments in marketing settings has used events such as weather, geographic boundaries, contract changes, shifts in firm policy, individual-level life changes, and regulatory changes to approximate a real experiment. In each case, an external shock creates a source of exogenous variation that the researcher uses to establish a causal relationship between the variation and the outcome of interest.Companies also use quasi-experimental methods to understand the consequences of key business actions. For example, [17] analyzed a quasi-experiment where eBay shut all the paid search advertising on Bing during a dispute with Microsoft but lost little traffic. These quasi-experimental results inspired a follow-up field experiment where eBay randomized suspension of its branded paid search advertising and found results consistent with the quasi-experiment. Reflecting the importance of such methods at firms, some companies provide causal inference training for their data scientists ([32]; [81]). The ability to make causal claims is highly valuable in academia and in practice. This article aims to help both marketing scholars and practitioners conduct and evaluate the credibility of quasi-experimental studies.Quasi-experimental research, as in much work in applied statistics, begins with the equation  y=f(X,ε;β)  . The focus is then on whether a change in a single covariate x in the vector of  X  can be demonstrated to cause a change in  y  . This focus often enables the exploration of foundational questions in marketing, because marketers often have data representing the actions of many individual consumers or clients and need to understand the causal relationship between a particular x and y to make decisions about whether and how much x to use.A marketing article that successfully uses the quasi-experimental econometric approach considers the following nine topics, which are echoed in the structure of this article: Research Question: Do We Care Whether x Causes y? Data Question: How Can Researchers Find Data with Quasi-Experimental Variation in x? Identification Strategy: Does x Cause y to Change? Empirical Analysis: How Can Researchers Estimate the Effect of x on y? Challenges to Research Design: What if Variation in x Is Not Exogenous? Robustness: How Robust Is the Effect of x on y? Mechanism: Why Does x Cause y to Change? External Validity: How Generalizable Is the Effect of x on y? Apologies: What Remains Unproven and What Are the Caveats?We start by explaining why quasi-experimental scholars may appear obsessed with identification, and how this influences the choice of research question and data setting. Quasi-experiments come in different shades, ranging from an almost completely random exogenous shock to where the treatment assignment is only partly random. We suggest different frameworks to accommodate various levels of evidence depending on the strength of the underlying identification argument. We then turn to the importance of understanding the underlying mechanism behind the causal result. Typically, this means showing that the effect is largest where theory would predict and is smallest where theory would predict a negligible effect. We also emphasize that researchers need to be clear about the external validity of their study and apologize for what remains unconvincing. Why the Focus on Identification?Why are quasi-experimental scholars seemingly obsessed with identification? Identification is defined by the challenge that ""many different theoretical models and hence many different causal interpretations may be consistent with the same data"" ([58], p. 47). However, effective decision making requires an understanding whether a measured relationship is indeed causal.One way to describe this issue is through the ""potential outcomes approach"" developed by Jerzy Neyman, Donald Rubin, and others ([86]).[ 5] This approach starts with the insight that for any discrete treatment—which could be an event or explicit policy (  D  )—each individual  i  has two possible outcomes: yi1  if the individual i experiences the treatment  Di=1  , and yi0  if the individual i does not experience the treatment  Di=0 The difference between the two is the causal effect. The identification problem occurs because a single individual i cannot both receive the treatment and not receive the treatment at the same time. Therefore, only one outcome is observed for each individual at any point in time. The unobserved outcome is called the ""counterfactual."" The unobservability of the counterfactual means that assumptions are required. The identification problem means that those who experience D, and those who do not, are different in unobserved ways.Random assignment solves the inference problem, as the ""unobserved ways"" should not matter ex ante ([31]). [88], p. 13) explain that ""if implemented correctly, random assignment creates two or more groups of units that are probabilistically similar to each other on the average."" With enough people assigned randomly to one group or another, the only meaningful difference between the groups will be a result of the treatment.Therefore, random assignment is often called the gold standard of identification ([70], p. 8). [12], p. 11) emphasize that ""the most credible and influential research designs use random assignment."" That said, we should be clear that field experiments are merely a gold standard for being able to plausibly claim causality, not the gold standard for empirical work ([34]). Indeed, in many marketing situations, experiments are not feasible, appropriate, or affordable ([44]).Quasi-experimental work, by contrast, is aimed to identify exogenous shocks or events that can approximate random assignment. Given that assignment is not random, a researcher's goal is to make the unobserved ways in which the treatment and control groups differ as untroubling as possible to the researcher and the reader and thereby mimic random assignment as closely as possible. Research Question: Do We Care Whether x Causes y?The first and hardest stage in this process is identifying a question in which marketing scholars, managers, or policy makers actually care whether x causes y. This is difficult because many of the  y  s and  x  s for which we can measure a causal relationship are (unfortunately) uninteresting. Therefore, researchers who do quasi-experimental research do best if they start not with the data or an exogenous shock but instead start by asking themselves, ""Suppose I convincingly showed that an increase in x increases  y  —who would care about this substantive issue?This means that the first stage requires the identification of a causal relationship that would be of interest to marketers or policy makers because their decisions will be usefully informed by a clear understanding of the consequences of a particular action. As marketing technology and practices change, the number of measurable, interesting, and unanswered questions grows. A variety of editorials in this journal and elsewhere focus on how researchers can identify important issues. For example, the January 2021 special issue of the Journal of Marketing was dedicated to finding important marketing research questions, as highlighted in the editorial ([36]. Other editorials that discuss ways to identify important research questions are [59]) and [26] in the Journal of Marketing, [87]) in the Journal of Consumer Research, [96] in Marketing Science, and [50]) in the Journal of Marketing Research. Data Question: How Can Researchers Find Data with Quasi-Experimental Variation in x?[12], p. 7) explain that an identification strategy ""describe[s] the manner in which a researcher uses observational data or data that is not generated as part of an intentional experiment, to approximate a real experiment."" They suggest first thinking of an ideal randomized experiment that can address the research question. This helps the researcher see clearly why an effect may not be identified causally in a nonexperimental setting.As [75], p. 151) discusses, ""Good natural experiments are studies in which there is a transparent exogenous source of variation in the explanatory variables that determine treatment assignment."" Unfortunately, there is no universally accepted interpretation of what it means to have a transparent exogenous source of variation. Therefore, [75] (p. 151) emphasizes the importance of clarifying identification assumptions and understanding the institutional setting, stating, ""If one cannot experimentally control the variation one is using, one should understand its source."" In the marketing context, [84] discusses the dangers of using methods in which the source of the exogenous variation is either poorly understood or only weakly related to the correlation of interest.Much of the work using quasi-experimental variation in marketing settings uses mundane but easily understood events such as contract changes, regulation, individual-level life changes, or shifts in firm policy that did not occur because of an anticipated effect on the outcome of interest. In some sense, some of the best sources of exogenous variation are mundane: nonmundane sources of variation such as global pandemics or earthquakes tend to be associated with other things happening that make it difficult to establish a clean causal relationship.Table 1 lists several example quasi-experimental papers published in 2018, 2019, and 2020 in the Journal of Marketing, the Journal of Marketing Research, and Marketing Science. This table also summarizes the source of variation these articles use, spanning contractual changes; ecological variation (e.g., weather); geography; and macroeconomic, individual, organizational, and regulatory changes. It is useful to consider in turn why each of these sources of variation can approximate random assignment.GraphTable 1. Examples of Quasi-Experiment Studies in Journal of Marketing, Journal of Marketing Research, and Marketing Science in 2018–2020. Quasi-Experimental VariationGeneral CategorySourceArticleResearch QuestionContractualTiming of American–Orbitz disputes to evaluate the absence of a major airline from a popular aggregator on consumer searchAkca and Rao (2020)Who has more market power in the airline-aggregator relationships?Timing of the introduction of the New York Times paywallPattabhiramaiah, Sriram, and Manchanda (2019)How does a paywall affect readership and site traffic?EcologicalVariation in the forecast error of the pollen levelsThomas (2020)How much does advertising affect purchases of allergy products?GeographicalDiscontinuities in the level of advertising at the borders of DMAsShapiro (2020)Does advertising affect consumer choice of health insurance?Discontinuities in the level of political ads at the borders of DMAsWang, Lewis, and Schweidel (2018)How does political advertising source and message tone affect vote shares and turnout rates in 2010 and 2012 Senatorial elections?IndividualTiming of users' adoption of a music streaming serviceDatta, Knox, and Bronnenberg (2018)How does a streaming service affect total music consumption?Variation in national ad exposures due to the local game outcomesHartmann and Klapper (2018)How do Super Bowl ads affect brand purchases?MacroeconomicVariation in income and wealth due to the recession between 2006 and 2009Dube, Hitsch, and Rossi (2018)Do income and wealth affect demand for private label products?OrganizationalDiscontinuity in the rounding rule that TripAdvisor uses to convert average ratings into displayed ratingsHollenbeck, Moorthy, and Proserpio (2019)How do online reviews affect advertising spending in the hotel industry?Timing of data breach and variation whether customer information was breached in a data breach eventJanakiraman, Lim, and Rishika (2018)How does a data breach announcement affect customer spending and channel migration?Variation in timing of adoption of front-of-package nutritional labels across categoriesLim et al. (2020)Do front-of-package nutritional labels affect nutritional quality for other brands in a category?RegulatoryTiming of the Massachusetts open payment lawGuo, Sriram, and Manchanda (2020)Do payment disclosure laws affect physician prescription behavior?Enforcement of minimum advertisement price policiesIsraeli (2018)What is the effect on violations if firms improve digital monitoring and enforcement of minimum advertised price policies?Timing of India's foreign direct investment liberalization reform in 1991Ramani and Srinivasan (2019)How do firms respond to foreign direct investment liberalization? 1 Notes: DMA = designated market area. ContractualTo find plausibly exogenous variation in timing, it often depends on an argument that the exact timing of a measure is plausibly exogenous. [28] argued that the timing of a dispute between the Associated Press and Google was essentially random as it was influenced by a contract negotiated many years previously, and so the timing could be used to study the effect of the removal of content from news aggregators on downstream news websites. EcologicalGenerally, within-season variation in weather is plausibly exogenous. For example, [95] uses quasi-experimental variation in actual and expected pollen counts. Key to the identification strategy is the focus on deviations from what was expected by firms. GeographicalWork using geographical boundaries often exploits the fact that people who live on either side of a demarcated geographic border are similar enough to be thought of as being randomized across them. For example, by looking at a remote border of Maryland that was geographically isolated from the rest of the state, [ 8] were able to argue that the imposition of sales tax for those who lived on one side of the border was random, relative to those people who lived nearby but just happened to be over the state border. MacroeconomicIt is also possible to take leverage of macroeconomic shocks. For example, [40] use the Great Recession as a key source of the variation on household incomes over time. They exploit the within-household variation in private label shares associated with within-household changes in income and wealth. The identifying assumption is that, conditional on all other factors, including an overall trend, within-household changes in income and wealth are as good as randomly assigned or exogenous changes. IndividualPlausibly exogenous variation can also be argued to occur at the individual level. For example, [20] use consumer migration to new locations as a quasi-experiment to study the causal impact of past experiences on current purchases. They argue that while migration is not necessarily random, the precise direction of migration can be, at least with respect to local brand market shares. OrganizationalShifts in firm policy and organizational events can also be leveraged as a source of variation. For example, [64] assess the change in customer behaviors between those whose information is breached and those whose information is not. The identification assumption is that the assignment of customers into the data breach group is likely to be random. RegulatoryMany papers also use the timing of regulatory changes as a source of variation. The argument here is typically that though the imposition of regulation may not be random, the timing of the regulation is. For example, [97] use a change in Massachusetts regulation of home sale listings to identify the effect of information about time on the market on house prices, and [76] use a change in the standardized nutrition labels on food products required by the Nutrition Labeling and Education Act and investigate how the Act changes brand nutritional quality.This discussion emphasizes that there are many potential sources of exogenous variation that can approximate a randomized experiment. We emphasize that typically the best papers focus on the research question first, and then imagine what the idealized experiment would look like to identify an actual quasi-experiment. Identification Strategy: Does x Really Cause y to Change?To convince a reader that an identification strategy is valid requires two steps. First, the researcher must explain where the variation they are calling exogenous comes from. This requires institutional knowledge and careful research into the setting. Second, the researcher needs to demonstrate that the relationship between the variation and the outcome of interest is very likely driven by the relationship between x and y and not by some other factor.To achieve the second requirement, it is useful to think about defending the experiment in terms of the exclusion restriction. Although the term ""exclusion restriction"" is often used specifically for instrumental variables, it is also a useful concept for other quasi-experiments. The exclusion restriction states that the quasi-experiment only affects y because it affects x.There are a variety of ways in which the exclusion restriction can fail, and so researchers look for exogenous variation in x that will have no direct effect on y. For example, [91] use wind speed as a quasi-experiment to provide an exogenous driver of posting to a user-generated content site about windsurfing. This allows them to understand the relationship between content creation and the creation of social ties. The argument for the exclusion restriction is that there is no other plausible way that wind could affect the creation of social ties except through content creation. As they mention in the paper, plausible challenges to this exclusion restriction are that windy days could affect friendship formation directly because users meet future online friends at windier surf locations. To address such challenges, the researchers present empirical data to suggest that the social ties that are being formed do not seem to reflect geography.Another example is [66], which examines the effect of delays in the early part of a banking technology adoption process on ultimate usage. Through a quasi-experiment that provides a source of exogenous variation in delays, they exploit the fact that Germany has a highly regulated system of public holidays and vacations that vary at the state level to prevent freeways from becoming overly congested. This leads to delays in technology adoption in that particular period to customers in one state, and not in others. The exclusion restriction is that there is no other reason that vacations or public holidays in the few days surrounding adoption would affect ultimate usage except through delaying the ability to navigate the security protocols required to sign up for the online banking service. One challenge for the exclusion restriction could be that individuals who sign up for a banking service around public holidays are somehow systematically different from others in terms of their laziness or motivation. To counter this challenge, the researchers present evidence that users are not different along any observable dimension.The exclusion restriction can also fail because of spillovers between groups that receive the exogenous shock or treatment and those that do not. The assumption that treatment of unit i affects only the outcome of unit i is called the stable unit treatment value assumption (SUTVA) in the treatment literature ([10]; [61]). This is not a trivial assumption. For example, [ 5] use the 2011 Orbitz–American Airlines disputes as an exogeneous event that led to a five-month period in which American fares were not displayed on Orbitz. The authors use this dispute to identify which company was hurt the most in terms of site visits and purchases. The SUTVA requires a valid control group such that the Orbitz–American Airlines disputes have no spillover on that group. As a result, the authors chose not to use airfare- or hotel-booking websites as a control due to the possible spillovers from Orbitz to other websites where customers can purchase. Instead, the authors used consumers' search of Lonely Planet as the control, because Lonely Planet is a travel website that is rarely used for bookings. The underlying idea is that an exclusion restriction cannot hold if the fact that one group was treated may also affect the control group's behavior. The SUTVA is therefore part of an argument that researchers make about an appropriate exclusion restriction.Importantly, there is no formula for a convincing explanation and defense of the empirical identification strategy in quasi-experiments. Except in cases of random assignment, it is not possible to prove that the identifying assumption is right. Instead, the objective for the authors is to pursue projects only when they can convince themselves (and their readers) that the causal interpretation is more plausible than other possible explanations. It is impossible to prove the validity of a quasi-experiment, such as whether one set of U.S. states serves as a legitimate control group for another or whether the exclusion restriction holds in instrumental variables. The credibility of any quasi-experimental work therefore relies on the plausibility of the argument for causality rather than on any formal statistical test. Empirical Analysis: How Can Researchers Estimate the Effect of x on y ?After establishing the identification assumption through the underlying framework of an exclusion restriction, the next step is to explore the data and conduct analysis that allows measurement of the effect of interest. This measured causal relationship is what has the potential to inform decision making. We discuss three different regression analysis frameworks using quasi-experiments: difference-in-differences (DID), regression discontinuity, and instrumental variables (IV). At the heart of all these strategies is a similar argument about the validity of the quasi-experiment.Table 2 outlines eight key steps in the three regression analysis frameworks. As pointed out by [54]) and others, the techniques are very similar in terms of the underlying econometric theory. However, though similar in the conceptual ideas, in terms of practical implementation, presentation, and how the researcher should best reassure their audience about the validity of the technique, there are some differences, which we expand on. The three frameworks differ in the first four implementation steps. We discuss the first four steps for each of the three regression analysis frameworks and highlight the issues in common across the three analysis frameworks in the last four steps. We also emphasize that many excellent papers do not implement each step, and this description is not intended to lead to unproductive dogmatism.GraphTable 2. Quasi-Experimental Regression Analysis Frameworks. Difference-in-DifferencesRegression DiscontinuityInstrumental VariablesIdentificationClarify the source of the shock, provide evidence why the shock can be seen as quasi-experimental, be clear on the identifying assumptions, and be transparent on the potential confoundedness.Justify the source of the fixed threshold, and whether the assignment to the treatment is determined, either completely or partly, by the value of the predictor on either side of a fixed threshold.Justify why the IV moves the endogenous covariate as if they are an experiment; explain the exclusion restriction.Raw dataTest whether those who receive the treatment are similar to those who do not; whether the parallel assumptions are satisfied; illustrate the trajectory.Provide evidence that the threshold is arbitrarily determined and not linked to underlying discontinuities in effects.Regress the outcome directly on the instrument and show that the instrument has the expected direct effect.Data analysisApply difference-in-differences regression framework in Equations 1 and 2 and adapt accordingly for other variations.Apply regression continuity framework in Equation 3.Report the first stage and determine whether the instruments are strong. Apply 2SLS in Equations 4 and 5 and conduct relevant tests.Standard errorsCluster at the level of treatment to account for within-unit correlation of the error term over time.Use robust standard errors, do not cluster on a discrete variableCluster at the level of treatment to account for within-unit correlation of the error term over time.Robustness checksConduct multiple robustness checks.Mechanism checksMeasure mediator variables or show moderation analysis.External validityDiscuss the assumptions required to capture the ATE.Apologies and caveatsApologize for all that is still unproven and give caveats. All of these methods implicitly rely on throwing out variation in the data that is not exogenous. In other words, they involve losing power to support the exogeneity assumption. This means that quasi-experimental work cannot use the R-squared as a useful summary of the appropriateness of the model. [41]) provide some useful evidence. While R-squared or a comparison of log-likelihoods is very useful in many other contexts (e.g., forecasts), benchmarking quasi-experimental analyses against other methods by using the R-squared will be misleading. Difference-in-Differences AnalysisA standard DID analysis compares a treatment group and a (quasi-) control group before and after the time of the treatment. The ""treatment"" is not truly a random experiment but, rather, some ""shock."" Unlike a simple comparison (or single-difference) analysis, DID methods generate a baseline for comparison between the treatment and the control group. By highlighting the change in the treatment group relative to the control group, DID enables the researcher to control for many of the most obvious sources of heterogeneity across groups.[47] is an example of a DID paper. The authors examine the impact of privacy regulation on the effectiveness of online advertising. In late 2003 and early 2004, many European countries implemented new restrictions on how firms could collect and use online data. The paper uses data on the success of nearly 10,000 online display advertising campaigns in Europe, the United States, and elsewhere between 2001 and 2008. The authors compare the change in effectiveness of the ad campaigns inside and outside Europe. Therefore, the first difference is the change in the campaign effectiveness, and the second difference is the change in Europe relative to elsewhere. Compared with before the regulation, ad campaigns became 2.8% less effective in Europe after the regulation. In contrast, compared with before the European regulation, ad campaigns became.1% more effective outside of Europe after the European regulation was implemented. Identification of Difference-in-DifferencesThe first step is to clearly lay out the identifying assumptions. [47], p. 63) state that ""the identification is based on the assumption that coinciding with the enactment of privacy laws, there was no systematic change in advertising effectiveness independent of the law"" and that ""the European campaigns and the European respondents do not systematically change over time for reasons other than the regulations."" A substantial portion of the paper is devoted to providing empirical evidence regarding whether ( 1) European ad agencies invest less in their ad creatives relative to non-European ad agencies after the laws, ( 2) the demographic profile of the respondents is representative of the general population of internet users, and ( 3) there may have been a change in European consumer attitudes and responsiveness to online advertising separate from the Privacy Directive.The analysis of consumer attitudes and ad responsiveness is based on a concern about unobservables, specifically whether there are alternative explanations for the measured changes in the attitudes of survey participants toward online advertising that were separate but contemporaneous with the change in European privacy laws. To check for such unobserved heterogeneity, [47]) examine the behavior of Europeans on non-European websites that are not covered by the European Privacy Directive to see if a similar shift in behavior can be observed, and they find evidence that changes in behavior are connected with the websites covered by the law, rather than the people taking the survey. The identification exclusion criterion is further validated by a mirror image of the falsification test by looking at residents of non–European Union (EU) countries who visited EU websites. When residents of non-EU countries visit EU websites, the ads are less effective in the postperiod. In contrast, when residents of these non-EU countries visit non-EU websites, there is no change in effectiveness before and after the EU regulation. Therefore, the results appear to be driven by what happens at EU websites rather than by a difference in how Europeans behave relative to non-Europeans. Raw Data Exploration of Difference-in-DifferencesThe second step is to explore the raw data. Before applying the DID framework, it is important to explore the raw data to assess whether the quasi-experiment appeared to have an effect. For example, when a treatment occurs in the middle of a time series, many papers use a graph that shows that before the treatment occurred, the treatment and control groups were on a similar trend and had similar values; then, after the treatment occurred, the trajectory of the treatment group diverged from the control group.Researchers should also assess whether their quasi-experimental setting meets the parallel trend assumption while exploring their raw data. This involves demonstrating that behaviors were similar in the period prior to the policy change across the treatment and control groups. Depending on the length of the time period, this can be done by conducting two-sample mean comparisons for each pretreatment period or by running a linear regression and looking at the time trend differences between the control and treatment groups. It is also often ideal to simply plot the raw data to support this point.Though it is desirable and convincing if the main effect of interest can be seen through descriptive statistics or visualization, we caution that this is not always possible. This may happen because effect sizes are small—as they often are in advertising—or because there is variation in the data that is best addressed using a regression framework. Analysis of Difference-in-DifferencesAlthough a DID regression can be represented in a 2 × 2 table, it is usually analyzed with regression analysis to allow researchers to control for factors that may change over time and across individuals. The simplest version of this regression is as follows: yit=α1TreatmentGroupi+α2AfterTreatmentt+βTreatmentGroupi×AfterTreatmentt+γXit+εit, Graph( 1)where y is the outcome of interest; i represents the individual, firm, or other cross-sectional unit of interest; t represents the time period; and  εit  represents the error. The key focus of the DID specification is on  β  , which captures the explanatory power of the crucial interaction term. Usually, researchers add controls  Xit  to address additional omitted variables concerns, such as an observed covariate that may not affect the treatment and control groups in the same way.When researchers have access to a panel, it is possible to address this concern directly by observing the same individuals, or the same campaigns, both before and after the timing of the treatment. It is then possible to add fixed effects to control for all individual-level (time-invariant) heterogeneity. Furthermore, if the data set includes more than two time periods, then adding time-specific fixed effects controls for all time-period-specific heterogeneity (across all individuals). With individual and time fixed effects, the DID regression is yit=βTreatmentGroupi×AfterTreatmentt+γXit+μi+τt+εit, Graph( 2)where  μi  is the individual-level fixed effect and  τt  is the time-period fixed effect. The fixed effects mean that the main effect of  TreatmentGroupi  and  AfterTreatmentt  drop out because they are collinear with the fixed effects. If possible, it is often desirable to difference out, rather than estimate, the fixed effects to avoid bias due to the incidental parameters problem (e.g., [67]). Most standard statistical packages automatically condition out the individual fixed effects from fixed effects panel data models where possible.[ 6]Though changes over time are common, DID methods do not require a time-series component. For example, [48] examine the impact of offline advertising restrictions on prices for keyword advertisements. The first difference is the keyword ad prices in states that have restrictions compared with states that do not. The second difference is the keywords that are affected by the restrictions compared with the keywords that are not.For quasi-experimental analyses that do examine changes over time, another tweak is that quasi-experimental treatment can occur at different times, meaning that individuals are treated at different times and that the  AfterTreatment  variable can change with subscripts i and t. For example, [27] study how a book review posted on Amazon affects sales of that book on Amazon, compared with sales of that book at barnesandnoble.com. Different books are reviewed at different times. Therefore, the treatment here is the review a book receives, and the  AfterTreatment  period occurs at different times for different books. [14], [19], [21], [35]), [49], and [85] explore the effects of variation in treatment timing. The issue is that because a fixed-effects DID estimator is a weighted sum of the treatment effect in each group and at each period, even though the weights sum to one, negative weights may arise when there is a substantial amount of heterogeneity in the treatment effects over time. A related concern has been highlighted by [45], who emphasize the problems that occur when both the treatment effect and treatment variance vary across groups.This means that researchers should be cautious in summarizing time-varying treatment effects with a homogeneous treatment effect as in the two-way DID framework if there is a substantial timing dimension. To address these issues, researchers have proposed a variety of estimators that allow for a cleaner comparison between the treated group and the control group. Both [21] and [35]) propose new estimands to estimate treatment effects in the presence of heterogeneity across groups and over time.[ 7] Another approach is taken by [93], who discuss corrections that should be applicable in a situation where leads or lags might be expected.Overall, DID is a powerful tool for helping identify the causal relationships that managers need for effective decision making. It can enable researchers to control for time-invariant individual-level heterogeneity, relying on the assumption that differences in the changes that the treatment and control groups experience over time are driven by the impact of the treatment. Regression Discontinuity AnalysisRegression discontinuity is a quasi-experimental technique in which the ""experiment"" relies on an exogenous arbitrary threshold. As [60], p. 616) put it, ""The basic idea behind the RD [regression discontinuity] design is that assignment to the treatment is determined, either completely or partly, by the value of the predictor being on either side of a fixed threshold."" Identification in regression discontinuityRegression discontinuity may be particularly useful to marketing scholars. [56] argue that many marketing interventions are based on thresholds of real or expected consumer or firm behavior. For example, direct mail companies use the scoring policies for recency, frequency, monetary models. Consumers just above and just below the cutoff should be similar in many dimensions, and their outcomes can be compared to assess the impact of the different mailings.Similarly, government policies based on firm size can provide a useful identification strategy for marketing scholars. For example, requirements for firms to post calories, undertake layoffs, and provide benefits often depend on the number of employees or other measures of firm size. By comparing firms just above and just below the threshold, it is possible to assess the effect of the policies on firm behavior.A regression discontinuity design implies that treatment is assigned depending on whether a continuous score  zi  crosses a cutoff  z¯  . The analysis then focuses on whether there is a change in the outcome of interest y in the neighborhood of  z¯  ([56]). In general, if a threshold is used as the source of the quasi-experiment, particular attention should be devoted to the source of the threshold and providing evidence that the threshold is essentially arbitrary and not likely to be linked to underlying discontinuities in behavior. Any discontinuity in the effect is assumed to be due to the treatment.This assumption is not always innocuous. Consider a $50 cutoff for receiving a marketing incentive. If the firm promotes the threshold and consumers try to achieve it, then there might be a substantial difference between people who spend $49 and people who spend $51. Those who spend $49 are likely to be unresponsive to the incentive because they did not try to cross the threshold to get the incentive. In contrast, those with exactly $50 in spending might have selectively chosen to spend exactly enough to get an incentive that they planned to use. It is important to address the potential for such concerns directly.This is reflected in a debate in economics about the effect of thresholds for low birth weight on medical outcomes. In an initial study, [ 6] used the fact that birth weight threshold of 1.5 kg is used to determine whether the newborn receives intensive medical treatment. In a critique of this work, [15] show that the children placed just at the cutoff seem to have significantly worse outcomes than babies on either side of the cutoff. This is evidence against use of this discontinuity for identification. [15] state, ""This may be a signal that poor-quality hospitals have relatively high propensities to round birth weights but is also consistent with manipulation of recorded birth weights by doctors, nurses, or parents to obtain favorable treatment for their children"" (p. 2119). Raw data exploration of regression discontinuityOnce the researcher has found a regression discontinuity setting, the first step is to explore whether the discontinuity is arbitrary and linked to discontinuities in any other variables. For example, [59] examine the relationship between online reviews and advertising spending in the hotel industry. They exploit the regression discontinuity design of the rounding rule that TripAdvisor uses to convert the average ratings of reviewers into the nearest half or full star (i.e., a rating of 3.74 is shown as 3.5 stars while a rating of 3.75 shown as 4 stars), building on work by [71]. The key identification argument is that the rounding mechanism creates discrete, random variations in perceived quality around the rounding threshold and is independent of a hotel's true quality.A threat to the arbitrary discontinuity threshold would be that hotels manipulate their average ratings around the rounding thresholds. [59] argue that if there is upward manipulation of ratings, there would be relatively few firms with average ratings just below the thresholds and a clump of firms with average ratings just above the thresholds. They show instead that the density of average ratings is uniform, with neither bumps nor dips above or below the round thresholds. They provide additional empirical evidence that characteristics of the hotels do not differ systematically above or below the threshold. Neither do they observe discontinuities in other key variables such as hotel prices and the number of five-star reviews. Analysis of regression discontinuityThe equation used for regression discontinuity can be written for panel data as yit=βI(zit≥z¯)+γXit+μi+τt+εit. Graph( 3)Here  β  is the treatment effect, the parameter of interest.  Xi  represents covariates.  I(zit≥z¯)  is an indicator function that equals one when  zit≥z¯  and zero otherwise. One final consideration is how to select the appropriate bandwidth for a regression discontinuity design, which is the question of how one decides on the sample to analyze, in terms of how far away the people in the sample are from the threshold where the discontinuity occurs. In general, such decisions have often been rather ad hoc, but there is an emerging literature that can help guide the researcher into thinking about how to take a more conservative approach to selecting bandwidth given the data at hand ([25]). The researcher should also ensure that their results are not sensitive to the choice of bandwidth. As with other quasi-experimental methods, the validity of the method cannot be statistically proven. Therefore, substantial emphasis must be placed on the explanation and defense of the quasi-experiment using raw data. Instrumental Variables AnalysisThe quasi-experimental perspective on IVs is somewhat different from the standard treatment in econometrics textbooks, which focuses on simultaneous equations and a more structural approach. The differences relate to justification and interpretation. The quasi-experimental approach emphasizes that the shocks that move the instrument should behave as if they are an experiment. The quasi-experimental approach gives a sense of the sign, significance, and magnitude of the causal effects. The structural approach emphasizes that the shocks should be motivated by an economic model that explains the exclusion restriction. The IV approach used in structural models gives elasticities that can be used to generate counterfactuals outside of the sample. Despite these differences in interpretation, it is important to remember that the underlying mathematics is identical. Identification of instrumental variablesThe basic idea behind using IVs is that the covariate of interest x contains both useful variation (to identify the causal effect of interest) and less useful variation (that confounds the effect). A good instrument z is strongly correlated with the useful variation but uncorrelated with the confounding variation. In other words, the researcher only uses the variation in x that can be explained by the exogenous shifter z.The standard two-stage model involves two steps. In the first-stage regression, a fitted value of  xi^  can be obtained by regressing x on instrument z and covariates  W  : xi=γzi+ϑWi+ηi. Graph( 4)In the second-stage regression, the IV estimator  β^  is obtained by regressing the outcome y on the fitted value of  x^  and covariates  W  : yi=βxi^+φWi+εi. Graph( 5)The identification of the effect of x on y relies on the following ""reduced form."" Inserting the predicted x to the y equation will give Equation 6. Here,  φ^  is used to highlight that when regressing y directly on instrument z and covariates W, the estimated covariate coefficient is rescaled as  φ^=βϑ+φ  . yi=βγzi+φ^Wi+εi. Graph( 6)Therefore, from the quasi-experimental point of view, an instrumental variable can be seen as a treatment that affects the endogenous covariate directly. This means that directly regressing the outcome of interest on the instrument (in one stage) will get the causal effect of interest, but it will not be properly scaled. The purpose of implementing two stages is to scale the treatment effect properly. There are many ways of operationalizing instrumental variables, and this can be a place for highly technical tools. We emphasize the simplest two-stage least squares (2SLS) approach, but the intuition behind the role of instrumental variables as an identification strategy remains regardless of functional form assumptions. Using two stages enables the researcher to disentangle  β  and  γ  . In other words, two stages are needed to get the elasticity right, but the experiment happens at the level of the instrument and so, even though the focus is on the relationship between x and y, the intuition on causality happens at the level of the relationship between z and y.Returning to [91], while the paper adds some additional necessary nuance to the estimation to fit the particular situation, the intuition on causality measures the impact of wind (the instrument  z  ) on social ties (the outcome of interest  y  ). This will be  βγ  . The relationship of interest, however, is the impact of posts (  x  ) on social ties (  y  ), which is measured as  β  .IV can be a less transparent solution to identifying causal effects compared with the other two analysis frameworks discussed previously (for a detailed discussion, see [84]). The distinction between the relationship of interest (  β  ) and the direct estimate from the quasi-experiment (  βγ  ) means that it is sometimes harder to visualize how the quasi-experimental variation works in IVs.Transparent communication of IV analysis is difficult for three reasons. First, in contrast to the binary nature of the exogenous variation in DID and regression discontinuity, instruments are often continuous. This makes it more difficult to communicate the intuition for why the variation is exogenous to the potential for omitted variables or simultaneity. The ability to use continuous instruments (and multiple instruments) can also be seen as a strength of IV techniques. They enable a more flexible set of counterfactuals because there are more treatments observed and used in the analysis. For example, while a discrete quasi-experiment on retailer discounts would allow the researcher to compare the impact of a small set of retailer discounts on sales, a continuous instrument for the discounts might allow the researcher to compare a variety of smaller and larger discounts.Second, weak instruments are a challenge. Instrumental variables techniques are consistent but biased, and this bias can matter even in seemingly large samples ([92]). Weak instruments can lead to incorrect inference in which the bias of the weak instrument dominates the potential bias of the omitted variables.Knowing the context and the institutional setting can be invaluable in identifying strong IVs. For example, [76] derived their instruments for brand taste and price from the authors' intimate knowledge of the regulation and food industry. There are also recent advances in econometric methods that allow for more accurate presentation of statistical significance when instruments are weak ([68]). As [11] point out, many of the challenges of weak instruments are magnified when authors use multiple instruments to deal with multiple sources of endogeneity. By contrast, a focus on a single endogenous variable with a single source of endogenous variation has attractive statistical properties as well as being more transparent to the reader.Third, many researchers present IV results with different tests and with different norms. This makes it difficult to read and assess the validity of papers with instruments. Raw data exploration and analysis of Instrumental Variables[12], pp. 212–13) provide a sequence of steps to follow in an attempt to standardize practice. In presenting this list, we hope that it does not lead to unproductive dogmatism, and we emphasize that this is just one possible way to communicate the rationale behind a causal interpretation of the results. Still, we hope that in following these steps to the extent possible, marketing scholars can avoid being subject to many of the criticisms highlighted by [84]. The steps are as follows: Regress the outcome directly on the instrument. When using IV techniques, it is also desirable to show the reduced form result of regressing the outcome directly on the instrument. Because this is an ordinary least squares regression, it is unbiased. At the very least, the researcher should be confident that the instrument (  z  ) has the expected direct effect on the outcome (  y  ). Report the first stage. Assess whether the signs and magnitudes of the coefficients make sense. Report the F-statistic on the excluded instruments. This helps determine whether the instruments are weak. [92] advise that F-statistics below 10 in case of only one instrument suggest weak instruments, though, as [12], p. 213) note, ""Obviously this cannot be a theorem."" Similarly, [84] suggests reporting the first stage with and without the instruments to document the incremental impact of the instruments on the R-squared. If there are multiple instruments, report the first- and second-stage results for each instrument separately (at least in the appendix) because bias is less likely if there is only one instrument. Presenting the results separately also helps the reader understand the intuition behind the quasi-experiment underlying each instrument—whether the multiple instruments use different variation in increasing the exogenous shift in x. If there are multiple instruments, an overidentification test such as the Sargan–Hansen J can be performed to test whether all instruments are uncorrelated with the 2SLS residuals.[ 8] However, given the difficulty of identifying a robust instrument, it is unusual for researchers to have convincing cases for multiple instruments in a way that leads their regression to be overidentified. In other words, increasingly, standard practice is to focus on one instrument rather than many ([11]). Conduct a Hausman test comparing ordinary least squares and instrumental variables. If the results change, reflect on whether they change in a direction that makes sense given the power of the instrument. Do not interpret the results of the Hausman test to prove that the endogeneity problem is irrelevant. As noted by [84], the instrument may not be valid and therefore the test would be uninformative. Assess whether there is a weak instrument problem. For example, in a linear model, compare the 2SLS results with the limited information maximum likelihood results. When there is a weak instrument, the two-stage least square estimators are biased in small sample. Limited information maximum likelihood estimators have better small sample properties than 2SLS with weak instruments. If the two estimates are different, there may be a weak instrument problem. Any inconsistency from a small violation of the exclusion restriction gets magnified by weak instruments. Presentation of Results and Clustering of ErrorsRegardless of which regression analysis framework to employ, presentation of baseline estimates and standard errors, along with a set of robustness checks ([59]) is standard. This typically appears in the form of a regression table with several different specifications. For example, the first column might not include any controls beyond the fixed effects, and the next set of columns might add controls. The economic magnitude of the coefficients should be discussed, both with respect to changes in the covariate of interest and relative to the range and standard deviation of the covariate and dependent variable.A key issue in quasi-experimental analysis is correlated errors in observations, because the outcome is often observed at a finer level than the treatment. For example, the researcher might observe treatment and control groups for several advertising campaigns over a long time period. For each campaign, the researcher might have data on many individuals per campaign and many time periods per individual; however, the choices of the same individual in many time periods are likely to be correlated. [16] emphasized that failure to control for the correlation between these choices will lead to an overstatement of the effective degrees of freedom in the data, and therefore, standard errors will be biased downward. They suggest clustering standard errors by individual over time to address this issue and provide Monte Carlo evidence that clustering is likely to lead to robust inference.Similarly, [38] emphasize that if individual responses to the same treatment are likely to be correlated, for example, because of close physical or social proximity, clustering standard errors by groups of individuals is a conservative and useful way to estimate standard errors. Researchers often need to decide on the size of the clusters. For example, in studying ready-to-eat breakfast cereals, is the correct unit the company such as General Mills, the brand such as Cheerios, or the sub-brand such as Honey Nut Cheerios? The answer depends on the data and research question. If the data are at a lower unit level (e.g., individuals) than a treatment that takes place at the firm level, cluster the standard errors at the level of the treatment. A useful perspective on this is provided by [ 2], who remind researchers that the major driver for clustering should be the experimental design rather than simple expectations of correlation. More recently, there has been evidence suggesting that it is undesirable to cluster on the variable that determines whether that observation is subject to the regression discontinuity design (e.g., age). The answer is often instead simply to reduce the bandwidth across which the regression discontinuity is studied ([65]).Clustered standard errors rely on consistency arguments and large samples. With a small number of clusters, alternative methods are needed, such as those developed by [22], [30], and [53]. For example, [43] investigate consumers' dynamic responses to price promotions in a retail setting that involved randomly assigning ten supermarkets into varying promotion depths. Given that treatment takes place at the store level while the observation is at the consumer level, each consumer's effective contribution to reducing standard error estimates is likely to be lower than in a setting where there is no correlation across observations. However, given the relatively small number of stores/clusters available in this setting, the authors implement the wild bootstrap procedure, as proposed by [22], to correct for downward bias potentially induced in small samples. However, [24] show that even this approach requires rather large assumptions. Challenges to Research Design: What if Variation in x is not Exogenous?A more general point is that quasi-experiments range in how plausible the exogenous variation underlying the paper is, ranging from cases where the allocation is almost completely random to less clear cases where a firm or consumer assignment to treatment or control is partly random and partly an endogenous choice. Perhaps the ideal thought experiment here is [101], whose treatment and control were a pair of kidneys from the same person. [101] finds that in the United States, even identical kidneys from the same donor are received differently depending on the observed number of rejections preceding the recipient in the queue. Most research settings are less favorable. In such settings, it is often useful to combine different approaches in the same paper. For example, [79] combines a DID strategy with counterfeit entry as the treatment with a convincing and high-powered instrument on government regulation.Still, there will be situations where a compelling exclusion restriction is lacking or the treatment–control allocation appears far from random. If the treatment and control groups are substantially different in the pretreatment or if the treatment appears to be applied based on selected characteristics, the control group is unlikely to be a good proxy for the counterfactual, and the quasi-experiment may be less likely to be valid.We provide a discussion of three methods that are further steps researchers can take when comparability between the control and treatment groups is violated. They vary in terms of the observed and potentially unobserved differences between the control and treatment groups. Table 3 provides a summary of the frameworks and when to apply them. The table emphasizes that researchers should be cautious about applying matching methods or correction for selection bias on the grounds that there are no plausible exclusion restrictions, because these methods still require the researcher to make an argument about an exclusion restriction. The technical details of matching methods or selection bias correction are different from the three methods described previously, but the idea is similar in nature. The main goal is to bring in additional data to create control and treatment groups that are like those in quasi-experiment studies.GraphTable 3. Steps if Researchers Are Worried They Do Not Meet the Exclusion Restriction. Propensity Score MatchingSynthetic ControlSelection Bias CorrectionAssumptionsObservable control variables are capable of identifying the selection into treatment and control conditionsThe counterfactual outcome of the treatment units can be imputed in a linear combination of control units in the absence of treatment.The unobservables that enter the treatment selection and the outcome are jointly distributed as bivariate normal.IdentificationThe exclusion restriction can be met conditional on the variables in the match.The exclusion restriction can be met conditional on the pretreatment outcomes.There is at least one variable for which a compelling argument can be made for the exclusion restriction in the selection equation.SettingsWhen matching is done to control the treatment and control pretreatment outcomes on a number of cross-sectional covariates.When the focus is on the evolution of the outcome and the pretreatment time period has rich data on treatment and control groups.When the allocation to the treatment condition is not fully random.CaveatsAssess the degree of overlap after matching, and assess sensitivity to potential selection on unobservables. Still need to justify the exclusion restriction.Harder to interpret the weights used to create the ""synthetic control."" Still need to justify the exclusion restriction.Justification of why certain observables only affect treatment selection but not the outcome variable. Still need to justify the exclusion restriction.  Propensity Score MatchingMatching methods, pioneered by [83], have been developed such that the outcomes of the treated are contrasted only against the outcomes of comparable untreated units. Many published articles in marketing have used propensity score matching when comparability between the control and treatment groups is violated. An assumption of propensity score matching is that there are observable control variables capable of identifying the selection into treatment and control conditions. This is not a trivial assumption. It suggests that propensity score matching is only good if the exclusion restriction is met conditional on the variables in the match. Any matching procedure to make the control and treatment more similar in the observables can be seen as a flexible functional form with adding ""control variables"" to an analysis framework. Propensity score matching requires subject-matter knowledge regarding the role of covariates in the treatment assignment decision and whether the exclusion restriction is satisfied conditional on the covariates. Therefore, we caution against applying matching methods without convincing justification of exclusion restriction.It is difficult to identify a standard procedure for propensity score matching. We refer to [61] as a good starting point. The general objective of propensity score matching is to estimate a score such that the distribution of all the observed variables and behaviors among the treated units is similar to that among the control units. In this discussion, we consider the set of treated units to be fixed a priori. Four steps are involved in the propensity-score-matching procedure.First, choose a functional form of the propensity score. The basic strategy uses logistic regression to model the probability of receiving the treatment given a set of observables. Second, measure the distance and apply a matching algorithm. Several possible matching methods are available including, for example, nearest-neighbor matching based on the distance in the estimated propensity score or multiple matching using all controls within some distance from the treated unit. Third, assess the degree of overlap in the distribution of the linearized propensity score after matching. Researchers typically plot and compare the histogram-based estimate of the distribution of the linearized propensity score (logarithm odds ratio) for the treatment and control groups. To inspect the match quality, it is useful to show tables on the distribution of the estimated propensity scores and the mean values of some key variables for the treated and untreated over different propensity score intervals.[ 9] Fourth and finally, calculate the average treatment effect (ATE) with the matched sample using, for example, the DID regression analysis framework discussed previously.There are at least two caveats regarding propensity score matching. First, the model for the propensity score may be misspecified. In that case, the balance in covariates conditional on the estimated propensity score may not hold, and the credibility of subsequent inferences may be compromised. This calls for a careful discussion on the role of covariates in the treatment assignment decision. Specifically, it is important to provide a discussion of whether the covariates can be considered exogenous to the treatment. Second, regardless of the number of observed covariates used, propensity score matching does not account for the potential selection on unobservables in treatment assignment. It is important to explain why controlling for observables will address concerns with the exclusion restriction or why unobservables are not an issue in treatment assignment. Synthetic Control MethodsIn some cases, even the closest match may not be close enough. This is particularly relevant when researchers are interested in how an event, regulatory intervention, or firm policy change affects the evolution of the outcome of interest, in contexts where only a modest number of treated units (possibly only a single one) and control units are observed for a large number of periods before and after the event. Two aspects make this setting different from the typical use of the propensity-score-matching method. First, matching is done over the pretreatment outcomes in each period rather than a number of covariates. Second, the number of control units and the number of pretreatment periods can be of similar magnitude. Synthetic controls use a different convex combination of the available control units ([ 3]; [ 4]; [39]). The intuition behind this method is that the created synthetic control unit closely represents the treated unit in all the pretreatment periods and affords time-varying causal inference on the trajectory of the outcome of interest.Synthetic control has been used in multiple recent studies with quasi-experimental design ([ 1]). For example, [51] analyze the causal effect of industry payment disclosure on physician prescription behavior, [99] assess the impact of mobile hailing technology adoption on drivers' hourly earnings, and [78] study the causal effect of online paywalls on the sales revenues of newspapers.Like propensity score matching, synthetic control methods are statistically rich, but they do not replace a carefully thought-out exclusion restriction and identification argument. Put differently, if propensity scores or synthetic controls appear to work when the treatment and control group are not similar, it is important to explain why controlling for observables will address issues with the exclusion restriction. In many cases, such explanations are weak and the exclusion restriction is unlikely to hold. Recent work in economics emphasizes this by showing the benefits of combining a synthetic control method with a strong exclusion restriction ([13]). Selection Bias Correction MethodMany papers written in marketing involve a comparison of potentially different groups that reflect endogenous choices by companies or consumers where the allocation to the treatment condition is not fully random. For example, [46] assess if the introduction of the free mobile app in a business-to-business context increases sales revenues from buyers who adopted the app. In an ideal setting, the company could randomize the treatment, then observe sales from buyers who did not get the app and sales from buyers who did get it. However, this company's app was available to all buyers. Therefore, the buyers' app adoption is not random, and self-selection into the treatment (adoption) group needs to be addressed. Omitted variables that drive strategic app adoption could correlate with the sales from these buyers.When this happens, it is sometimes useful to estimate a Heckman selection model ([57]), which explicitly models selection into the treatment as a two-step process. As [100], p. 564) pointed out, the exclusion criterion is still key to the identification of the treatment effect of interest in the two-step estimation procedure. Without the exclusion criterion, the effect of the treatment is identified only due to the nonlinearity in the functional form (specifically through the inverse Mills ratio). This may lead to severe collinearity and imprecision in the standard errors. More importantly, without a strong and credible exclusion restriction, identification in this setting is driven by the assumed functional form.In other words, although the Heckman correction will provide an estimate without an exclusion restriction, that estimate depends entirely on the assumption that the error structure is bivariate normal. When there is an argument for the exclusion restriction, a selection model is helpful. In the absence of the exclusion restriction, even if combined with other techniques such as propensity score matching, the results would be identified off the functional form assumption alone. Put differently, if one of the covariates in the correction equation satisfies the exclusion restriction, then it is the variation in that variable that identifies the control for selection. In contrast, if the covariates in the first step are all also in the second step, then it is only the assumed error structure that identifies the control for selection.There are both similarities and differences between selection bias correction and instrumental variable approaches. There are also similarities with the control function approach in terms of the importance of functional form assumptions on the errors in the absence of an exclusion restriction. Control functions are not part of the standard quasi-experimental toolkit, so we do not provide a detailed discussion. The selection bias correction approach uses the instrument to control for the effect of unobservables, while the instrumental variable approach attempts to eliminate the threat of endogeneity by only leveraging the useful variation created by the instrument. Yet, the two approaches share the basic idea of using an exclusion criterion (or instrument). Ultimately, both rely on the ability to find an exclusion restriction that creates useful and exogenous variation. This is why we emphasize the importance of identification in quasi-experiments and caution against blindly applying a correction for selection bias without carefully thinking about the identification assumption and providing a justification for why the exclusion restriction holds. Selection bias correction approaches are therefore only useful for causal inference in the presence of a strong credible exclusion restriction. Robustness: How Robust is the Effect of x on y ?The specific robustness checks chosen will depend on the exact context. With electronic appendices and increasingly cheap computation, it is possible to show robustness to a large number of alternative specifications. Here, empirical work with quasi-experimental methods differs substantially from research using forecasting models. The aim is not to show one specification (or model) and defend it. Instead, the idea is to show that the sign, significance, and magnitude of the estimate of  β  remain broadly consistent across a vast range of possible models ([59]). Often these robustness checks are dropped from the published version of the article, though they are very useful in the referee process and can end up as part of an online appendix. The following subsections describe some examples of useful robustness checks. Different ControlsCompare the coefficient of interest in the models with and without controls. For example, if the coefficient changes from 2.5 to 3.5, then this change (+1.0 in this example) is informative about how big the impact of the omitted variables has to be relative to the observed controls for the omitted variables to drive the result. [ 7] provide a method to examine how much the effect of interest changes as controls are added, and then to assess how important the omitted variables would have to be for the treatment effect to disappear. The method is based on Rosenbaum bounds ([37]; [82]). It has been applied in the marketing literature by [73] and extended by [90]. Although the formal method is useful, as discussed in [77], many researchers ([ 9]; [74]) use the more basic insight that there is information in the impact of the controls on the measured effect of interest. This does not mean that results are invalid if the controls do change the estimated effect substantially, but documenting that adding seemingly relevant controls does not change the results can provide further support for the causal interpretation. Different Functional FormsResults should not depend on arbitrary choices of functional form. For example, if using a linear probability model, show robustness to logit and probit. The choice between linear probability models and nonlinear models such as logit is widely debated. [12] argue for linear probability models because they are simple to interpret and consistent under a basic set of assumptions. Others argue against them because they are inefficient (and inconsistent if the assumptions are violated). In cases like this, where the literature does not give clear guidance on the choice of model, showing robustness to different choices is optimal. Different Choices of the Time Period Under StudyResearchers often can choose when to start and end the sample. For example, for a treatment that occurs in 2004, researchers should be comfortable that the results are robust to the arbitrary choice of whether the period studied is 2002 to 2006, 2000 to 2008, 1995 to 2015, and so on. Different Dependent VariablesThere might be several different dependent variables that relate to the outcome of interest. Showing robustness to these related outcomes increases confidence in results. Different Choices of the Size of the Control GroupResearchers choose whether all the data should be used in the control group, or only a subset of the data that is ""close"" to the treatment group (e.g., as measured by a propensity score). Researchers can also choose how to define the treatment group. Placebo TestsThe idea of a placebo test is to repeat your analysis using a different part of the data set where no intervention occurred. For example, if the quasi-experimental shock happens this year, instead of comparing the difference in the outcome between last year and this year between the control and treatment groups, you can conduct a placebo test by redoing the analysis and compare the difference in the outcome between the control and treatment groups using periods with no intervention shocks. Alternatively, analysis can be conducted on an outcome that should be unrelated to the intervention being studied. The goal is to establish a null effect when there is not supposed to be one.It is unlikely that every robustness check will yield the same level of significance or the same-sized point estimate as the initial specification. Researchers (and reviewers) should therefore not expect every specification to yield the exact same results. The key is to communicate when the results hold up. This will consequently help inform the reader what drives the statistical power behind the results.Broadly, quasi-experimental research aspires to identify effects that do not rely on the underlying assumptions outside of the experimental variation. There are many places where that can break down, including functional form assumptions, external validity, and various confounding effects. The focus is on a robust single causal relationship. Mechanism: Why Does x Cause y to Change?The most effective papers typically do not stop with identifying a causal effect and its magnitude. After identifying a likely causal relationship, it is important to assess why x causes y to shift. Understanding mechanisms is often a key goal of social science. There are at least three benefits of establishing mechanisms. First, it provides a rationale for why the effect should exist in the first place. It requires the authors to think about the theoretical contribution of their research more carefully and helps make the argument for causal identification more convincing. Second, identifying mechanisms can help evaluate the benefits and negative consequences of the intervention and identify avenues for course correction, if needed. Third, understanding mechanisms allows for the possibility to extrapolate the findings to other contexts. Research needs to provide guidance on when and why the causal relationship is relevant. Assessing the Mechanism Through Mediation AnalysisWhen the data afford a direct measure of mediator variables, mechanisms can be inferred by mediator analysis. To illustrate how quasi-experiments can show process through mediation, we use [52] as an example. They investigate whether a variable compensation scheme increases salespeople's stress, resulting in emotional exhaustion and more sick days, and counteracts the sales benefits companies might expect from variable compensation schemes. In one of their empirical analyses, they use a natural experiment where a company dropped the variable compensation share from 80% to 20% in one of its business units. To test the health state as a possible mediator variable, they were able to measure sick days both before and after the change in the variable compensation share. In the country of study, sick days are strictly regulated by law and require certification by a physician (at the latest on the third day of the leave). Those who take more than three sick days in a given month are more likely to have substantial health problems. They measure the sick days counting after the third sick day in a month.Combining the DID analysis with mediator analysis, [52] show that the direct effect of the treatment (drop in variable compensation share) on sales performance is significant and negative, and that the indirect effect of the treatment on sales performance via sick days is positive and significant. The mediator analysis suggests that a higher variable compensation share is associated with enhanced sales performance but also with more sick days, which, in turn, reduce the gains to sales performance. Assessing the Mechanism Through Moderation AnalysisHeterogeneous treatment effects can be used to test behavioral mechanisms. In a quasi-experimental setting, mechanism checks via heterogeneous treatment effects, sometimes referred to as falsification checks, are not simply equal to identifying moderators. They involve identifying which groups would be affected by a certain mechanism that would display the causal effect of interest, and which other groups would not display the causal effect of interest by the proposed mechanism.Moderation analysis therefore serves a broader purpose by providing an opportunity to help explore the behavioral mechanism. If the effect goes away when theory suggests it should, then this helps identify why it happens. If the effect is larger when theory suggests it should be, then this also helps identify the mechanism. A simple approach is to estimate the effect separately by whether an individual is a member of a group that theory suggests should experience a bigger effect. Formal testing of whether the difference is statistically significant requires a three-way interaction between x, the source of variation, and group membership.There are many relevant examples in marketing of the use of moderation analyses to demonstrate a mechanism if there is a reason to believe the boundary of underlying process exists or the magnitude of the treatment effect varies by some observables. For example, after showing the European privacy regulation hurt online advertising, [47] ran a falsification check demonstrating that European consumers behaved like Americans when visiting American websites and that American consumers behaved like Europeans when visiting European websites. The paper then explored the mechanism and showed that the regulation especially hurt unobtrusive advertising and advertising on general interest websites, two situations where using data to target advertising is particularly valuable.Overall, mechanism checks through mediator or moderation analyses are important because they distinguish the goal of the marketing scholar from the marketing practitioner. Marketing practitioners run experiments and analyze data to understand what they should do in the particular situation they are facing. Marketing scholars need to have a broader sense of applicability beyond the specific setting being studied. Mediation and moderation analyses provide an understanding of when a marketing action will and will not lead to the desired behavior. For this reason, marketing papers are more likely to be remembered for the evidence that is shown in support of a theory explaining why the result holds. External Validity: How Generalizable is the Effect of x on y ?The external validity discussion in a paper should recognize the assumptions required for the analysis to capture the ATE across the population of interest, rather than a more local effect that is an artifact of the data sample or the source of quasi-experimental variation. A key concept is the ATE across the entire population. This is the difference in outcomes that would occur by moving the entire population from the control group to the treatment group. However, in some cases, the ATE may not be particularly relevant, because it averages across the entire population and includes units that would never be eligible for treatment ([100], p. 604). For example, we would not want to include millionaires in computing the ATE of a job training program. To address this, the researcher could use the average treatment effect on the treated, which measures the expected effect of treatment for those who actually were in the treatment condition.One reason why a research setting may fail to be externally valid is if the treated population is unrepresentative ([72]). A concern that will drive whether the treated population is unrepresentative is whether those affected could self-select into and out of the treatment. For example, [28] study a rule change by Google that allowed non–trademark holders to use trademarks in search advertising copy. They study the rule change's effect on user click behavior. In this case, many advertisers did not alter their advertising copy strategy, for a variety of reasons. These advertisers may be systematically different from the advertisers that did change their strategy. Because these advertisers were not forced to change their strategy, we will never know what would have happened if they did. When faced with such issues, it is best to spell out the potential for self-selection and discuss whether it makes the paper more or less relevant. In this case, it would be accurate to say that the researchers captured the effect of a loosening of trademark restrictions, because it is unlikely that a search engine would force its advertisers into using other advertisers' trademarks. However, it would not be accurate to claim that the researchers capture the broader effect of all advertisers using other advertisers' trademarks in their copy.The treated population may also be unrepresentative if the treatment impacts a subpopulation to change behavior, but not the main population of interest. This means that the measured effect is localized to that subpopulation, and it is referred to in the literature as the local average treatment effect (LATE). For example, in the context of regression discontinuity, the LATE is the average of the treatment effect over the individuals who would have been in the counterfactual condition if the discontinuity threshold were changed. A limitation of regression discontinuity is that the results directly apply only to populations around the threshold. For example, comparing the $49 spend with the $51 spend may be informative about the impact of the marketing incentive on consumers who spend around $50; however, consumers who typically spend a lot more or a lot less might be different. The idea of LATE also has implications for the interpretation of instrumental variables estimates, as any IV estimate is the LATE for the observations in the regression who experienced the kind of variation exploited by the instrument.[10]More broadly, as with other aspects of quasi-experimental research, the best practice regarding the external validity of results is to clearly lay out the assumptions and limitations. For example, [94] use a quasi-experiment and DID to examine the impact of advertising revenue on the type of content posted on Chinese blogs. While it might be tempting to interpret the results as suggestive of a broader impact of commercial interests on media, they are careful to emphasize the many differences between blogs and other media, between China and the rest of the world, and between the way the bloggers were compensated and other online advertising models. In this way, Sun and Zhu's article explicitly limits the temptation of the reader to extrapolate too much.An internally valid quasi-experimental estimate can have broader external validity when used to identify relationships such as elasticities and then to use a structural model to identify the counterfactual of interest. In these cases, under the assumption that the model is a useful representation of reality, quasi-experimental methods serve as a complement for, rather than a substitute to, structure. For example, [ 9] use quasi-experimental methods to identify the impact of the automotive brand preferences of parents on the brand preferences of their children. They then use structural methods to estimate the implications for firm strategy. [42] use quasi-experimental variation in health insurance prices to identify price elasticity and then combine this measure with a structural model to estimate the welfare implications of adverse selection. [29] use quasi-experimental variation around set quotas to identify the relationship between commissions and sales, and then use this variation in a structural model to determine optimal compensation schemes.Overall, effective quasi-experimental research requires an understanding of the underlying assumptions behind any broad interpretation of quasi-experimental results. Quasi-experiments often require a focus on a narrow slice of the data, and therefore, it is important to consider the degree to which the results apply to a broader population. Apologies: What Remains Unproven and What Are the Caveats?Any identification strategy relies on a set of assumptions. These assumptions need to be explicit throughout the paper. There are always some tests that cannot be run, for example, due to lack of data. There are always some robustness checks that are weaker than others. There are always some steps from data to interpretation. While apologies do not mean all is forgiven, the objective should be to clarify the boundaries of the claims. Obfuscation is much worse than a clear summary of the identifying assumptions.As an example, [51] employ a DID research design to study the effect of the payment disclosure law introduced in Massachusetts in June 2009. The research design uses the setting that physicians located in the border counties of Massachusetts and its neighboring states did not have disclosure laws during this period. They lay out the assumptions underlying their estimation:Our identification of the effect of disclosure legislation relies on the change in new prescriptions by physicians located in Massachusetts (MA) after the policy intervention, relative to their counterparts from ""control"" states in which no such law existed in the same period.... To assess potential threats to the validity of our research design, we verify if the result was driven by changes in physician payments as a result of the MA disclosure law. If such payment changes were primarily driven by local pharmaceutical reps reallocating their marketing budgets across physicians operating on either side of state borders, this would render the border identification strategy problematic.([51], p. 517)This example communicates three distinct points. First, it explains the identification strategy. Second, it details the main threats to the validity of this identification strategy. Third, it describes what they do to address it. These points suggest that effective apologies focus on demonstrating what interpretations are reasonable, and what might be a stretch of the results. The goal is not to show that in all circumstances and every conceivable way the identification is perfect. That is not possible. Instead, the goal is to provide clear bounds on the interpretation. The paper's contribution is then a function of whether it provides new knowledge under this bounded interpretation. ConclusionQuasi-experimental techniques are an important tool for marketers. First, marketing scholars need to be able to inform marketing practitioners—both managers and policy makers—about the causal effect to allow practitioners to make superior decisions. Second, the best quasi-experimental papers do not simply prove a causal effect but delve into the underlying mechanism, which is key to marketing scholarship's goal of generalizability. Third, such techniques become more important as the scope and span of marketing practice expands and there are new settings and more varied sources of data that allow their application.The objective of a quasi-experimental research paper is to answer an interesting and important research question about a causal relationship and provide evidence suggesting the mechanism behind the relationship. The choice of method (DID, regression discontinuity, or instrumental variables) depends on the nature of the quasi-experiment. The framework we present focuses on understanding how exogenous variation helps uncover causal relationships and why specific actions affect behavior. Of course the details of the methods will evolve over time as new research appears. Because marketing scholars are often interested in providing generalizable insights about how marketing actions change the behavior of individual consumers, the quasi-experimental framework is particularly useful. Similarly, firms that want to use those insights benefit. As the availability of detailed data grows and marketing technology changes, these methods will enable marketing scholars to provide assessments of a wide variety of situations in which a particular marketing action is likely to change consumer behavior or market dynamics.  "
4,"Conducting Research in Marketing with Quasi-Experiments This article aims to broaden the understanding of quasi-experimental methods among marketing scholars and those who read their work by describing the underlying logic and set of actions that make their work convincing. The purpose of quasi-experimental methods is, in the absence of experimental variation, to determine the presence of a causal relationship. First, the authors explore how to identify settings and data where it is interesting to understand whether an action causally affects a marketing outcome. Second, they outline how to structure an empirical strategy to identify a causal empirical relationship. The article details the application of various methods to identify how an action affects an outcome in marketing, including difference-in-differences, regression discontinuity, instrumental variables, propensity score matching, synthetic control, and selection bias correction. The authors emphasize the importance of clearly communicating the identifying assumptions underlying the assertion of causality. Last, they explain how exploring the behavioral mechanism—whether individual, organizational, or market level—can actually reinforce arguments of causality.Keywords: quasi-experiments; marketing methods; econometricsQuasi-experimental methods have been widely applied in marketing to explain changes in consumer behavior, firm behavior, and market-level outcomes. ""Quasi-experiment"" refers to the use of an experimental mode of analysis and interpretation to data sets where the data-generating process is not itself intentionally experimental ([23]). Instead, quasi-experimental research uses variation that occurs without experimental intervention but is nonetheless exogenous to the particular research setting. Work using quasi-experiments in marketing settings has used events such as weather, geographic boundaries, contract changes, shifts in firm policy, individual-level life changes, and regulatory changes to approximate a real experiment. In each case, an external shock creates a source of exogenous variation that the researcher uses to establish a causal relationship between the variation and the outcome of interest.Companies also use quasi-experimental methods to understand the consequences of key business actions. For example, [17] analyzed a quasi-experiment where eBay shut all the paid search advertising on Bing during a dispute with Microsoft but lost little traffic. These quasi-experimental results inspired a follow-up field experiment where eBay randomized suspension of its branded paid search advertising and found results consistent with the quasi-experiment. Reflecting the importance of such methods at firms, some companies provide causal inference training for their data scientists ([32]; [81]). The ability to make causal claims is highly valuable in academia and in practice. This article aims to help both marketing scholars and practitioners conduct and evaluate the credibility of quasi-experimental studies.Quasi-experimental research, as in much work in applied statistics, begins with the equation  y=f(X,ε;β)  . The focus is then on whether a change in a single covariate x in the vector of  X  can be demonstrated to cause a change in  y  . This focus often enables the exploration of foundational questions in marketing, because marketers often have data representing the actions of many individual consumers or clients and need to understand the causal relationship between a particular x and y to make decisions about whether and how much x to use.A marketing article that successfully uses the quasi-experimental econometric approach considers the following nine topics, which are echoed in the structure of this article: Research Question: Do We Care Whether x Causes y? Data Question: How Can Researchers Find Data with Quasi-Experimental Variation in x? Identification Strategy: Does x Cause y to Change? Empirical Analysis: How Can Researchers Estimate the Effect of x on y? Challenges to Research Design: What if Variation in x Is Not Exogenous? Robustness: How Robust Is the Effect of x on y? Mechanism: Why Does x Cause y to Change? External Validity: How Generalizable Is the Effect of x on y? Apologies: What Remains Unproven and What Are the Caveats?We start by explaining why quasi-experimental scholars may appear obsessed with identification, and how this influences the choice of research question and data setting. Quasi-experiments come in different shades, ranging from an almost completely random exogenous shock to where the treatment assignment is only partly random. We suggest different frameworks to accommodate various levels of evidence depending on the strength of the underlying identification argument. We then turn to the importance of understanding the underlying mechanism behind the causal result. Typically, this means showing that the effect is largest where theory would predict and is smallest where theory would predict a negligible effect. We also emphasize that researchers need to be clear about the external validity of their study and apologize for what remains unconvincing. Why the Focus on Identification?Why are quasi-experimental scholars seemingly obsessed with identification? Identification is defined by the challenge that ""many different theoretical models and hence many different causal interpretations may be consistent with the same data"" ([58], p. 47). However, effective decision making requires an understanding whether a measured relationship is indeed causal.One way to describe this issue is through the ""potential outcomes approach"" developed by Jerzy Neyman, Donald Rubin, and others ([86]).[ 5] This approach starts with the insight that for any discrete treatment—which could be an event or explicit policy (  D  )—each individual  i  has two possible outcomes: yi1  if the individual i experiences the treatment  Di=1  , and yi0  if the individual i does not experience the treatment  Di=0 The difference between the two is the causal effect. The identification problem occurs because a single individual i cannot both receive the treatment and not receive the treatment at the same time. Therefore, only one outcome is observed for each individual at any point in time. The unobserved outcome is called the ""counterfactual."" The unobservability of the counterfactual means that assumptions are required. The identification problem means that those who experience D, and those who do not, are different in unobserved ways.Random assignment solves the inference problem, as the ""unobserved ways"" should not matter ex ante ([31]). [88], p. 13) explain that ""if implemented correctly, random assignment creates two or more groups of units that are probabilistically similar to each other on the average."" With enough people assigned randomly to one group or another, the only meaningful difference between the groups will be a result of the treatment.Therefore, random assignment is often called the gold standard of identification ([70], p. 8). [12], p. 11) emphasize that ""the most credible and influential research designs use random assignment."" That said, we should be clear that field experiments are merely a gold standard for being able to plausibly claim causality, not the gold standard for empirical work ([34]). Indeed, in many marketing situations, experiments are not feasible, appropriate, or affordable ([44]).Quasi-experimental work, by contrast, is aimed to identify exogenous shocks or events that can approximate random assignment. Given that assignment is not random, a researcher's goal is to make the unobserved ways in which the treatment and control groups differ as untroubling as possible to the researcher and the reader and thereby mimic random assignment as closely as possible. Research Question: Do We Care Whether x Causes y?The first and hardest stage in this process is identifying a question in which marketing scholars, managers, or policy makers actually care whether x causes y. This is difficult because many of the  y  s and  x  s for which we can measure a causal relationship are (unfortunately) uninteresting. Therefore, researchers who do quasi-experimental research do best if they start not with the data or an exogenous shock but instead start by asking themselves, ""Suppose I convincingly showed that an increase in x increases  y  —who would care about this substantive issue?This means that the first stage requires the identification of a causal relationship that would be of interest to marketers or policy makers because their decisions will be usefully informed by a clear understanding of the consequences of a particular action. As marketing technology and practices change, the number of measurable, interesting, and unanswered questions grows. A variety of editorials in this journal and elsewhere focus on how researchers can identify important issues. For example, the January 2021 special issue of the Journal of Marketing was dedicated to finding important marketing research questions, as highlighted in the editorial ([36]. Other editorials that discuss ways to identify important research questions are [59]) and [26] in the Journal of Marketing, [87]) in the Journal of Consumer Research, [96] in Marketing Science, and [50]) in the Journal of Marketing Research. Data Question: How Can Researchers Find Data with Quasi-Experimental Variation in x?[12], p. 7) explain that an identification strategy ""describe[s] the manner in which a researcher uses observational data or data that is not generated as part of an intentional experiment, to approximate a real experiment."" They suggest first thinking of an ideal randomized experiment that can address the research question. This helps the researcher see clearly why an effect may not be identified causally in a nonexperimental setting.As [75], p. 151) discusses, ""Good natural experiments are studies in which there is a transparent exogenous source of variation in the explanatory variables that determine treatment assignment."" Unfortunately, there is no universally accepted interpretation of what it means to have a transparent exogenous source of variation. Therefore, [75] (p. 151) emphasizes the importance of clarifying identification assumptions and understanding the institutional setting, stating, ""If one cannot experimentally control the variation one is using, one should understand its source."" In the marketing context, [84] discusses the dangers of using methods in which the source of the exogenous variation is either poorly understood or only weakly related to the correlation of interest.Much of the work using quasi-experimental variation in marketing settings uses mundane but easily understood events such as contract changes, regulation, individual-level life changes, or shifts in firm policy that did not occur because of an anticipated effect on the outcome of interest. In some sense, some of the best sources of exogenous variation are mundane: nonmundane sources of variation such as global pandemics or earthquakes tend to be associated with other things happening that make it difficult to establish a clean causal relationship.Table 1 lists several example quasi-experimental papers published in 2018, 2019, and 2020 in the Journal of Marketing, the Journal of Marketing Research, and Marketing Science. This table also summarizes the source of variation these articles use, spanning contractual changes; ecological variation (e.g., weather); geography; and macroeconomic, individual, organizational, and regulatory changes. It is useful to consider in turn why each of these sources of variation can approximate random assignment.GraphTable 1. Examples of Quasi-Experiment Studies in Journal of Marketing, Journal of Marketing Research, and Marketing Science in 2018–2020. Quasi-Experimental VariationGeneral CategorySourceArticleResearch QuestionContractualTiming of American–Orbitz disputes to evaluate the absence of a major airline from a popular aggregator on consumer searchAkca and Rao (2020)Who has more market power in the airline-aggregator relationships?Timing of the introduction of the New York Times paywallPattabhiramaiah, Sriram, and Manchanda (2019)How does a paywall affect readership and site traffic?EcologicalVariation in the forecast error of the pollen levelsThomas (2020)How much does advertising affect purchases of allergy products?GeographicalDiscontinuities in the level of advertising at the borders of DMAsShapiro (2020)Does advertising affect consumer choice of health insurance?Discontinuities in the level of political ads at the borders of DMAsWang, Lewis, and Schweidel (2018)How does political advertising source and message tone affect vote shares and turnout rates in 2010 and 2012 Senatorial elections?IndividualTiming of users' adoption of a music streaming serviceDatta, Knox, and Bronnenberg (2018)How does a streaming service affect total music consumption?Variation in national ad exposures due to the local game outcomesHartmann and Klapper (2018)How do Super Bowl ads affect brand purchases?MacroeconomicVariation in income and wealth due to the recession between 2006 and 2009Dube, Hitsch, and Rossi (2018)Do income and wealth affect demand for private label products?OrganizationalDiscontinuity in the rounding rule that TripAdvisor uses to convert average ratings into displayed ratingsHollenbeck, Moorthy, and Proserpio (2019)How do online reviews affect advertising spending in the hotel industry?Timing of data breach and variation whether customer information was breached in a data breach eventJanakiraman, Lim, and Rishika (2018)How does a data breach announcement affect customer spending and channel migration?Variation in timing of adoption of front-of-package nutritional labels across categoriesLim et al. (2020)Do front-of-package nutritional labels affect nutritional quality for other brands in a category?RegulatoryTiming of the Massachusetts open payment lawGuo, Sriram, and Manchanda (2020)Do payment disclosure laws affect physician prescription behavior?Enforcement of minimum advertisement price policiesIsraeli (2018)What is the effect on violations if firms improve digital monitoring and enforcement of minimum advertised price policies?Timing of India's foreign direct investment liberalization reform in 1991Ramani and Srinivasan (2019)How do firms respond to foreign direct investment liberalization? 1 Notes: DMA = designated market area. ContractualTo find plausibly exogenous variation in timing, it often depends on an argument that the exact timing of a measure is plausibly exogenous. [28] argued that the timing of a dispute between the Associated Press and Google was essentially random as it was influenced by a contract negotiated many years previously, and so the timing could be used to study the effect of the removal of content from news aggregators on downstream news websites. EcologicalGenerally, within-season variation in weather is plausibly exogenous. For example, [95] uses quasi-experimental variation in actual and expected pollen counts. Key to the identification strategy is the focus on deviations from what was expected by firms. GeographicalWork using geographical boundaries often exploits the fact that people who live on either side of a demarcated geographic border are similar enough to be thought of as being randomized across them. For example, by looking at a remote border of Maryland that was geographically isolated from the rest of the state, [ 8] were able to argue that the imposition of sales tax for those who lived on one side of the border was random, relative to those people who lived nearby but just happened to be over the state border. MacroeconomicIt is also possible to take leverage of macroeconomic shocks. For example, [40] use the Great Recession as a key source of the variation on household incomes over time. They exploit the within-household variation in private label shares associated with within-household changes in income and wealth. The identifying assumption is that, conditional on all other factors, including an overall trend, within-household changes in income and wealth are as good as randomly assigned or exogenous changes. IndividualPlausibly exogenous variation can also be argued to occur at the individual level. For example, [20] use consumer migration to new locations as a quasi-experiment to study the causal impact of past experiences on current purchases. They argue that while migration is not necessarily random, the precise direction of migration can be, at least with respect to local brand market shares. OrganizationalShifts in firm policy and organizational events can also be leveraged as a source of variation. For example, [64] assess the change in customer behaviors between those whose information is breached and those whose information is not. The identification assumption is that the assignment of customers into the data breach group is likely to be random. RegulatoryMany papers also use the timing of regulatory changes as a source of variation. The argument here is typically that though the imposition of regulation may not be random, the timing of the regulation is. For example, [97] use a change in Massachusetts regulation of home sale listings to identify the effect of information about time on the market on house prices, and [76] use a change in the standardized nutrition labels on food products required by the Nutrition Labeling and Education Act and investigate how the Act changes brand nutritional quality.This discussion emphasizes that there are many potential sources of exogenous variation that can approximate a randomized experiment. We emphasize that typically the best papers focus on the research question first, and then imagine what the idealized experiment would look like to identify an actual quasi-experiment. Identification Strategy: Does x Really Cause y to Change?To convince a reader that an identification strategy is valid requires two steps. First, the researcher must explain where the variation they are calling exogenous comes from. This requires institutional knowledge and careful research into the setting. Second, the researcher needs to demonstrate that the relationship between the variation and the outcome of interest is very likely driven by the relationship between x and y and not by some other factor.To achieve the second requirement, it is useful to think about defending the experiment in terms of the exclusion restriction. Although the term ""exclusion restriction"" is often used specifically for instrumental variables, it is also a useful concept for other quasi-experiments. The exclusion restriction states that the quasi-experiment only affects y because it affects x.There are a variety of ways in which the exclusion restriction can fail, and so researchers look for exogenous variation in x that will have no direct effect on y. For example, [91] use wind speed as a quasi-experiment to provide an exogenous driver of posting to a user-generated content site about windsurfing. This allows them to understand the relationship between content creation and the creation of social ties. The argument for the exclusion restriction is that there is no other plausible way that wind could affect the creation of social ties except through content creation. As they mention in the paper, plausible challenges to this exclusion restriction are that windy days could affect friendship formation directly because users meet future online friends at windier surf locations. To address such challenges, the researchers present empirical data to suggest that the social ties that are being formed do not seem to reflect geography.Another example is [66], which examines the effect of delays in the early part of a banking technology adoption process on ultimate usage. Through a quasi-experiment that provides a source of exogenous variation in delays, they exploit the fact that Germany has a highly regulated system of public holidays and vacations that vary at the state level to prevent freeways from becoming overly congested. This leads to delays in technology adoption in that particular period to customers in one state, and not in others. The exclusion restriction is that there is no other reason that vacations or public holidays in the few days surrounding adoption would affect ultimate usage except through delaying the ability to navigate the security protocols required to sign up for the online banking service. One challenge for the exclusion restriction could be that individuals who sign up for a banking service around public holidays are somehow systematically different from others in terms of their laziness or motivation. To counter this challenge, the researchers present evidence that users are not different along any observable dimension.The exclusion restriction can also fail because of spillovers between groups that receive the exogenous shock or treatment and those that do not. The assumption that treatment of unit i affects only the outcome of unit i is called the stable unit treatment value assumption (SUTVA) in the treatment literature ([10]; [61]). This is not a trivial assumption. For example, [ 5] use the 2011 Orbitz–American Airlines disputes as an exogeneous event that led to a five-month period in which American fares were not displayed on Orbitz. The authors use this dispute to identify which company was hurt the most in terms of site visits and purchases. The SUTVA requires a valid control group such that the Orbitz–American Airlines disputes have no spillover on that group. As a result, the authors chose not to use airfare- or hotel-booking websites as a control due to the possible spillovers from Orbitz to other websites where customers can purchase. Instead, the authors used consumers' search of Lonely Planet as the control, because Lonely Planet is a travel website that is rarely used for bookings. The underlying idea is that an exclusion restriction cannot hold if the fact that one group was treated may also affect the control group's behavior. The SUTVA is therefore part of an argument that researchers make about an appropriate exclusion restriction.Importantly, there is no formula for a convincing explanation and defense of the empirical identification strategy in quasi-experiments. Except in cases of random assignment, it is not possible to prove that the identifying assumption is right. Instead, the objective for the authors is to pursue projects only when they can convince themselves (and their readers) that the causal interpretation is more plausible than other possible explanations. It is impossible to prove the validity of a quasi-experiment, such as whether one set of U.S. states serves as a legitimate control group for another or whether the exclusion restriction holds in instrumental variables. The credibility of any quasi-experimental work therefore relies on the plausibility of the argument for causality rather than on any formal statistical test. Empirical Analysis: How Can Researchers Estimate the Effect of x on y ?After establishing the identification assumption through the underlying framework of an exclusion restriction, the next step is to explore the data and conduct analysis that allows measurement of the effect of interest. This measured causal relationship is what has the potential to inform decision making. We discuss three different regression analysis frameworks using quasi-experiments: difference-in-differences (DID), regression discontinuity, and instrumental variables (IV). At the heart of all these strategies is a similar argument about the validity of the quasi-experiment.Table 2 outlines eight key steps in the three regression analysis frameworks. As pointed out by [54]) and others, the techniques are very similar in terms of the underlying econometric theory. However, though similar in the conceptual ideas, in terms of practical implementation, presentation, and how the researcher should best reassure their audience about the validity of the technique, there are some differences, which we expand on. The three frameworks differ in the first four implementation steps. We discuss the first four steps for each of the three regression analysis frameworks and highlight the issues in common across the three analysis frameworks in the last four steps. We also emphasize that many excellent papers do not implement each step, and this description is not intended to lead to unproductive dogmatism.GraphTable 2. Quasi-Experimental Regression Analysis Frameworks. Difference-in-DifferencesRegression DiscontinuityInstrumental VariablesIdentificationClarify the source of the shock, provide evidence why the shock can be seen as quasi-experimental, be clear on the identifying assumptions, and be transparent on the potential confoundedness.Justify the source of the fixed threshold, and whether the assignment to the treatment is determined, either completely or partly, by the value of the predictor on either side of a fixed threshold.Justify why the IV moves the endogenous covariate as if they are an experiment; explain the exclusion restriction.Raw dataTest whether those who receive the treatment are similar to those who do not; whether the parallel assumptions are satisfied; illustrate the trajectory.Provide evidence that the threshold is arbitrarily determined and not linked to underlying discontinuities in effects.Regress the outcome directly on the instrument and show that the instrument has the expected direct effect.Data analysisApply difference-in-differences regression framework in Equations 1 and 2 and adapt accordingly for other variations.Apply regression continuity framework in Equation 3.Report the first stage and determine whether the instruments are strong. Apply 2SLS in Equations 4 and 5 and conduct relevant tests.Standard errorsCluster at the level of treatment to account for within-unit correlation of the error term over time.Use robust standard errors, do not cluster on a discrete variableCluster at the level of treatment to account for within-unit correlation of the error term over time.Robustness checksConduct multiple robustness checks.Mechanism checksMeasure mediator variables or show moderation analysis.External validityDiscuss the assumptions required to capture the ATE.Apologies and caveatsApologize for all that is still unproven and give caveats. All of these methods implicitly rely on throwing out variation in the data that is not exogenous. In other words, they involve losing power to support the exogeneity assumption. This means that quasi-experimental work cannot use the R-squared as a useful summary of the appropriateness of the model. [41]) provide some useful evidence. While R-squared or a comparison of log-likelihoods is very useful in many other contexts (e.g., forecasts), benchmarking quasi-experimental analyses against other methods by using the R-squared will be misleading. Difference-in-Differences AnalysisA standard DID analysis compares a treatment group and a (quasi-) control group before and after the time of the treatment. The ""treatment"" is not truly a random experiment but, rather, some ""shock."" Unlike a simple comparison (or single-difference) analysis, DID methods generate a baseline for comparison between the treatment and the control group. By highlighting the change in the treatment group relative to the control group, DID enables the researcher to control for many of the most obvious sources of heterogeneity across groups.[47] is an example of a DID paper. The authors examine the impact of privacy regulation on the effectiveness of online advertising. In late 2003 and early 2004, many European countries implemented new restrictions on how firms could collect and use online data. The paper uses data on the success of nearly 10,000 online display advertising campaigns in Europe, the United States, and elsewhere between 2001 and 2008. The authors compare the change in effectiveness of the ad campaigns inside and outside Europe. Therefore, the first difference is the change in the campaign effectiveness, and the second difference is the change in Europe relative to elsewhere. Compared with before the regulation, ad campaigns became 2.8% less effective in Europe after the regulation. In contrast, compared with before the European regulation, ad campaigns became.1% more effective outside of Europe after the European regulation was implemented. Identification of Difference-in-DifferencesThe first step is to clearly lay out the identifying assumptions. [47], p. 63) state that ""the identification is based on the assumption that coinciding with the enactment of privacy laws, there was no systematic change in advertising effectiveness independent of the law"" and that ""the European campaigns and the European respondents do not systematically change over time for reasons other than the regulations."" A substantial portion of the paper is devoted to providing empirical evidence regarding whether ( 1) European ad agencies invest less in their ad creatives relative to non-European ad agencies after the laws, ( 2) the demographic profile of the respondents is representative of the general population of internet users, and ( 3) there may have been a change in European consumer attitudes and responsiveness to online advertising separate from the Privacy Directive.The analysis of consumer attitudes and ad responsiveness is based on a concern about unobservables, specifically whether there are alternative explanations for the measured changes in the attitudes of survey participants toward online advertising that were separate but contemporaneous with the change in European privacy laws. To check for such unobserved heterogeneity, [47]) examine the behavior of Europeans on non-European websites that are not covered by the European Privacy Directive to see if a similar shift in behavior can be observed, and they find evidence that changes in behavior are connected with the websites covered by the law, rather than the people taking the survey. The identification exclusion criterion is further validated by a mirror image of the falsification test by looking at residents of non–European Union (EU) countries who visited EU websites. When residents of non-EU countries visit EU websites, the ads are less effective in the postperiod. In contrast, when residents of these non-EU countries visit non-EU websites, there is no change in effectiveness before and after the EU regulation. Therefore, the results appear to be driven by what happens at EU websites rather than by a difference in how Europeans behave relative to non-Europeans. Raw Data Exploration of Difference-in-DifferencesThe second step is to explore the raw data. Before applying the DID framework, it is important to explore the raw data to assess whether the quasi-experiment appeared to have an effect. For example, when a treatment occurs in the middle of a time series, many papers use a graph that shows that before the treatment occurred, the treatment and control groups were on a similar trend and had similar values; then, after the treatment occurred, the trajectory of the treatment group diverged from the control group.Researchers should also assess whether their quasi-experimental setting meets the parallel trend assumption while exploring their raw data. This involves demonstrating that behaviors were similar in the period prior to the policy change across the treatment and control groups. Depending on the length of the time period, this can be done by conducting two-sample mean comparisons for each pretreatment period or by running a linear regression and looking at the time trend differences between the control and treatment groups. It is also often ideal to simply plot the raw data to support this point.Though it is desirable and convincing if the main effect of interest can be seen through descriptive statistics or visualization, we caution that this is not always possible. This may happen because effect sizes are small—as they often are in advertising—or because there is variation in the data that is best addressed using a regression framework. Analysis of Difference-in-DifferencesAlthough a DID regression can be represented in a 2 × 2 table, it is usually analyzed with regression analysis to allow researchers to control for factors that may change over time and across individuals. The simplest version of this regression is as follows: yit=α1TreatmentGroupi+α2AfterTreatmentt+βTreatmentGroupi×AfterTreatmentt+γXit+εit, Graph( 1)where y is the outcome of interest; i represents the individual, firm, or other cross-sectional unit of interest; t represents the time period; and  εit  represents the error. The key focus of the DID specification is on  β  , which captures the explanatory power of the crucial interaction term. Usually, researchers add controls  Xit  to address additional omitted variables concerns, such as an observed covariate that may not affect the treatment and control groups in the same way.When researchers have access to a panel, it is possible to address this concern directly by observing the same individuals, or the same campaigns, both before and after the timing of the treatment. It is then possible to add fixed effects to control for all individual-level (time-invariant) heterogeneity. Furthermore, if the data set includes more than two time periods, then adding time-specific fixed effects controls for all time-period-specific heterogeneity (across all individuals). With individual and time fixed effects, the DID regression is yit=βTreatmentGroupi×AfterTreatmentt+γXit+μi+τt+εit, Graph( 2)where  μi  is the individual-level fixed effect and  τt  is the time-period fixed effect. The fixed effects mean that the main effect of  TreatmentGroupi  and  AfterTreatmentt  drop out because they are collinear with the fixed effects. If possible, it is often desirable to difference out, rather than estimate, the fixed effects to avoid bias due to the incidental parameters problem (e.g., [67]). Most standard statistical packages automatically condition out the individual fixed effects from fixed effects panel data models where possible.[ 6]Though changes over time are common, DID methods do not require a time-series component. For example, [48] examine the impact of offline advertising restrictions on prices for keyword advertisements. The first difference is the keyword ad prices in states that have restrictions compared with states that do not. The second difference is the keywords that are affected by the restrictions compared with the keywords that are not.For quasi-experimental analyses that do examine changes over time, another tweak is that quasi-experimental treatment can occur at different times, meaning that individuals are treated at different times and that the  AfterTreatment  variable can change with subscripts i and t. For example, [27] study how a book review posted on Amazon affects sales of that book on Amazon, compared with sales of that book at barnesandnoble.com. Different books are reviewed at different times. Therefore, the treatment here is the review a book receives, and the  AfterTreatment  period occurs at different times for different books. [14], [19], [21], [35]), [49], and [85] explore the effects of variation in treatment timing. The issue is that because a fixed-effects DID estimator is a weighted sum of the treatment effect in each group and at each period, even though the weights sum to one, negative weights may arise when there is a substantial amount of heterogeneity in the treatment effects over time. A related concern has been highlighted by [45], who emphasize the problems that occur when both the treatment effect and treatment variance vary across groups.This means that researchers should be cautious in summarizing time-varying treatment effects with a homogeneous treatment effect as in the two-way DID framework if there is a substantial timing dimension. To address these issues, researchers have proposed a variety of estimators that allow for a cleaner comparison between the treated group and the control group. Both [21] and [35]) propose new estimands to estimate treatment effects in the presence of heterogeneity across groups and over time.[ 7] Another approach is taken by [93], who discuss corrections that should be applicable in a situation where leads or lags might be expected.Overall, DID is a powerful tool for helping identify the causal relationships that managers need for effective decision making. It can enable researchers to control for time-invariant individual-level heterogeneity, relying on the assumption that differences in the changes that the treatment and control groups experience over time are driven by the impact of the treatment. Regression Discontinuity AnalysisRegression discontinuity is a quasi-experimental technique in which the ""experiment"" relies on an exogenous arbitrary threshold. As [60], p. 616) put it, ""The basic idea behind the RD [regression discontinuity] design is that assignment to the treatment is determined, either completely or partly, by the value of the predictor being on either side of a fixed threshold."" Identification in regression discontinuityRegression discontinuity may be particularly useful to marketing scholars. [56] argue that many marketing interventions are based on thresholds of real or expected consumer or firm behavior. For example, direct mail companies use the scoring policies for recency, frequency, monetary models. Consumers just above and just below the cutoff should be similar in many dimensions, and their outcomes can be compared to assess the impact of the different mailings.Similarly, government policies based on firm size can provide a useful identification strategy for marketing scholars. For example, requirements for firms to post calories, undertake layoffs, and provide benefits often depend on the number of employees or other measures of firm size. By comparing firms just above and just below the threshold, it is possible to assess the effect of the policies on firm behavior.A regression discontinuity design implies that treatment is assigned depending on whether a continuous score  zi  crosses a cutoff  z¯  . The analysis then focuses on whether there is a change in the outcome of interest y in the neighborhood of  z¯  ([56]). In general, if a threshold is used as the source of the quasi-experiment, particular attention should be devoted to the source of the threshold and providing evidence that the threshold is essentially arbitrary and not likely to be linked to underlying discontinuities in behavior. Any discontinuity in the effect is assumed to be due to the treatment.This assumption is not always innocuous. Consider a $50 cutoff for receiving a marketing incentive. If the firm promotes the threshold and consumers try to achieve it, then there might be a substantial difference between people who spend $49 and people who spend $51. Those who spend $49 are likely to be unresponsive to the incentive because they did not try to cross the threshold to get the incentive. In contrast, those with exactly $50 in spending might have selectively chosen to spend exactly enough to get an incentive that they planned to use. It is important to address the potential for such concerns directly.This is reflected in a debate in economics about the effect of thresholds for low birth weight on medical outcomes. In an initial study, [ 6] used the fact that birth weight threshold of 1.5 kg is used to determine whether the newborn receives intensive medical treatment. In a critique of this work, [15] show that the children placed just at the cutoff seem to have significantly worse outcomes than babies on either side of the cutoff. This is evidence against use of this discontinuity for identification. [15] state, ""This may be a signal that poor-quality hospitals have relatively high propensities to round birth weights but is also consistent with manipulation of recorded birth weights by doctors, nurses, or parents to obtain favorable treatment for their children"" (p. 2119). Raw data exploration of regression discontinuityOnce the researcher has found a regression discontinuity setting, the first step is to explore whether the discontinuity is arbitrary and linked to discontinuities in any other variables. For example, [59] examine the relationship between online reviews and advertising spending in the hotel industry. They exploit the regression discontinuity design of the rounding rule that TripAdvisor uses to convert the average ratings of reviewers into the nearest half or full star (i.e., a rating of 3.74 is shown as 3.5 stars while a rating of 3.75 shown as 4 stars), building on work by [71]. The key identification argument is that the rounding mechanism creates discrete, random variations in perceived quality around the rounding threshold and is independent of a hotel's true quality.A threat to the arbitrary discontinuity threshold would be that hotels manipulate their average ratings around the rounding thresholds. [59] argue that if there is upward manipulation of ratings, there would be relatively few firms with average ratings just below the thresholds and a clump of firms with average ratings just above the thresholds. They show instead that the density of average ratings is uniform, with neither bumps nor dips above or below the round thresholds. They provide additional empirical evidence that characteristics of the hotels do not differ systematically above or below the threshold. Neither do they observe discontinuities in other key variables such as hotel prices and the number of five-star reviews. Analysis of regression discontinuityThe equation used for regression discontinuity can be written for panel data as yit=βI(zit≥z¯)+γXit+μi+τt+εit. Graph( 3)Here  β  is the treatment effect, the parameter of interest.  Xi  represents covariates.  I(zit≥z¯)  is an indicator function that equals one when  zit≥z¯  and zero otherwise. One final consideration is how to select the appropriate bandwidth for a regression discontinuity design, which is the question of how one decides on the sample to analyze, in terms of how far away the people in the sample are from the threshold where the discontinuity occurs. In general, such decisions have often been rather ad hoc, but there is an emerging literature that can help guide the researcher into thinking about how to take a more conservative approach to selecting bandwidth given the data at hand ([25]). The researcher should also ensure that their results are not sensitive to the choice of bandwidth. As with other quasi-experimental methods, the validity of the method cannot be statistically proven. Therefore, substantial emphasis must be placed on the explanation and defense of the quasi-experiment using raw data. Instrumental Variables AnalysisThe quasi-experimental perspective on IVs is somewhat different from the standard treatment in econometrics textbooks, which focuses on simultaneous equations and a more structural approach. The differences relate to justification and interpretation. The quasi-experimental approach emphasizes that the shocks that move the instrument should behave as if they are an experiment. The quasi-experimental approach gives a sense of the sign, significance, and magnitude of the causal effects. The structural approach emphasizes that the shocks should be motivated by an economic model that explains the exclusion restriction. The IV approach used in structural models gives elasticities that can be used to generate counterfactuals outside of the sample. Despite these differences in interpretation, it is important to remember that the underlying mathematics is identical. Identification of instrumental variablesThe basic idea behind using IVs is that the covariate of interest x contains both useful variation (to identify the causal effect of interest) and less useful variation (that confounds the effect). A good instrument z is strongly correlated with the useful variation but uncorrelated with the confounding variation. In other words, the researcher only uses the variation in x that can be explained by the exogenous shifter z.The standard two-stage model involves two steps. In the first-stage regression, a fitted value of  xi^  can be obtained by regressing x on instrument z and covariates  W  : xi=γzi+ϑWi+ηi. Graph( 4)In the second-stage regression, the IV estimator  β^  is obtained by regressing the outcome y on the fitted value of  x^  and covariates  W  : yi=βxi^+φWi+εi. Graph( 5)The identification of the effect of x on y relies on the following ""reduced form."" Inserting the predicted x to the y equation will give Equation 6. Here,  φ^  is used to highlight that when regressing y directly on instrument z and covariates W, the estimated covariate coefficient is rescaled as  φ^=βϑ+φ  . yi=βγzi+φ^Wi+εi. Graph( 6)Therefore, from the quasi-experimental point of view, an instrumental variable can be seen as a treatment that affects the endogenous covariate directly. This means that directly regressing the outcome of interest on the instrument (in one stage) will get the causal effect of interest, but it will not be properly scaled. The purpose of implementing two stages is to scale the treatment effect properly. There are many ways of operationalizing instrumental variables, and this can be a place for highly technical tools. We emphasize the simplest two-stage least squares (2SLS) approach, but the intuition behind the role of instrumental variables as an identification strategy remains regardless of functional form assumptions. Using two stages enables the researcher to disentangle  β  and  γ  . In other words, two stages are needed to get the elasticity right, but the experiment happens at the level of the instrument and so, even though the focus is on the relationship between x and y, the intuition on causality happens at the level of the relationship between z and y.Returning to [91], while the paper adds some additional necessary nuance to the estimation to fit the particular situation, the intuition on causality measures the impact of wind (the instrument  z  ) on social ties (the outcome of interest  y  ). This will be  βγ  . The relationship of interest, however, is the impact of posts (  x  ) on social ties (  y  ), which is measured as  β  .IV can be a less transparent solution to identifying causal effects compared with the other two analysis frameworks discussed previously (for a detailed discussion, see [84]). The distinction between the relationship of interest (  β  ) and the direct estimate from the quasi-experiment (  βγ  ) means that it is sometimes harder to visualize how the quasi-experimental variation works in IVs.Transparent communication of IV analysis is difficult for three reasons. First, in contrast to the binary nature of the exogenous variation in DID and regression discontinuity, instruments are often continuous. This makes it more difficult to communicate the intuition for why the variation is exogenous to the potential for omitted variables or simultaneity. The ability to use continuous instruments (and multiple instruments) can also be seen as a strength of IV techniques. They enable a more flexible set of counterfactuals because there are more treatments observed and used in the analysis. For example, while a discrete quasi-experiment on retailer discounts would allow the researcher to compare the impact of a small set of retailer discounts on sales, a continuous instrument for the discounts might allow the researcher to compare a variety of smaller and larger discounts.Second, weak instruments are a challenge. Instrumental variables techniques are consistent but biased, and this bias can matter even in seemingly large samples ([92]). Weak instruments can lead to incorrect inference in which the bias of the weak instrument dominates the potential bias of the omitted variables.Knowing the context and the institutional setting can be invaluable in identifying strong IVs. For example, [76] derived their instruments for brand taste and price from the authors' intimate knowledge of the regulation and food industry. There are also recent advances in econometric methods that allow for more accurate presentation of statistical significance when instruments are weak ([68]). As [11] point out, many of the challenges of weak instruments are magnified when authors use multiple instruments to deal with multiple sources of endogeneity. By contrast, a focus on a single endogenous variable with a single source of endogenous variation has attractive statistical properties as well as being more transparent to the reader.Third, many researchers present IV results with different tests and with different norms. This makes it difficult to read and assess the validity of papers with instruments. Raw data exploration and analysis of Instrumental Variables[12], pp. 212–13) provide a sequence of steps to follow in an attempt to standardize practice. In presenting this list, we hope that it does not lead to unproductive dogmatism, and we emphasize that this is just one possible way to communicate the rationale behind a causal interpretation of the results. Still, we hope that in following these steps to the extent possible, marketing scholars can avoid being subject to many of the criticisms highlighted by [84]. The steps are as follows: Regress the outcome directly on the instrument. When using IV techniques, it is also desirable to show the reduced form result of regressing the outcome directly on the instrument. Because this is an ordinary least squares regression, it is unbiased. At the very least, the researcher should be confident that the instrument (  z  ) has the expected direct effect on the outcome (  y  ). Report the first stage. Assess whether the signs and magnitudes of the coefficients make sense. Report the F-statistic on the excluded instruments. This helps determine whether the instruments are weak. [92] advise that F-statistics below 10 in case of only one instrument suggest weak instruments, though, as [12], p. 213) note, ""Obviously this cannot be a theorem."" Similarly, [84] suggests reporting the first stage with and without the instruments to document the incremental impact of the instruments on the R-squared. If there are multiple instruments, report the first- and second-stage results for each instrument separately (at least in the appendix) because bias is less likely if there is only one instrument. Presenting the results separately also helps the reader understand the intuition behind the quasi-experiment underlying each instrument—whether the multiple instruments use different variation in increasing the exogenous shift in x. If there are multiple instruments, an overidentification test such as the Sargan–Hansen J can be performed to test whether all instruments are uncorrelated with the 2SLS residuals.[ 8] However, given the difficulty of identifying a robust instrument, it is unusual for researchers to have convincing cases for multiple instruments in a way that leads their regression to be overidentified. In other words, increasingly, standard practice is to focus on one instrument rather than many ([11]). Conduct a Hausman test comparing ordinary least squares and instrumental variables. If the results change, reflect on whether they change in a direction that makes sense given the power of the instrument. Do not interpret the results of the Hausman test to prove that the endogeneity problem is irrelevant. As noted by [84], the instrument may not be valid and therefore the test would be uninformative. Assess whether there is a weak instrument problem. For example, in a linear model, compare the 2SLS results with the limited information maximum likelihood results. When there is a weak instrument, the two-stage least square estimators are biased in small sample. Limited information maximum likelihood estimators have better small sample properties than 2SLS with weak instruments. If the two estimates are different, there may be a weak instrument problem. Any inconsistency from a small violation of the exclusion restriction gets magnified by weak instruments. Presentation of Results and Clustering of ErrorsRegardless of which regression analysis framework to employ, presentation of baseline estimates and standard errors, along with a set of robustness checks ([59]) is standard. This typically appears in the form of a regression table with several different specifications. For example, the first column might not include any controls beyond the fixed effects, and the next set of columns might add controls. The economic magnitude of the coefficients should be discussed, both with respect to changes in the covariate of interest and relative to the range and standard deviation of the covariate and dependent variable.A key issue in quasi-experimental analysis is correlated errors in observations, because the outcome is often observed at a finer level than the treatment. For example, the researcher might observe treatment and control groups for several advertising campaigns over a long time period. For each campaign, the researcher might have data on many individuals per campaign and many time periods per individual; however, the choices of the same individual in many time periods are likely to be correlated. [16] emphasized that failure to control for the correlation between these choices will lead to an overstatement of the effective degrees of freedom in the data, and therefore, standard errors will be biased downward. They suggest clustering standard errors by individual over time to address this issue and provide Monte Carlo evidence that clustering is likely to lead to robust inference.Similarly, [38] emphasize that if individual responses to the same treatment are likely to be correlated, for example, because of close physical or social proximity, clustering standard errors by groups of individuals is a conservative and useful way to estimate standard errors. Researchers often need to decide on the size of the clusters. For example, in studying ready-to-eat breakfast cereals, is the correct unit the company such as General Mills, the brand such as Cheerios, or the sub-brand such as Honey Nut Cheerios? The answer depends on the data and research question. If the data are at a lower unit level (e.g., individuals) than a treatment that takes place at the firm level, cluster the standard errors at the level of the treatment. A useful perspective on this is provided by [ 2], who remind researchers that the major driver for clustering should be the experimental design rather than simple expectations of correlation. More recently, there has been evidence suggesting that it is undesirable to cluster on the variable that determines whether that observation is subject to the regression discontinuity design (e.g., age). The answer is often instead simply to reduce the bandwidth across which the regression discontinuity is studied ([65]).Clustered standard errors rely on consistency arguments and large samples. With a small number of clusters, alternative methods are needed, such as those developed by [22], [30], and [53]. For example, [43] investigate consumers' dynamic responses to price promotions in a retail setting that involved randomly assigning ten supermarkets into varying promotion depths. Given that treatment takes place at the store level while the observation is at the consumer level, each consumer's effective contribution to reducing standard error estimates is likely to be lower than in a setting where there is no correlation across observations. However, given the relatively small number of stores/clusters available in this setting, the authors implement the wild bootstrap procedure, as proposed by [22], to correct for downward bias potentially induced in small samples. However, [24] show that even this approach requires rather large assumptions. Challenges to Research Design: What if Variation in x is not Exogenous?A more general point is that quasi-experiments range in how plausible the exogenous variation underlying the paper is, ranging from cases where the allocation is almost completely random to less clear cases where a firm or consumer assignment to treatment or control is partly random and partly an endogenous choice. Perhaps the ideal thought experiment here is [101], whose treatment and control were a pair of kidneys from the same person. [101] finds that in the United States, even identical kidneys from the same donor are received differently depending on the observed number of rejections preceding the recipient in the queue. Most research settings are less favorable. In such settings, it is often useful to combine different approaches in the same paper. For example, [79] combines a DID strategy with counterfeit entry as the treatment with a convincing and high-powered instrument on government regulation.Still, there will be situations where a compelling exclusion restriction is lacking or the treatment–control allocation appears far from random. If the treatment and control groups are substantially different in the pretreatment or if the treatment appears to be applied based on selected characteristics, the control group is unlikely to be a good proxy for the counterfactual, and the quasi-experiment may be less likely to be valid.We provide a discussion of three methods that are further steps researchers can take when comparability between the control and treatment groups is violated. They vary in terms of the observed and potentially unobserved differences between the control and treatment groups. Table 3 provides a summary of the frameworks and when to apply them. The table emphasizes that researchers should be cautious about applying matching methods or correction for selection bias on the grounds that there are no plausible exclusion restrictions, because these methods still require the researcher to make an argument about an exclusion restriction. The technical details of matching methods or selection bias correction are different from the three methods described previously, but the idea is similar in nature. The main goal is to bring in additional data to create control and treatment groups that are like those in quasi-experiment studies.GraphTable 3. Steps if Researchers Are Worried They Do Not Meet the Exclusion Restriction. Propensity Score MatchingSynthetic ControlSelection Bias CorrectionAssumptionsObservable control variables are capable of identifying the selection into treatment and control conditionsThe counterfactual outcome of the treatment units can be imputed in a linear combination of control units in the absence of treatment.The unobservables that enter the treatment selection and the outcome are jointly distributed as bivariate normal.IdentificationThe exclusion restriction can be met conditional on the variables in the match.The exclusion restriction can be met conditional on the pretreatment outcomes.There is at least one variable for which a compelling argument can be made for the exclusion restriction in the selection equation.SettingsWhen matching is done to control the treatment and control pretreatment outcomes on a number of cross-sectional covariates.When the focus is on the evolution of the outcome and the pretreatment time period has rich data on treatment and control groups.When the allocation to the treatment condition is not fully random.CaveatsAssess the degree of overlap after matching, and assess sensitivity to potential selection on unobservables. Still need to justify the exclusion restriction.Harder to interpret the weights used to create the ""synthetic control."" Still need to justify the exclusion restriction.Justification of why certain observables only affect treatment selection but not the outcome variable. Still need to justify the exclusion restriction.  Propensity Score MatchingMatching methods, pioneered by [83], have been developed such that the outcomes of the treated are contrasted only against the outcomes of comparable untreated units. Many published articles in marketing have used propensity score matching when comparability between the control and treatment groups is violated. An assumption of propensity score matching is that there are observable control variables capable of identifying the selection into treatment and control conditions. This is not a trivial assumption. It suggests that propensity score matching is only good if the exclusion restriction is met conditional on the variables in the match. Any matching procedure to make the control and treatment more similar in the observables can be seen as a flexible functional form with adding ""control variables"" to an analysis framework. Propensity score matching requires subject-matter knowledge regarding the role of covariates in the treatment assignment decision and whether the exclusion restriction is satisfied conditional on the covariates. Therefore, we caution against applying matching methods without convincing justification of exclusion restriction.It is difficult to identify a standard procedure for propensity score matching. We refer to [61] as a good starting point. The general objective of propensity score matching is to estimate a score such that the distribution of all the observed variables and behaviors among the treated units is similar to that among the control units. In this discussion, we consider the set of treated units to be fixed a priori. Four steps are involved in the propensity-score-matching procedure.First, choose a functional form of the propensity score. The basic strategy uses logistic regression to model the probability of receiving the treatment given a set of observables. Second, measure the distance and apply a matching algorithm. Several possible matching methods are available including, for example, nearest-neighbor matching based on the distance in the estimated propensity score or multiple matching using all controls within some distance from the treated unit. Third, assess the degree of overlap in the distribution of the linearized propensity score after matching. Researchers typically plot and compare the histogram-based estimate of the distribution of the linearized propensity score (logarithm odds ratio) for the treatment and control groups. To inspect the match quality, it is useful to show tables on the distribution of the estimated propensity scores and the mean values of some key variables for the treated and untreated over different propensity score intervals.[ 9] Fourth and finally, calculate the average treatment effect (ATE) with the matched sample using, for example, the DID regression analysis framework discussed previously.There are at least two caveats regarding propensity score matching. First, the model for the propensity score may be misspecified. In that case, the balance in covariates conditional on the estimated propensity score may not hold, and the credibility of subsequent inferences may be compromised. This calls for a careful discussion on the role of covariates in the treatment assignment decision. Specifically, it is important to provide a discussion of whether the covariates can be considered exogenous to the treatment. Second, regardless of the number of observed covariates used, propensity score matching does not account for the potential selection on unobservables in treatment assignment. It is important to explain why controlling for observables will address concerns with the exclusion restriction or why unobservables are not an issue in treatment assignment. Synthetic Control MethodsIn some cases, even the closest match may not be close enough. This is particularly relevant when researchers are interested in how an event, regulatory intervention, or firm policy change affects the evolution of the outcome of interest, in contexts where only a modest number of treated units (possibly only a single one) and control units are observed for a large number of periods before and after the event. Two aspects make this setting different from the typical use of the propensity-score-matching method. First, matching is done over the pretreatment outcomes in each period rather than a number of covariates. Second, the number of control units and the number of pretreatment periods can be of similar magnitude. Synthetic controls use a different convex combination of the available control units ([ 3]; [ 4]; [39]). The intuition behind this method is that the created synthetic control unit closely represents the treated unit in all the pretreatment periods and affords time-varying causal inference on the trajectory of the outcome of interest.Synthetic control has been used in multiple recent studies with quasi-experimental design ([ 1]). For example, [51] analyze the causal effect of industry payment disclosure on physician prescription behavior, [99] assess the impact of mobile hailing technology adoption on drivers' hourly earnings, and [78] study the causal effect of online paywalls on the sales revenues of newspapers.Like propensity score matching, synthetic control methods are statistically rich, but they do not replace a carefully thought-out exclusion restriction and identification argument. Put differently, if propensity scores or synthetic controls appear to work when the treatment and control group are not similar, it is important to explain why controlling for observables will address issues with the exclusion restriction. In many cases, such explanations are weak and the exclusion restriction is unlikely to hold. Recent work in economics emphasizes this by showing the benefits of combining a synthetic control method with a strong exclusion restriction ([13]). Selection Bias Correction MethodMany papers written in marketing involve a comparison of potentially different groups that reflect endogenous choices by companies or consumers where the allocation to the treatment condition is not fully random. For example, [46] assess if the introduction of the free mobile app in a business-to-business context increases sales revenues from buyers who adopted the app. In an ideal setting, the company could randomize the treatment, then observe sales from buyers who did not get the app and sales from buyers who did get it. However, this company's app was available to all buyers. Therefore, the buyers' app adoption is not random, and self-selection into the treatment (adoption) group needs to be addressed. Omitted variables that drive strategic app adoption could correlate with the sales from these buyers.When this happens, it is sometimes useful to estimate a Heckman selection model ([57]), which explicitly models selection into the treatment as a two-step process. As [100], p. 564) pointed out, the exclusion criterion is still key to the identification of the treatment effect of interest in the two-step estimation procedure. Without the exclusion criterion, the effect of the treatment is identified only due to the nonlinearity in the functional form (specifically through the inverse Mills ratio). This may lead to severe collinearity and imprecision in the standard errors. More importantly, without a strong and credible exclusion restriction, identification in this setting is driven by the assumed functional form.In other words, although the Heckman correction will provide an estimate without an exclusion restriction, that estimate depends entirely on the assumption that the error structure is bivariate normal. When there is an argument for the exclusion restriction, a selection model is helpful. In the absence of the exclusion restriction, even if combined with other techniques such as propensity score matching, the results would be identified off the functional form assumption alone. Put differently, if one of the covariates in the correction equation satisfies the exclusion restriction, then it is the variation in that variable that identifies the control for selection. In contrast, if the covariates in the first step are all also in the second step, then it is only the assumed error structure that identifies the control for selection.There are both similarities and differences between selection bias correction and instrumental variable approaches. There are also similarities with the control function approach in terms of the importance of functional form assumptions on the errors in the absence of an exclusion restriction. Control functions are not part of the standard quasi-experimental toolkit, so we do not provide a detailed discussion. The selection bias correction approach uses the instrument to control for the effect of unobservables, while the instrumental variable approach attempts to eliminate the threat of endogeneity by only leveraging the useful variation created by the instrument. Yet, the two approaches share the basic idea of using an exclusion criterion (or instrument). Ultimately, both rely on the ability to find an exclusion restriction that creates useful and exogenous variation. This is why we emphasize the importance of identification in quasi-experiments and caution against blindly applying a correction for selection bias without carefully thinking about the identification assumption and providing a justification for why the exclusion restriction holds. Selection bias correction approaches are therefore only useful for causal inference in the presence of a strong credible exclusion restriction. Robustness: How Robust is the Effect of x on y ?The specific robustness checks chosen will depend on the exact context. With electronic appendices and increasingly cheap computation, it is possible to show robustness to a large number of alternative specifications. Here, empirical work with quasi-experimental methods differs substantially from research using forecasting models. The aim is not to show one specification (or model) and defend it. Instead, the idea is to show that the sign, significance, and magnitude of the estimate of  β  remain broadly consistent across a vast range of possible models ([59]). Often these robustness checks are dropped from the published version of the article, though they are very useful in the referee process and can end up as part of an online appendix. The following subsections describe some examples of useful robustness checks. Different ControlsCompare the coefficient of interest in the models with and without controls. For example, if the coefficient changes from 2.5 to 3.5, then this change (+1.0 in this example) is informative about how big the impact of the omitted variables has to be relative to the observed controls for the omitted variables to drive the result. [ 7] provide a method to examine how much the effect of interest changes as controls are added, and then to assess how important the omitted variables would have to be for the treatment effect to disappear. The method is based on Rosenbaum bounds ([37]; [82]). It has been applied in the marketing literature by [73] and extended by [90]. Although the formal method is useful, as discussed in [77], many researchers ([ 9]; [74]) use the more basic insight that there is information in the impact of the controls on the measured effect of interest. This does not mean that results are invalid if the controls do change the estimated effect substantially, but documenting that adding seemingly relevant controls does not change the results can provide further support for the causal interpretation. Different Functional FormsResults should not depend on arbitrary choices of functional form. For example, if using a linear probability model, show robustness to logit and probit. The choice between linear probability models and nonlinear models such as logit is widely debated. [12] argue for linear probability models because they are simple to interpret and consistent under a basic set of assumptions. Others argue against them because they are inefficient (and inconsistent if the assumptions are violated). In cases like this, where the literature does not give clear guidance on the choice of model, showing robustness to different choices is optimal. Different Choices of the Time Period Under StudyResearchers often can choose when to start and end the sample. For example, for a treatment that occurs in 2004, researchers should be comfortable that the results are robust to the arbitrary choice of whether the period studied is 2002 to 2006, 2000 to 2008, 1995 to 2015, and so on. Different Dependent VariablesThere might be several different dependent variables that relate to the outcome of interest. Showing robustness to these related outcomes increases confidence in results. Different Choices of the Size of the Control GroupResearchers choose whether all the data should be used in the control group, or only a subset of the data that is ""close"" to the treatment group (e.g., as measured by a propensity score). Researchers can also choose how to define the treatment group. Placebo TestsThe idea of a placebo test is to repeat your analysis using a different part of the data set where no intervention occurred. For example, if the quasi-experimental shock happens this year, instead of comparing the difference in the outcome between last year and this year between the control and treatment groups, you can conduct a placebo test by redoing the analysis and compare the difference in the outcome between the control and treatment groups using periods with no intervention shocks. Alternatively, analysis can be conducted on an outcome that should be unrelated to the intervention being studied. The goal is to establish a null effect when there is not supposed to be one.It is unlikely that every robustness check will yield the same level of significance or the same-sized point estimate as the initial specification. Researchers (and reviewers) should therefore not expect every specification to yield the exact same results. The key is to communicate when the results hold up. This will consequently help inform the reader what drives the statistical power behind the results.Broadly, quasi-experimental research aspires to identify effects that do not rely on the underlying assumptions outside of the experimental variation. There are many places where that can break down, including functional form assumptions, external validity, and various confounding effects. The focus is on a robust single causal relationship. Mechanism: Why Does x Cause y to Change?The most effective papers typically do not stop with identifying a causal effect and its magnitude. After identifying a likely causal relationship, it is important to assess why x causes y to shift. Understanding mechanisms is often a key goal of social science. There are at least three benefits of establishing mechanisms. First, it provides a rationale for why the effect should exist in the first place. It requires the authors to think about the theoretical contribution of their research more carefully and helps make the argument for causal identification more convincing. Second, identifying mechanisms can help evaluate the benefits and negative consequences of the intervention and identify avenues for course correction, if needed. Third, understanding mechanisms allows for the possibility to extrapolate the findings to other contexts. Research needs to provide guidance on when and why the causal relationship is relevant. Assessing the Mechanism Through Mediation AnalysisWhen the data afford a direct measure of mediator variables, mechanisms can be inferred by mediator analysis. To illustrate how quasi-experiments can show process through mediation, we use [52] as an example. They investigate whether a variable compensation scheme increases salespeople's stress, resulting in emotional exhaustion and more sick days, and counteracts the sales benefits companies might expect from variable compensation schemes. In one of their empirical analyses, they use a natural experiment where a company dropped the variable compensation share from 80% to 20% in one of its business units. To test the health state as a possible mediator variable, they were able to measure sick days both before and after the change in the variable compensation share. In the country of study, sick days are strictly regulated by law and require certification by a physician (at the latest on the third day of the leave). Those who take more than three sick days in a given month are more likely to have substantial health problems. They measure the sick days counting after the third sick day in a month.Combining the DID analysis with mediator analysis, [52] show that the direct effect of the treatment (drop in variable compensation share) on sales performance is significant and negative, and that the indirect effect of the treatment on sales performance via sick days is positive and significant. The mediator analysis suggests that a higher variable compensation share is associated with enhanced sales performance but also with more sick days, which, in turn, reduce the gains to sales performance. Assessing the Mechanism Through Moderation AnalysisHeterogeneous treatment effects can be used to test behavioral mechanisms. In a quasi-experimental setting, mechanism checks via heterogeneous treatment effects, sometimes referred to as falsification checks, are not simply equal to identifying moderators. They involve identifying which groups would be affected by a certain mechanism that would display the causal effect of interest, and which other groups would not display the causal effect of interest by the proposed mechanism.Moderation analysis therefore serves a broader purpose by providing an opportunity to help explore the behavioral mechanism. If the effect goes away when theory suggests it should, then this helps identify why it happens. If the effect is larger when theory suggests it should be, then this also helps identify the mechanism. A simple approach is to estimate the effect separately by whether an individual is a member of a group that theory suggests should experience a bigger effect. Formal testing of whether the difference is statistically significant requires a three-way interaction between x, the source of variation, and group membership.There are many relevant examples in marketing of the use of moderation analyses to demonstrate a mechanism if there is a reason to believe the boundary of underlying process exists or the magnitude of the treatment effect varies by some observables. For example, after showing the European privacy regulation hurt online advertising, [47] ran a falsification check demonstrating that European consumers behaved like Americans when visiting American websites and that American consumers behaved like Europeans when visiting European websites. The paper then explored the mechanism and showed that the regulation especially hurt unobtrusive advertising and advertising on general interest websites, two situations where using data to target advertising is particularly valuable.Overall, mechanism checks through mediator or moderation analyses are important because they distinguish the goal of the marketing scholar from the marketing practitioner. Marketing practitioners run experiments and analyze data to understand what they should do in the particular situation they are facing. Marketing scholars need to have a broader sense of applicability beyond the specific setting being studied. Mediation and moderation analyses provide an understanding of when a marketing action will and will not lead to the desired behavior. For this reason, marketing papers are more likely to be remembered for the evidence that is shown in support of a theory explaining why the result holds. External Validity: How Generalizable is the Effect of x on y ?The external validity discussion in a paper should recognize the assumptions required for the analysis to capture the ATE across the population of interest, rather than a more local effect that is an artifact of the data sample or the source of quasi-experimental variation. A key concept is the ATE across the entire population. This is the difference in outcomes that would occur by moving the entire population from the control group to the treatment group. However, in some cases, the ATE may not be particularly relevant, because it averages across the entire population and includes units that would never be eligible for treatment ([100], p. 604). For example, we would not want to include millionaires in computing the ATE of a job training program. To address this, the researcher could use the average treatment effect on the treated, which measures the expected effect of treatment for those who actually were in the treatment condition.One reason why a research setting may fail to be externally valid is if the treated population is unrepresentative ([72]). A concern that will drive whether the treated population is unrepresentative is whether those affected could self-select into and out of the treatment. For example, [28] study a rule change by Google that allowed non–trademark holders to use trademarks in search advertising copy. They study the rule change's effect on user click behavior. In this case, many advertisers did not alter their advertising copy strategy, for a variety of reasons. These advertisers may be systematically different from the advertisers that did change their strategy. Because these advertisers were not forced to change their strategy, we will never know what would have happened if they did. When faced with such issues, it is best to spell out the potential for self-selection and discuss whether it makes the paper more or less relevant. In this case, it would be accurate to say that the researchers captured the effect of a loosening of trademark restrictions, because it is unlikely that a search engine would force its advertisers into using other advertisers' trademarks. However, it would not be accurate to claim that the researchers capture the broader effect of all advertisers using other advertisers' trademarks in their copy.The treated population may also be unrepresentative if the treatment impacts a subpopulation to change behavior, but not the main population of interest. This means that the measured effect is localized to that subpopulation, and it is referred to in the literature as the local average treatment effect (LATE). For example, in the context of regression discontinuity, the LATE is the average of the treatment effect over the individuals who would have been in the counterfactual condition if the discontinuity threshold were changed. A limitation of regression discontinuity is that the results directly apply only to populations around the threshold. For example, comparing the $49 spend with the $51 spend may be informative about the impact of the marketing incentive on consumers who spend around $50; however, consumers who typically spend a lot more or a lot less might be different. The idea of LATE also has implications for the interpretation of instrumental variables estimates, as any IV estimate is the LATE for the observations in the regression who experienced the kind of variation exploited by the instrument.[10]More broadly, as with other aspects of quasi-experimental research, the best practice regarding the external validity of results is to clearly lay out the assumptions and limitations. For example, [94] use a quasi-experiment and DID to examine the impact of advertising revenue on the type of content posted on Chinese blogs. While it might be tempting to interpret the results as suggestive of a broader impact of commercial interests on media, they are careful to emphasize the many differences between blogs and other media, between China and the rest of the world, and between the way the bloggers were compensated and other online advertising models. In this way, Sun and Zhu's article explicitly limits the temptation of the reader to extrapolate too much.An internally valid quasi-experimental estimate can have broader external validity when used to identify relationships such as elasticities and then to use a structural model to identify the counterfactual of interest. In these cases, under the assumption that the model is a useful representation of reality, quasi-experimental methods serve as a complement for, rather than a substitute to, structure. For example, [ 9] use quasi-experimental methods to identify the impact of the automotive brand preferences of parents on the brand preferences of their children. They then use structural methods to estimate the implications for firm strategy. [42] use quasi-experimental variation in health insurance prices to identify price elasticity and then combine this measure with a structural model to estimate the welfare implications of adverse selection. [29] use quasi-experimental variation around set quotas to identify the relationship between commissions and sales, and then use this variation in a structural model to determine optimal compensation schemes.Overall, effective quasi-experimental research requires an understanding of the underlying assumptions behind any broad interpretation of quasi-experimental results. Quasi-experiments often require a focus on a narrow slice of the data, and therefore, it is important to consider the degree to which the results apply to a broader population. Apologies: What Remains Unproven and What Are the Caveats?Any identification strategy relies on a set of assumptions. These assumptions need to be explicit throughout the paper. There are always some tests that cannot be run, for example, due to lack of data. There are always some robustness checks that are weaker than others. There are always some steps from data to interpretation. While apologies do not mean all is forgiven, the objective should be to clarify the boundaries of the claims. Obfuscation is much worse than a clear summary of the identifying assumptions.As an example, [51] employ a DID research design to study the effect of the payment disclosure law introduced in Massachusetts in June 2009. The research design uses the setting that physicians located in the border counties of Massachusetts and its neighboring states did not have disclosure laws during this period. They lay out the assumptions underlying their estimation:Our identification of the effect of disclosure legislation relies on the change in new prescriptions by physicians located in Massachusetts (MA) after the policy intervention, relative to their counterparts from ""control"" states in which no such law existed in the same period.... To assess potential threats to the validity of our research design, we verify if the result was driven by changes in physician payments as a result of the MA disclosure law. If such payment changes were primarily driven by local pharmaceutical reps reallocating their marketing budgets across physicians operating on either side of state borders, this would render the border identification strategy problematic.([51], p. 517)This example communicates three distinct points. First, it explains the identification strategy. Second, it details the main threats to the validity of this identification strategy. Third, it describes what they do to address it. These points suggest that effective apologies focus on demonstrating what interpretations are reasonable, and what might be a stretch of the results. The goal is not to show that in all circumstances and every conceivable way the identification is perfect. That is not possible. Instead, the goal is to provide clear bounds on the interpretation. The paper's contribution is then a function of whether it provides new knowledge under this bounded interpretation. ConclusionQuasi-experimental techniques are an important tool for marketers. First, marketing scholars need to be able to inform marketing practitioners—both managers and policy makers—about the causal effect to allow practitioners to make superior decisions. Second, the best quasi-experimental papers do not simply prove a causal effect but delve into the underlying mechanism, which is key to marketing scholarship's goal of generalizability. Third, such techniques become more important as the scope and span of marketing practice expands and there are new settings and more varied sources of data that allow their application.The objective of a quasi-experimental research paper is to answer an interesting and important research question about a causal relationship and provide evidence suggesting the mechanism behind the relationship. The choice of method (DID, regression discontinuity, or instrumental variables) depends on the nature of the quasi-experiment. The framework we present focuses on understanding how exogenous variation helps uncover causal relationships and why specific actions affect behavior. Of course the details of the methods will evolve over time as new research appears. Because marketing scholars are often interested in providing generalizable insights about how marketing actions change the behavior of individual consumers, the quasi-experimental framework is particularly useful. Similarly, firms that want to use those insights benefit. As the availability of detailed data grows and marketing technology changes, these methods will enable marketing scholars to provide assessments of a wide variety of situations in which a particular marketing action is likely to change consumer behavior or market dynamics.  "
5,"Connecting to Place, People, and Past: How Products Make Us Feel Grounded Consumption can provide a feeling of groundedness or being emotionally rooted. This can occur when products connect consumers to their physical (place), social (people), and historic (past) environment. The authors introduce the concept of groundedness to the literature and show that it increases consumer choice; happiness; and feelings of safety, strength, and stability. Following these consequential outcomes, the authors demonstrate how marketers can provide consumers with a feeling of groundedness through product designs, distribution channels, and marketing communications. They also show how marketers might segment the market using observable proxies for consumers' need for groundedness, such as high computer use, high socioeconomic status, or life changes brought on by the COVID-19 pandemic. Taken together, the findings show that groundedness is a powerful concept providing a comprehensive explanation for a variety of consumer trends, including the popularity of local, artisanal, and nostalgic products. It seems that in times of digitization, urbanization, and global challenges, the need to feel grounded has become particularly acute.Keywords: connectedness; alienation; need to belong; groundedness; local; rootedness; terroir; traditionalTo be rooted is perhaps the most important and least recognized need of the human soul.—[47], p. 43).Dual forces of digitization and globalization have made our social and work lives become increasingly virtual, fast-paced, and mobile, leaving many consumers feeling like trees with weak roots, at risk of being torn from the earth. In response, we observe consumers trying to (re)connect to place, people, and past—to get anchored. Against this backdrop, we propose and test an important driver of consumer behavior that has largely been overlooked in marketing literature: the feeling of groundedness.We believe that many consumers have a need to feel grounded, which we define as a feeling of emotional rootedness. This feeling emanates from connections to one's physical, social, and historic environment and provides a sense of strength, safety, and stability. Although the concept has received scant attention in prior marketing, consumer behavior, and social psychology research, the feeling of groundedness appears to be a familiar one among lay consumers. For example, we might feel grounded when returning to our birthplace, sitting at our grandparents' kitchen table while enjoying a pie made with apples from their backyard tree and according to a recipe passed down for generations. Similarly, we may have experienced feeling grounded when shopping at the local farmers market or foraging a basket of mushrooms from a nearby forest.We argue that there are at least three conceptually separable (but in practice often intertwined) sources of feelings of groundedness: connectedness to place, people, and/or past. Collectively, connections to place, people, and past engender feelings of groundedness by ""rooting"" us in our physical, social, and historic sphere. These connections may be established through many different objects, activities, and types of interactions. In this article, we focus on the role of products in providing customers with a connection to place, people, and past.Indeed, numerous marketplace examples illustrate increasing consumer demand for products that presumably make them feel more connected and thus grounded: Spearheading a renaissance of artisan, indie, and craft production, for example, locally rooted (micro)breweries have gained substantial market share in recent years. In 2019, craft beer accounted for 13.6% of total beer volume sales—a number that had increased by 4% even as overall U.S. beer volume sales had decreased by 2% ([ 6]). Similarly, sales estimates of local food increased from US$6.1 billion in 2012 to US$8.7 billion in 2015 ([24]; [45]) and farmers markets—which afford a connection to the land and to the people behind the food—are on the rise. In 2014, there were 8,268 farmers markets across the United States: a growth of 180% since 2006 ([24]). Beyond the food industry, online marketplaces such as Etsy connect consumers to handcrafted products and to the craftspeople that sell them. Impressively, Etsy reported 81.9 million users and US$10.3 billion gross merchandise sales worldwide in 2020 ([10]).This trend in demand for local, personal, and traditional products is surprising when considered against the backdrop of globalization, digitization, and modern society's penchant for technology and innovation. Marketers have begun to capitalize on these shifts in demand—for example, by stocking and promoting local products, encouraging contact with the people who make the products, and highlighting traditional ingredients or production methods. We have recently also observed marketers referring to the concept of groundedness. The Austrian grocery chain BILLA ran a national advertising campaign in fall 2020 referring to the farmers behind their products as ""The people who make us grounded"" (""Wer uns erdet"").In light of these trends, we contend that products can metaphorically connect us to place, people, and past, and thereby make us feel grounded. For brevity, we hereinafter refer to products that can make consumers feel grounded as ""grounding products."" We argue that the ability of products to provide a feeling of groundedness will make them more attractive to consumers. We further propose that feeling grounded may contribute to consumer well-being. Groundedness—understood as a feeling of deep-rootedness, having a strong foundation, and being securely anchored—gives consumers feelings of safety, strength, and stability as well as confidence that they can withstand adversity. As such, feelings of groundedness might provide consumers with a sense of happiness, thus adding to their overall well-being.This work makes several contributions. First, it introduces the feeling of groundedness as a driver of consumer behavior and consumer welfare. Second, it provides an overarching theoretical explanation for a variety of major consumer trends, such as the desire for local, craft, and traditional products. Third, it highlights that consumers experience a feeling of groundedness when products connect consumers to their physical (place), social (people), and historic (past) environment. Fourth, the studies offer various actionable marketing implications for products aimed at helping consumers connect to place, people, and past. Groundedness Groundedness in the LiteratureAs a personal characteristic, to be ""grounded"" is a common concept in everyday parlance, easily found in any dictionary. In contrast to everyday parlance, we found groundedness to be a fairly novel and underresearched construct in the literature. There are few direct references to groundedness in the marketing, consumer behavior, or social psychology literature streams. The mentions we did find in other literature (e.g., psychotherapy, environmental or educational psychology) are relatively obscure, only loosely related, or speculative (for an overview of relevant research, see Web Appendix A). For example, educational psychologist [29] writes about ""rootedness"" and develops a measure of rootedness for college students. However, McAndrew's explanation of rootedness is limited to location. Similarly, environmental psychologists (e.g., [27]; [33]) have studied connectedness to nature, which is also a more limited construct. We found a more closely related conception of groundedness in a psychotherapy doctoral dissertation, where [31], pp. 82–83) describes rootedness in terms of ""the personal, social, environmental, and economic anchoring that sees us through tough times. Within rootedness, there is a sense of togetherness, a combination of personal identity and group identity, past and present, and people and places.""In philosophy, [47], p. 43) points to the importance of being rooted. She notes:A human being has roots by virtue of his real, active, and natural participation in the life of a community, which preserves in living shape certain particular treasures of the past and certain particular expectations of the future. This participation is a natural one, in the sense that it is automatically brought about by place, conditions of birth, profession, and social surroundings.[12] likewise writes about rootedness in terms of the need to establish roots and feel at home in the world, while [41] refers to a connection to the land as a source of well-being that is undermined by technological forces that separate people from their roots in nature.In marketing, [42] examine rootedness in the context of community-supported agriculture (CSA), arguing that by connecting consumers to the land and producers, CSA membership may help consumers reconnect to their ""material, historical, and spiritual roots"" (p. 141). [ 2] also touch on some of the elements, antecedents, and consequences of groundedness, such as community and traditions.In summary, we believe the idea of groundedness has not been formally developed as a concept, nor have the full scope of the construct and its implications for consumer behavior and marketing been identified. We aim to fill this gap in the literature. The Construct of GroundednessWe argue that many consumers have a need to feel grounded, which we define as a feeling of emotional rootedness. The feeling of groundedness results from being metaphorically embedded in one's physical, social, and historical environment. Like the roots of a tree or the foundation of a house, a feeling of groundedness connects a person to their ""terroir"" (where the French word terroir not only refers to the land per se but also includes its cultural history and human capital [[35]]). Consistent with relevant dictionary definitions—which include being mentally and emotionally stable or firmly established[ 5]—we argue that the feeling of groundedness provides a solid foundation that imparts a sense of strength, safety, stability, and confidence that one can withstand adversity. Connection to placeConsistent with the idea of ""spreading one's roots into the ground,"" and the literal translation of terroir as ""land"" or ""soil"" ([35]), the feeling of groundedness can be obtained from a connection to a physical environment or place. This connection can be physical in the literal sense, as when working with actual, tangible objects that originate in the local environment, or when immersing in the natural environment itself. We find examples of such immersion in, and connection to, the natural sphere in the East Asian tradition of shinrin yoku, or forest bathing ([19]), and the Nordic cultures' idea of outdoor life (Friluftsliv), which, according to [15], p. 3), provides ""a biological, social, aesthetic, spiritual and philosophical experience of closeness to a place, the landscape, and the more-than-human world; an experience most urban people today lack."" In the same vein, connection to place may be experienced when directly drawing from the earth, as popularly pursued in urban gardening and farming. Indeed, one of [42], pp. 140–41) informants states, ""That's what farming actually is [a connection to the earth].... You are working with the living world. It's the connection you give people to the farm."" In addition to a physical connection, consumers can also connect to place in a more symbolic sense. They may do so, for example, by consuming locally produced goods, such as a beer from a nearby brewery. Establishing a connection to one's place to feel grounded may have become especially important as a consequence of migration and mobility. For example, a consumer who has recently been relocated to a certain town may particularly desire to consume products local to that town, thus enabling them to build a connection to that place. Connection to peopleFeelings of groundedness can also arise from a connection to one's social environment. Just as the meaning of terroir also includes its human capital ([35]), the idea of a ""place"" that provides groundedness, such as home, is often strongly shaped by the people and community associated with that place.In the social psychology literature, the human need for connectedness or belongingness to other people ([ 4]) has been well established. Running counter to that need is the phenomenon of modern-day alienation ([26]). The concept has been revived by marketing scholars to describe alienation of the consumer from the marketplace ([ 1]), and from a product's producer ([46]). Along the same lines, [ 2] observe postmodern consumers' feelings of personal meaninglessness and loss of moorings brought on by globalization and technology, while stressing the importance of identity, home, and community as antidotes to these feelings.Although the strongest route to groundedness via people might be connecting to one's closest social surroundings (e.g., one's family), we also see customers trying to reestablish a connection to people by means of certain product choices. Both online and offline, consumers may obtain groundedness by buying directly from the producer. At a farmers market, consumers may buy eggs directly from the person who fed the chickens and collected their eggs. On Etsy, online shoppers can order a breakfast mug from the very person who designed and shaped the piece with their own hands; the shopper might even be able to communicate directly with that person and learn how they developed their passion for handicraft. Either way, this enables the customer to get ""closer to the creator"" ([39]). On the business side, many firms, big and small, try to facilitate connections between customers and the people behind their products: for example, featuring individual producers on the packaging, indicating the name and address of food suppliers, or communicating via the company's founder or chief executive officer ([14]). Connection to pastThe human environment, or terroir, also includes a historical dimension ([35]). We suggest that feelings of groundedness can also be experienced based on a connection to the past. The past provides a foundation of memories, traditions, and cultural values for individuals to be grounded in.Examples from the marketing literature illustrate how consumption behavior establishes a connection to the past and begets feelings of groundedness. In [42], some respondent quotes suggest that community-supported farms provide not merely a connection with their local physical environment and the people around them but also a symbolic connection to past generations within one's own family (e.g., a connection to ancestors who were farmers). [44], who investigated Nordic consumers' food consumption motives, state that ""in the end, it is the caring food-producer who can bring the ubiquitous brand consumption back to where we were before industrialism"" (p. 230). Similarly, [ 3] find that visits to local farmers markets allow consumers to ""reconnect with their agrarian roots"" (p. 567), searching for ""food that is embedded in their personal and shared social histories"" (p. 564). In the consumer product domain, we see a resurgence of historic brands such as Converse ([23]) and observe companies helping consumers get connected to, or grounded in, the past. For example, firms may purposefully manufacture according to traditional and artisanal methods, such as making things by hand ([13]), or return to using older, often more ""natural"" materials and ingredients.Building on this conceptualization, our first prediction is as follows: H1:  Products that connect consumers to place, people, and past provide consumers with the feeling of groundedness. How Groundedness Is Distinct from Related ConstructsProducts that connect consumers to place, people, and past frequently differ from other products in more aspects than their affordance of feelings of groundedness. For example, a local, traditional product is probably also more authentic ([32]; [34]). Likewise, products that connect to place, people, and past could be deemed higher quality or costlier to produce. They may be more unique ([25]), or perceived as made with love ([13]). Consumers may feel a stronger brand attachment to such products ([43]). These products may also provide a greater sense of human contact ([38]), brand experience ([ 5]), brand community (e.g., [28]), and sense of nostalgia ([ 9]). Products that provide a feeling of groundedness may also evoke a feeling of being true to oneself (i.e., self-authenticity or existential authenticity; e.g., [ 2]; [16]), a feeling of knowing who one is (self-identity), a general sense of belonging ([ 4]) that is not about feeling grounded and deep-rooted, or a general sense of meaning in life ([21]; [37]; [40])—all of which could increase one's well-being.While these related constructs are relevant, we argue that they play different conceptual roles than groundedness. First, some constructs—such as product authenticity, product quality, or product uniqueness—are characteristics of products. They logically cannot cast doubt about the existence of groundedness, which is a feeling about the self.Second, other alternative constructs could be classified not as characteristics of brands but as feelings about brands. For example, brand attachment is a feeling of connection to a brand. In some situations, feeling connected to a brand might be a consequence of a brand's relationship to a place, people, or the past that a consumer longs to feel a connection with. For example, a consumer may be more likely to feel attached to a wine brand from their own region (or to their favorite laptop brand, which may have nothing to do with feeling connected to place, people, or past). However, this feeling of brand attachment is not a feeling about the self. Thus, it cannot be the same as the feeling of groundedness.A third category of constructs relates to connectedness but is focused on only one of the three sources. For example, nostalgia, as ""a sentimental longing or wistful affection for a period in the past,""[ 6] is related to the past but not necessarily people or place. Likewise, these constructs might be alternative explanations for one of the antecedents of groundedness but not groundedness itself. In addition, nostalgia describes a state of longing or affection, but it does not stipulate that this longing has been satisfied by an actual connection to the past. Thus, nostalgia is conceptually more closely related to the need for groundedness than to actually feeling grounded.Finally, there are some constructs involving feelings about the self that might be driven by similar antecedents or generate similar consequences as the feeling of groundedness; these include feeling true to oneself (i.e., self-authenticity), a sense of belonging that does not involve a feeling of deep-rootedness, a sense of self-identity, and a general sense of meaning in life. Our studies will assess these alternative constructs to groundedness. FrameworkFigure 1 depicts our conceptual framework. At the core of this framework and as summarized in H1 is that there are at least three immediate sources of groundedness: connection to the physical environment, or to place; connection to the social environment, or to people; and connection to the historical environment, or to the past.Figure 1 further depicts our hypotheses about the consequences of the feeling of groundedness; in particular, we consider product attractiveness (H2) and consumer well-being (H3) as important outcome variables. We then examine ways in which marketers can leverage groundedness on the basis of marketing-mix elements (H4) and consumer characteristics (H5).Graph: Figure 1. Conceptual framework. How Groundedness Affects Consumer ChoiceIn our predictions about downstream effects of groundedness, we hypothesize that groundedness increases product attractiveness and, thus, affects consumer choice. In particular, we suggest that products providing a connection to place, people, and past beget feelings of groundedness for the customer and may therefore be more attractive than their competitors that do not. We thus predict that customers will prefer these products and have stronger intent to purchase and higher willingness to pay (WTP). More formally, H2:  Products' ability to provide consumers with the feeling of groundedness makes those products more attractive to consumers. How Groundedness Affects Consumer Well-BeingBeyond marketplace outcomes, we hypothesize in our predictions that groundedness increases consumer well-being. In particular, we suggest that feeling grounded provides consumers with a sense of strength, stability, safety, and confidence in one's ability to withstand adversity. As such, feelings of groundedness might provide consumers with a sense of happiness, thus adding to their well-being. We find conceptual support for these predictions in the descriptions of [31] and [42]. [31], p. 82) refers to rootedness as providing ""a sense of balance, belonging, and fitting to one's place."" Further specifying the elements of well-being afforded by groundedness, Ndi (p. 59) says that rootedness is ""the ultimate feeling that provides stability, harmony, and happiness among people and their community,"" whereas a lack of rootedness leaves a person with a sense of meaninglessness, disconnectedness, emptiness, vulnerability, and unhappiness. Building on [41] work in biodynamics, [42], p. 140) also suggest that emotional connections to one's environment ""are a primordial source of spiritual sustenance and a foundation of social and personal well-being and, conversely, that psychological and societal unrest are precipitated by technological forces that separate humanity from its roots in nature."" Research on constructs related to groundedness also provides indirect, suggestive evidence for our proposition that groundedness increases consumers' well-being. [27], for example, find that connectedness to nature is positively correlated with subjective well-being. We predict the following: H3:  The feeling of groundedness increases consumers' subjective well-being. How Marketers Can Leverage Groundedness Marketing-mix strategiesMarketers can use several marketing-mix variables that help connect consumers to place, people, and past and thus make them feel grounded. Marketers can promote the location where the product is made or ingredients are sourced, engage in storytelling about the history of the brand, or introduce the people who produce the products ([14]; [46]). Marketers can design products in a local or traditional style; use local, ethnic, or traditional ingredients; or employ traditional production processes (e.g., in ""indie"" products). Marketers can also adjust their channels of distribution to help customers connect to place, people, and past. For example, farms and small producers can use farmers markets (vs. supermarkets) that connect consumers with place, people, and past. Retailers can employ traditional store designs or focus their assortments on more traditional products. We propose the following: H4:  Marketing-mix variables such as communication, product design, and channels of distribution can be designed to increase the feeling of groundedness. Consumer segmentation strategiesWe expect that consumers differ in how important feelings of groundedness are to them. That is, the level of need for connection with place, people, and past, and thus, for groundedness, varies across consumers. We examine three reasons why the need for groundedness might be heightened in certain consumer segments. First, the need for groundedness should be particularly strong when consumers' life and work make it difficult to establish and maintain strong connections with place, people, and past. We suggest that living in large cities (which are often inhabited by people who did not grow up there, are characterized by social anonymity, and tend to showcase modernity) is a predictor of need for groundedness. With regard to work, we expect that performing mostly computerized work, confined to the limits of one's desktop, puts a distance between individuals and other people as well as the physical environment. We consequently argue that computerized work is associated with a stronger need for groundedness.Second, we propose that the need for groundedness is stronger when consumers' foundations are shaken or connections with place, people, and past are severed or under pressure. We expect this to have been the case, for example, during the COVID-19 pandemic, a global event that indeed disrupted many people's lives. Accordingly, those who the pandemic had more strongly put in a state of flux should have experienced a higher need for groundedness.Third, we suggest that the need for groundedness will be more prominent for consumers whose more basic needs are satisfied. Respective proxies such as consumers' socioeconomic status (SES) should thus be correlated with their felt need for groundedness. We predict the following: H5:  The feeling of groundedness is more important to consumers when their work and life do not provide a strong connection to place, people, and past; when life events shake their foundation; or when their basic needs are already sufficiently met. Overview of StudiesWith a view to robustness and generalizability, we test our predictions in eight experiments and one consumer survey, based on a variety of samples and data collection techniques (students in behavioral labs at universities, online platforms, and professional market research panels, both in the United States and in Europe). For managerial usability, our study paradigms include both consequential outcome measures as well as marketing-relevant factors that can be manipulated or measured. Study 1 provides evidence that groundedness increases product attractiveness in real economic terms using an incentive-compatible measure of WTP. Studies 2a–c show that groundedness has explanatory value above and beyond alternative constructs. These studies also explore how a product's affordance of groundedness depends on the closeness of the consumer's connection to the provenance of the product or the producer of the product. Studies 3 and 4 provide concrete implications for marketing practice by manipulating product design and assortment, showing how demand for traditional versus innovative products is affected by consumers' current need for groundedness, and exploring proxies that might allow managers to assess said need. In Studies 5a and 5b we focus on psychological effects on consumers. Study 5a shows that groundedness has a positive effect on consumer happiness, whereas Study 5b examines the effect of a grounding product on one's feelings of strength, stability, and safety. Study 1: Groundedness and Product AttractivenessStudy 1 tests the effect of groundedness on product attractiveness (H2). We do so in a study paradigm that aims to showcase the managerial relevance of the focal effect. Specifically, we exposed participants of a consumer panel to a more grounding ""indie"" brand of soap versus a less grounding industrial brand and took an incentive-compatible measure of participants' WTP for each product. We separately tested the extent to which the two brands provide a connection to place, people, and past (see Web Appendix B). We also measured a moderator—importance of the product category to the consumer—to provide further insight into the process and strengthen internal validity (e.g., to alleviate any concerns about demand effects). We reasoned that the self-related benefit of groundedness afforded by indie (vs. industrial) brands should be more pronounced when the product category is more central to the self (i.e., more important to the consumer). MethodAn age- and gender-representative sample of 311 Austrian consumers from a professional market research panel participated for monetary compensation (Mage = 41.8 years; 50.2% female; for instructions and stimuli of this and all following studies, see Web Appendices B–F). All participants were exposed to a color picture and verbal description for two bars of soap. An almond-scented soap made by Firm A was always presented on the left. An olive-scented soap from Firm B was always presented on the right. We manipulated which firm was described as indie (""makes high-quality products that are produced in a small and independent craft business"") versus industrial (""makes high-quality products that are industrially produced at scale in a large factory"").[ 7] Participants indicated their WTP for a bar of soap from both companies separately using an incentive-compatible elicitation method (dual-lottery Becker–DeGroot–Marschak procedure; e.g., [13]). This method provides an incentive-compatible measure of what the product is worth to participants.Next, participants indicated which soap provided relatively stronger feelings of groundedness by rating agreement with the following two statements (translated from the original German): ""When I think of this firm's soap ... I feel deep-rooted and firmly anchored ('grounded')"" and ""I can firmly feel my feet on the ground."" Participants also indicated how well a graphic depicting a human form with branches for arms and a deep, wide root system instead of legs (see Figure 1) represented their emotional state. The three items were measured on a seven-point scale (1 = ""true for Firm B,"" and 7 = ""true for Firm A"") and were averaged to create a groundedness index (α = .87).[ 8] We captured the importance of the underlying product category to the consumer with a three-item measure (e.g., ""The product category 'soap' is very important to me""). All measurement items used in this and subsequent studies, as well as their reliability statistics, are listed in Web Appendices B–F. Unless indicated differently, items are measured on seven-point scales (where ""strongly disagree/does not describe my feelings at all/not true of me at all/true for Brand B,"" etc. is coded as 1, and ""strongly agree/describes my feelings very well/very true of me/true for Brand A,"" etc. is coded as 7). Results and DiscussionWe ran a repeated-measures analysis of variance (ANOVA) with consumers' WTP in euros as the repeated-measures factor and our indie versus industrial counterbalancing manipulation as the between-subjects factor (for complete results, see Web Appendix B). We find the expected interaction effect (F( 1, 309) = 174.51, p < .001). Follow-up contrast analyses show that participants are willing to pay more for the soap of Firm A if that product is portrayed as an indie (Mindie = €3.29) versus as an industrial (Mindustrial = €1.91; F( 1, 309) = 37.47, p < .001) brand. Likewise, the soap of Firm B is valued more when Firm B is described as an indie (vs. industrial) company (Mindie = €3.12, Mindustrial = €2.11; F( 1, 309) = 20.67, p < .001)—a notable 60% increase in value. For moderation and mediation analyses, we calculated the intraindividual delta WTP (WTPFirm A − WTPFirm B: MFirm A indie = €1.18, MFirm A industrial = −€1.21; F( 1, 309) = 174.51, p < .001).An ANOVA on the groundedness measure indicates a significant effect: when Firm A is described as indie, participants more strongly declare that Firm A makes them feel grounded (MFirm A indie = 5.15) compared with when Firm A is described as industrial (MFirm A industrial = 2.92; F( 1, 309) = 269.58, p < .001). Mediation analysis ([20], Model 4, 10,000 bootstrap samples) shows that the WTP effect is mediated by feelings of groundedness (indirect effect = 1.24, 95% confidence interval [CI95%]: [.87, 1.67]). A moderation analysis ([20], Model 1) with the delta WTP measure as dependent variable confirms the hypothesis that the indie premium increases as the category importance increases (p < .001; for details, see Web Appendix B). Finally, a moderated mediation analysis ([20], Model 8) shows that this interaction effect is mediated by groundedness: the indirect effect of indie versus industrial on delta WTP through feelings of groundedness is always significant but stronger at high versus low levels of category importance (indirect effect16th percentile = .79, CI95%: [.51, 1.12]; indirect effect50th percentile = 1.13, CI95%: [.77, 1.55]; indirect effect84th percentile = 1.54, CI95%: [1.04, 2.16]; index of moderated mediation = .21, CI95%: [.11,.34]).Study 1 finds that products making a connection to the past, to people, and to a place make consumers feel more grounded, which increases their WTP. Thus, the result in Study 1 supports H2. The effect is managerially relevant: the more grounding product yielded a notable 60% increase in WTP. In addition, Study 1 shows that the effect is moderated by the importance of the product category. The pattern of moderated mediation, where the indie versus industrial nature of the brand is less important to feelings of groundedness when the product is less important to the consumer's identity, provides further evidence for our process.A limitation of Study 1 is that indie versus industrial products may differ in more aspects than their ability to provide a feeling of groundedness. For example, an indie brand might provide higher value to consumers by being perceived as more authentic ([32]) and more unique ([25]) than an industrial brand. Further, the description of the indie brand and its production method might give consumers a greater sense of love ([13]), human contact ([38]), attachment ([43]), brand experience ([ 5]), and brand community (e.g., [28]). Or the indie brand might simply be higher quality and costlier to produce. Our mediation and moderated mediation provide initial evidence for the proposed groundedness process, suggesting that these alternative processes are not the only drivers of the effects on WTP. We explicitly address these alternative explanations in Study 2. Study 2: Connectedness to Place or People, Groundedness, and Product AttractivenessOne major element of our theory is that the feeling of groundedness afforded by a product results from the connection that product provides to place, people, and past (H1). If products are indeed connectors between customers and their place, people, and past, we should be able to affect groundedness—and product attractiveness (H2)—not just by manipulating the place, people, or past of the product as we did in Study 1 but also by manipulating the place, people, or past of the customer. Thus, in Study 2a, we keep brands and products constant and manipulate how much groundedness a brand is able to provide as a function of a customer characteristic (i.e., customer location), rather than a product characteristic. Study 2a MethodWe asked 172 students (Mage = 21.9 years; 79.7% female) at a Northeastern U.S. university (n = 89, for a gift voucher and cookies) and an Austrian university (n = 83, for course credit) to imagine that they had just moved to either Karlstad or Umeå in Sweden. We then asked them to choose (using a three-item measure, e.g., ""Which of the two craft beers do you choose?"") which of two real Swedish craft beer brands, Good Guys Brew from Karlstad and Beer Studio from Umeå, they would purchase on their first night out. Next, participants reported which of the two brands they perceived would make them feel more grounded (""In the situation described, this brand would make me feel deep-rooted,"" ""This brand would make me feel well-grounded,"" and ""In a metaphorical sense: Which of the two craft beers would rather make you feel as illustrated by the following picture?"" [showing the picture of a human/tree form with deep roots]; α = .90). All items in this study were captured on seven-point scales where one anchor was the beer from Karlstad and the other anchor the beer from Umeå. We counterbalanced which beer was shown on the left- versus right-hand side. Before the participant location manipulation, we also asked participants to rate the two brands on a relative scale regarding nine product characteristics that might make either product more attractive. Because these were product characteristics that should not have been influenced by the participant's location, and because they were measured before the location manipulation, they did not—and could not—explain our results (for results regarding the control variables in this and all subsequent studies, see Web Appendices C–F). At the end of the study, we captured some information about the participants' relation to beer and to Sweden (e.g., ""Have you ever been to Sweden?,"" ""How much do you like beer in general?"").[ 9] Results and discussionA one-way ANOVA shows that participants who moved to Karlstad prefer the Karlstad-based beer significantly more than those who moved to Umeå (MKarlstad = 4.80, MUmeå = 4.14; F( 1, 170) = 6.70, p = .010). Similarly, the Karlstad-based beer provides relatively more groundedness to participants who moved to Karlstad versus Umeå (MKarlstad = 4.29, MUmeå = 3.79; F( 1, 170) = 5.77, p = .017). Groundedness mediates the effect of residence location on preference (indirect effect = .40, CI95%: [.07,.74]; [20], Model 4). For each of the nine alternative constructs, the focal indirect effect via groundedness remains significant when we include the alternative construct as a rival mediator.Study 2a shows that groundedness drives product attractiveness (H2) when we keep products constant but manipulate the place of the customer. This study highlights that the groundedness effect depends not only on the features of the product but also on the situation of the customer. Managerially, the study shows that local brands are particularly grounding and thus attractive to local consumers. Study 2a manipulated how participants relate to a place that is connected to a focal product, and thus how much groundedness it affords them. Unlike Study 2a, Study 2b capitalizes on participants' existing relationship to a place. Study 2b further addresses alternative constructs to groundedness by measuring them after the focal manipulation. Study 2b MethodThe week before Christmas, we asked 1,306 Austrian students from a university in Vienna (Mage = 22.8 years; 55.4% female; compensated by a lottery for an iPhone 11 and five €10 gift vouchers, prescreened for having grown up in Austria but outside Vienna and for celebrating Christmas) to imagine they were celebrating Christmas in Vienna this year and looking to buy a Christmas tree at a local market. We then varied between-subjects whether the market's Christmas trees originated from the state the participant grew up in or from a randomly selected other Austrian state. The trees were thus not connected to participants' current place (i.e., where they were studying and buying the tree) but to either the place where they grew up or a third location in Austria. Then, we assessed purchase intent for the Christmas tree using four items (e.g., ""I would very much like to buy a Christmas tree at this market""). We next captured feelings of groundedness from purchasing a Christmas tree at that market, using the same three items as in Study 2a. Finally, participants completed two-item measures of alternative constructs (the product's authenticity, uniqueness, quality, love, production costs, sense of human contact, brand experience, feeling of belonging to a brand community, and attachment). In addition, we measured participants' desire to support the producer as a possible alternative explanation. Due to this study's use of multiple items for each construct, we were able to ascertain that groundedness is empirically distinct from the other constructs captured (purchase intent and alternative constructs) using the [11] criterion. We performed the same tests in all subsequent studies with multi-item measures of our dependent variables (see Web Appendices C–F). Results and discussionParticipants are more intent on buying a Christmas tree from the focal market if it is from their own state (Mown place = 5.35) versus another state in the same country (Mother place = 4.95; F( 1, 1,304) = 24.27, p < .001). Further, when the trees originate from participants' own state, participants experience stronger feelings of groundedness than when the trees are from another state (Mown place = 3.39, Mother place = 3.15; F( 1, 1,304) = 8.43, p = .004), which is in line with H1. We do not find significant differences between conditions with regard to the alternative explanations captured (ps >.087). Differences in perceived production costs (Mown place = 4.17, Mother place = 4.30; F( 1, 1,304) = 2.92, p = .088) are marginally significant but run in the opposite direction of the dependent variable. Thus, they are unable to explain our results. Consistent with H2, a mediation model ([20], Model 4) shows that groundedness mediates the treatment effect on purchase intent (indirect effect = .11, CI95% [.03,.18]). For each of the ten alternative constructs, the focal indirect effect via groundedness remains significant when we include the alternative construct as a rival mediator.Studies 2a and 2b show that a product that connects a consumer to a place they relate to (a city they move to, the state they are from) makes them feel more grounded and is more attractive than a product originating from a specified place they do not relate to (another city or state in the same country). One pertinent question is how much that feeling of groundedness depends on the closeness of the connection to place, people, and past. While the more grounding option in Studies 2a and 2b connects customers to their own (""my"") current or past place, the indie brand utilized in Study 1 merely provided a connection to ""a"" place (and ""the"" people who made it and ""the"" past, respectively). Our view is that, ceteris paribus, the depth of groundedness gradually increases with the closeness of the connection. The closer the personal relationship of the customer to the place, people, and past represented by the product, the stronger the connection and thus feelings of groundedness established via the product. We test this prediction in the context of a customer connecting to the people dimension next. Study 2cStudy 2c addresses whether differences in closeness indeed matter—that is, whether they afford different levels of feelings of groundedness when compared directly. Beyond that, the study isolates connection to people as a potential driver of groundedness (H1). MethodTwo hundred U.K. crowd workers on Prolific (Mage = 33.8 years; 55.0% female; for monetary compensation) were asked to indicate their feelings of groundedness associated with the use of a coffee mug (using the same measure as in Studies 2a and 2b). To sample different levels of personal closeness along the proposed continuum, the producer of the mug was manipulated to be either ""an artisan that is personally close to you (e.g., a close friend, relative, partner, etc.)"" or ""an artisan that is a distant acquaintance of yours (e.g., a colleague from work, a neighbor, a friend of a friend, etc.)."" We measured perceived connection to people through the mug using three items (e.g., ""Drinking from this mug, I somehow feel a connection to 'my people'""). We used the same control measures as in Study 2b (except for the motivation to provide financial support by purchasing a product, given that there was no purchase in this study). Results and discussionFirst, the pattern of results for groundedness and connection to people supports our theorizing about a continuum of closeness and, thus, groundedness: perceived connection to people is significantly higher when the artisan producer is a close other versus when they are merely an acquaintance (Mclose = 4.34, Mdistant = 3.75; F( 1, 198) = 6.63, p = .011). The same is true for feelings of groundedness: participants experience stronger feelings of groundedness when considering the coffee mug produced by an artisan that is a close other versus one that is merely a distant acquaintance (Mclose = 4.14, Mdistant = 3.29; F( 1, 198) = 14.78, p < .001). Further, a mediation model ([20], Model 4) shows that producer closeness mediates the effect on groundedness (indirect effect = .42, CI95%: [.09,.75]). Importantly, for each of the nine alternative constructs, the focal indirect effect via groundedness remains significant when we include the alternative construct as a rival mediator.Thus, Study 2c shows that being personally closer to one of the sources of groundedness enables consumers to experience stronger feelings of groundedness. More precisely, groundedness is a function of how close the consumer's relationship is to the product's place, people (e.g., the product's producer), or past. As for different routes to groundedness, the study shows that a product's people dimension alone (e.g., its producer) can boost groundedness via a stronger perceived connection to people established by the product. Managerially, the findings are important because marketers can choose the extent to which they highlight the closeness or similarity between customers and producers. In addition, the study highlights that managers may need to search for personally relevant and close sources of groundedness from the perspective of a given target customer.The next set of studies investigates how the groundedness effect can be leveraged via marketing-mix elements (Studies 3a and 3b) and which types of customers have a particularly high need for groundedness (Studies 3a and 4). Study 3: Marketing Mix, Connectedness to Past, Groundedness, and Product AttractivenessStudy 3a focuses on connections to past as a source of groundedness (H1) by manipulating product design (H4). We also examine how the effect of groundedness on product attractiveness (H2) varies across consumers by capturing their chronic need to connect to the past (the higher this need, the stronger the groundedness effect should become). Study 3b manipulates consumers' state need for groundedness and addresses category management considerations by testing how consumers' need for groundedness impacts the preference for traditional versus innovative products. Study 3a MethodWe showed 223 students in the behavioral laboratory of a large European university (Mage = 23.9 years; 65.5% female; for monetary compensation or course credit) two sets of cutlery (from Brand A and Brand B) side by side, stipulating that they were of comparable price and quality. We manipulated product design to provide more versus less connection to the past by using a more traditional versus modern product design. We manipulated which set of cutlery was presented on the left- versus right-hand side (i.e., as Brand A vs. B). Using adapted versions of the measures in Studies 1 and 2, we asked participants to indicate which of the two brands they would rather purchase, which would make them feel more grounded, and which evoked a stronger connection to the past. Need to connect to the past as a chronic consumer trait—our moderator—was measured in terms of agreement with three items (e.g., ""I generally try to see if I can somehow satisfy my desire to [metaphorically] 'connect to the past'"").[10] Results and discussionOur manipulation proved effective: Participants more strongly associate Brand A ( = 7, Brand B = 1) with a connection to the past when Brand A cutlery had a traditional design (MBrand_A_traditional = 5.49, MBrand_A_modern = 1.97; F( 1, 221) = 405.25, p < .001). As expected (H4), we find a significant effect on groundedness—Brand A is perceived to provide more groundedness (relative to Brand B) when Brand A features traditional design (MBrand_A_traditional = 4.39, MBrand_A_modern = 3.65; F( 1, 221) = 18.50, p < .001). For product preference, we find an overall preference for the modern cutlery (MBrand_A_traditional = 3.69, MBrand_A_modern = 4.48; F( 1, 221) = 9.01, p = .003; of course, the fact that traditional products provide a stronger sense of groundedness does not preclude that many people might still prefer a specific set of modern cutlery over a specific set of traditional cutlery, or modern designs over traditional ones in general). More importantly, and as expected (H2), we find a positive effect of groundedness on product preference (b = .61, p < .001), and a positive indirect effect ([20], Model 4) of traditional (vs. modern) design on preference through groundedness (indirect effect = .55, CI95%: [.27,.89]). As one would expect, preference becomes even stronger for the modern cutlery when the groundedness path is controlled for (estimated MBrand_A_traditional = 3.40, estimated MBrand_A_Modern = 4.74).As anticipated, we find that one's general need to connect to the past significantly moderates purchase preference (p < .001; [20], Model 1). Thus, participants with a low need to connect to the past have a more pronounced preference for the modern cutlery; conversely, participants with a high need to connect to the past show a preference for the traditional cutlery (e.g., at need to connect to past = 1, conditional effect = −2.53, CI95%: [−3.58, −1.48]; at need to connect to the past = 7, conditional effect = 1.29, CI95%: [.002, 2.57]). A moderated mediation analysis ([20], Model 58; see Web Appendix D) shows that traditional design affords a stronger feeling of groundedness, and that groundedness becomes a more important driver of preference as general need to connect to the past increases. In fact, at very low levels of general need to connect to the past, a product's ability to provide feelings of groundedness no longer significantly impacts product preference (e.g., at need to connect to the past = 1, conditional effect = .33, CI95%: [−.05,.71]).In summary, Study 3a shows that by varying a marketing-mix element (product design) to be more traditional (vs. modern), marketers can affect customer preference via feelings of groundedness. This is because the marketing-mix element directly caters to a source of groundedness (H4). Study 3bStudy 3b investigates preference for traditional versus innovative products as a direct function of consumers' current need for groundedness and manipulates this need. We also perform a test of how the relative interest in different product categories—traditional versus innovative—is affected by different levels of need for groundedness, pointing to potential boundary conditions of the groundedness effect. MethodTwo hundred crowd workers on Prolific (Mage = 33.4 years; 54.0% female) from the United Kingdom took part in this study for monetary compensation. Participants filled out two ostensibly unrelated surveys. The first manipulated participants' current need for groundedness. Participants in the high-need condition read, ""Research has shown that feelings of groundedness can be positive or negative depending on the context and situation we are in."" They were then asked to describe a recent situation where feeling grounded was desirable to them because ""you metaphorically felt your roots were too loose and weak with respect to your connection to a place, to people, and the past."" Conversely, participants in the low-need condition read, ""Research has shown that feelings of groundedness can be negative or positive,"" and were asked to describe a situation where groundedness was undesirable to them because ""you metaphorically felt your roots were too dense and strong."" After completing the writing task and reporting their current need for groundedness on a version of our three-item groundedness scale, participants were thanked and told they would be forwarded to another study. Here, participants were introduced to two different online stores, presented side by side: one specializing in ""the best traditional products"" and one specializing in ""the best innovative products."" We then asked participants to indicate which of the stores they would prefer to shop at on a seven-point scale, with Store A and Store B as anchors. We alternated which of the stores (A vs. B) was presented as traditional versus innovative in our stimuli. We subsequently reversed the Store A versus B preference scores for half the data set, so that the innovative store preference was always anchored at 1 and the traditional store preference was always anchored at 7. Results and discussionOur manipulation was effective: participants who wrote about a situation where their need for groundedness was high reported experiencing a higher need for groundedness (M = 5.25) than those who wrote about a situation where need for groundedness was low (M = 4.11; F( 1, 198) = 41.19, p < .001). In terms of shopping preferences, participants in the high-need-for-groundedness condition showed a stronger preference for the online store with traditional (vs. innovative) products (M = 4.00) than those in the low-need-for-groundedness condition (M = 3.47; F( 1, 198) = 4.17, p = .043).Thus, and in line with H4, Study 3b shows that relative interest in purchasing traditional products is higher in situations and contexts where consumers' need for groundedness is high. In situations and contexts where groundedness is less sought after, innovative products become relatively more interesting. Study 4: Consumer Characteristics and Need for GroundednessStudies 3a and 3b suggest that groundedness is not equally attractive and relevant to all consumers in all situations. For segmentation purposes, it is important to know which consumers are more likely to have a strong enduring need for groundedness. As predicted in H5, we argue that the feeling of groundedness is more important to consumers when their work and life (e.g., computerized desktop work, living in a large city) do not provide a strong connection to place, people, and past; when certain life events (e.g., the COVID-19 crisis) shake their foundation; or when their basic needs are already sufficiently met (e.g., when they have higher SES). In Study 4, we use a survey to measure these consumer characteristics, along with need for groundedness and preference for products that connect to place, people, and past. The study was conducted in spring 2020, at the beginning of the COVID-19 pandemic and first lockdown. This enabled us to assess the impact of a disruptive life event on the need for groundedness. MethodAn age- and gender-representative sample from a U.S. consumer panel completed this survey for monetary compensation (N = 325; Mage = 45.5 years; 51.1% female). We first measured product preference and need for groundedness: preference for products connected to one's place, people, and past were measured (in random order) using three items each (e.g., ""I like to purchase products that connect me to 'my place' ['my people'/'my past'], i.e., my physical [social/historic] environment""). We merged these into one global index of purchase interest. Need for groundedness was measured using a version of our three-item scale, adapted to measure general need for groundedness (e.g., ""In general, I want to feel deep-rooted""). We next captured a series of demographic and lifestyle variables.To assess a potential lack of connection to people, place, and past in consumers' work and social lives, we captured three variables. First, we asked respondents about the type of area they live in (1 = ""in the countryside,"" and 7 = ""in a big city""). We hypothesized that living in large cities (which are often inhabited by people who did not grow up there, are characterized by social anonymity, and tend to showcase modernity) is a predictor of need for groundedness. Second, we assessed participants' desktop work using two items (e.g., ""During the week [e.g., when being at work] ... I primarily work at the computer""). We expected a positive relationship between desktop work and need for groundedness, because a disproportionate amount of computerized work (while confined to one's desktop) separates individuals from other people as well as the physical environment. A similar logic might apply to people whose job is characterized as ""work of the head"" (i.e., work that contains many abstract tasks), as opposed to people who perform manual labor (""work of the hands"") or work in social jobs (""work of the heart""; [17]). Respondents accordingly indicated which of these three categories their current or most recent job fell into.Next, to assess a potential link between need for groundedness and a disruptive major life event, we examined perceived impact of the COVID-19 crisis on the consumer's life. We assessed this with a single item (""Due to the current Corona [COVID-19] crisis, I feel that my life is in a state of major change""). Last, we theorized that the need for groundedness should become more prominent when basic needs such as food and shelter are not a concern. Therefore, we tested whether higher SES (measured on a three-item scale [e.g., ""I have enough money to buy things I want""]) might be an effective proxy for one's need for groundedness. No other measures were taken. Results and DiscussionFirst, and as expected, we find a significant and positive correlation between one's need for groundedness and purchase intent for products connecting to place, people, and past (r = .57, p < .001). Second, we analyzed the correlations of all proposed indicators with the need for groundedness. In particular, need for groundedness correlates positively with desktop work (r = .26, p < .001), SES (r = .30, p < .001), change experienced as a result of COVID-19 (r = .12, p = .030), and living in a big city rather than the countryside (r = .10, p = .079), but correlates negatively with performing work of the hands (r = −.11, p = .040; for a complete correlation table of this study; see Web Appendix E).Third, we ran multivariate ordinary least squares (OLS) regressions with all predictor variables on both need for groundedness and purchase intent. For those variables that emerged as significant predictors for both the need for groundedness and purchase intent, we examined whether the need for groundedness mediates the respective effects on purchase intent while entering all other variables as covariates. For conciseness, we report only significant results hereinafter (see Table 1 for details).GraphTable 1. Multivariate OLS Regression Models (Study 4). Need for GroundednessPurchase Interest in Products Connected to Place, People, and PastUnstandardized Coef.Standardized Coef.Unstandardized Coef.Standardized Coef.bSEβtp-valuebSEβtp-value(Constant)2.983***.4177.157<.001.682.461.481.140Living environment.038.034.0611.11.270.08*.038.1012.105.036Desktop work.143***.037.2483.892<.001.24***.041.3245.905<.001Work of the hands−.17.19−.061−.9.370−.228.21−.064−1.091.276(1 = hands,0 = otherwise)Work of the head−.281.172−.117−1.64.102−.699***.19−.226−3.686<.001(1 = head,0 = otherwise)Change through COVID-19.091*.04.1232.302.022.234***.044.2475.356<.001SES.185***.037.2725.008<.001.287***.041.337.038<.001Age.01*.004.1362.357.019.000.004.005.104.917Gender (1 = male, 0 = female).004.13.002.034.973.465**.144.1513.228.001R2 = .162, d.f. = 8, 316R2 = .379, d.f. = 8, 316 1 *p < .05.2 **p < .01.3 ***p < .001.4 Notes: Mediation models ([20], Model 4): Mediator = need for groundedness, DV = purchase interest; ( 1) IV = SES: indirect effect = .10, CI95%: [.05,.15]; ( 2) IV = desktop work: indirect effect = .07, CI95%: [.03,.12]; ( 3) IV = change through COVID-19: indirect effect = .05, CI95%: [.002,.10].The multivariate OLS models showed that three predictors remain significant for both the need for groundedness (NG) and purchase intent (PI) when simultaneously including all variables in the model: ( 1) desktop work (NG: b = .14, SE = .04, t(316) = 3.89, p < .001; PI: b = .24, SE = .04, t(316) = 5.91, p < .001), ( 2) SES (NG: b = .19, SE = .04, t(316) = 5.01, p < .001; PI: b = .29, SE = .04, t(316) = 7.04, p < .001), and ( 3) change related to COVID-19 (NG: b = .09, SE = .04, t(316) = 2.30, p = .022; PI: b = .23, SE = .04, t(316) = 5.36, p < .001). Need for groundedness mediates the effect of all three variables on purchase intent (in line with H2; see Table 1 and Web Appendix E).Our ""work of the head"" dummy was not significant in the multivariate OLS model. We conclude that the ""work of the head/heart/hands"" measure was probably too rough and thus unable to adequately detect the important nuances in job characteristics that affect the need for groundedness. We were also surprised that one's living environment did not emerge as a significant predictor for need for groundedness in the multivariate OLS model. A closer look at the data reveals, however, that a disproportionately large number (29.2%) of respondents in our sample indicated living in big cities (i.e., chose the endpoint of the scale). When dichotomizing the measure (i.e., living in big city vs. not), we find the predicted positive effect: people living in a big city have a heightened need for groundedness (see Web Appendix E).In summary, Study 4 finds that a higher need for groundedness is apparent in consumer profiles characterized by larger societal trends: living in big cities (urbanization), doing desktop work at the computer (digitization), and undergoing major change (such as during the COVID-19 pandemic). Further, groundedness seems to be more relevant for high-SES consumers.Thus far, we have provided a cohesive picture of groundedness in terms of both triggers (H1) and market-relevant outcomes (H2), as well as ways for marketers to leverage groundedness (H4, H5). In the final two studies, we examine the implications of groundedness for consumers' psychological well-being (H3). Study 5: Connectedness, Groundedness, and Consumer Well-BeingTo test our hypothesis that feeling grounded increases consumers' subjective well-being (H3), Study 5a measures happiness as a consequence of attaining groundedness. We also test another managerial manipulation: channel type (H4). Study 5b expands into a broader range of psychological outcomes; as outlined in our conceptual framework, the feeling of groundedness should provide consumers with a sense of strength, stability, safety, and self-confidence. We test these outcomes in the context of using locally grown ingredients and also investigate alternative constructs to groundedness, such as self-authenticity, meaning in life, or sense of identity. Study 5a MethodWe randomly assigned 190 Austrian students (Mage = 22.5 years; 50.5% female; lab-based, for monetary compensation) to think about shopping at a supermarket or local farmers market. We then asked about their feelings of groundedness; happiness; and being connected to place, people, and past. Happiness was measured using three items (e.g., ""In the situation just described, how happy would you feel?""). Feelings of groundedness were measured using our three-item measure. Connection to place, people, and past were captured separately using three items each (e.g., ""Having been in the supermarket [to the farmers market] makes me feel connected to my physical/social/historic environment""). The order of the dependent measures (happiness, groundedness), as well as the order of the item blocks capturing connection to place, people, and past, were counterbalanced. Perceived quality and price were measured as control variables. Results and discussionChannel type has a significant effect on groundedness and happiness. Participants who thought about shopping at the farmers market reported feeling significantly more grounded (Mfarmersmarket = 4.66 vs. Msupermarket = 3.80; F( 1, 188) = 18.19, p < .001) and happier (Mfarmersmarket = 5.32 vs. Msupermarket = 4.87; F( 1, 188) = 7.94, p = .005). Consistent with our theorizing (H4), shopping at the farmers market leads to significantly higher perceived connection to place (Mfarmersmarket = 4.96 vs. Msupermarket = 4.03; F( 1,188) = 15.74, p < .001), people (Mfarmersmarket = 4.58 vs. Msupermarket = 3.45; F( 1, 188) = 24.60, p < .001), and past (Mfarmersmarket = 3.73 vs. Msupermarket = 2.52; F( 1, 188) = 25.47, p < .001). We also find support for serial mediation such that the effect of channel on happiness is mediated, in series, by connection to place, people, and past, and groundedness (for mediation results, see Web Appendix F). All effects remain robust when we enter quality and price as covariates.Study 5a thus supports our prediction that the feeling of groundedness increases consumers' subjective well-being (H3) while providing converging evidence for H1. Finally, the manipulation of distribution channel (H4) offers an actionable strategy for marketers to leverage groundedness.In our last study, we employ the context of locally grown ingredients to test a broader range of psychological outcomes of groundedness. We also test the explanatory value of groundedness against alternative constructs that are self-related, such as feelings of self-authenticity or meaning in life. Study 5b MethodThree hundred four students from a major European university completed Study 5b's online study for course credit. We excluded 12 participants for failing our reading check, leaving us with a final data set of 292 participants (Mage = 22.3 years; 69.5% female). Participants were asked to think about making apple pie on a Saturday; specifically, a pie with Boskoop apples—their favorite pie-making variety. In addition, they were told that these apples were from either an orchard only 12 kilometers from their home or an orchard 1,200 kilometers from their home. Participants then completed a short survey that measured five downstream psychological outcomes of groundedness using a five-item scale: ""I feel truly safe as a person,"" ""I experience a feeling of inner strength,"" ""I feel truly stable,"" ""I have a strong feeling of basic trust and confidence in myself,"" and ""I feel that nothing can stir me up"" (α = .89). Afterward, we measured feelings of groundedness using our three-item measure. Finally, participants completed four multi-item measures intended to capture alternative explanations (self-authenticity [e.g., ""I feel out of touch with the 'real me'""], meaning in life [e.g., ""I have a good sense of what makes my life meaningful""], self-identity [e.g., ""I have the feeling that I know who I am""], feeling of belonging [e.g., ""I have a feeling of belonging""]). Results and discussionParticipants who considered making apple pie with apples grown close to home scored significantly higher in terms of experiencing the related psychological downstream consequences than those using apples grown far away (Mlocal = 5.08, Mnonlocal = 4.61; F( 1, 290) = 12.22, p = .001). Thus, the apple pie made with local products boosted participants' personal feelings of strength, safety, and stability (for effects on the individual dependent variable items, see Web Appendix F). They also reported significantly stronger feelings of groundedness (Mlocal = 4.65, Mnonlocal = 4.06; F( 1, 290) = 15.20, p < .001). A mediation model ([20], Model 4) shows that the downstream consequences are mediated by feelings of groundedness (indirect effect = .27, CI95%: [.13,.44]). Importantly, the indirect effect via feelings of groundedness on the downstream consequences holds when we add, one at a time, each of the four alternative explanations as a rival mediator.Study 5b thus confirms positive psychological downstream effects of groundedness (H3) tested in the realm of local products. Products grown closer to the consumer—that is, products that are more strongly connected to one's place—make consumers feel not only more grounded but also stronger, safer, and more stable. General DiscussionIn this research, we have provided systematic evidence that products can provide consumers with feelings of groundedness by giving them a sense of connection to place, people, and past. We do so across nine studies (eight experiments and one survey), both online and in the lab, using different populations (business students, crowd workers on Amazon Mechanical Turk and Prolific, and members of commercial, representative panels) across two continents (total N > 3,000). We have tested our theory for robustness across a variety of product domains, including both disposable and durable consumer goods (food, care products, seasonal products, and tableware), using real brands to strengthen external validity as well as highly controlled stimuli for internal validity. We have provided process evidence via mediation, moderation, and moderated mediation. Theoretical ImplicationsThis work introduces feelings of groundedness to the marketing literature by identifying these feelings as an important construct for marketing research and systematically examining it as a driver of consumer behavior. While references to groundedness and related constructs can be found in philosophy (e.g., [47]), different domains of psychology (e.g., [29]), and psychotherapy ([31]), the concept of groundedness is new to experimental research in marketing, consumer behavior, and mainstream psychology. Existing research in consumer culture theory has given passing treatment to concepts such as ""rooted connections"" ([42]) and has definitely been inspirational to this work. However, it has neither discussed nor empirically explored the full concept of groundedness with its antecedents, proxies, boundary conditions, and consequences, which we have aimed to do here.We also contribute to the growing literature on consumer well-being. [47], p. 43) proposes that ""every human being needs to have multiple roots. It is necessary for him to draw well-nigh the whole of his moral, intellectual, and spiritual life by way of the environment of which he forms a natural part."" Our work indeed shows that groundedness is related to happiness and a sense of strength, stability, and safety; thus, we propose groundedness as a novel antecedent of these outcomes.We also theorized about three sources of feelings of groundedness: connections to place, people, and past. Although the three sources are often empirically intertwined, we show that they are theoretically distinct and powerful in fueling consumers' feelings of groundedness. Our analysis further provides rich insight on the nature of these connections by showing that the extent to which products provide feelings of groundedness is a graded function of closeness. That is, a product provides stronger feelings of groundedness when the product's place, people, or past is closer to the consumer. Finally, by identifying the role of groundedness and its sources, we offer an overarching theoretical explanation for major current consumer trends, such as buying local products (connected to place), produced by people we relate to (connected to people), and according to traditional production methods (connected to the past). Marketing ImplicationsFeelings of groundedness are worthy of managers' attention because these feelings have important downstream consequences as shown across our studies. In particular, feelings of groundedness impact consumers' brand preference and WTP. In Study 1, for example, consumers were willing to pay a price premium of about 60% for the product that provided more groundedness.Our work also provides actionable implications for product and brand management: we give concrete approaches regarding how firms can elicit groundedness by showing consumers their product's connection to place, people, and past. For example, our results in Studies 1, 2a, and 2b show how presenting a product as artisanal or highlighting the local origin of a product can provide feelings of groundedness. In Studies 3a and 3b, we have shown that managers can utilize other marketing-mix elements such as product design or retail assortment and configure them (e.g., as more traditional instead of modern) to provide a stronger connection to the past. Similarly, Study 5a shows that a marketer's choice of distribution channel (e.g., farmers market) has an impact on feelings of groundedness.In terms of customer targeting, we have pointed out when and for whom groundedness is more important. In particular, we have shown that traditional (vs. innovative) products benefit from situational differences in the need for groundedness (Study 3b). On the level of individual differences, in Study 3a, only consumers with a high chronic need to connect to the past preferred the more traditional cutlery design. Our representative survey (Study 4) further showed a higher need for groundedness among consumers who are particularly affected by large global trends or major disruptive events. These global trends (e.g., digitization, urbanization) and major life events (e.g., the COVID-19 pandemic) make it harder for consumers to feel connected to people, place, and past. From a groundedness perspective, it is not surprising that during the safety- and stability-threatening COVID-19 pandemic, customers returned to the familiar grocery brands consumed with their families while growing up ([ 7]). There are probably multiple drivers for this behavior, but it is likely that consumers chose these products, at least in part, because of the connections to place, people, and past—and thus feeling of groundedness—they provide. Limitations and Future ResearchThis is the first series of experimental studies investigating feelings of groundedness. As such, many questions remain for future research. With regard to antecedents, for example, we have focused on products as means for consumers to experience feelings of groundedness. However, anecdotal evidence suggests that there are other ways for consumers to feel more connected to place, people, and past and, consequently, more grounded: for example, through services such as genealogy websites, cooking classes, lectures on local history, or yoga and meditation classes providing ""grounding"" exercises.The scope of Study 4 has allowed us to identify an initial set of indicators for who has a higher need for groundedness and why, but it is clear there will be additional consumer characteristics and lifestyle variables helpful to marketers in identifying relevant customer segments. For example, people who travel frequently for work and have little chance to connect to their current physical environment may seize opportunities to (re)-connect to place—such as through a local craft beer—to feel more grounded. Likewise, pandemics such as COVID-19 are not the only type of events that can shake a person's foundation. Stressful life events such as separation or loss, starting a new job, or moving homes may cause a higher need to feel grounded. Similarly, the need for groundedness may be subject to seasonal variations. Preliminary insights from our own qualitative explorations suggest that individuals' need for groundedness may be particularly high during the holiday season and other festive occasions, such as Christmas, Thanksgiving, Ramadan, and one's own birthday. Apart from that, interestingly, the need for groundedness appears to be higher during the colder seasons. We believe a more thorough testing of these hypotheses seems promising and would likely have important implications. If the initial signals are correct, for example, studies of scanner or panel data should reveal variations in the demand for products that connect to place, people, and past across the year.Finally, we have only begun to examine boundary conditions. For example, it seems possible that in some situations strong roots not only provide strength and stability but could also constrain movement, thus giving consumers the feeling of being ""stuck"" and unable to escape their roots. Imagine growing up on a farm, surrounded by one's family, and doing things day after day in the same way they have traditionally been done by previous generations. A person in this situation will likely feel grounded but might also feel more motivated to break free, move away, or challenge the status quo. If such is the case, too much groundedness might even backfire. Future research might thus enrich the present investigation by focusing on potential downsides of groundedness. ConclusionThis research introduced feelings of groundedness as a relevant construct for marketing research and consumer behavior. We have demonstrated its importance to marketers by documenting that it increases product attractiveness and that it can be manipulated through a variety of marketing-mix strategies and used for targeting consumer segments prone to a lack of groundedness. We also have shown that groundedness is important to consumer well-being, pointing to important consumer welfare and policy implications. We expect that the importance of this topic to consumers and marketers will only increase as digitization, urbanization, and global migration continue to challenge consumers' connections to place, people, and past.  "
5,"Connecting to Place, People, and Past: How Products Make Us Feel Grounded Consumption can provide a feeling of groundedness or being emotionally rooted. This can occur when products connect consumers to their physical (place), social (people), and historic (past) environment. The authors introduce the concept of groundedness to the literature and show that it increases consumer choice; happiness; and feelings of safety, strength, and stability. Following these consequential outcomes, the authors demonstrate how marketers can provide consumers with a feeling of groundedness through product designs, distribution channels, and marketing communications. They also show how marketers might segment the market using observable proxies for consumers' need for groundedness, such as high computer use, high socioeconomic status, or life changes brought on by the COVID-19 pandemic. Taken together, the findings show that groundedness is a powerful concept providing a comprehensive explanation for a variety of consumer trends, including the popularity of local, artisanal, and nostalgic products. It seems that in times of digitization, urbanization, and global challenges, the need to feel grounded has become particularly acute.Keywords: connectedness; alienation; need to belong; groundedness; local; rootedness; terroir; traditionalTo be rooted is perhaps the most important and least recognized need of the human soul.—[47], p. 43).Dual forces of digitization and globalization have made our social and work lives become increasingly virtual, fast-paced, and mobile, leaving many consumers feeling like trees with weak roots, at risk of being torn from the earth. In response, we observe consumers trying to (re)connect to place, people, and past—to get anchored. Against this backdrop, we propose and test an important driver of consumer behavior that has largely been overlooked in marketing literature: the feeling of groundedness.We believe that many consumers have a need to feel grounded, which we define as a feeling of emotional rootedness. This feeling emanates from connections to one's physical, social, and historic environment and provides a sense of strength, safety, and stability. Although the concept has received scant attention in prior marketing, consumer behavior, and social psychology research, the feeling of groundedness appears to be a familiar one among lay consumers. For example, we might feel grounded when returning to our birthplace, sitting at our grandparents' kitchen table while enjoying a pie made with apples from their backyard tree and according to a recipe passed down for generations. Similarly, we may have experienced feeling grounded when shopping at the local farmers market or foraging a basket of mushrooms from a nearby forest.We argue that there are at least three conceptually separable (but in practice often intertwined) sources of feelings of groundedness: connectedness to place, people, and/or past. Collectively, connections to place, people, and past engender feelings of groundedness by ""rooting"" us in our physical, social, and historic sphere. These connections may be established through many different objects, activities, and types of interactions. In this article, we focus on the role of products in providing customers with a connection to place, people, and past.Indeed, numerous marketplace examples illustrate increasing consumer demand for products that presumably make them feel more connected and thus grounded: Spearheading a renaissance of artisan, indie, and craft production, for example, locally rooted (micro)breweries have gained substantial market share in recent years. In 2019, craft beer accounted for 13.6% of total beer volume sales—a number that had increased by 4% even as overall U.S. beer volume sales had decreased by 2% ([ 6]). Similarly, sales estimates of local food increased from US$6.1 billion in 2012 to US$8.7 billion in 2015 ([24]; [45]) and farmers markets—which afford a connection to the land and to the people behind the food—are on the rise. In 2014, there were 8,268 farmers markets across the United States: a growth of 180% since 2006 ([24]). Beyond the food industry, online marketplaces such as Etsy connect consumers to handcrafted products and to the craftspeople that sell them. Impressively, Etsy reported 81.9 million users and US$10.3 billion gross merchandise sales worldwide in 2020 ([10]).This trend in demand for local, personal, and traditional products is surprising when considered against the backdrop of globalization, digitization, and modern society's penchant for technology and innovation. Marketers have begun to capitalize on these shifts in demand—for example, by stocking and promoting local products, encouraging contact with the people who make the products, and highlighting traditional ingredients or production methods. We have recently also observed marketers referring to the concept of groundedness. The Austrian grocery chain BILLA ran a national advertising campaign in fall 2020 referring to the farmers behind their products as ""The people who make us grounded"" (""Wer uns erdet"").In light of these trends, we contend that products can metaphorically connect us to place, people, and past, and thereby make us feel grounded. For brevity, we hereinafter refer to products that can make consumers feel grounded as ""grounding products."" We argue that the ability of products to provide a feeling of groundedness will make them more attractive to consumers. We further propose that feeling grounded may contribute to consumer well-being. Groundedness—understood as a feeling of deep-rootedness, having a strong foundation, and being securely anchored—gives consumers feelings of safety, strength, and stability as well as confidence that they can withstand adversity. As such, feelings of groundedness might provide consumers with a sense of happiness, thus adding to their overall well-being.This work makes several contributions. First, it introduces the feeling of groundedness as a driver of consumer behavior and consumer welfare. Second, it provides an overarching theoretical explanation for a variety of major consumer trends, such as the desire for local, craft, and traditional products. Third, it highlights that consumers experience a feeling of groundedness when products connect consumers to their physical (place), social (people), and historic (past) environment. Fourth, the studies offer various actionable marketing implications for products aimed at helping consumers connect to place, people, and past. Groundedness Groundedness in the LiteratureAs a personal characteristic, to be ""grounded"" is a common concept in everyday parlance, easily found in any dictionary. In contrast to everyday parlance, we found groundedness to be a fairly novel and underresearched construct in the literature. There are few direct references to groundedness in the marketing, consumer behavior, or social psychology literature streams. The mentions we did find in other literature (e.g., psychotherapy, environmental or educational psychology) are relatively obscure, only loosely related, or speculative (for an overview of relevant research, see Web Appendix A). For example, educational psychologist [29] writes about ""rootedness"" and develops a measure of rootedness for college students. However, McAndrew's explanation of rootedness is limited to location. Similarly, environmental psychologists (e.g., [27]; [33]) have studied connectedness to nature, which is also a more limited construct. We found a more closely related conception of groundedness in a psychotherapy doctoral dissertation, where [31], pp. 82–83) describes rootedness in terms of ""the personal, social, environmental, and economic anchoring that sees us through tough times. Within rootedness, there is a sense of togetherness, a combination of personal identity and group identity, past and present, and people and places.""In philosophy, [47], p. 43) points to the importance of being rooted. She notes:A human being has roots by virtue of his real, active, and natural participation in the life of a community, which preserves in living shape certain particular treasures of the past and certain particular expectations of the future. This participation is a natural one, in the sense that it is automatically brought about by place, conditions of birth, profession, and social surroundings.[12] likewise writes about rootedness in terms of the need to establish roots and feel at home in the world, while [41] refers to a connection to the land as a source of well-being that is undermined by technological forces that separate people from their roots in nature.In marketing, [42] examine rootedness in the context of community-supported agriculture (CSA), arguing that by connecting consumers to the land and producers, CSA membership may help consumers reconnect to their ""material, historical, and spiritual roots"" (p. 141). [ 2] also touch on some of the elements, antecedents, and consequences of groundedness, such as community and traditions.In summary, we believe the idea of groundedness has not been formally developed as a concept, nor have the full scope of the construct and its implications for consumer behavior and marketing been identified. We aim to fill this gap in the literature. The Construct of GroundednessWe argue that many consumers have a need to feel grounded, which we define as a feeling of emotional rootedness. The feeling of groundedness results from being metaphorically embedded in one's physical, social, and historical environment. Like the roots of a tree or the foundation of a house, a feeling of groundedness connects a person to their ""terroir"" (where the French word terroir not only refers to the land per se but also includes its cultural history and human capital [[35]]). Consistent with relevant dictionary definitions—which include being mentally and emotionally stable or firmly established[ 5]—we argue that the feeling of groundedness provides a solid foundation that imparts a sense of strength, safety, stability, and confidence that one can withstand adversity. Connection to placeConsistent with the idea of ""spreading one's roots into the ground,"" and the literal translation of terroir as ""land"" or ""soil"" ([35]), the feeling of groundedness can be obtained from a connection to a physical environment or place. This connection can be physical in the literal sense, as when working with actual, tangible objects that originate in the local environment, or when immersing in the natural environment itself. We find examples of such immersion in, and connection to, the natural sphere in the East Asian tradition of shinrin yoku, or forest bathing ([19]), and the Nordic cultures' idea of outdoor life (Friluftsliv), which, according to [15], p. 3), provides ""a biological, social, aesthetic, spiritual and philosophical experience of closeness to a place, the landscape, and the more-than-human world; an experience most urban people today lack."" In the same vein, connection to place may be experienced when directly drawing from the earth, as popularly pursued in urban gardening and farming. Indeed, one of [42], pp. 140–41) informants states, ""That's what farming actually is [a connection to the earth].... You are working with the living world. It's the connection you give people to the farm."" In addition to a physical connection, consumers can also connect to place in a more symbolic sense. They may do so, for example, by consuming locally produced goods, such as a beer from a nearby brewery. Establishing a connection to one's place to feel grounded may have become especially important as a consequence of migration and mobility. For example, a consumer who has recently been relocated to a certain town may particularly desire to consume products local to that town, thus enabling them to build a connection to that place. Connection to peopleFeelings of groundedness can also arise from a connection to one's social environment. Just as the meaning of terroir also includes its human capital ([35]), the idea of a ""place"" that provides groundedness, such as home, is often strongly shaped by the people and community associated with that place.In the social psychology literature, the human need for connectedness or belongingness to other people ([ 4]) has been well established. Running counter to that need is the phenomenon of modern-day alienation ([26]). The concept has been revived by marketing scholars to describe alienation of the consumer from the marketplace ([ 1]), and from a product's producer ([46]). Along the same lines, [ 2] observe postmodern consumers' feelings of personal meaninglessness and loss of moorings brought on by globalization and technology, while stressing the importance of identity, home, and community as antidotes to these feelings.Although the strongest route to groundedness via people might be connecting to one's closest social surroundings (e.g., one's family), we also see customers trying to reestablish a connection to people by means of certain product choices. Both online and offline, consumers may obtain groundedness by buying directly from the producer. At a farmers market, consumers may buy eggs directly from the person who fed the chickens and collected their eggs. On Etsy, online shoppers can order a breakfast mug from the very person who designed and shaped the piece with their own hands; the shopper might even be able to communicate directly with that person and learn how they developed their passion for handicraft. Either way, this enables the customer to get ""closer to the creator"" ([39]). On the business side, many firms, big and small, try to facilitate connections between customers and the people behind their products: for example, featuring individual producers on the packaging, indicating the name and address of food suppliers, or communicating via the company's founder or chief executive officer ([14]). Connection to pastThe human environment, or terroir, also includes a historical dimension ([35]). We suggest that feelings of groundedness can also be experienced based on a connection to the past. The past provides a foundation of memories, traditions, and cultural values for individuals to be grounded in.Examples from the marketing literature illustrate how consumption behavior establishes a connection to the past and begets feelings of groundedness. In [42], some respondent quotes suggest that community-supported farms provide not merely a connection with their local physical environment and the people around them but also a symbolic connection to past generations within one's own family (e.g., a connection to ancestors who were farmers). [44], who investigated Nordic consumers' food consumption motives, state that ""in the end, it is the caring food-producer who can bring the ubiquitous brand consumption back to where we were before industrialism"" (p. 230). Similarly, [ 3] find that visits to local farmers markets allow consumers to ""reconnect with their agrarian roots"" (p. 567), searching for ""food that is embedded in their personal and shared social histories"" (p. 564). In the consumer product domain, we see a resurgence of historic brands such as Converse ([23]) and observe companies helping consumers get connected to, or grounded in, the past. For example, firms may purposefully manufacture according to traditional and artisanal methods, such as making things by hand ([13]), or return to using older, often more ""natural"" materials and ingredients.Building on this conceptualization, our first prediction is as follows: H1:  Products that connect consumers to place, people, and past provide consumers with the feeling of groundedness. How Groundedness Is Distinct from Related ConstructsProducts that connect consumers to place, people, and past frequently differ from other products in more aspects than their affordance of feelings of groundedness. For example, a local, traditional product is probably also more authentic ([32]; [34]). Likewise, products that connect to place, people, and past could be deemed higher quality or costlier to produce. They may be more unique ([25]), or perceived as made with love ([13]). Consumers may feel a stronger brand attachment to such products ([43]). These products may also provide a greater sense of human contact ([38]), brand experience ([ 5]), brand community (e.g., [28]), and sense of nostalgia ([ 9]). Products that provide a feeling of groundedness may also evoke a feeling of being true to oneself (i.e., self-authenticity or existential authenticity; e.g., [ 2]; [16]), a feeling of knowing who one is (self-identity), a general sense of belonging ([ 4]) that is not about feeling grounded and deep-rooted, or a general sense of meaning in life ([21]; [37]; [40])—all of which could increase one's well-being.While these related constructs are relevant, we argue that they play different conceptual roles than groundedness. First, some constructs—such as product authenticity, product quality, or product uniqueness—are characteristics of products. They logically cannot cast doubt about the existence of groundedness, which is a feeling about the self.Second, other alternative constructs could be classified not as characteristics of brands but as feelings about brands. For example, brand attachment is a feeling of connection to a brand. In some situations, feeling connected to a brand might be a consequence of a brand's relationship to a place, people, or the past that a consumer longs to feel a connection with. For example, a consumer may be more likely to feel attached to a wine brand from their own region (or to their favorite laptop brand, which may have nothing to do with feeling connected to place, people, or past). However, this feeling of brand attachment is not a feeling about the self. Thus, it cannot be the same as the feeling of groundedness.A third category of constructs relates to connectedness but is focused on only one of the three sources. For example, nostalgia, as ""a sentimental longing or wistful affection for a period in the past,""[ 6] is related to the past but not necessarily people or place. Likewise, these constructs might be alternative explanations for one of the antecedents of groundedness but not groundedness itself. In addition, nostalgia describes a state of longing or affection, but it does not stipulate that this longing has been satisfied by an actual connection to the past. Thus, nostalgia is conceptually more closely related to the need for groundedness than to actually feeling grounded.Finally, there are some constructs involving feelings about the self that might be driven by similar antecedents or generate similar consequences as the feeling of groundedness; these include feeling true to oneself (i.e., self-authenticity), a sense of belonging that does not involve a feeling of deep-rootedness, a sense of self-identity, and a general sense of meaning in life. Our studies will assess these alternative constructs to groundedness. FrameworkFigure 1 depicts our conceptual framework. At the core of this framework and as summarized in H1 is that there are at least three immediate sources of groundedness: connection to the physical environment, or to place; connection to the social environment, or to people; and connection to the historical environment, or to the past.Figure 1 further depicts our hypotheses about the consequences of the feeling of groundedness; in particular, we consider product attractiveness (H2) and consumer well-being (H3) as important outcome variables. We then examine ways in which marketers can leverage groundedness on the basis of marketing-mix elements (H4) and consumer characteristics (H5).Graph: Figure 1. Conceptual framework. How Groundedness Affects Consumer ChoiceIn our predictions about downstream effects of groundedness, we hypothesize that groundedness increases product attractiveness and, thus, affects consumer choice. In particular, we suggest that products providing a connection to place, people, and past beget feelings of groundedness for the customer and may therefore be more attractive than their competitors that do not. We thus predict that customers will prefer these products and have stronger intent to purchase and higher willingness to pay (WTP). More formally, H2:  Products' ability to provide consumers with the feeling of groundedness makes those products more attractive to consumers. How Groundedness Affects Consumer Well-BeingBeyond marketplace outcomes, we hypothesize in our predictions that groundedness increases consumer well-being. In particular, we suggest that feeling grounded provides consumers with a sense of strength, stability, safety, and confidence in one's ability to withstand adversity. As such, feelings of groundedness might provide consumers with a sense of happiness, thus adding to their well-being. We find conceptual support for these predictions in the descriptions of [31] and [42]. [31], p. 82) refers to rootedness as providing ""a sense of balance, belonging, and fitting to one's place."" Further specifying the elements of well-being afforded by groundedness, Ndi (p. 59) says that rootedness is ""the ultimate feeling that provides stability, harmony, and happiness among people and their community,"" whereas a lack of rootedness leaves a person with a sense of meaninglessness, disconnectedness, emptiness, vulnerability, and unhappiness. Building on [41] work in biodynamics, [42], p. 140) also suggest that emotional connections to one's environment ""are a primordial source of spiritual sustenance and a foundation of social and personal well-being and, conversely, that psychological and societal unrest are precipitated by technological forces that separate humanity from its roots in nature."" Research on constructs related to groundedness also provides indirect, suggestive evidence for our proposition that groundedness increases consumers' well-being. [27], for example, find that connectedness to nature is positively correlated with subjective well-being. We predict the following: H3:  The feeling of groundedness increases consumers' subjective well-being. How Marketers Can Leverage Groundedness Marketing-mix strategiesMarketers can use several marketing-mix variables that help connect consumers to place, people, and past and thus make them feel grounded. Marketers can promote the location where the product is made or ingredients are sourced, engage in storytelling about the history of the brand, or introduce the people who produce the products ([14]; [46]). Marketers can design products in a local or traditional style; use local, ethnic, or traditional ingredients; or employ traditional production processes (e.g., in ""indie"" products). Marketers can also adjust their channels of distribution to help customers connect to place, people, and past. For example, farms and small producers can use farmers markets (vs. supermarkets) that connect consumers with place, people, and past. Retailers can employ traditional store designs or focus their assortments on more traditional products. We propose the following: H4:  Marketing-mix variables such as communication, product design, and channels of distribution can be designed to increase the feeling of groundedness. Consumer segmentation strategiesWe expect that consumers differ in how important feelings of groundedness are to them. That is, the level of need for connection with place, people, and past, and thus, for groundedness, varies across consumers. We examine three reasons why the need for groundedness might be heightened in certain consumer segments. First, the need for groundedness should be particularly strong when consumers' life and work make it difficult to establish and maintain strong connections with place, people, and past. We suggest that living in large cities (which are often inhabited by people who did not grow up there, are characterized by social anonymity, and tend to showcase modernity) is a predictor of need for groundedness. With regard to work, we expect that performing mostly computerized work, confined to the limits of one's desktop, puts a distance between individuals and other people as well as the physical environment. We consequently argue that computerized work is associated with a stronger need for groundedness.Second, we propose that the need for groundedness is stronger when consumers' foundations are shaken or connections with place, people, and past are severed or under pressure. We expect this to have been the case, for example, during the COVID-19 pandemic, a global event that indeed disrupted many people's lives. Accordingly, those who the pandemic had more strongly put in a state of flux should have experienced a higher need for groundedness.Third, we suggest that the need for groundedness will be more prominent for consumers whose more basic needs are satisfied. Respective proxies such as consumers' socioeconomic status (SES) should thus be correlated with their felt need for groundedness. We predict the following: H5:  The feeling of groundedness is more important to consumers when their work and life do not provide a strong connection to place, people, and past; when life events shake their foundation; or when their basic needs are already sufficiently met. Overview of StudiesWith a view to robustness and generalizability, we test our predictions in eight experiments and one consumer survey, based on a variety of samples and data collection techniques (students in behavioral labs at universities, online platforms, and professional market research panels, both in the United States and in Europe). For managerial usability, our study paradigms include both consequential outcome measures as well as marketing-relevant factors that can be manipulated or measured. Study 1 provides evidence that groundedness increases product attractiveness in real economic terms using an incentive-compatible measure of WTP. Studies 2a–c show that groundedness has explanatory value above and beyond alternative constructs. These studies also explore how a product's affordance of groundedness depends on the closeness of the consumer's connection to the provenance of the product or the producer of the product. Studies 3 and 4 provide concrete implications for marketing practice by manipulating product design and assortment, showing how demand for traditional versus innovative products is affected by consumers' current need for groundedness, and exploring proxies that might allow managers to assess said need. In Studies 5a and 5b we focus on psychological effects on consumers. Study 5a shows that groundedness has a positive effect on consumer happiness, whereas Study 5b examines the effect of a grounding product on one's feelings of strength, stability, and safety. Study 1: Groundedness and Product AttractivenessStudy 1 tests the effect of groundedness on product attractiveness (H2). We do so in a study paradigm that aims to showcase the managerial relevance of the focal effect. Specifically, we exposed participants of a consumer panel to a more grounding ""indie"" brand of soap versus a less grounding industrial brand and took an incentive-compatible measure of participants' WTP for each product. We separately tested the extent to which the two brands provide a connection to place, people, and past (see Web Appendix B). We also measured a moderator—importance of the product category to the consumer—to provide further insight into the process and strengthen internal validity (e.g., to alleviate any concerns about demand effects). We reasoned that the self-related benefit of groundedness afforded by indie (vs. industrial) brands should be more pronounced when the product category is more central to the self (i.e., more important to the consumer). MethodAn age- and gender-representative sample of 311 Austrian consumers from a professional market research panel participated for monetary compensation (Mage = 41.8 years; 50.2% female; for instructions and stimuli of this and all following studies, see Web Appendices B–F). All participants were exposed to a color picture and verbal description for two bars of soap. An almond-scented soap made by Firm A was always presented on the left. An olive-scented soap from Firm B was always presented on the right. We manipulated which firm was described as indie (""makes high-quality products that are produced in a small and independent craft business"") versus industrial (""makes high-quality products that are industrially produced at scale in a large factory"").[ 7] Participants indicated their WTP for a bar of soap from both companies separately using an incentive-compatible elicitation method (dual-lottery Becker–DeGroot–Marschak procedure; e.g., [13]). This method provides an incentive-compatible measure of what the product is worth to participants.Next, participants indicated which soap provided relatively stronger feelings of groundedness by rating agreement with the following two statements (translated from the original German): ""When I think of this firm's soap ... I feel deep-rooted and firmly anchored ('grounded')"" and ""I can firmly feel my feet on the ground."" Participants also indicated how well a graphic depicting a human form with branches for arms and a deep, wide root system instead of legs (see Figure 1) represented their emotional state. The three items were measured on a seven-point scale (1 = ""true for Firm B,"" and 7 = ""true for Firm A"") and were averaged to create a groundedness index (α = .87).[ 8] We captured the importance of the underlying product category to the consumer with a three-item measure (e.g., ""The product category 'soap' is very important to me""). All measurement items used in this and subsequent studies, as well as their reliability statistics, are listed in Web Appendices B–F. Unless indicated differently, items are measured on seven-point scales (where ""strongly disagree/does not describe my feelings at all/not true of me at all/true for Brand B,"" etc. is coded as 1, and ""strongly agree/describes my feelings very well/very true of me/true for Brand A,"" etc. is coded as 7). Results and DiscussionWe ran a repeated-measures analysis of variance (ANOVA) with consumers' WTP in euros as the repeated-measures factor and our indie versus industrial counterbalancing manipulation as the between-subjects factor (for complete results, see Web Appendix B). We find the expected interaction effect (F( 1, 309) = 174.51, p < .001). Follow-up contrast analyses show that participants are willing to pay more for the soap of Firm A if that product is portrayed as an indie (Mindie = €3.29) versus as an industrial (Mindustrial = €1.91; F( 1, 309) = 37.47, p < .001) brand. Likewise, the soap of Firm B is valued more when Firm B is described as an indie (vs. industrial) company (Mindie = €3.12, Mindustrial = €2.11; F( 1, 309) = 20.67, p < .001)—a notable 60% increase in value. For moderation and mediation analyses, we calculated the intraindividual delta WTP (WTPFirm A − WTPFirm B: MFirm A indie = €1.18, MFirm A industrial = −€1.21; F( 1, 309) = 174.51, p < .001).An ANOVA on the groundedness measure indicates a significant effect: when Firm A is described as indie, participants more strongly declare that Firm A makes them feel grounded (MFirm A indie = 5.15) compared with when Firm A is described as industrial (MFirm A industrial = 2.92; F( 1, 309) = 269.58, p < .001). Mediation analysis ([20], Model 4, 10,000 bootstrap samples) shows that the WTP effect is mediated by feelings of groundedness (indirect effect = 1.24, 95% confidence interval [CI95%]: [.87, 1.67]). A moderation analysis ([20], Model 1) with the delta WTP measure as dependent variable confirms the hypothesis that the indie premium increases as the category importance increases (p < .001; for details, see Web Appendix B). Finally, a moderated mediation analysis ([20], Model 8) shows that this interaction effect is mediated by groundedness: the indirect effect of indie versus industrial on delta WTP through feelings of groundedness is always significant but stronger at high versus low levels of category importance (indirect effect16th percentile = .79, CI95%: [.51, 1.12]; indirect effect50th percentile = 1.13, CI95%: [.77, 1.55]; indirect effect84th percentile = 1.54, CI95%: [1.04, 2.16]; index of moderated mediation = .21, CI95%: [.11,.34]).Study 1 finds that products making a connection to the past, to people, and to a place make consumers feel more grounded, which increases their WTP. Thus, the result in Study 1 supports H2. The effect is managerially relevant: the more grounding product yielded a notable 60% increase in WTP. In addition, Study 1 shows that the effect is moderated by the importance of the product category. The pattern of moderated mediation, where the indie versus industrial nature of the brand is less important to feelings of groundedness when the product is less important to the consumer's identity, provides further evidence for our process.A limitation of Study 1 is that indie versus industrial products may differ in more aspects than their ability to provide a feeling of groundedness. For example, an indie brand might provide higher value to consumers by being perceived as more authentic ([32]) and more unique ([25]) than an industrial brand. Further, the description of the indie brand and its production method might give consumers a greater sense of love ([13]), human contact ([38]), attachment ([43]), brand experience ([ 5]), and brand community (e.g., [28]). Or the indie brand might simply be higher quality and costlier to produce. Our mediation and moderated mediation provide initial evidence for the proposed groundedness process, suggesting that these alternative processes are not the only drivers of the effects on WTP. We explicitly address these alternative explanations in Study 2. Study 2: Connectedness to Place or People, Groundedness, and Product AttractivenessOne major element of our theory is that the feeling of groundedness afforded by a product results from the connection that product provides to place, people, and past (H1). If products are indeed connectors between customers and their place, people, and past, we should be able to affect groundedness—and product attractiveness (H2)—not just by manipulating the place, people, or past of the product as we did in Study 1 but also by manipulating the place, people, or past of the customer. Thus, in Study 2a, we keep brands and products constant and manipulate how much groundedness a brand is able to provide as a function of a customer characteristic (i.e., customer location), rather than a product characteristic. Study 2a MethodWe asked 172 students (Mage = 21.9 years; 79.7% female) at a Northeastern U.S. university (n = 89, for a gift voucher and cookies) and an Austrian university (n = 83, for course credit) to imagine that they had just moved to either Karlstad or Umeå in Sweden. We then asked them to choose (using a three-item measure, e.g., ""Which of the two craft beers do you choose?"") which of two real Swedish craft beer brands, Good Guys Brew from Karlstad and Beer Studio from Umeå, they would purchase on their first night out. Next, participants reported which of the two brands they perceived would make them feel more grounded (""In the situation described, this brand would make me feel deep-rooted,"" ""This brand would make me feel well-grounded,"" and ""In a metaphorical sense: Which of the two craft beers would rather make you feel as illustrated by the following picture?"" [showing the picture of a human/tree form with deep roots]; α = .90). All items in this study were captured on seven-point scales where one anchor was the beer from Karlstad and the other anchor the beer from Umeå. We counterbalanced which beer was shown on the left- versus right-hand side. Before the participant location manipulation, we also asked participants to rate the two brands on a relative scale regarding nine product characteristics that might make either product more attractive. Because these were product characteristics that should not have been influenced by the participant's location, and because they were measured before the location manipulation, they did not—and could not—explain our results (for results regarding the control variables in this and all subsequent studies, see Web Appendices C–F). At the end of the study, we captured some information about the participants' relation to beer and to Sweden (e.g., ""Have you ever been to Sweden?,"" ""How much do you like beer in general?"").[ 9] Results and discussionA one-way ANOVA shows that participants who moved to Karlstad prefer the Karlstad-based beer significantly more than those who moved to Umeå (MKarlstad = 4.80, MUmeå = 4.14; F( 1, 170) = 6.70, p = .010). Similarly, the Karlstad-based beer provides relatively more groundedness to participants who moved to Karlstad versus Umeå (MKarlstad = 4.29, MUmeå = 3.79; F( 1, 170) = 5.77, p = .017). Groundedness mediates the effect of residence location on preference (indirect effect = .40, CI95%: [.07,.74]; [20], Model 4). For each of the nine alternative constructs, the focal indirect effect via groundedness remains significant when we include the alternative construct as a rival mediator.Study 2a shows that groundedness drives product attractiveness (H2) when we keep products constant but manipulate the place of the customer. This study highlights that the groundedness effect depends not only on the features of the product but also on the situation of the customer. Managerially, the study shows that local brands are particularly grounding and thus attractive to local consumers. Study 2a manipulated how participants relate to a place that is connected to a focal product, and thus how much groundedness it affords them. Unlike Study 2a, Study 2b capitalizes on participants' existing relationship to a place. Study 2b further addresses alternative constructs to groundedness by measuring them after the focal manipulation. Study 2b MethodThe week before Christmas, we asked 1,306 Austrian students from a university in Vienna (Mage = 22.8 years; 55.4% female; compensated by a lottery for an iPhone 11 and five €10 gift vouchers, prescreened for having grown up in Austria but outside Vienna and for celebrating Christmas) to imagine they were celebrating Christmas in Vienna this year and looking to buy a Christmas tree at a local market. We then varied between-subjects whether the market's Christmas trees originated from the state the participant grew up in or from a randomly selected other Austrian state. The trees were thus not connected to participants' current place (i.e., where they were studying and buying the tree) but to either the place where they grew up or a third location in Austria. Then, we assessed purchase intent for the Christmas tree using four items (e.g., ""I would very much like to buy a Christmas tree at this market""). We next captured feelings of groundedness from purchasing a Christmas tree at that market, using the same three items as in Study 2a. Finally, participants completed two-item measures of alternative constructs (the product's authenticity, uniqueness, quality, love, production costs, sense of human contact, brand experience, feeling of belonging to a brand community, and attachment). In addition, we measured participants' desire to support the producer as a possible alternative explanation. Due to this study's use of multiple items for each construct, we were able to ascertain that groundedness is empirically distinct from the other constructs captured (purchase intent and alternative constructs) using the [11] criterion. We performed the same tests in all subsequent studies with multi-item measures of our dependent variables (see Web Appendices C–F). Results and discussionParticipants are more intent on buying a Christmas tree from the focal market if it is from their own state (Mown place = 5.35) versus another state in the same country (Mother place = 4.95; F( 1, 1,304) = 24.27, p < .001). Further, when the trees originate from participants' own state, participants experience stronger feelings of groundedness than when the trees are from another state (Mown place = 3.39, Mother place = 3.15; F( 1, 1,304) = 8.43, p = .004), which is in line with H1. We do not find significant differences between conditions with regard to the alternative explanations captured (ps >.087). Differences in perceived production costs (Mown place = 4.17, Mother place = 4.30; F( 1, 1,304) = 2.92, p = .088) are marginally significant but run in the opposite direction of the dependent variable. Thus, they are unable to explain our results. Consistent with H2, a mediation model ([20], Model 4) shows that groundedness mediates the treatment effect on purchase intent (indirect effect = .11, CI95% [.03,.18]). For each of the ten alternative constructs, the focal indirect effect via groundedness remains significant when we include the alternative construct as a rival mediator.Studies 2a and 2b show that a product that connects a consumer to a place they relate to (a city they move to, the state they are from) makes them feel more grounded and is more attractive than a product originating from a specified place they do not relate to (another city or state in the same country). One pertinent question is how much that feeling of groundedness depends on the closeness of the connection to place, people, and past. While the more grounding option in Studies 2a and 2b connects customers to their own (""my"") current or past place, the indie brand utilized in Study 1 merely provided a connection to ""a"" place (and ""the"" people who made it and ""the"" past, respectively). Our view is that, ceteris paribus, the depth of groundedness gradually increases with the closeness of the connection. The closer the personal relationship of the customer to the place, people, and past represented by the product, the stronger the connection and thus feelings of groundedness established via the product. We test this prediction in the context of a customer connecting to the people dimension next. Study 2cStudy 2c addresses whether differences in closeness indeed matter—that is, whether they afford different levels of feelings of groundedness when compared directly. Beyond that, the study isolates connection to people as a potential driver of groundedness (H1). MethodTwo hundred U.K. crowd workers on Prolific (Mage = 33.8 years; 55.0% female; for monetary compensation) were asked to indicate their feelings of groundedness associated with the use of a coffee mug (using the same measure as in Studies 2a and 2b). To sample different levels of personal closeness along the proposed continuum, the producer of the mug was manipulated to be either ""an artisan that is personally close to you (e.g., a close friend, relative, partner, etc.)"" or ""an artisan that is a distant acquaintance of yours (e.g., a colleague from work, a neighbor, a friend of a friend, etc.)."" We measured perceived connection to people through the mug using three items (e.g., ""Drinking from this mug, I somehow feel a connection to 'my people'""). We used the same control measures as in Study 2b (except for the motivation to provide financial support by purchasing a product, given that there was no purchase in this study). Results and discussionFirst, the pattern of results for groundedness and connection to people supports our theorizing about a continuum of closeness and, thus, groundedness: perceived connection to people is significantly higher when the artisan producer is a close other versus when they are merely an acquaintance (Mclose = 4.34, Mdistant = 3.75; F( 1, 198) = 6.63, p = .011). The same is true for feelings of groundedness: participants experience stronger feelings of groundedness when considering the coffee mug produced by an artisan that is a close other versus one that is merely a distant acquaintance (Mclose = 4.14, Mdistant = 3.29; F( 1, 198) = 14.78, p < .001). Further, a mediation model ([20], Model 4) shows that producer closeness mediates the effect on groundedness (indirect effect = .42, CI95%: [.09,.75]). Importantly, for each of the nine alternative constructs, the focal indirect effect via groundedness remains significant when we include the alternative construct as a rival mediator.Thus, Study 2c shows that being personally closer to one of the sources of groundedness enables consumers to experience stronger feelings of groundedness. More precisely, groundedness is a function of how close the consumer's relationship is to the product's place, people (e.g., the product's producer), or past. As for different routes to groundedness, the study shows that a product's people dimension alone (e.g., its producer) can boost groundedness via a stronger perceived connection to people established by the product. Managerially, the findings are important because marketers can choose the extent to which they highlight the closeness or similarity between customers and producers. In addition, the study highlights that managers may need to search for personally relevant and close sources of groundedness from the perspective of a given target customer.The next set of studies investigates how the groundedness effect can be leveraged via marketing-mix elements (Studies 3a and 3b) and which types of customers have a particularly high need for groundedness (Studies 3a and 4). Study 3: Marketing Mix, Connectedness to Past, Groundedness, and Product AttractivenessStudy 3a focuses on connections to past as a source of groundedness (H1) by manipulating product design (H4). We also examine how the effect of groundedness on product attractiveness (H2) varies across consumers by capturing their chronic need to connect to the past (the higher this need, the stronger the groundedness effect should become). Study 3b manipulates consumers' state need for groundedness and addresses category management considerations by testing how consumers' need for groundedness impacts the preference for traditional versus innovative products. Study 3a MethodWe showed 223 students in the behavioral laboratory of a large European university (Mage = 23.9 years; 65.5% female; for monetary compensation or course credit) two sets of cutlery (from Brand A and Brand B) side by side, stipulating that they were of comparable price and quality. We manipulated product design to provide more versus less connection to the past by using a more traditional versus modern product design. We manipulated which set of cutlery was presented on the left- versus right-hand side (i.e., as Brand A vs. B). Using adapted versions of the measures in Studies 1 and 2, we asked participants to indicate which of the two brands they would rather purchase, which would make them feel more grounded, and which evoked a stronger connection to the past. Need to connect to the past as a chronic consumer trait—our moderator—was measured in terms of agreement with three items (e.g., ""I generally try to see if I can somehow satisfy my desire to [metaphorically] 'connect to the past'"").[10] Results and discussionOur manipulation proved effective: Participants more strongly associate Brand A ( = 7, Brand B = 1) with a connection to the past when Brand A cutlery had a traditional design (MBrand_A_traditional = 5.49, MBrand_A_modern = 1.97; F( 1, 221) = 405.25, p < .001). As expected (H4), we find a significant effect on groundedness—Brand A is perceived to provide more groundedness (relative to Brand B) when Brand A features traditional design (MBrand_A_traditional = 4.39, MBrand_A_modern = 3.65; F( 1, 221) = 18.50, p < .001). For product preference, we find an overall preference for the modern cutlery (MBrand_A_traditional = 3.69, MBrand_A_modern = 4.48; F( 1, 221) = 9.01, p = .003; of course, the fact that traditional products provide a stronger sense of groundedness does not preclude that many people might still prefer a specific set of modern cutlery over a specific set of traditional cutlery, or modern designs over traditional ones in general). More importantly, and as expected (H2), we find a positive effect of groundedness on product preference (b = .61, p < .001), and a positive indirect effect ([20], Model 4) of traditional (vs. modern) design on preference through groundedness (indirect effect = .55, CI95%: [.27,.89]). As one would expect, preference becomes even stronger for the modern cutlery when the groundedness path is controlled for (estimated MBrand_A_traditional = 3.40, estimated MBrand_A_Modern = 4.74).As anticipated, we find that one's general need to connect to the past significantly moderates purchase preference (p < .001; [20], Model 1). Thus, participants with a low need to connect to the past have a more pronounced preference for the modern cutlery; conversely, participants with a high need to connect to the past show a preference for the traditional cutlery (e.g., at need to connect to past = 1, conditional effect = −2.53, CI95%: [−3.58, −1.48]; at need to connect to the past = 7, conditional effect = 1.29, CI95%: [.002, 2.57]). A moderated mediation analysis ([20], Model 58; see Web Appendix D) shows that traditional design affords a stronger feeling of groundedness, and that groundedness becomes a more important driver of preference as general need to connect to the past increases. In fact, at very low levels of general need to connect to the past, a product's ability to provide feelings of groundedness no longer significantly impacts product preference (e.g., at need to connect to the past = 1, conditional effect = .33, CI95%: [−.05,.71]).In summary, Study 3a shows that by varying a marketing-mix element (product design) to be more traditional (vs. modern), marketers can affect customer preference via feelings of groundedness. This is because the marketing-mix element directly caters to a source of groundedness (H4). Study 3bStudy 3b investigates preference for traditional versus innovative products as a direct function of consumers' current need for groundedness and manipulates this need. We also perform a test of how the relative interest in different product categories—traditional versus innovative—is affected by different levels of need for groundedness, pointing to potential boundary conditions of the groundedness effect. MethodTwo hundred crowd workers on Prolific (Mage = 33.4 years; 54.0% female) from the United Kingdom took part in this study for monetary compensation. Participants filled out two ostensibly unrelated surveys. The first manipulated participants' current need for groundedness. Participants in the high-need condition read, ""Research has shown that feelings of groundedness can be positive or negative depending on the context and situation we are in."" They were then asked to describe a recent situation where feeling grounded was desirable to them because ""you metaphorically felt your roots were too loose and weak with respect to your connection to a place, to people, and the past."" Conversely, participants in the low-need condition read, ""Research has shown that feelings of groundedness can be negative or positive,"" and were asked to describe a situation where groundedness was undesirable to them because ""you metaphorically felt your roots were too dense and strong."" After completing the writing task and reporting their current need for groundedness on a version of our three-item groundedness scale, participants were thanked and told they would be forwarded to another study. Here, participants were introduced to two different online stores, presented side by side: one specializing in ""the best traditional products"" and one specializing in ""the best innovative products."" We then asked participants to indicate which of the stores they would prefer to shop at on a seven-point scale, with Store A and Store B as anchors. We alternated which of the stores (A vs. B) was presented as traditional versus innovative in our stimuli. We subsequently reversed the Store A versus B preference scores for half the data set, so that the innovative store preference was always anchored at 1 and the traditional store preference was always anchored at 7. Results and discussionOur manipulation was effective: participants who wrote about a situation where their need for groundedness was high reported experiencing a higher need for groundedness (M = 5.25) than those who wrote about a situation where need for groundedness was low (M = 4.11; F( 1, 198) = 41.19, p < .001). In terms of shopping preferences, participants in the high-need-for-groundedness condition showed a stronger preference for the online store with traditional (vs. innovative) products (M = 4.00) than those in the low-need-for-groundedness condition (M = 3.47; F( 1, 198) = 4.17, p = .043).Thus, and in line with H4, Study 3b shows that relative interest in purchasing traditional products is higher in situations and contexts where consumers' need for groundedness is high. In situations and contexts where groundedness is less sought after, innovative products become relatively more interesting. Study 4: Consumer Characteristics and Need for GroundednessStudies 3a and 3b suggest that groundedness is not equally attractive and relevant to all consumers in all situations. For segmentation purposes, it is important to know which consumers are more likely to have a strong enduring need for groundedness. As predicted in H5, we argue that the feeling of groundedness is more important to consumers when their work and life (e.g., computerized desktop work, living in a large city) do not provide a strong connection to place, people, and past; when certain life events (e.g., the COVID-19 crisis) shake their foundation; or when their basic needs are already sufficiently met (e.g., when they have higher SES). In Study 4, we use a survey to measure these consumer characteristics, along with need for groundedness and preference for products that connect to place, people, and past. The study was conducted in spring 2020, at the beginning of the COVID-19 pandemic and first lockdown. This enabled us to assess the impact of a disruptive life event on the need for groundedness. MethodAn age- and gender-representative sample from a U.S. consumer panel completed this survey for monetary compensation (N = 325; Mage = 45.5 years; 51.1% female). We first measured product preference and need for groundedness: preference for products connected to one's place, people, and past were measured (in random order) using three items each (e.g., ""I like to purchase products that connect me to 'my place' ['my people'/'my past'], i.e., my physical [social/historic] environment""). We merged these into one global index of purchase interest. Need for groundedness was measured using a version of our three-item scale, adapted to measure general need for groundedness (e.g., ""In general, I want to feel deep-rooted""). We next captured a series of demographic and lifestyle variables.To assess a potential lack of connection to people, place, and past in consumers' work and social lives, we captured three variables. First, we asked respondents about the type of area they live in (1 = ""in the countryside,"" and 7 = ""in a big city""). We hypothesized that living in large cities (which are often inhabited by people who did not grow up there, are characterized by social anonymity, and tend to showcase modernity) is a predictor of need for groundedness. Second, we assessed participants' desktop work using two items (e.g., ""During the week [e.g., when being at work] ... I primarily work at the computer""). We expected a positive relationship between desktop work and need for groundedness, because a disproportionate amount of computerized work (while confined to one's desktop) separates individuals from other people as well as the physical environment. A similar logic might apply to people whose job is characterized as ""work of the head"" (i.e., work that contains many abstract tasks), as opposed to people who perform manual labor (""work of the hands"") or work in social jobs (""work of the heart""; [17]). Respondents accordingly indicated which of these three categories their current or most recent job fell into.Next, to assess a potential link between need for groundedness and a disruptive major life event, we examined perceived impact of the COVID-19 crisis on the consumer's life. We assessed this with a single item (""Due to the current Corona [COVID-19] crisis, I feel that my life is in a state of major change""). Last, we theorized that the need for groundedness should become more prominent when basic needs such as food and shelter are not a concern. Therefore, we tested whether higher SES (measured on a three-item scale [e.g., ""I have enough money to buy things I want""]) might be an effective proxy for one's need for groundedness. No other measures were taken. Results and DiscussionFirst, and as expected, we find a significant and positive correlation between one's need for groundedness and purchase intent for products connecting to place, people, and past (r = .57, p < .001). Second, we analyzed the correlations of all proposed indicators with the need for groundedness. In particular, need for groundedness correlates positively with desktop work (r = .26, p < .001), SES (r = .30, p < .001), change experienced as a result of COVID-19 (r = .12, p = .030), and living in a big city rather than the countryside (r = .10, p = .079), but correlates negatively with performing work of the hands (r = −.11, p = .040; for a complete correlation table of this study; see Web Appendix E).Third, we ran multivariate ordinary least squares (OLS) regressions with all predictor variables on both need for groundedness and purchase intent. For those variables that emerged as significant predictors for both the need for groundedness and purchase intent, we examined whether the need for groundedness mediates the respective effects on purchase intent while entering all other variables as covariates. For conciseness, we report only significant results hereinafter (see Table 1 for details).GraphTable 1. Multivariate OLS Regression Models (Study 4). Need for GroundednessPurchase Interest in Products Connected to Place, People, and PastUnstandardized Coef.Standardized Coef.Unstandardized Coef.Standardized Coef.bSEβtp-valuebSEβtp-value(Constant)2.983***.4177.157<.001.682.461.481.140Living environment.038.034.0611.11.270.08*.038.1012.105.036Desktop work.143***.037.2483.892<.001.24***.041.3245.905<.001Work of the hands−.17.19−.061−.9.370−.228.21−.064−1.091.276(1 = hands,0 = otherwise)Work of the head−.281.172−.117−1.64.102−.699***.19−.226−3.686<.001(1 = head,0 = otherwise)Change through COVID-19.091*.04.1232.302.022.234***.044.2475.356<.001SES.185***.037.2725.008<.001.287***.041.337.038<.001Age.01*.004.1362.357.019.000.004.005.104.917Gender (1 = male, 0 = female).004.13.002.034.973.465**.144.1513.228.001R2 = .162, d.f. = 8, 316R2 = .379, d.f. = 8, 316 1 *p < .05.2 **p < .01.3 ***p < .001.4 Notes: Mediation models ([20], Model 4): Mediator = need for groundedness, DV = purchase interest; ( 1) IV = SES: indirect effect = .10, CI95%: [.05,.15]; ( 2) IV = desktop work: indirect effect = .07, CI95%: [.03,.12]; ( 3) IV = change through COVID-19: indirect effect = .05, CI95%: [.002,.10].The multivariate OLS models showed that three predictors remain significant for both the need for groundedness (NG) and purchase intent (PI) when simultaneously including all variables in the model: ( 1) desktop work (NG: b = .14, SE = .04, t(316) = 3.89, p < .001; PI: b = .24, SE = .04, t(316) = 5.91, p < .001), ( 2) SES (NG: b = .19, SE = .04, t(316) = 5.01, p < .001; PI: b = .29, SE = .04, t(316) = 7.04, p < .001), and ( 3) change related to COVID-19 (NG: b = .09, SE = .04, t(316) = 2.30, p = .022; PI: b = .23, SE = .04, t(316) = 5.36, p < .001). Need for groundedness mediates the effect of all three variables on purchase intent (in line with H2; see Table 1 and Web Appendix E).Our ""work of the head"" dummy was not significant in the multivariate OLS model. We conclude that the ""work of the head/heart/hands"" measure was probably too rough and thus unable to adequately detect the important nuances in job characteristics that affect the need for groundedness. We were also surprised that one's living environment did not emerge as a significant predictor for need for groundedness in the multivariate OLS model. A closer look at the data reveals, however, that a disproportionately large number (29.2%) of respondents in our sample indicated living in big cities (i.e., chose the endpoint of the scale). When dichotomizing the measure (i.e., living in big city vs. not), we find the predicted positive effect: people living in a big city have a heightened need for groundedness (see Web Appendix E).In summary, Study 4 finds that a higher need for groundedness is apparent in consumer profiles characterized by larger societal trends: living in big cities (urbanization), doing desktop work at the computer (digitization), and undergoing major change (such as during the COVID-19 pandemic). Further, groundedness seems to be more relevant for high-SES consumers.Thus far, we have provided a cohesive picture of groundedness in terms of both triggers (H1) and market-relevant outcomes (H2), as well as ways for marketers to leverage groundedness (H4, H5). In the final two studies, we examine the implications of groundedness for consumers' psychological well-being (H3). Study 5: Connectedness, Groundedness, and Consumer Well-BeingTo test our hypothesis that feeling grounded increases consumers' subjective well-being (H3), Study 5a measures happiness as a consequence of attaining groundedness. We also test another managerial manipulation: channel type (H4). Study 5b expands into a broader range of psychological outcomes; as outlined in our conceptual framework, the feeling of groundedness should provide consumers with a sense of strength, stability, safety, and self-confidence. We test these outcomes in the context of using locally grown ingredients and also investigate alternative constructs to groundedness, such as self-authenticity, meaning in life, or sense of identity. Study 5a MethodWe randomly assigned 190 Austrian students (Mage = 22.5 years; 50.5% female; lab-based, for monetary compensation) to think about shopping at a supermarket or local farmers market. We then asked about their feelings of groundedness; happiness; and being connected to place, people, and past. Happiness was measured using three items (e.g., ""In the situation just described, how happy would you feel?""). Feelings of groundedness were measured using our three-item measure. Connection to place, people, and past were captured separately using three items each (e.g., ""Having been in the supermarket [to the farmers market] makes me feel connected to my physical/social/historic environment""). The order of the dependent measures (happiness, groundedness), as well as the order of the item blocks capturing connection to place, people, and past, were counterbalanced. Perceived quality and price were measured as control variables. Results and discussionChannel type has a significant effect on groundedness and happiness. Participants who thought about shopping at the farmers market reported feeling significantly more grounded (Mfarmersmarket = 4.66 vs. Msupermarket = 3.80; F( 1, 188) = 18.19, p < .001) and happier (Mfarmersmarket = 5.32 vs. Msupermarket = 4.87; F( 1, 188) = 7.94, p = .005). Consistent with our theorizing (H4), shopping at the farmers market leads to significantly higher perceived connection to place (Mfarmersmarket = 4.96 vs. Msupermarket = 4.03; F( 1,188) = 15.74, p < .001), people (Mfarmersmarket = 4.58 vs. Msupermarket = 3.45; F( 1, 188) = 24.60, p < .001), and past (Mfarmersmarket = 3.73 vs. Msupermarket = 2.52; F( 1, 188) = 25.47, p < .001). We also find support for serial mediation such that the effect of channel on happiness is mediated, in series, by connection to place, people, and past, and groundedness (for mediation results, see Web Appendix F). All effects remain robust when we enter quality and price as covariates.Study 5a thus supports our prediction that the feeling of groundedness increases consumers' subjective well-being (H3) while providing converging evidence for H1. Finally, the manipulation of distribution channel (H4) offers an actionable strategy for marketers to leverage groundedness.In our last study, we employ the context of locally grown ingredients to test a broader range of psychological outcomes of groundedness. We also test the explanatory value of groundedness against alternative constructs that are self-related, such as feelings of self-authenticity or meaning in life. Study 5b MethodThree hundred four students from a major European university completed Study 5b's online study for course credit. We excluded 12 participants for failing our reading check, leaving us with a final data set of 292 participants (Mage = 22.3 years; 69.5% female). Participants were asked to think about making apple pie on a Saturday; specifically, a pie with Boskoop apples—their favorite pie-making variety. In addition, they were told that these apples were from either an orchard only 12 kilometers from their home or an orchard 1,200 kilometers from their home. Participants then completed a short survey that measured five downstream psychological outcomes of groundedness using a five-item scale: ""I feel truly safe as a person,"" ""I experience a feeling of inner strength,"" ""I feel truly stable,"" ""I have a strong feeling of basic trust and confidence in myself,"" and ""I feel that nothing can stir me up"" (α = .89). Afterward, we measured feelings of groundedness using our three-item measure. Finally, participants completed four multi-item measures intended to capture alternative explanations (self-authenticity [e.g., ""I feel out of touch with the 'real me'""], meaning in life [e.g., ""I have a good sense of what makes my life meaningful""], self-identity [e.g., ""I have the feeling that I know who I am""], feeling of belonging [e.g., ""I have a feeling of belonging""]). Results and discussionParticipants who considered making apple pie with apples grown close to home scored significantly higher in terms of experiencing the related psychological downstream consequences than those using apples grown far away (Mlocal = 5.08, Mnonlocal = 4.61; F( 1, 290) = 12.22, p = .001). Thus, the apple pie made with local products boosted participants' personal feelings of strength, safety, and stability (for effects on the individual dependent variable items, see Web Appendix F). They also reported significantly stronger feelings of groundedness (Mlocal = 4.65, Mnonlocal = 4.06; F( 1, 290) = 15.20, p < .001). A mediation model ([20], Model 4) shows that the downstream consequences are mediated by feelings of groundedness (indirect effect = .27, CI95%: [.13,.44]). Importantly, the indirect effect via feelings of groundedness on the downstream consequences holds when we add, one at a time, each of the four alternative explanations as a rival mediator.Study 5b thus confirms positive psychological downstream effects of groundedness (H3) tested in the realm of local products. Products grown closer to the consumer—that is, products that are more strongly connected to one's place—make consumers feel not only more grounded but also stronger, safer, and more stable. General DiscussionIn this research, we have provided systematic evidence that products can provide consumers with feelings of groundedness by giving them a sense of connection to place, people, and past. We do so across nine studies (eight experiments and one survey), both online and in the lab, using different populations (business students, crowd workers on Amazon Mechanical Turk and Prolific, and members of commercial, representative panels) across two continents (total N > 3,000). We have tested our theory for robustness across a variety of product domains, including both disposable and durable consumer goods (food, care products, seasonal products, and tableware), using real brands to strengthen external validity as well as highly controlled stimuli for internal validity. We have provided process evidence via mediation, moderation, and moderated mediation. Theoretical ImplicationsThis work introduces feelings of groundedness to the marketing literature by identifying these feelings as an important construct for marketing research and systematically examining it as a driver of consumer behavior. While references to groundedness and related constructs can be found in philosophy (e.g., [47]), different domains of psychology (e.g., [29]), and psychotherapy ([31]), the concept of groundedness is new to experimental research in marketing, consumer behavior, and mainstream psychology. Existing research in consumer culture theory has given passing treatment to concepts such as ""rooted connections"" ([42]) and has definitely been inspirational to this work. However, it has neither discussed nor empirically explored the full concept of groundedness with its antecedents, proxies, boundary conditions, and consequences, which we have aimed to do here.We also contribute to the growing literature on consumer well-being. [47], p. 43) proposes that ""every human being needs to have multiple roots. It is necessary for him to draw well-nigh the whole of his moral, intellectual, and spiritual life by way of the environment of which he forms a natural part."" Our work indeed shows that groundedness is related to happiness and a sense of strength, stability, and safety; thus, we propose groundedness as a novel antecedent of these outcomes.We also theorized about three sources of feelings of groundedness: connections to place, people, and past. Although the three sources are often empirically intertwined, we show that they are theoretically distinct and powerful in fueling consumers' feelings of groundedness. Our analysis further provides rich insight on the nature of these connections by showing that the extent to which products provide feelings of groundedness is a graded function of closeness. That is, a product provides stronger feelings of groundedness when the product's place, people, or past is closer to the consumer. Finally, by identifying the role of groundedness and its sources, we offer an overarching theoretical explanation for major current consumer trends, such as buying local products (connected to place), produced by people we relate to (connected to people), and according to traditional production methods (connected to the past). Marketing ImplicationsFeelings of groundedness are worthy of managers' attention because these feelings have important downstream consequences as shown across our studies. In particular, feelings of groundedness impact consumers' brand preference and WTP. In Study 1, for example, consumers were willing to pay a price premium of about 60% for the product that provided more groundedness.Our work also provides actionable implications for product and brand management: we give concrete approaches regarding how firms can elicit groundedness by showing consumers their product's connection to place, people, and past. For example, our results in Studies 1, 2a, and 2b show how presenting a product as artisanal or highlighting the local origin of a product can provide feelings of groundedness. In Studies 3a and 3b, we have shown that managers can utilize other marketing-mix elements such as product design or retail assortment and configure them (e.g., as more traditional instead of modern) to provide a stronger connection to the past. Similarly, Study 5a shows that a marketer's choice of distribution channel (e.g., farmers market) has an impact on feelings of groundedness.In terms of customer targeting, we have pointed out when and for whom groundedness is more important. In particular, we have shown that traditional (vs. innovative) products benefit from situational differences in the need for groundedness (Study 3b). On the level of individual differences, in Study 3a, only consumers with a high chronic need to connect to the past preferred the more traditional cutlery design. Our representative survey (Study 4) further showed a higher need for groundedness among consumers who are particularly affected by large global trends or major disruptive events. These global trends (e.g., digitization, urbanization) and major life events (e.g., the COVID-19 pandemic) make it harder for consumers to feel connected to people, place, and past. From a groundedness perspective, it is not surprising that during the safety- and stability-threatening COVID-19 pandemic, customers returned to the familiar grocery brands consumed with their families while growing up ([ 7]). There are probably multiple drivers for this behavior, but it is likely that consumers chose these products, at least in part, because of the connections to place, people, and past—and thus feeling of groundedness—they provide. Limitations and Future ResearchThis is the first series of experimental studies investigating feelings of groundedness. As such, many questions remain for future research. With regard to antecedents, for example, we have focused on products as means for consumers to experience feelings of groundedness. However, anecdotal evidence suggests that there are other ways for consumers to feel more connected to place, people, and past and, consequently, more grounded: for example, through services such as genealogy websites, cooking classes, lectures on local history, or yoga and meditation classes providing ""grounding"" exercises.The scope of Study 4 has allowed us to identify an initial set of indicators for who has a higher need for groundedness and why, but it is clear there will be additional consumer characteristics and lifestyle variables helpful to marketers in identifying relevant customer segments. For example, people who travel frequently for work and have little chance to connect to their current physical environment may seize opportunities to (re)-connect to place—such as through a local craft beer—to feel more grounded. Likewise, pandemics such as COVID-19 are not the only type of events that can shake a person's foundation. Stressful life events such as separation or loss, starting a new job, or moving homes may cause a higher need to feel grounded. Similarly, the need for groundedness may be subject to seasonal variations. Preliminary insights from our own qualitative explorations suggest that individuals' need for groundedness may be particularly high during the holiday season and other festive occasions, such as Christmas, Thanksgiving, Ramadan, and one's own birthday. Apart from that, interestingly, the need for groundedness appears to be higher during the colder seasons. We believe a more thorough testing of these hypotheses seems promising and would likely have important implications. If the initial signals are correct, for example, studies of scanner or panel data should reveal variations in the demand for products that connect to place, people, and past across the year.Finally, we have only begun to examine boundary conditions. For example, it seems possible that in some situations strong roots not only provide strength and stability but could also constrain movement, thus giving consumers the feeling of being ""stuck"" and unable to escape their roots. Imagine growing up on a farm, surrounded by one's family, and doing things day after day in the same way they have traditionally been done by previous generations. A person in this situation will likely feel grounded but might also feel more motivated to break free, move away, or challenge the status quo. If such is the case, too much groundedness might even backfire. Future research might thus enrich the present investigation by focusing on potential downsides of groundedness. ConclusionThis research introduced feelings of groundedness as a relevant construct for marketing research and consumer behavior. We have demonstrated its importance to marketers by documenting that it increases product attractiveness and that it can be manipulated through a variety of marketing-mix strategies and used for targeting consumer segments prone to a lack of groundedness. We also have shown that groundedness is important to consumer well-being, pointing to important consumer welfare and policy implications. We expect that the importance of this topic to consumers and marketers will only increase as digitization, urbanization, and global migration continue to challenge consumers' connections to place, people, and past.  "
6,"Do Backer Affiliations Help or Hurt Crowdfunding Success? Crowdfunding has emerged as a mechanism to raise funds for entrepreneurial ideas. On crowdfunding platforms, backers (i.e., individuals who fund ideas) jointly fund the same idea, leading to affiliations, or overlaps, within the community. The authors find that while an increase in the total number of backers may positively affect funding behavior, the resulting affiliations affect funding negatively. They reason that when affiliated others fund a new idea, individuals may feel less of a need to fund, a process known as ""vicarious moral licensing."" Drawing on data collected from 2,021 ideas on a prominent crowdfunding platform, the authors show that prior affiliation among backers negatively affects an idea's funding amount and eventual funding success. Creator engagement (i.e., idea description and updates) and backer engagement (i.e., Facebook shares) moderate this negative effect. The effect of affiliation is robust across several instrumental variables, model specifications, measures of affiliation, and multiple crowdfunding outcomes. Results from three experiments, a survey, and interviews with backers support the negative effect of affiliation and show that it can be explained by vicarious moral licensing. The authors develop actionable insights for creators to mitigate the negative effects of affiliation with the language used in idea descriptions and updates.Keywords: backer affiliation; crowdfunding; prosocial; social structure; vicarious moral licensingCrowdfunding has emerged as a dominant mechanism to harness the power of crowds in raising funds for innovative ideas. Interest in crowdfunding has surged in recent years. Facebook acquired Oculus 3D visualization device, a crowdfunded idea on Kickstarter, for US$3 billion ([14]). Peloton, the highly successful exercise bike, started as a Kickstarter project. The global crowdfunding market is expected to be well over US$40 billion by 2026 ([54]). Brands such as GE ([11]) and Unilever use crowdfunding to spur innovation ([53]), and academic research on the phenomenon and its role in the digital economy is emerging ([ 2]; [12]). Crowdfunding is a form of crowdsourcing in which participants, hereinafter referred to as ""backers,"" are recruited to raise funds for ideas (e.g., [16]; [62]). As some backers fund the same ideas (i.e., ""cobacking""), overlaps develop between these backers. These overlaps, called ""affiliations,"" are key building blocks of the community's network structure and have been studied in other crowdsourcing communities (e.g., [48]). In this research, we explore how affiliation, defined as the number of cobacking relationships between potential backers and those who have previously funded the focal idea, might affect the idea's crowdfunding success. We illustrate affiliation in crowdfunding using a stylized example in Figure 1.Graph: Figure 1. Illustration of affiliation in crowdfunding.We know that crowd size affects outcomes positively as participants look to anonymous others for cues to decide which ideas to fund, a phenomenon referred to as ""herding"" (e.g., [66]). Previous research shows that attracting more backers positively impacts crowdfunding outcomes ([24]), an insight that many creators seem to grasp. However, crowd size does not represent the social structure (i.e., the pattern of connections in the community). In crowdfunding, as in other contexts in which shared communal goals exist (e.g., Wikipedia), social structure plays a more prominent role (e.g., [48]; [62]).Our primary contribution is in showing that while the total number of backers (i.e., crowd size) may positively affect funding behavior and idea success (e.g., [66]), adding backers may not be unilaterally beneficial as the ensuing affiliation between backers negatively affects funding. Our analysis reveals that the negative effect of backer affiliation is above and beyond the positive effect of number of backers (i.e., the herding effect), highlighting the tension between the benefits of adding more backers and the adverse effects of backer affiliation. In other words, while adding a new backer (e.g., the focal backer in Figure 1) may positively affect the focal idea's success, adding this focal backer may not be equally beneficial across the three scenarios in Figure 1 as the degree of affiliation differs. We propose that the affiliation between the focal backer and other backers will influence the amount that the focal backer puts toward the focal idea and, thus, the idea's funding success.Affiliation is a powerful force because it makes affiliated others' actions lead to changes in one's subsequent behavior (e.g., [35]; [57]). In some contexts, affiliation positively affects behavior as individuals desire to belong and therefore conform to affiliated others' behavior (e.g., [32]). However, in crowdfunding communities where individuals are often motivated by prosocial goals (e.g., [50]), we propose that such affiliation can negatively affect behavior. When individual actions benefit a social cause, seeing affiliated others participate may make individuals feel less of a need to do so, a process referred to as ""vicarious moral licensing"" (e.g., [13]; [22]; [37]). Thus, we propose that when backers decide whether to fund an idea, they are less likely to do so or more likely to fund a lower amount if affiliated others have already done so.While affiliations develop in the community through cobacking, creators and backers also engage through nonmonetary actions, thereby driving social interaction. Therefore, to develop further substantive implications about the effect of affiliation, we examine the moderating role of both creator and backer engagement (e.g., [ 4]; [35]). For example, creators communicate with backers through the description of the idea on its homepage, perceived to be an important determinant of an idea's success ([38]; [64]), and by posting updates about progress. Backers engage with the community by sharing ideas on social media. We aim to understand how the effect of affiliation varies due to creator and backer engagement, as they help shed light on the underlying mechanism that drives the effect of affiliation.We use multiple methods and data sets, including secondary data and experiments, to provide convergent validity to our findings. We also conduct interviews with 6 backers, survey 100 backers, and analyze 572 posts on backer forums to develop insights about the mechanism driving the effect of affiliation on funding outcomes. First, we assemble a comprehensive data set of daily funding for 2,021 new crowdfunded ideas listed on Kickstarter. We study two crowdfunding outcomes: ( 1) the monetary amount of funding received by an idea on any given day and ( 2) whether the idea raises sufficient funds during the funding window to meet or exceed its funding goal. We measure affiliation of an idea on the focal day as the number of cobacking relationships of backers who back on the focal day, with backers who funded until the day before the focal day (e.g., [35]; [41]). We estimate an instrumental variables regression model with fixed effects to assess the impact of affiliation among an idea's backers on the daily funding amount and report results from several robustness analyses. Second, we present results from controlled experiments, where we exogenously manipulate affiliation, and across three experiments, we replicate the negative effect of affiliation on funding, examine the underlying mechanism, and uncover the role of a key moderator. We find that the negative effect is stronger when creators use more communal words—both in the initial description of the idea and in subsequent updates—and when more backers share the idea on social media. Thus, creator and backer engagement may moderate prosocial motives to fund, further validating the proposed licensing mechanism.We make several contributions. We are the first to show that affiliation among backers affects crowdfunding success in statistically and economically significant ways after controlling for herding and accounting for several alternative explanations. A 10% daily increase in number of backers would lead to an additional 20.2% in funding or an increase of US$83/day (i.e., the herding effect). In contrast, a 10% daily increase in backer affiliation would lead to an 8.7% decrease in funding or a decrease of US$36/day, offsetting the increase due to number of backers by about 43%. Thus, adding backers is good, but if the additional backers increase affiliation, the positive effect of adding these backers is smaller in the scenario where affiliation is high. We isolate vicarious moral licensing as a theoretical mechanism that drives the negative effect of affiliation through experiments. We explore the role of factors related to the idea, the creator, and the backers, all of which interact with affiliation. Theoretical Background Social Influences in CrowdfundingAlthough crowdfunding has emerged as a dominant force for funding new ideas, research on crowdfunding is limited. Most early research focuses on microlending ([34]; [66]) or on crowdfunding platforms for music and journalism (e.g., [ 1]; [ 8]). Topics such as proximity to the deadline ([12]) and the text of content (e.g., [42]) have also garnered attention. Researchers have studied a variety of social factors that influence crowdfunding, in particular, the relationship between creators and individual backers, including the role of offline friendship ([34]), geographic proximity ([ 1]), and social interactions ([28]). We present a summary of representative research in Table 1.GraphTable 1. Comparison with Relevant Empirical Research. References (Published)Dependent VariablesExplanatory VariablesEmpirical Model FeaturesData ContextExperimentsFindingsOur researchFundingAffiliationMediator: vicarious moral licensingModerators: creator and backer engagementFixed-effects log-linear regressionEndogeneity (instrument)Robustness checks: probit, logit, and TobitCrowdfunding(Kickstarter, experiments)Three experimentsPrior backer affiliation decreases funding; the negative effect is due to vicarious moral licensing. This effect is stronger for ideas with communal descriptions, more communal updates, and more backer sharing on social media.Wei, Hong, and Tellis (2021)Success of fundingSimilarity between ideasNetwork similarityBinary regressionCrowdfunding(Kickstarter)NoPrior success of similar ideas affects success. Funding performance increases as an idea's novel and imitative characteristics are balanced. The optimal funding level is closer to the level of similar ideas.Netzer, Lemaire, and Herzenstein (2019)Loan paybackLoan descriptionText analyticsBinary regressionCrowdlending(Prosper)NoBorrowers who use certain types of words are more likely to default.Dai and Zhang (2019)Funding time elapsedGoing past deadlineProsocial motivationCreator is individualRegression continuityCrowdfunding(Kickstarter)NoBackers might be driven by prosocial motives around deadline following goal pursuit.Kim et al. (2020)Goal completionForward lookingSocial interactionsBayesian IJC methodTwo-stepCounterfactualsCrowdfunding (music)(Sellaband)NoForward-looking investment behavior as well as contemporaneous and forward-looking social interactions impact share purchases and goal completion.Burtch et al. (2013)ContributionsConcealmentSocial normsTobitEndogeneity (IV)Crowdfunding(Data set undisclosed)NoConcealment hurts the likelihood of contribution and contribution. Social norms drive concealment.Agrawal, Catalini, and Goldfarb (2015)Decision to investGeographyLinear regressionFixed effectsCrowdfunding (music)(Sellaband)NoLocal backers are not influenced by artist. The effect does not persist past the first investment, indicating the role of search but not monitoring.Burtch et al. (2013)Contribution frequencyCrowdingFunding windowDegree of exposureLog-linear regressionEndogeneity (GMM)Crowdfunding (journalism)(Data set undisclosed)NoPartial crowding-out effect. Backers experience lower marginal utility of giving as the funds become less relevant to the recipient. The funding window and degree of exposure have a positive effect, after publication of the story.Lin, Prabhala, and Viswanathan (2013)Interest rate, default rateFriendshipsProbit regressionCrowdlending(Prosper)NoOnline friendships act as signals of credit quality, increase the probability of funding, lower interest rates, and result in lower ex post default rates—gradation in effects based on roles and identities of friends.Zhang and Liu (2012)Loan amountsCrowdingHazard modelFixed effectsFirst differencesCrowdlending(Prosper)NoWell-funded borrowers attract more funding. Lenders learn from peer decisions and do not mimic. 1 Notes: IV = instrumental variable, GMM = generalized method of moments; IJC = [26].In addition to the relationship between creators and backers, there are several ways in which others' actions might inform backers' funding decisions. For example, [66] report that potential lenders assess borrowers' creditworthiness by observing other lenders. They attribute the positive effect of the number of other lenders to herding, wherein crowd size becomes a beacon for others to decide which ideas to fund. This finding might suggest that the mere addition of more supporters unilaterally benefits crowdfunding outcomes as potential backers simply follow other backers. What are some factors that might limit the positive impact of the crowd's behavior on crowdfunding? To answer this question, we note that most research has considered the presence of the anonymous crowd as the cause for a social effect that is generally positive. However, crowd size does not account for an important aspect of networks (i.e., the structure of connections among the community's participants).Thus, what is missing in extant research is an explicit acknowledgment of social structure beyond crowd size and an exploration of how it impacts crowdfunding outcomes. Social structure arises due to coparticipation in events, in our case, cobacking across ideas, a phenomenon referred to as affiliation (e.g., [17]; [60]). Affiliation, identified as an important phenomenon in the new digital economy dominated by crowdsharing ([15]), is the central focus of our research. Affiliation in CrowdfundingCommunities evolve through repeated interactions between members, which give rise to affiliations or overlaps. As affiliations grow, the interconnectivity among backers leads to scaffolding structures that hold the community together through both first- and second-order ties. Affiliations have been studied in interfirm relationships ([58]), board interlocks ([52]), product development (e.g., [35]), and wiki contributions ([48]). Regardless of the context, research suggests that ( 1) individuals notice affiliated others' behavior, ( 2) individuals feel a sense of connectedness and shared identity with affiliated others, and as such, ( 3) affiliated others' actions lead to changes in one's subsequent behavior (e.g., [35]; [57]).To establish that participants notice affiliated others' behavior when visiting crowdfunding platforms, we ran a pilot study with actual backers prescreened on the basis of their prior crowdfunding behavior. Participants were shown a screenshot of a crowdfunding page created by a web designer. To assess which information captured participants' attention, we used a standard heat-mapping approach for measuring visual attention ([ 5]). Invisible boxes around various pieces of information (e.g., idea title, backer information, idea description) coded visual attention as participants read and clicked on information, as per instructions. We found that many participants read and clicked on backer information, more so than other potentially relevant information such as the number of shares and creator information. Further, of the available backer information, affiliation ranked as highly important (for details, see Web Appendix A). Discussions on crowdfunding message boards and websites, as well as results from a survey that we conducted (discussed in the following sections), further support this idea, suggesting that among all available information, backers do consider affiliated others' behavior as they make funding decisions. Next, to confirm that affiliation affects perceptions of connectedness and shared identity in crowdfunding communities, we ran a pilot study with 150 Amazon Mechanical Turk (MTurk) participants. We find that affiliation significantly increased perceptions of connectedness and shared identity with other backers (see Web Appendix B).If potential backers notice affiliated others' behaviors and feel a sense of connectedness with these affiliated others, how might affiliated others' behavior influence their own funding decisions? To answer this important question, we next examine crowdfunding platforms and how they differ from noncommunal (i.e., transactional) contexts. Vicarious Moral Licensing in Crowdfunding CommunitiesCrowdfunding is a communal endeavor in which individuals collaborate to achieve shared goals, and platforms grow due to members' participation and interactions ([50]). Individuals behave differently in communal contexts than in noncommunal (i.e., transactional) contexts (e.g., [10]; [18]). For example, in communal contexts, individuals are more likely to request help from others, keep track of others' needs, respond to them, and report more positive emotions while doing so ([18]). In crowdfunding, these prosocial goals are reflected in the desire to help others, achieve funding goals, and be part of a community ([19]).In crowdfunding communities where individuals are often motivated by prosocial goals (e.g., [50]), we propose that seeing affiliated others fund may make individuals feel less of a need to do so, a process referred to as vicarious moral licensing (e.g., [13]; [22]; [37]). Vicarious moral licensing occurs when individuals see affiliated others' actions as satisfying their own goals, which changes their perceived moral imperative and subsequent behavior. For example, learning that affiliated others demonstrate environmentally friendly behavior makes individuals less likely to do so ([37]). It is important to recognize that this effect is not merely akin to strangers' behavior in a crowd (i.e., the bystander effect; e.g., [31]), but that it is those with whom an individual perceives a social connection (i.e., affiliation) that drives the focal effect.To confirm the importance of affiliated others' behavior and further validate the proposed mechanism, we conducted in-depth interviews of 6 backers, surveyed 100 backers, and coded 572 posts from KickstarterForum.org, the dominant crowdfunding discussion forum (see Web Appendix C). The findings confirmed the prominence of prosocial (i.e., communal) motives on crowdfunding decisions, the importance of affiliated others' behavior on backers' own funding behavior, and the role of vicarious moral licensing. For example, as one interview participant explained, ""I look at other funders only to further discover related projects. It's an interesting way to discover—because some people are more involved than you are.... It's interesting to follow that rabbit trail and see, 'Oh, this person supported this, and look at what else they fund.'"" Another stated, ""You are dealing with finite resources in terms of what you are willing to spend. If you support one thing, I don't know, for me, if I see someone supporting something else, I think, well yeah, they supported that. I'm sure I could find a bunch of other people that support a bunch of other things. I just gave X amount of dollars, whatever amount I have, and I'm not going to be giving any more than that right now.""Although we propose vicarious moral licensing as the mechanism underlying the focal effect of affiliation and initial evidence indicates this to be the case, we acknowledge the complexity of social interactions in crowdfunding. Because these social interactions are likely to be subject to several factors, we consider uniqueness as an alternative explanation for the negative effect of affiliation on funding. Backers may try to identify ideas that have received less funding from affiliated others. By doing so, backers can distinguish themselves from these affiliated others, fulfilling a need for uniqueness (e.g., [59]). In our analysis, we report results from an experiment where we test vicarious moral licensing and uniqueness as potential explanations for the negative effect of affiliation on funding. Moderators of the Effect of Affiliation on Crowdfunding SuccessCrowdfunding platforms are characterized by contributions from both creators and backers (e.g., [ 4]; [48]) as these interactions create and sustain the community's viability. Therefore, we explore the role of creator and backer engagement in moderating the impact of affiliation on crowdfunding.Previous research has found that while prosocial goals may be common in crowdfunding platforms (e.g., [50]), an idea's description can further induce prosocial motivation and behavior when it emphasizes communal language ([23]). We propose that the vicarious moral licensing effect (i.e., the negative effect of affiliation on funding) is driven by the communal context and the prosocial behavior it prompts and that this behavior is further heightened by creators describing their ideas with communal words like ""together"" and asking backers to ""partner"" with them by providing financial ""support"" ([47]). As such, ideas described with more (vs. less) communal words will exhibit a stronger negative effect of affiliation on funding outcomes.Creators can also engage with the backer community by posting updates to highlight their strategic goals and the idea's progress. Updates provide diagnostic information concerning an idea's success (e.g., [ 4]; [35]). Updates might draw backers' attention to the idea's characteristics and evolution, and lessen attention toward cobackers and affiliation. Consistent with the vicarious moral licensing mechanism, we expect updates that use more (vs. less) communal words to strengthen affiliation's negative effect. As such, we estimate the moderating effects of communal words in the creator's updates.We also explore how backers' engagement might moderate the affiliation effect by exploring the role of social media sharing of the focal idea by backers. While sharing behavior on social media could have several motivations, altruism is perceived as a primary motivator, and others seeing the shares likely view them as such ([29]; [33]). We expect that such sharing heightens funders' prosocial motives and vicarious moral licensing, further strengthening the negative effect of affiliation. Next, we describe our data and methodology. Data and MethodologyWe employed a multimethod approach to investigate the phenomenon. We collected and analyzed two types of data: observational data from a crowdfunding platform and experimental data from lab settings. We begin by describing the observational data, the empirical model and identification strategy, the results, and robustness checks. Then, we describe three experiments in which we identify the primary effect in a controlled setting and shed light on the mechanism underlying the primary effect and its moderator. The first experiment demonstrates the negative effect of affiliation on funding behavior. The second experiment validates vicarious moral licensing as an underlying mechanism and rules out uniqueness as one potential alternative explanation. The third experiment examines how the idea's description moderates the effect of affiliation. Collection and Analysis of Observational DataWe collected data on Kickstarter, the world's largest and most prominent crowdfunding platform. We utilized a web crawler to visit the new ideas page listed on Kickstarter beginning December 18, 2013. From that day and every subsequent day of data collection, the crawler visited the pages of the ideas that were started on the first day of the crawl, in addition to all the ideas that were started on the subsequent days. We stopped the crawler after 37 days, giving us data on 2,021 new ideas. We acknowledge that our research's funding constraints affected the number of days, but we went one week past the most common deadline of 30 days. We note that while some ideas in our sample received funding after data collection stopped, our results are robust to this truncation.[ 7]For the data collection, the crawler began with ideas that started receiving funds on the day of the crawl, and it identified every backer who funded the focal idea, the funded amount, and the calendar date. The crawler then visited every backer's history and collected information on all the other ideas that the backer had funded in the past. At the time of data collection, Kickstarter made all backers visible to all prospective backers. The list of backers on Kickstarter was available by clicking the ""community"" link that prominently appears on the focal idea's web page.[ 8] This process allowed us to construct the network, giving us the structure of relationships to calculate affiliation. The crawler also collected other relevant information from the page, including the idea's description, number and text of updates, and the number of Facebook shares of the idea to measure backer engagement.Our unit of analysis for the daily amount funded is an idea-day, and our final sample had 32,438 observations at the idea-day level. This specification makes the most sense because, for a data set with idea-day-backer as the unit of analysis, the funded amount (for an idea on a day) takes zero values for over 99.9% of observations, making such a specification noninformative. Next, we describe the key measures. Daily amount fundedConsistent with prior literature ([ 1]; [ 8]), our funding success measure is the amount of funding received by an idea on any given day. Across all crowdfunding platforms, this measure is always easily and prominently visible on the idea's webpage. Subsequently, we show that our results are robust to other measures of success. AffiliationConsistent with prior literature (e.g., [35]; [41]), we posit that two backers are affiliated if they have funded at least one common idea on the platform and are not affiliated if all the ideas that they have funded are mutually exclusive. Thus, the backer affiliation for a focal idea on a focal day is the number of cobacking relationships between those backers who fund the focal idea on the focal day and all backers who have funded the focal idea at any time before the focal day.Consider a backer of a focal idea who funds the focal idea on the focal day. Consider another backer of the focal idea who funds the focal idea any time before the focal day. A cobacking relationship exists between these two backers if they have both funded one idea (other than the focal idea) any time before the focal day. One cobacking relationship represents one unit of affiliation. Affiliation increases both with the number of backers who coback and with the number of cobacked ideas.To elaborate, consider the following examples. In each example, idea i is launched on day t = 1, say December 13. Further, Jack funds idea i on December 17 (t = 5), and the goal is to calculate affiliation as of December 17 (t = 5).Example 1: Tom funds idea i on December 13. Also, before December 17, Jack and Tom both fund another idea j. As there is one cobacking relationship (that between Jack and Tom for cobacking idea j), Affiliationi, t = 5 = 1.Example 2: Tom funds idea i on December 13. Jack and Jill both fund idea i on December 17. Before December 17, Jack and Tom both fund another idea j. Furthermore, before December 17, Jill and Tom both fund another idea k. As there are two cobacking relationships (those between Jack and Tom for cobacking idea j, and between Jill and Tom for cobacking idea k), Affiliationi, t = 5 = 2.Example 3: Tom funds idea i on December 13. Jane funds idea i on December 14. Before December 17, Tom, Jack, and Jane funded another idea j. As there are two cobacking relationships (those between Jack and Tom for cobacking idea j, and between Jack and Jane for cobacking idea j), Affiliationi, t = 5 = 2.We present summary statistics for Kickstarter in Table 2. The median number of backers who fund an idea in a day is 1, and most ideas have only a few backers. When a backer funds an idea, the median number of past backers of that idea is 10 backers (i.e., the median of the variable cumulative number of backers funding idea i before day t is 10). In the six months preceding data collection, 82% of backers in our data had not funded any idea on Kickstarter. Thus, the odds of having to remember multiple cobacking relationships are relatively low. Most importantly, the median value of affiliation is zero, and the mean is 3.3. In other words, a large majority of backers in our data must process a very small amount of information to infer affiliation. Our measure of affiliation reflects a more nuanced and disaggregated conceptualization of affiliations than the number of ""cobacked ideas"" or the number of ""common backers."" Other measures are likely sparser than our measure. Subsequently, we show that our results are robust to alternate measures of affiliation.GraphTable 2. Summary Statistics for Kickstarter. VariableMeanSDMinMedianMaxAmount of funding of idea i (in $) on day t409.344,764.7200593,731Backer affiliation of idea i by day t  −  1 (Affilit  −  1)3.3310.6600477Cumulative number of backers funding idea i by day t − 1 (CumBackersit − 1)53.85357.8701017,018Number of backers funding idea i on day t − 1 (Backersit − 1)6.71142.990117,010Cumulative number of updates by creator of idea i by day t − 1 (CumUpdatesit − 1)8.8223.6400524Proportion of funding goal of idea i achieved by day t − 1 (PropGoalit − 1).492.140.13132.63Proportion of funding duration of idea i completed by day t − 1 (PropDurationit − 1).31.240.27.97Closeness centrality of idea i as of day t − 15.67 × 10 − 95.01 × 10 − 83.49 × 10 − 112.50 × 10 − 102.2 × 10 − 6Betweenness centrality of idea i as of day t − 11,258.078,698.4900335,213Eigenvector centrality of idea i as of day t − 1.002.0302.50 × 10 − 91Last week (1 if day t is in the last week of funding of idea i, 0 otherwise).08.28001Cumulative number of communal words in updates by creator of idea i by day t − 1 (CommunalUpdatesit − 1)20.171,696.2700230,232  UpdatesWe measure the creator's engagement using the number of creator's updates on the idea page and separately measure the level of communal content in each update. To code communal words, we created a dictionary to capture words that reflect the use of communal language. For this, we asked two graduate research assistants to read descriptions of a random sample of 100 ideas (from our data) and identify words that reflected a ""communal"" idea while coding each description on whether it was communal. We provided the Merriam-Webster definition of ""communal"" (""of or relating to a community"") to the two coders along with synonyms from a thesaurus. Then, we cross-verified these words with LIWC's category for ""affiliation,"" comprising 248 words ([46]). Communal words that appear at least once in our corpus are member, team, group, groups, family, friends, affiliation, affiliate, relation, connection, alliance, relationship, partner, partners, partnership, link, merge, cooperate, cooperation, together, join, thanks, thank you, appreciate, our, and we. We measure backer engagement as the number of Facebook shares of the idea by backers, which we collected when the web crawler visited an idea's webpage. Model-free evidenceTo explore model-free evidence, we present summary statistics about three regimes of the distribution of the amount of daily funding achieved for Kickstarter in Table 3: ( 1) idea-day-specific observations when there is no funding, ( 2) when the daily funding is positive but does not exceed the mean level in the data ($409.34), and ( 3) when the daily funding exceeds the mean level. Backer affiliation is highest when ideas do not receive any funding and lowest when ideas achieve the highest funding. The measure of backer affiliation for an idea on a given day is based on cobacking relationships of backers that fund the idea on that specific day with backers who funded before that day. If no backer funds on a specific day, the affiliation measure for that day is zero. The measure is not cumulative, and it does not increase over time. Thus, there is model-free evidence for the negative effect of affiliation on funding outcomes. We collected similar data from another crowdfunding platform, Indiegogo, which we use in the robustness analysis. Additional details about the Kickstarter data and summary statistics for the Indiegogo data appear in Web Appendix D. We illustrate affiliation in Figures W1–W5 and the sample's network structure and growth in Figures W6–W8 in Web Appendix E. We estimate the primary empirical model on Kickstarter data.GraphTable 3. Means of Backer Affiliation and Other Time-Varying Covariates at Different Levels of Daily Funding (Kickstarter). VariableAmount Funded (yit) = 0Amount Funded 0 < (yit) ≤ $409.34Amount Funded (yit) > $409.34Number of observations17,50511,0713,863Proportion of all observations53.96%34.13%11.91%Amount of funding of idea i (in $) in day t0109.293,214.89Backer affiliation of idea i by day t − 1 (Affilit − 1)4.222.261.79Cumulative number of backers funding idea i by day t − 1(CumBackersit − 1)22.6544.23222.89Cumulative number of updates by creator of idea i by day t − 1 (CumUpdatesit − 1)6.2110.5415.76Proportion of funding goal of idea i achieved by day t − 1 (PropGoalit − 1).26.531.69Proportion of funding duration of idea i completed by day t − 1 (PropDurationit − 1).34.29.27Closeness centrality of idea i as of day t − 15.68 × 10−94.56 × 10−99.49 × 10−9Betweenness centrality of idea i as of day t − 1473.991,178.215,974.58Eigenvector centrality of idea i as of day t − 1.001.000.012Last week (1 if day t is in the last week of funding of idea i, 0 otherwise).10.07.06Cumulative number of communal words in updates by creator of idea i by day t − 1 (CommunalUpdatesit − 1).691.93199.38  Empirical modelFollowing [ 8] and [66], our primary dependent variable (yit) is the monetary funding received by an idea i (i = 1,...N) on day t (t = 1,...Ti). As a starting point, we incorporate backer affiliation and several controls in a fixed-effects regression model as follows: log(yit)=αi+αt+β1log(Affilit–1)+β2log(CumBackersit–1)+β3log(CumUpdatesit–1)+β4log(yit–1)+β5PropGoalit–1+β6PropDurationit–1+β7LastWeekit–1+β8Networkit–1+β9log(CommunalUpdatesit–1)+β10log(Affilit–1)×CommunalUpdatesit–1+β11log(Affilit–1)×FBSharesi+eit. Graph( 1)To account for nonnegativity, we log-transform all variables that are not proportions. For variables that can take zero values, we take the logarithm of the variable added to.001. Replacing this constant with other constants does not affect our results. Estimating the model without taking logarithms of any variable gave us consistent results.To control idea-specific confounding factors such as inherent differences in idea quality, the novelty of idea description, creator expertise, and so on, we employ idea-specific fixed effects αi, a vector of 2,021 elements for the Kickstarter data set. We incorporate fixed effects for each day in the idea's funding window to control temporal patterns in funding and changes in the Kickstarter environment over time. These are denoted by the vector αt. Error terms are assumed normally distributed and clustered at the idea level.Our key independent variable is Affilit − 1. This is the number of cobacking relationships between those backers who fund idea i on day t − 1 and all backers who have funded this idea before day t − 1. Subsequently, we report robustness checks to alternate measures of backer affiliation. Although our fixed-effects specification controls for confounds at the idea level and the day level, we need to control idea-specific factors that are time varying. Chief among these is the amount of funding received by the focal idea on day t − 1 ([ 8]), enabling us to control those time-varying idea-specific unobservables, which may be serially correlated (e.g., word of mouth about the idea) and to attenuate serial correlation among the residuals. This also accounts for the alternate explanation that affiliation on day t − 1 affects funding on day t − 1, but not on day t. By incorporating the lagged measure of funding, we can account for all factors that affect funding until the day t − 1.We next discuss other time-varying idea-specific controls. First, the number of affiliations among backers is correlated with the number of backers. There can be no backer affiliations without backers; more backers could result in more possibilities for affiliation. To control for the possibility that the number of backers drives the effect of affiliation on funding, we include CumBackersit − 1, the cumulative number of backers funding idea i by day t − 1, as a control variable. Also, to the extent that ideas with more backers attract more funding ([66]), this serves as a measure for herding behavior. Second, creators communicate with backers via updates, a means to elevate idea visibility and signal effort ([12]). To understand how creator actions might drive funding, we include CumUpdatesit − 1, the cumulative number of updates by the creator of the idea i by day t − 1. In addition, CommunalUpdatesit − 1 is the number of communal words contained in the updates.Third, the funding window of an idea influences its funding outcomes. Ideas receive more funding in the later stages of the funding window as the funding deadline nears (e.g., [12]; [31]). To account for this, we include the duration of the funding window completed for the idea (PropDurationit − 1) as a proportion of the total funding window (typically 30 days). Furthermore, ideas receive greater funding as they get closer to meeting their funding goals ([12]). Although daily fixed effects account for temporal variations in funding, they might not capture the effect of proximity to the funding goal. Therefore, we include PropGoalit − 1, the proportion of the funding goal of the idea that has been achieved until day t − 1, and LastWeekit − 1, a dummy variable for whether the observation belongs to the last week of the funding window.Finally, structural measures of network centrality might affect the outcome. Because these measures capture the extent of social capital that accrues to ideas due to being associated with certain backers, we want to control for the effects of these measures. We compute and include three of the most widely used network measures in marketing (e.g., [35]; [48]; [58]), (Networkit − 1): closeness centrality, betweenness centrality, and eigenvector centrality of idea i on day t. Closeness centrality in our context is how close the focal idea is from all the backers (connected and not connected) in the network, betweenness centrality is the extent to which the focal idea lies on the common paths between all pairs of backers in the network, and eigenvector centrality is the extent to which the focal idea's backers are prolific in backing other ideas. We computed both bipartite and single-mode network variants of each of these measures.[ 9] Given the high correlation across the bipartite and single-mode versions of each measure, we included in the model the version of each measure that leads to a more significant improvement in R2. As shown in the correlation matrix of all variables (Table 4), these variables are not highly correlated with our measure of affiliation, suggesting that affiliation captures the network's unique structural properties based on counts of overlaps. To assess interaction effects, we interact affiliation with CommunalUpdatesit − 1[10] and with the number of Facebook shares of the idea by backers (FBSharesi).GraphTable 4. Pairwise Correlation Coefficients of All Variables (Kickstarter). Variable12345678910111. Amount of funding of (in $) in t12. Backer affiliation i by t − 1−.0713. Cum. number of backers funding by t − 1.13.1414. Cum. number of updates by creator i by t − 1.13.15.1815. Amount of funding (in $) in t − 1.49−.11.17.1516. Proportion of funding goal achieved by t − 1.15.00.12.20.1917. Proportion of funding duration completed by t − 1−.11.37.05.30−.19.0518. Closeness centrality as of t − 1.01−.07−.00−.04.14−.01−.1219. Betweenness centrality i as of t − 1.09.37.23.29.07.07.32−.07110. Eigenvector centrality as of t − 1.05.04.33.09.07.04.00.07.09111. Last week (1 if day t is in the last week of funding)−.05.14.02.15−.05.05.61.03.01.00112. Cum. no. of communal words in updates by t − 1.02.02.26.01.03.00−.00.01.00.09−.00 2 Notes: We take logarithms of all variables, which are not proportions. For variables that can take zero values, we take the logarithm of the variable added to.001. All variables pertain to the focal idea. Coefficients with p < .05 are in boldface.We use lags of all covariates because information about the focal day is not updated in real time and is unavailable until the following day.[11] We present the correlation matrix of all variables in Table 4; most correlations are less than.3, allaying multicollinearity's ill effects. Empirical strategyWe first discuss how our work is different from the peer effects literature and then explain our identification strategy. The prototypical problem in the marketing literature on the identification of peer effects (e.g., [40]) is to estimate the likelihood of agent A adopting a product (e.g., buying an online game) under the knowledge that agent B (a self-identified ""friend"" or influencer) has already adopted that same product. The herding literature has conclusively documented positive peer effects across various consumer contexts (e.g., [57]; [66]). If there are positive effects due to the size of the crowd or the number of peers, that would be equivalent to herding, not our study's primary focus. In other words, our main objective is not to estimate how backer A will fund the focal idea if another backer B has previously funded it. We account for herding in our model by incorporating the prior number of backers of the focal idea as a control. Instead, our objective is to study the effect of affiliation, which is formed when two backers back an idea that is not the focal idea. Affiliation arises in collaborative contexts (e.g., board interlocks, product development teams) rather than common product purchases. We note that this is a key difference of our article from other contexts. In addition, our interest is less in modeling agent behavior (e.g., an individual's rating of a product in [57]) than in modeling product success (i.e., funding success of an idea). We next address three main issues that could confound identifying the causal effect of affiliation on the focal idea's funding. Correlated unobservablesIdea-specific characteristics that are not observable to the researcher could be correlated with our affiliation measure and affect the focal idea's funding. Perhaps highly affiliated backers are attracted to ideas with high (or low) unobserved quality. The inability to control for quality dimensions might induce an upward (or downward) bias in our estimate of the effect of affiliation. Following [40] and [57], we incorporate idea-specific fixed effects. These effectively control for all idea-specific factors that might be correlated with affiliation. Next, there could be time-varying factors across the funding window that might be correlated with affiliation and funding. For example, affiliation and funding are both likely to be low in the first few days of funding. We control for all day-specific trends by incorporating day fixed effects. Finally, the presence of idea-specific time-varying factors cannot be ruled out. We control for funding received by the focal idea on day t − 1. As mentioned previously, this approach enables us to control those time-varying idea-specific unobservables, which may be serially correlated, and to attenuate serial correlation among the residuals. SimultaneityIn the context of peer influence, simultaneity implies that not only can the influencer influence the focal individual but the focal individual could also affect the influencer's actions, leading to an upward bias in the estimate of peer effects. In our context, affiliations formed on a focal day may affect the focal idea's funding. Simultaneously, the focal idea's funding on a focal day also affects affiliation formation on that day. Following recent literature (e.g., [45]), we use the lagged measures of affiliation in the model. While affiliation before the focal day can affect funding on the focal day, the reverse is not possible. Endogenous group formation (or homophily)Backers with similar preferences may be more likely to behave similarly. In such a scenario, the effect of prior affiliation on subsequent funding of the focal idea might manifest these common preferences. The literature on consumer peer effects has used consumer-specific fixed effects to deal with this. However, crowdfunding is different from consumer contexts in that while consumers buy (and evaluate) several products, backers typically fund very few ideas on a platform.Moreover, unlike crowdfunding, consumer contexts generally focus on the individual more than collective action ([50]). So, backer-specific fixed effects are econometrically infeasible to estimate for both the researcher and the platform. Instead, we first include the cumulative number of backers and several other network measures as controls. Next, we note that controlling for lagged funding of the idea also controls backer characteristics that have affected funding before the focal day.Finally, we include an instrument for affiliation. If our measure of affiliation is correlated with the error term in Equation 1, its coefficient could be biased. In our primary analysis, we use an observed instrument to estimate a two-stage least-squares instrumental variable regression model. As [49], p. 4) mentions, the ideal solution for endogeneity is to conduct an experiment where the endogenous variable is uncorrelated with the construction's dependent variable. Therefore, we ran controlled experiments, which we explain subsequently, where participants were randomly assigned to different affiliation levels, creating exogenous variation.For the primary instrumental variable approach, we follow recent research (e.g., [20]; [51]) that uses instruments based on agent behavior in categories (or firms) different from the focal category (or a firm). Following this approach, we use the mean (across ideas) of affiliations on day t − 1 of all ideas in our Kickstarter data, which are in a category different from that of the focal idea as the primary instrument for Affilit − 1 in Kickstarter. For example, for an observation about an idea on movies on December 22, this instrument is the mean of affiliations on December 22 of all ideas in our data that are not in the movies category. This instrument is correlated with Affilit − 1 (correlation = .16).Conceptually, this instrument is appealing because of the interdependencies across different parts of the global affiliation network on Kickstarter (i.e., the affiliation network across all ideas seeking funding concurrently), thus satisfying the relevance criterion. However, because most backers only back one idea (i.e., affiliation is sparse), the mean affiliation across ideas in other categories is very unlikely to be related to the unobserved component of the focal idea's funding outcome in Equation 1 providing the basis for identification. Further, a category-level measure of affiliation should remain unaffected by idea-level factors, especially if the idea is from a different category. A category-level measure should not correlate strongly with idea-day-level idiosyncratic shocks from another category, thus meeting the exclusion criterion. The first-stage equation is specified as log(Affilit–1)=λ0+λ1IVit–1+λ2log(CumBackersit–1)+λ3log(CumUpdatesit–1)+λ4log(yit–1)+λ5PropGoalit–1+λ6PropDurationit–1+λ7LastWeekit–1+λ8Networkit–1+λ9log(CommunalUpdatesit–1)+δit–1. Graph( 2)The R2 for the first stage regression without the instrument (i.e., assuming that λ1 = 0) is.365 and with the instrument is.385, showing that the instrument's addition improves the in-sample model fit. The estimate of λ1 is.42 (p < .01). The corresponding F-statistic for the F-test of excluded instruments is 879.83, far exceeding the threshold value of 10 ([55], p. 522). The large value of the Anderson–Rubin statistic (F( 1, 28,300) = 298.41) rejects the null hypothesis that the instrument is weak. We show in robustness analyses that the estimates are consistent across the use of alternative instruments. We also instrument for the interaction of affiliation and the number of communal words contained in the updates (CommunalUpdatesit − 1). Following [44], the instrument for this interaction variable is the interaction of the instrument for affiliation and CommunalUpdatesit − 1. We do not instrument for the interaction of affiliation and the number of Facebook shares, because the Durbin–Wu–Hausman test of the hypothesis that this regressor is exogenous could not be rejected (χ2 = .055, p > .1). Furthermore, the sharing activity of a specific idea on a social media platform other than Kickstarter is conceptually independent of its funding outcome on Kickstarter. ResultsFirst, we present the parameter estimates of the instrumental variable regression models estimated on the Kickstarter data and then discuss robustness checks. We present estimates of five models, with and without instruments, and the sequential addition of interactions in Table 5. M1–M4 do not have interaction effects, and while M1 ignores endogeneity, M2, M3, and M4 correct for it and show that the results are robust to different instruments. The results from the full model specified in Equation 1 are reported in M5, which we discuss next.GraphTable 5. Coefficient Estimates of the Fixed-Effects Regression Model of Daily Funding of Ideas on Kickstarter. VariableM1M2M3M4M5 (Final Model)Backer affiliation of idea i by day t − 1 (Affilit − 1)−.04***(.01)−.86***(.06)−1.88***(.41)−.80***(.06)−.87***(.06)Cumulative number of backers funding idea i by day t − 1 (CumBackersit − 1).15**(.06)1.92***(.15)4.13***(.90)1.79***(.15)2.02***(.16)Cumulative number of updates by creator of idea i by day t − 1 (CumUpdatesit− 1)−.05**(.02)−.06**(.03)−.07*(.04)−.06**(.03)−.05*(.03)Amount of funding of idea i (in $) on day t − 1−.03(.02).01(.02).07**(.03).01(.02).01(.02)Proportion of funding goal of idea i achieved by day t − 1−.09(.06)−.23***(.07)−.39**(.10)−.22**(.07)−.17**(.07)Proportion of funding duration of idea i completed by day t − 1−.27(.99)1.12(1.10)2.84*(1.65)1.02(1.09).63(1.10)Closeness centrality of idea i as of day t − 12.54(3.12)2.43(3.54)8.46(5.30)1.99(3.48)2.71(3.54)Betweenness centrality of idea i as of day t − 1−.02**(.01)−.02**(.01)−.03*(.02)−.02**(.01)−.02*(.01)Eigenvector centrality of idea i as of day t − 1−.74(.82)−4.39*(2.33)−8.95*(4.66)−4.13*(2.21)−4.40*(2.67)Last week (1 if day t is in the last week of funding of idea i, 0 otherwise).25(.21).13(.24).03(.35).14(.24).09(.25)Cumulative number of communal words in updates by creator of idea i by day t − 1 (CommunalUpdatesit − 1)−.04(.03)−.02(.03)−.01(.04)−.02(.03)−.29*(.15)Interactions of Backer AffiliationsAffilit − 1 × CommunalUpdatesit − 1−7.68**(3.80)Affilit − 1 × Number of Facebook shares of idea i−.006***(.001)Fixed effects for each idea iYesYesYesYesYesFixed effects for each day tYesYesYesYesYesInstrument for Affilit − 1NoYesConstraOtherYes 3 *p < .10.4 **p < .05.5 ***p < .01.6 Notes:""Constra"" refers to [ 7] measure of constraint of the focal idea. ""Other"" instrument refers to the instrument constructed from Indiegogo data. Full model resultsWe find that affiliation among backers has a consistent negative effect on the funding of ideas on Kickstarter (β = −.87, p < .01). This effect persists despite the inclusion of idea-specific fixed effects, daily fixed effects, controlling for lagged funding, and the prior number of backers of the idea. We corroborate extant findings on herding (e.g., [66]) and additionally show that affiliation plays a key role and that its effect is negative.Concerning the moderators, the creator's engagement measured as using communal words in updates further strengthens the negative effect of affiliation, perhaps because of a heightened licensing effect (β = −7.68, p < .01). For backer engagement, we find the negative effect of affiliation is stronger as backer engagement, measured as the number of Facebook shares of the idea by backers, increases (β = −.006, p < .01). One explanation of this is that while individuals share on Facebook for various motives, the primary motivation is prosocial, and others seeing the shares likely see them as such, strengthening the vicarious moral licensing effect ([29]).Concerning control variables, the greater the number of backers of an idea before the focal day, a measure of herding, the more funding the idea will attract on the focal day (β = 2.02, p < .01). This indicates that the total number of backers for an idea may act as a signal of its quality or potential worthiness, a finding that is consistent with prior research (e.g., [34]). The current research replicates this effect and demonstrates that social structure influences behavior beyond the herding effect. Moreover, this theory supports our contention that affiliation, measured by cobacking, drives the negative effect, not herding. We also find that the total number of updates posted by the creator has a negative effect on crowdfunding success (β = −.05, p < .10), although this effect is not significant across all model specifications. The effect of the proportion of the funding goal which was achieved on the previous day is negative (β = −.17, p < .05), perhaps suggesting a preference to fund underfunded ideas. For network centrality measures, we find that betweenness (β = −.02, p < .05) and eigenvector centrality (β = −4.40, p < .05) have a negative effect on funding. The negative effects of these second-order network measures, compared with the positive effect of number of backers (proxy for first-order network effect), highlight the complexity in flow of information on the network and are consistent with findings from prior studies (e.g., [35]). This is perhaps because these measures indicate the backers' ability to identify and fund salient opportunities, or access to information from their overall networks about idea quality based on indirect ties across the whole network, not just direct ties. Thus, the effects also highlight the importance of distinguishing direct and indirect aspects of how networks operate in community contexts.To ensure that outliers are not driving our results, we estimate the main model (M5) after dropping the top 10th percentile of observations (which have affiliation values greater than 7), yielding a significant and negative estimate of the affiliation coefficient (β = −.87, p < .01). We find a similar negative effect in models estimated on various subsets of the data. To investigate if specific categories of ideas drive our results, we estimate the model separately for each category's ideas. We find a negative effect of affiliation for 11 out of 12 categories, with the most negative effect of affiliation in the ideas from the photography and technology categories. Our estimate of affiliation's effect is negative but not statistically significant for the ""dance"" category, which accounts for just 21 out of 2,021 ideas in our data. Robustness analysesWe conducted several robustness analyses. First, we estimated the model on Indiegogo data; the results are quite consistent (see Web Appendix F). Second, we estimated the model on Kickstarter data using three alternative sets of instruments: discrete latent instrumental variables, an instrument constructed using affiliation from another platform, and a network-based instrument (see Web Appendix G). Third, we estimated probit, logit, and Tobit models of funding success and checked the robustness of our results to two alternative measures of affiliation (Web Appendix H). All analyses show that our results are robust. Next, we report three experiments in which we probe the effect of affiliation, the underlying process, and a moderating factor to further validate our empirical model. Collection and Analysis of Experimental DataIn the first experiment, we demonstrate the negative effect of affiliation in a controlled experimental setting. In the second experiment, we validate vicarious moral licensing as an underlying mechanism and rule out uniqueness as one potential alternative explanation. In the third experiment, we show how the idea's description might moderate the effect of affiliation. Experiment 1We conducted Experiment 1 on MTurk with 200 North American residents[12] (Mage = 35.26 years; 49.8% women; 42.6% have previously funded a crowdfunding idea). We presented participants with two ideas seeking funding (both real ideas from Kickstarter; see Web Appendix I). First, participants saw a screenshot of a website created by a graphic designer to look like an idea page on a real crowdfunding platform (e.g., [ 8]; [65]).Consistent with prior research, participants were given money beyond study payment, creating an incentive-compatible dependent measure ([21]; [39]). Participants were told, ""As part of this study, you will receive a $2 bonus. You can use some or all of this money to fund this project."" They were then asked how much they would give toward the idea on a nine-point scale with dollar amounts in $.25 intervals, ranging from $0 to $2.00. If participants chose ""$0"" and opted to keep the full bonus, they were then forwarded to the end of the survey and were paid the original MTurk fee as well as the $2 bonus. If participants used any of their bonus to fund the first idea, they were included in our primary analyses. Ninety-three participants opted not to fund the first idea, leaving us with 107 participants. Four participants were removed who indicated that they had a child affected by autism, the focus of one of the two ideas, and were inclined toward funding but would opt to put the money toward helping their child. All participants completed the dependent measures. Two participants were removed for spending less than a second on the manipulation, leaving us with 101 participants. Participants then saw a screenshot of a second website designed to look like an idea on a crowdfunding platform (for details, see Web Appendix I).The screenshot included idea information and a list of recent backers shown on the screen's right side. Participants in the high-affiliation condition saw a high overlap in the number of backers across the two ideas. Participants in the control-affiliation condition saw the same number of backers, but the names on the two lists did not overlap. A manipulation check confirmed the effectiveness of the manipulation. All participants who funded the first idea were told that they would receive an additional $2 bonus to keep or use to fund the second idea. Their decision on a nine-point scale ranging from $0 to $2.00 in $.25 intervals served as the outcome. At the end of the study, participants were given the money that they chose to keep as a bonus, and the remainder (i.e., what they chose to fund each of the ideas) was put toward each crowdfunding idea. Finally, participants responded to a set of demographic measures (e.g., age, gender, whether they had previously funded an idea on a crowdfunding platform). A one-way analysis of variance showed a significant effect of affiliation on the funding of the second idea (F( 1, 99) = 4.05, p < .05). As we expected, those in the high-affiliation condition funded less than those in the control-affiliation condition (Mhigh = 4.27, SD = 2.27 vs. Mcontrol = 5.27, SD = 2.70). Of the $2 bonus, those in the high-affiliation condition chose to fund $.82 toward the focal idea, while those in the control-affiliation condition chose to fund $1.07, on average.The first experiment confirmed the negative effect of affiliation in the lab setting, validating our primary empirical finding that affiliation negatively affects crowdfunding success. Experiment 2In the second experiment, we measured two potential mediators in an attempt to document ""a"" mediating process (i.e., the mediating process given our stimuli and procedures) as opposed to ""the"" mediating process (i.e., a single mediating process that is operative across all crowdfunding contexts; e.g., [ 6]). We propose vicarious moral licensing as a mechanism for the negative impact of affiliation on funding and test need for uniqueness as an alternative mechanism ([59]).We conducted the study on MTurk with 228 North American residents (Mage = 39.57 years; 54.4% women; 38.2% had previously funded an idea on an online crowdfunding platform). All participants spent adequate time on the manipulation. Three participants did not complete the dependent measures, resulting in an effective sample of 225 participants. Participants were told to imagine that they had $50 and were asked to choose one idea to fund from a set of four real ideas seeking funding on Kickstarter and across categories (e.g., technology, nonprofits, arts/film); details appear in Web Appendix I. After this decision, they read about a second idea that they were told is seeking funding. Those in the high-affiliation condition were told that many of the backers who funded the first idea they chose also funded the focal idea. Those in the control affiliation condition were provided no information about other backers' funding decisions. A pretest confirmed the effectiveness of the manipulation (see Web Appendix I). Next, participants responded to two items to capture vicarious moral licensing (""Based on the funding behavior of cobackers, I do not feel the need to fund [focal idea]"" and ""Based on the funding behavior of cobackers, I do not feel obligated to fund [focal idea]""; M = 4.10, SD = 1.48; r = .72) and two items to capture uniqueness (""If I funded [focal idea], my decision to fund would say a lot about me as a unique individual"" and ""If I funded [focal idea], it would help me stand out from the crowd""; M = 3.68, SD = 1.45; r = .81).Next, we asked participants how much money they would pledge toward funding the subsequent focal idea (range: $0–$5,000, the total needed to hit the focal idea's funding goal). Consistent with prior research and our empirical model, we log-transformed funding ([36]). Finally, participants completed demographic questions.As expected, we found a negative effect of affiliation on funding (F( 1, 223) = 4.29, p < .04) such that those in the high-affiliation condition reported a lower funding amount than those in the control condition (Mhigh = 3.17, SD = 2.49 vs. Mcontrol = 3.82, SD = 2.24) or in raw numbers (Mhigh = $256.96, SD = $674.40 vs. Mcontrol = $339.88, SD = $875.90). A one-way analysis of variance showed a significant effect of affiliation on the licensing measure (F( 1, 223) = 3.89, p = .05). As we expected, those in the high-affiliation condition agreed more with the licensing measure, indicating less need to fund than those in the control condition (Mhigh = 4.29, SD = 1.57 vs. Mcontrol = 3.90, SD = 1.37). However, there was no significant effect of affiliation on uniqueness (Mhigh = 3.56, SD = 1.52 vs. Mcontrol = 3.80, SD = 1.36; F( 1, 223) = 1.53, p = .22). We then assessed the indirect effects of the two mediators on funding. The results indicate that licensing was a significant mediator (95% confidence interval does not include 0: [−.4423, −.0003]), but uniqueness was not (95% confidence interval: [−.5479,.1142]).In this experiment, we replicated the negative effect of affiliation and uncovered vicarious moral licensing as an underlying mechanism. Although we did not find an effect of affiliation on uniqueness in this study, we note that uniqueness may operate more strongly for some ideas and some individuals, providing an interesting avenue for future research on crowdfunding ([59]). Experiment 3In Experiment 3, we explored the role of a moderator: how the creator describes the idea. We theorized that the negative effect of affiliation occurs in a crowdfunding context, at least partly due to its communal nature and how the ideas are presented to potential backers. We conducted the third experiment on MTurk with 206 North American residents (Mage = 38.81 years; 46.1% women; 42.2% have previously funded an idea on an online crowdfunding platform). All participants completed the dependent measures. Three participants who spent less than one second reading the manipulation were removed, resulting in N = 203. We manipulated two factors between participants: ( 1) affiliation (high vs. control) and ( 2) idea description (more vs. less communal).As in Experiment 2, participants read about an idea currently seeking funding on Kickstarter and were told to imagine that they had funded this idea (see Web Appendix I). We used the same manipulation of affiliation as in Experiment 2. Those in the high-affiliation condition were told that many backers who funded the first idea they chose also funded the focal idea. Those in the control-affiliation condition were not provided any information about other backers' funding decisions. Participants then read about diveLIVE, a technology that allows divers to talk underwater while streaming live video to the internet. diveLIVE, the focal idea, was described as more or less communal with small changes (e.g., ""Let's learn about the oceans"" vs. ""This product uses technology to take videos of the oceans"").Next, participants indicated how much money they would pledge toward diveLIVE, the focal idea (range: $0–$20,000, the total needed to hit the focal idea's funding goal). Consistent with prior research, our empirical model, and Experiment 2, we log-transformed funding ([36]) for analysis but provide results in raw numbers for ease of interpretation. Finally, participants completed demographic questions.We found evidence for a main effect of idea description (F( 1, 199) = 13.86, p < .01) consistent with prior research, which finds that ideas described as more communal tend to be more successful than those described as an investment opportunity ([ 3]). More importantly, we found an interaction between the two manipulated factors (F( 1, 199) = 5.84, p < .02). As we expected, when the idea was described as more communal, those in the high-affiliation condition reported lower funding than those in the control-affiliation condition (Mhigh = $2,155.32, SD = $3,998.08 vs. Mcontrol = $4,073.04, SD = $5,316.08; t(199) = 2.09, p < .04). When the idea was described as less communal, there was no effect of affiliation on funding (Mhigh = $2,572.33, SD = $4,933.01 vs. Mcontrol = $1,868.68, SD = $4,039.03; t(199) = −1.34, p = .18; see Figure W9 in Web Appendix I). The third experiment established that the negative effect of affiliation is stronger when creator's use more communal words in the description of the idea. Validating Moderation with Observational DataAs discussed previously, we find a negative moderating effect of the number of communal words in updates posted by creators. To validate the third experiment with converging evidence, we returned to our secondary data to examine how the number of communal words in the idea description influenced the relationship between affiliation and funding behavior across thousands of crowdfunding ideas (e.g., [42]). This would establish how the use of communal words in creator's updates as well as in the idea's description would influence the effect of backer affiliation and highlight the importance of the communal mechanism. We used the same text dictionary that we created for coding communal words in updates and coded the description of every idea in our sample. The median number of communal words in an idea description is 3 (M = 6.1). We then created two subsets of our data based on a median split of the number of communal words used in describing the idea. We estimated the model separately on each subset and find that the coefficient of affiliation is less negative for ideas described using three or fewer communal words (M = −.92, SE = .09) than for ideas described using four or more communal words (M = −1.25, SE = .15). Replacing the number of communal words in this analysis with the ratio of the number of communal words to the total number of words does not affect this result, nor does splitting the data on the basis of the average number of communal words instead of the median. Finally, the effect of affiliation is less negative for ideas with no communal words than for ideas with at least one communal word. This provides real-world evidence for the role of idea description on the relationship between affiliation and funding behavior, validating our theory and experimental evidence.In summary, these findings further support our reasoning that the negative effect of affiliation is driven, at least in part, by the communal nature of crowdfunding and the prosocial mindset that it prompts ([50]). When an idea is described as more communal, these prosocial goals are exacerbated, leading potential backers to feel that they do not need to fund the idea because these affiliated others are funding it (e.g., [37]). However, when an idea is described as less communal, this effect is mitigated. Next, we discuss our results and develop implications for theory and practice. DiscussionWe establish a negative effect of affiliation on the crowdfunding success of ideas using a large empirical study and then validating the effect through experiments. We provide preliminary insights into the role of vicarious moral licensing as the underlying mechanism for this effect and investigate the moderating role of creator and backer engagement. The licensing effect and its role in reducing backers' perceived obligation to fund ideas could make backers less likely to fund or fund with less money if they opt to fund, both of which could explain the negative effect at the idea level. We begin with a focus on the novel contribution of our finding concerning affiliation, discuss the economic implications of our results, and identify the primary contributions of our research and how it paves the way for future research.The negative effect of affiliation among backers in crowdfunding is distinct from and in addition to the positive effect of herding due to the crowd's size shown in prior research (e.g., [66]). We establish an inherent tension between the positive effect of crowd size and the negative effect of backer affiliation in crowdfunding. Thus, we show that, in addition to relying on crowd size, backers make inferences based on the behavior of affiliated others in a crowdfunding context. A 10% daily increase in number of backers leads to an additional 20.2% in funding or an increase of US$83/day (i.e., the herding effect). In contrast, a 10% daily increase in backer affiliation leads to an 8.7% decrease in funding or a decrease of US$36/day, offsetting the increase due to number of backers by 43%. Our results concerning affiliation are both statistically and economically meaningful and highlight the need to recognize the tension between increasing the number of backers and limiting the ill effects of affiliation.Interestingly, Kickstarter stopped disclosing the prior backers' list on an idea's page as of the time of writing this article. This policy change is consistent with our results. If backer identities remain unknown, potential backers cannot infer affiliation, and therefore ideas cannot be negatively impacted by backer affiliation. Other crowdfunding platforms should reevaluate disclosure policies about past backers of an idea or perhaps reconsider whom they show at the top of their backer lists.So how might creators mitigate the negative effects of affiliation? The moderation effects from our results provide actionable insights for creators seeking crowdfunding from potential backers and considering what platforms to pursue. Our results concerning the interaction between affiliation and creator engagement show that creators can subdue the negative effects of affiliation by carefully crafting the idea description and updates, avoiding communal language.Further, while it appears that encouraging backers to share the idea on social media might be counterproductive because it strengthens affiliation's negative effect, the impact is small and should not be a major concern. The change in the marginal effect of affiliation as sharing by backers increases is small, indicating that change in backers' engagement, while statistically significant, does not have a meaningful effect on crowdfunding. Doubling the number of Facebook shares from its mean of 79 to 148 strengthens the negative effect of affiliation by.42% and translates to a decline of US$1.72/day.We developed recommendations for creators and examples of best practices from our data set (see Table 6). For example, creators should focus on the idea's inherent purpose and objective value in its description and avoid using too much communal language (e.g., cooperate, partner, support) in the idea description and updates. Overall, we recommend that platforms educate creators on how best to structure communication with backers and guide creators in meeting their goals. Backers could perhaps learn to interpret such updates better and use the information provided by the backer to qualify what they infer from the community.GraphTable 6. Actionable Outcomes for Managers Recommendations for Idea Descriptions and Updates. FindingRecommendations for CreatorsExamples from Kickstarter DataInteraction between affiliation and communal words in idea descriptionFocus on the idea's inherent purpose as opposed to a focus on community.The Drone PocketIdea Description: ""The world's first multicopter that'spowerful enough to carry a high-quality action camera and folds up smaller than a 7 in tablet.""Key technology features outlined prominently on idea's home page.Total Amount Raised: $929,212Pegasus Touch Laser SLA 3D PrinterIdea page includes recent press articles with links that highlight idea's featuresTotal Amount Raised: $819,535Use noncommunal words (e.g., ""you"" vs. ""we"") in idea description.The Floyd Leg""The Floyd Leg gives you the framework to take ownership of your furniture by allowing you to create a table from any flat surface"" (emphasis added)Total Amount Raised: $256,273Avoid thanking backers too much in idea description, as it can make the project appear needy.""First off, I want to say thanks for checking out of project. Every single person that takes the time to look at our project means the world to us.""Do not describe idea with overemphasis on communal language (e.g., ""support,"" ""team"").""As we approach Thanksgiving, I continue to be thankful for the patience and support that the unsung backers have shown with our team.""Interaction between affiliation and communal words in updatesDo not show too much appreciation via updates for funding as it is progressing.""Thanks to all of you who pledged for this campaign. We really appreciate your continued support.""Minimize communal language (e.g., partner) in updates.""Your first duty as partners with us on this project; should you choose to accept..."" Our results about the mechanism provide insights on how platforms and creators should engage with backers. Research has shown that licensing is a nonconscious effect and can be mitigated by making individuals aware of their behavior ([27]). Particularly in this type of vicarious moral licensing, highlighting individuals' uniqueness and independent identity may also mitigate the negative effect of affiliation on funding ([30]; [37]; [43]). If creators expect high overlap among backers, they could describe their ideas using less communal language, thereby lowering the licensing effect. Our results suggest that vicarious licensing might overwhelm other relevant idea information, potentially leading to suboptimal backer decisions. In line with our findings, backers might, in some cases, pay more attention to signals from affiliated others rather than from the whole crowd.For crowdfunding platforms, our findings provide a rationale for why there might be room for new crowdfunding platforms to thrive and grow. Although several crowdfunding platforms have flourished in the past decade, Kickstarter, Indiegogo, and GoFundMe have arguably dominated the market. Other once-popular platforms, such as Sellaband and PledgeMusic, have failed. Large platforms with millions of backers might pose high entry barriers to new entrants. However, our findings point to one source of competitive advantage for newer platforms: negative affiliation effects are more likely to occur in well-established platforms with large backer communities. Strategically building diverse and unaffiliated communities of backers might confer a competitive advantage to new platforms. Our results show that this can be achieved by expanding the number of categories of ideas, as affiliation's negative effect may be mitigated as backers of ideas across different categories may be less likely to coback ideas. The failure of category-specific platforms such as Sellaband (music), and the relative success of platforms hosting diverse ideas, such as Kickstarter, provides support for this reasoning. Second, platforms allocating marketing resources across existing and new backers (e.g., allocating social media spending across established markets such as Los Angeles and new markets such as Lima) could perhaps view our results as a reason to divert resources away from backer-dense markets. Third, platforms that provide backer information may also want to use algorithms that promote unaffiliated (vs. affiliated) backers, for example, by highlighting first-time backers. Finally, drawing on our results about creator engagement, we recommend that platforms educate creators on how to design better backer communication.Insights from our study are relevant to other types of crowdsourcing platforms as well. For example, participants on LEGO's Ideas, which focuses on ideation, and SeedInvest, which helps raise equity, could mitigate the negative effects of affiliation, for example, by describing initiatives as less communal and by posting updates with less communal language. Our findings are also applicable to crowdfunding contests (e.g., [ 9]; [25]), where participants could be encouraged to vote across categories to reduce coparticipation and help them break away from the adverse effects of groupthink.We highlight several areas of inquiry for future research. Reward structures could impact the role of affiliation in crowdfunding and thus merit attention (e.g., [56]). Fake reviews have been investigated in the online context (e.g., [67]), and it would be interesting to explore the veracity of idea descriptions and creator updates. In addition to affiliation, which we study, other network characteristics such as clans and core–periphery structures ([60]) could explain the nature of information flow across affiliation structures.As interest in crowdfunding increases, interesting research questions continue to emerge. We believe that our research explores important questions concerning crowdfunding that involve backer affiliation and community structure, and we hope to lay the foundation for future studies in the domain. Supplemental Materialsj-pdf-1-jmx-10.1177_00222429211031814 - Supplemental material for Do Backer Affiliations Help or Hurt Crowdfunding Success?Supplemental material, sj-pdf-1-jmx-10.1177_00222429211031814 for Do Backer Affiliations Help or Hurt Crowdfunding Success? by Kelly B. Herd, Girish Mallapragada, and Vishal Narayan in Journal of Marketing  "
6,"Do Backer Affiliations Help or Hurt Crowdfunding Success? Crowdfunding has emerged as a mechanism to raise funds for entrepreneurial ideas. On crowdfunding platforms, backers (i.e., individuals who fund ideas) jointly fund the same idea, leading to affiliations, or overlaps, within the community. The authors find that while an increase in the total number of backers may positively affect funding behavior, the resulting affiliations affect funding negatively. They reason that when affiliated others fund a new idea, individuals may feel less of a need to fund, a process known as ""vicarious moral licensing."" Drawing on data collected from 2,021 ideas on a prominent crowdfunding platform, the authors show that prior affiliation among backers negatively affects an idea's funding amount and eventual funding success. Creator engagement (i.e., idea description and updates) and backer engagement (i.e., Facebook shares) moderate this negative effect. The effect of affiliation is robust across several instrumental variables, model specifications, measures of affiliation, and multiple crowdfunding outcomes. Results from three experiments, a survey, and interviews with backers support the negative effect of affiliation and show that it can be explained by vicarious moral licensing. The authors develop actionable insights for creators to mitigate the negative effects of affiliation with the language used in idea descriptions and updates.Keywords: backer affiliation; crowdfunding; prosocial; social structure; vicarious moral licensingCrowdfunding has emerged as a dominant mechanism to harness the power of crowds in raising funds for innovative ideas. Interest in crowdfunding has surged in recent years. Facebook acquired Oculus 3D visualization device, a crowdfunded idea on Kickstarter, for US$3 billion ([14]). Peloton, the highly successful exercise bike, started as a Kickstarter project. The global crowdfunding market is expected to be well over US$40 billion by 2026 ([54]). Brands such as GE ([11]) and Unilever use crowdfunding to spur innovation ([53]), and academic research on the phenomenon and its role in the digital economy is emerging ([ 2]; [12]). Crowdfunding is a form of crowdsourcing in which participants, hereinafter referred to as ""backers,"" are recruited to raise funds for ideas (e.g., [16]; [62]). As some backers fund the same ideas (i.e., ""cobacking""), overlaps develop between these backers. These overlaps, called ""affiliations,"" are key building blocks of the community's network structure and have been studied in other crowdsourcing communities (e.g., [48]). In this research, we explore how affiliation, defined as the number of cobacking relationships between potential backers and those who have previously funded the focal idea, might affect the idea's crowdfunding success. We illustrate affiliation in crowdfunding using a stylized example in Figure 1.Graph: Figure 1. Illustration of affiliation in crowdfunding.We know that crowd size affects outcomes positively as participants look to anonymous others for cues to decide which ideas to fund, a phenomenon referred to as ""herding"" (e.g., [66]). Previous research shows that attracting more backers positively impacts crowdfunding outcomes ([24]), an insight that many creators seem to grasp. However, crowd size does not represent the social structure (i.e., the pattern of connections in the community). In crowdfunding, as in other contexts in which shared communal goals exist (e.g., Wikipedia), social structure plays a more prominent role (e.g., [48]; [62]).Our primary contribution is in showing that while the total number of backers (i.e., crowd size) may positively affect funding behavior and idea success (e.g., [66]), adding backers may not be unilaterally beneficial as the ensuing affiliation between backers negatively affects funding. Our analysis reveals that the negative effect of backer affiliation is above and beyond the positive effect of number of backers (i.e., the herding effect), highlighting the tension between the benefits of adding more backers and the adverse effects of backer affiliation. In other words, while adding a new backer (e.g., the focal backer in Figure 1) may positively affect the focal idea's success, adding this focal backer may not be equally beneficial across the three scenarios in Figure 1 as the degree of affiliation differs. We propose that the affiliation between the focal backer and other backers will influence the amount that the focal backer puts toward the focal idea and, thus, the idea's funding success.Affiliation is a powerful force because it makes affiliated others' actions lead to changes in one's subsequent behavior (e.g., [35]; [57]). In some contexts, affiliation positively affects behavior as individuals desire to belong and therefore conform to affiliated others' behavior (e.g., [32]). However, in crowdfunding communities where individuals are often motivated by prosocial goals (e.g., [50]), we propose that such affiliation can negatively affect behavior. When individual actions benefit a social cause, seeing affiliated others participate may make individuals feel less of a need to do so, a process referred to as ""vicarious moral licensing"" (e.g., [13]; [22]; [37]). Thus, we propose that when backers decide whether to fund an idea, they are less likely to do so or more likely to fund a lower amount if affiliated others have already done so.While affiliations develop in the community through cobacking, creators and backers also engage through nonmonetary actions, thereby driving social interaction. Therefore, to develop further substantive implications about the effect of affiliation, we examine the moderating role of both creator and backer engagement (e.g., [ 4]; [35]). For example, creators communicate with backers through the description of the idea on its homepage, perceived to be an important determinant of an idea's success ([38]; [64]), and by posting updates about progress. Backers engage with the community by sharing ideas on social media. We aim to understand how the effect of affiliation varies due to creator and backer engagement, as they help shed light on the underlying mechanism that drives the effect of affiliation.We use multiple methods and data sets, including secondary data and experiments, to provide convergent validity to our findings. We also conduct interviews with 6 backers, survey 100 backers, and analyze 572 posts on backer forums to develop insights about the mechanism driving the effect of affiliation on funding outcomes. First, we assemble a comprehensive data set of daily funding for 2,021 new crowdfunded ideas listed on Kickstarter. We study two crowdfunding outcomes: ( 1) the monetary amount of funding received by an idea on any given day and ( 2) whether the idea raises sufficient funds during the funding window to meet or exceed its funding goal. We measure affiliation of an idea on the focal day as the number of cobacking relationships of backers who back on the focal day, with backers who funded until the day before the focal day (e.g., [35]; [41]). We estimate an instrumental variables regression model with fixed effects to assess the impact of affiliation among an idea's backers on the daily funding amount and report results from several robustness analyses. Second, we present results from controlled experiments, where we exogenously manipulate affiliation, and across three experiments, we replicate the negative effect of affiliation on funding, examine the underlying mechanism, and uncover the role of a key moderator. We find that the negative effect is stronger when creators use more communal words—both in the initial description of the idea and in subsequent updates—and when more backers share the idea on social media. Thus, creator and backer engagement may moderate prosocial motives to fund, further validating the proposed licensing mechanism.We make several contributions. We are the first to show that affiliation among backers affects crowdfunding success in statistically and economically significant ways after controlling for herding and accounting for several alternative explanations. A 10% daily increase in number of backers would lead to an additional 20.2% in funding or an increase of US$83/day (i.e., the herding effect). In contrast, a 10% daily increase in backer affiliation would lead to an 8.7% decrease in funding or a decrease of US$36/day, offsetting the increase due to number of backers by about 43%. Thus, adding backers is good, but if the additional backers increase affiliation, the positive effect of adding these backers is smaller in the scenario where affiliation is high. We isolate vicarious moral licensing as a theoretical mechanism that drives the negative effect of affiliation through experiments. We explore the role of factors related to the idea, the creator, and the backers, all of which interact with affiliation. Theoretical Background Social Influences in CrowdfundingAlthough crowdfunding has emerged as a dominant force for funding new ideas, research on crowdfunding is limited. Most early research focuses on microlending ([34]; [66]) or on crowdfunding platforms for music and journalism (e.g., [ 1]; [ 8]). Topics such as proximity to the deadline ([12]) and the text of content (e.g., [42]) have also garnered attention. Researchers have studied a variety of social factors that influence crowdfunding, in particular, the relationship between creators and individual backers, including the role of offline friendship ([34]), geographic proximity ([ 1]), and social interactions ([28]). We present a summary of representative research in Table 1.GraphTable 1. Comparison with Relevant Empirical Research. References (Published)Dependent VariablesExplanatory VariablesEmpirical Model FeaturesData ContextExperimentsFindingsOur researchFundingAffiliationMediator: vicarious moral licensingModerators: creator and backer engagementFixed-effects log-linear regressionEndogeneity (instrument)Robustness checks: probit, logit, and TobitCrowdfunding(Kickstarter, experiments)Three experimentsPrior backer affiliation decreases funding; the negative effect is due to vicarious moral licensing. This effect is stronger for ideas with communal descriptions, more communal updates, and more backer sharing on social media.Wei, Hong, and Tellis (2021)Success of fundingSimilarity between ideasNetwork similarityBinary regressionCrowdfunding(Kickstarter)NoPrior success of similar ideas affects success. Funding performance increases as an idea's novel and imitative characteristics are balanced. The optimal funding level is closer to the level of similar ideas.Netzer, Lemaire, and Herzenstein (2019)Loan paybackLoan descriptionText analyticsBinary regressionCrowdlending(Prosper)NoBorrowers who use certain types of words are more likely to default.Dai and Zhang (2019)Funding time elapsedGoing past deadlineProsocial motivationCreator is individualRegression continuityCrowdfunding(Kickstarter)NoBackers might be driven by prosocial motives around deadline following goal pursuit.Kim et al. (2020)Goal completionForward lookingSocial interactionsBayesian IJC methodTwo-stepCounterfactualsCrowdfunding (music)(Sellaband)NoForward-looking investment behavior as well as contemporaneous and forward-looking social interactions impact share purchases and goal completion.Burtch et al. (2013)ContributionsConcealmentSocial normsTobitEndogeneity (IV)Crowdfunding(Data set undisclosed)NoConcealment hurts the likelihood of contribution and contribution. Social norms drive concealment.Agrawal, Catalini, and Goldfarb (2015)Decision to investGeographyLinear regressionFixed effectsCrowdfunding (music)(Sellaband)NoLocal backers are not influenced by artist. The effect does not persist past the first investment, indicating the role of search but not monitoring.Burtch et al. (2013)Contribution frequencyCrowdingFunding windowDegree of exposureLog-linear regressionEndogeneity (GMM)Crowdfunding (journalism)(Data set undisclosed)NoPartial crowding-out effect. Backers experience lower marginal utility of giving as the funds become less relevant to the recipient. The funding window and degree of exposure have a positive effect, after publication of the story.Lin, Prabhala, and Viswanathan (2013)Interest rate, default rateFriendshipsProbit regressionCrowdlending(Prosper)NoOnline friendships act as signals of credit quality, increase the probability of funding, lower interest rates, and result in lower ex post default rates—gradation in effects based on roles and identities of friends.Zhang and Liu (2012)Loan amountsCrowdingHazard modelFixed effectsFirst differencesCrowdlending(Prosper)NoWell-funded borrowers attract more funding. Lenders learn from peer decisions and do not mimic. 1 Notes: IV = instrumental variable, GMM = generalized method of moments; IJC = [26].In addition to the relationship between creators and backers, there are several ways in which others' actions might inform backers' funding decisions. For example, [66] report that potential lenders assess borrowers' creditworthiness by observing other lenders. They attribute the positive effect of the number of other lenders to herding, wherein crowd size becomes a beacon for others to decide which ideas to fund. This finding might suggest that the mere addition of more supporters unilaterally benefits crowdfunding outcomes as potential backers simply follow other backers. What are some factors that might limit the positive impact of the crowd's behavior on crowdfunding? To answer this question, we note that most research has considered the presence of the anonymous crowd as the cause for a social effect that is generally positive. However, crowd size does not account for an important aspect of networks (i.e., the structure of connections among the community's participants).Thus, what is missing in extant research is an explicit acknowledgment of social structure beyond crowd size and an exploration of how it impacts crowdfunding outcomes. Social structure arises due to coparticipation in events, in our case, cobacking across ideas, a phenomenon referred to as affiliation (e.g., [17]; [60]). Affiliation, identified as an important phenomenon in the new digital economy dominated by crowdsharing ([15]), is the central focus of our research. Affiliation in CrowdfundingCommunities evolve through repeated interactions between members, which give rise to affiliations or overlaps. As affiliations grow, the interconnectivity among backers leads to scaffolding structures that hold the community together through both first- and second-order ties. Affiliations have been studied in interfirm relationships ([58]), board interlocks ([52]), product development (e.g., [35]), and wiki contributions ([48]). Regardless of the context, research suggests that ( 1) individuals notice affiliated others' behavior, ( 2) individuals feel a sense of connectedness and shared identity with affiliated others, and as such, ( 3) affiliated others' actions lead to changes in one's subsequent behavior (e.g., [35]; [57]).To establish that participants notice affiliated others' behavior when visiting crowdfunding platforms, we ran a pilot study with actual backers prescreened on the basis of their prior crowdfunding behavior. Participants were shown a screenshot of a crowdfunding page created by a web designer. To assess which information captured participants' attention, we used a standard heat-mapping approach for measuring visual attention ([ 5]). Invisible boxes around various pieces of information (e.g., idea title, backer information, idea description) coded visual attention as participants read and clicked on information, as per instructions. We found that many participants read and clicked on backer information, more so than other potentially relevant information such as the number of shares and creator information. Further, of the available backer information, affiliation ranked as highly important (for details, see Web Appendix A). Discussions on crowdfunding message boards and websites, as well as results from a survey that we conducted (discussed in the following sections), further support this idea, suggesting that among all available information, backers do consider affiliated others' behavior as they make funding decisions. Next, to confirm that affiliation affects perceptions of connectedness and shared identity in crowdfunding communities, we ran a pilot study with 150 Amazon Mechanical Turk (MTurk) participants. We find that affiliation significantly increased perceptions of connectedness and shared identity with other backers (see Web Appendix B).If potential backers notice affiliated others' behaviors and feel a sense of connectedness with these affiliated others, how might affiliated others' behavior influence their own funding decisions? To answer this important question, we next examine crowdfunding platforms and how they differ from noncommunal (i.e., transactional) contexts. Vicarious Moral Licensing in Crowdfunding CommunitiesCrowdfunding is a communal endeavor in which individuals collaborate to achieve shared goals, and platforms grow due to members' participation and interactions ([50]). Individuals behave differently in communal contexts than in noncommunal (i.e., transactional) contexts (e.g., [10]; [18]). For example, in communal contexts, individuals are more likely to request help from others, keep track of others' needs, respond to them, and report more positive emotions while doing so ([18]). In crowdfunding, these prosocial goals are reflected in the desire to help others, achieve funding goals, and be part of a community ([19]).In crowdfunding communities where individuals are often motivated by prosocial goals (e.g., [50]), we propose that seeing affiliated others fund may make individuals feel less of a need to do so, a process referred to as vicarious moral licensing (e.g., [13]; [22]; [37]). Vicarious moral licensing occurs when individuals see affiliated others' actions as satisfying their own goals, which changes their perceived moral imperative and subsequent behavior. For example, learning that affiliated others demonstrate environmentally friendly behavior makes individuals less likely to do so ([37]). It is important to recognize that this effect is not merely akin to strangers' behavior in a crowd (i.e., the bystander effect; e.g., [31]), but that it is those with whom an individual perceives a social connection (i.e., affiliation) that drives the focal effect.To confirm the importance of affiliated others' behavior and further validate the proposed mechanism, we conducted in-depth interviews of 6 backers, surveyed 100 backers, and coded 572 posts from KickstarterForum.org, the dominant crowdfunding discussion forum (see Web Appendix C). The findings confirmed the prominence of prosocial (i.e., communal) motives on crowdfunding decisions, the importance of affiliated others' behavior on backers' own funding behavior, and the role of vicarious moral licensing. For example, as one interview participant explained, ""I look at other funders only to further discover related projects. It's an interesting way to discover—because some people are more involved than you are.... It's interesting to follow that rabbit trail and see, 'Oh, this person supported this, and look at what else they fund.'"" Another stated, ""You are dealing with finite resources in terms of what you are willing to spend. If you support one thing, I don't know, for me, if I see someone supporting something else, I think, well yeah, they supported that. I'm sure I could find a bunch of other people that support a bunch of other things. I just gave X amount of dollars, whatever amount I have, and I'm not going to be giving any more than that right now.""Although we propose vicarious moral licensing as the mechanism underlying the focal effect of affiliation and initial evidence indicates this to be the case, we acknowledge the complexity of social interactions in crowdfunding. Because these social interactions are likely to be subject to several factors, we consider uniqueness as an alternative explanation for the negative effect of affiliation on funding. Backers may try to identify ideas that have received less funding from affiliated others. By doing so, backers can distinguish themselves from these affiliated others, fulfilling a need for uniqueness (e.g., [59]). In our analysis, we report results from an experiment where we test vicarious moral licensing and uniqueness as potential explanations for the negative effect of affiliation on funding. Moderators of the Effect of Affiliation on Crowdfunding SuccessCrowdfunding platforms are characterized by contributions from both creators and backers (e.g., [ 4]; [48]) as these interactions create and sustain the community's viability. Therefore, we explore the role of creator and backer engagement in moderating the impact of affiliation on crowdfunding.Previous research has found that while prosocial goals may be common in crowdfunding platforms (e.g., [50]), an idea's description can further induce prosocial motivation and behavior when it emphasizes communal language ([23]). We propose that the vicarious moral licensing effect (i.e., the negative effect of affiliation on funding) is driven by the communal context and the prosocial behavior it prompts and that this behavior is further heightened by creators describing their ideas with communal words like ""together"" and asking backers to ""partner"" with them by providing financial ""support"" ([47]). As such, ideas described with more (vs. less) communal words will exhibit a stronger negative effect of affiliation on funding outcomes.Creators can also engage with the backer community by posting updates to highlight their strategic goals and the idea's progress. Updates provide diagnostic information concerning an idea's success (e.g., [ 4]; [35]). Updates might draw backers' attention to the idea's characteristics and evolution, and lessen attention toward cobackers and affiliation. Consistent with the vicarious moral licensing mechanism, we expect updates that use more (vs. less) communal words to strengthen affiliation's negative effect. As such, we estimate the moderating effects of communal words in the creator's updates.We also explore how backers' engagement might moderate the affiliation effect by exploring the role of social media sharing of the focal idea by backers. While sharing behavior on social media could have several motivations, altruism is perceived as a primary motivator, and others seeing the shares likely view them as such ([29]; [33]). We expect that such sharing heightens funders' prosocial motives and vicarious moral licensing, further strengthening the negative effect of affiliation. Next, we describe our data and methodology. Data and MethodologyWe employed a multimethod approach to investigate the phenomenon. We collected and analyzed two types of data: observational data from a crowdfunding platform and experimental data from lab settings. We begin by describing the observational data, the empirical model and identification strategy, the results, and robustness checks. Then, we describe three experiments in which we identify the primary effect in a controlled setting and shed light on the mechanism underlying the primary effect and its moderator. The first experiment demonstrates the negative effect of affiliation on funding behavior. The second experiment validates vicarious moral licensing as an underlying mechanism and rules out uniqueness as one potential alternative explanation. The third experiment examines how the idea's description moderates the effect of affiliation. Collection and Analysis of Observational DataWe collected data on Kickstarter, the world's largest and most prominent crowdfunding platform. We utilized a web crawler to visit the new ideas page listed on Kickstarter beginning December 18, 2013. From that day and every subsequent day of data collection, the crawler visited the pages of the ideas that were started on the first day of the crawl, in addition to all the ideas that were started on the subsequent days. We stopped the crawler after 37 days, giving us data on 2,021 new ideas. We acknowledge that our research's funding constraints affected the number of days, but we went one week past the most common deadline of 30 days. We note that while some ideas in our sample received funding after data collection stopped, our results are robust to this truncation.[ 7]For the data collection, the crawler began with ideas that started receiving funds on the day of the crawl, and it identified every backer who funded the focal idea, the funded amount, and the calendar date. The crawler then visited every backer's history and collected information on all the other ideas that the backer had funded in the past. At the time of data collection, Kickstarter made all backers visible to all prospective backers. The list of backers on Kickstarter was available by clicking the ""community"" link that prominently appears on the focal idea's web page.[ 8] This process allowed us to construct the network, giving us the structure of relationships to calculate affiliation. The crawler also collected other relevant information from the page, including the idea's description, number and text of updates, and the number of Facebook shares of the idea to measure backer engagement.Our unit of analysis for the daily amount funded is an idea-day, and our final sample had 32,438 observations at the idea-day level. This specification makes the most sense because, for a data set with idea-day-backer as the unit of analysis, the funded amount (for an idea on a day) takes zero values for over 99.9% of observations, making such a specification noninformative. Next, we describe the key measures. Daily amount fundedConsistent with prior literature ([ 1]; [ 8]), our funding success measure is the amount of funding received by an idea on any given day. Across all crowdfunding platforms, this measure is always easily and prominently visible on the idea's webpage. Subsequently, we show that our results are robust to other measures of success. AffiliationConsistent with prior literature (e.g., [35]; [41]), we posit that two backers are affiliated if they have funded at least one common idea on the platform and are not affiliated if all the ideas that they have funded are mutually exclusive. Thus, the backer affiliation for a focal idea on a focal day is the number of cobacking relationships between those backers who fund the focal idea on the focal day and all backers who have funded the focal idea at any time before the focal day.Consider a backer of a focal idea who funds the focal idea on the focal day. Consider another backer of the focal idea who funds the focal idea any time before the focal day. A cobacking relationship exists between these two backers if they have both funded one idea (other than the focal idea) any time before the focal day. One cobacking relationship represents one unit of affiliation. Affiliation increases both with the number of backers who coback and with the number of cobacked ideas.To elaborate, consider the following examples. In each example, idea i is launched on day t = 1, say December 13. Further, Jack funds idea i on December 17 (t = 5), and the goal is to calculate affiliation as of December 17 (t = 5).Example 1: Tom funds idea i on December 13. Also, before December 17, Jack and Tom both fund another idea j. As there is one cobacking relationship (that between Jack and Tom for cobacking idea j), Affiliationi, t = 5 = 1.Example 2: Tom funds idea i on December 13. Jack and Jill both fund idea i on December 17. Before December 17, Jack and Tom both fund another idea j. Furthermore, before December 17, Jill and Tom both fund another idea k. As there are two cobacking relationships (those between Jack and Tom for cobacking idea j, and between Jill and Tom for cobacking idea k), Affiliationi, t = 5 = 2.Example 3: Tom funds idea i on December 13. Jane funds idea i on December 14. Before December 17, Tom, Jack, and Jane funded another idea j. As there are two cobacking relationships (those between Jack and Tom for cobacking idea j, and between Jack and Jane for cobacking idea j), Affiliationi, t = 5 = 2.We present summary statistics for Kickstarter in Table 2. The median number of backers who fund an idea in a day is 1, and most ideas have only a few backers. When a backer funds an idea, the median number of past backers of that idea is 10 backers (i.e., the median of the variable cumulative number of backers funding idea i before day t is 10). In the six months preceding data collection, 82% of backers in our data had not funded any idea on Kickstarter. Thus, the odds of having to remember multiple cobacking relationships are relatively low. Most importantly, the median value of affiliation is zero, and the mean is 3.3. In other words, a large majority of backers in our data must process a very small amount of information to infer affiliation. Our measure of affiliation reflects a more nuanced and disaggregated conceptualization of affiliations than the number of ""cobacked ideas"" or the number of ""common backers."" Other measures are likely sparser than our measure. Subsequently, we show that our results are robust to alternate measures of affiliation.GraphTable 2. Summary Statistics for Kickstarter. VariableMeanSDMinMedianMaxAmount of funding of idea i (in $) on day t409.344,764.7200593,731Backer affiliation of idea i by day t  −  1 (Affilit  −  1)3.3310.6600477Cumulative number of backers funding idea i by day t − 1 (CumBackersit − 1)53.85357.8701017,018Number of backers funding idea i on day t − 1 (Backersit − 1)6.71142.990117,010Cumulative number of updates by creator of idea i by day t − 1 (CumUpdatesit − 1)8.8223.6400524Proportion of funding goal of idea i achieved by day t − 1 (PropGoalit − 1).492.140.13132.63Proportion of funding duration of idea i completed by day t − 1 (PropDurationit − 1).31.240.27.97Closeness centrality of idea i as of day t − 15.67 × 10 − 95.01 × 10 − 83.49 × 10 − 112.50 × 10 − 102.2 × 10 − 6Betweenness centrality of idea i as of day t − 11,258.078,698.4900335,213Eigenvector centrality of idea i as of day t − 1.002.0302.50 × 10 − 91Last week (1 if day t is in the last week of funding of idea i, 0 otherwise).08.28001Cumulative number of communal words in updates by creator of idea i by day t − 1 (CommunalUpdatesit − 1)20.171,696.2700230,232  UpdatesWe measure the creator's engagement using the number of creator's updates on the idea page and separately measure the level of communal content in each update. To code communal words, we created a dictionary to capture words that reflect the use of communal language. For this, we asked two graduate research assistants to read descriptions of a random sample of 100 ideas (from our data) and identify words that reflected a ""communal"" idea while coding each description on whether it was communal. We provided the Merriam-Webster definition of ""communal"" (""of or relating to a community"") to the two coders along with synonyms from a thesaurus. Then, we cross-verified these words with LIWC's category for ""affiliation,"" comprising 248 words ([46]). Communal words that appear at least once in our corpus are member, team, group, groups, family, friends, affiliation, affiliate, relation, connection, alliance, relationship, partner, partners, partnership, link, merge, cooperate, cooperation, together, join, thanks, thank you, appreciate, our, and we. We measure backer engagement as the number of Facebook shares of the idea by backers, which we collected when the web crawler visited an idea's webpage. Model-free evidenceTo explore model-free evidence, we present summary statistics about three regimes of the distribution of the amount of daily funding achieved for Kickstarter in Table 3: ( 1) idea-day-specific observations when there is no funding, ( 2) when the daily funding is positive but does not exceed the mean level in the data ($409.34), and ( 3) when the daily funding exceeds the mean level. Backer affiliation is highest when ideas do not receive any funding and lowest when ideas achieve the highest funding. The measure of backer affiliation for an idea on a given day is based on cobacking relationships of backers that fund the idea on that specific day with backers who funded before that day. If no backer funds on a specific day, the affiliation measure for that day is zero. The measure is not cumulative, and it does not increase over time. Thus, there is model-free evidence for the negative effect of affiliation on funding outcomes. We collected similar data from another crowdfunding platform, Indiegogo, which we use in the robustness analysis. Additional details about the Kickstarter data and summary statistics for the Indiegogo data appear in Web Appendix D. We illustrate affiliation in Figures W1–W5 and the sample's network structure and growth in Figures W6–W8 in Web Appendix E. We estimate the primary empirical model on Kickstarter data.GraphTable 3. Means of Backer Affiliation and Other Time-Varying Covariates at Different Levels of Daily Funding (Kickstarter). VariableAmount Funded (yit) = 0Amount Funded 0 < (yit) ≤ $409.34Amount Funded (yit) > $409.34Number of observations17,50511,0713,863Proportion of all observations53.96%34.13%11.91%Amount of funding of idea i (in $) in day t0109.293,214.89Backer affiliation of idea i by day t − 1 (Affilit − 1)4.222.261.79Cumulative number of backers funding idea i by day t − 1(CumBackersit − 1)22.6544.23222.89Cumulative number of updates by creator of idea i by day t − 1 (CumUpdatesit − 1)6.2110.5415.76Proportion of funding goal of idea i achieved by day t − 1 (PropGoalit − 1).26.531.69Proportion of funding duration of idea i completed by day t − 1 (PropDurationit − 1).34.29.27Closeness centrality of idea i as of day t − 15.68 × 10−94.56 × 10−99.49 × 10−9Betweenness centrality of idea i as of day t − 1473.991,178.215,974.58Eigenvector centrality of idea i as of day t − 1.001.000.012Last week (1 if day t is in the last week of funding of idea i, 0 otherwise).10.07.06Cumulative number of communal words in updates by creator of idea i by day t − 1 (CommunalUpdatesit − 1).691.93199.38  Empirical modelFollowing [ 8] and [66], our primary dependent variable (yit) is the monetary funding received by an idea i (i = 1,...N) on day t (t = 1,...Ti). As a starting point, we incorporate backer affiliation and several controls in a fixed-effects regression model as follows: log(yit)=αi+αt+β1log(Affilit–1)+β2log(CumBackersit–1)+β3log(CumUpdatesit–1)+β4log(yit–1)+β5PropGoalit–1+β6PropDurationit–1+β7LastWeekit–1+β8Networkit–1+β9log(CommunalUpdatesit–1)+β10log(Affilit–1)×CommunalUpdatesit–1+β11log(Affilit–1)×FBSharesi+eit. Graph( 1)To account for nonnegativity, we log-transform all variables that are not proportions. For variables that can take zero values, we take the logarithm of the variable added to.001. Replacing this constant with other constants does not affect our results. Estimating the model without taking logarithms of any variable gave us consistent results.To control idea-specific confounding factors such as inherent differences in idea quality, the novelty of idea description, creator expertise, and so on, we employ idea-specific fixed effects αi, a vector of 2,021 elements for the Kickstarter data set. We incorporate fixed effects for each day in the idea's funding window to control temporal patterns in funding and changes in the Kickstarter environment over time. These are denoted by the vector αt. Error terms are assumed normally distributed and clustered at the idea level.Our key independent variable is Affilit − 1. This is the number of cobacking relationships between those backers who fund idea i on day t − 1 and all backers who have funded this idea before day t − 1. Subsequently, we report robustness checks to alternate measures of backer affiliation. Although our fixed-effects specification controls for confounds at the idea level and the day level, we need to control idea-specific factors that are time varying. Chief among these is the amount of funding received by the focal idea on day t − 1 ([ 8]), enabling us to control those time-varying idea-specific unobservables, which may be serially correlated (e.g., word of mouth about the idea) and to attenuate serial correlation among the residuals. This also accounts for the alternate explanation that affiliation on day t − 1 affects funding on day t − 1, but not on day t. By incorporating the lagged measure of funding, we can account for all factors that affect funding until the day t − 1.We next discuss other time-varying idea-specific controls. First, the number of affiliations among backers is correlated with the number of backers. There can be no backer affiliations without backers; more backers could result in more possibilities for affiliation. To control for the possibility that the number of backers drives the effect of affiliation on funding, we include CumBackersit − 1, the cumulative number of backers funding idea i by day t − 1, as a control variable. Also, to the extent that ideas with more backers attract more funding ([66]), this serves as a measure for herding behavior. Second, creators communicate with backers via updates, a means to elevate idea visibility and signal effort ([12]). To understand how creator actions might drive funding, we include CumUpdatesit − 1, the cumulative number of updates by the creator of the idea i by day t − 1. In addition, CommunalUpdatesit − 1 is the number of communal words contained in the updates.Third, the funding window of an idea influences its funding outcomes. Ideas receive more funding in the later stages of the funding window as the funding deadline nears (e.g., [12]; [31]). To account for this, we include the duration of the funding window completed for the idea (PropDurationit − 1) as a proportion of the total funding window (typically 30 days). Furthermore, ideas receive greater funding as they get closer to meeting their funding goals ([12]). Although daily fixed effects account for temporal variations in funding, they might not capture the effect of proximity to the funding goal. Therefore, we include PropGoalit − 1, the proportion of the funding goal of the idea that has been achieved until day t − 1, and LastWeekit − 1, a dummy variable for whether the observation belongs to the last week of the funding window.Finally, structural measures of network centrality might affect the outcome. Because these measures capture the extent of social capital that accrues to ideas due to being associated with certain backers, we want to control for the effects of these measures. We compute and include three of the most widely used network measures in marketing (e.g., [35]; [48]; [58]), (Networkit − 1): closeness centrality, betweenness centrality, and eigenvector centrality of idea i on day t. Closeness centrality in our context is how close the focal idea is from all the backers (connected and not connected) in the network, betweenness centrality is the extent to which the focal idea lies on the common paths between all pairs of backers in the network, and eigenvector centrality is the extent to which the focal idea's backers are prolific in backing other ideas. We computed both bipartite and single-mode network variants of each of these measures.[ 9] Given the high correlation across the bipartite and single-mode versions of each measure, we included in the model the version of each measure that leads to a more significant improvement in R2. As shown in the correlation matrix of all variables (Table 4), these variables are not highly correlated with our measure of affiliation, suggesting that affiliation captures the network's unique structural properties based on counts of overlaps. To assess interaction effects, we interact affiliation with CommunalUpdatesit − 1[10] and with the number of Facebook shares of the idea by backers (FBSharesi).GraphTable 4. Pairwise Correlation Coefficients of All Variables (Kickstarter). Variable12345678910111. Amount of funding of (in $) in t12. Backer affiliation i by t − 1−.0713. Cum. number of backers funding by t − 1.13.1414. Cum. number of updates by creator i by t − 1.13.15.1815. Amount of funding (in $) in t − 1.49−.11.17.1516. Proportion of funding goal achieved by t − 1.15.00.12.20.1917. Proportion of funding duration completed by t − 1−.11.37.05.30−.19.0518. Closeness centrality as of t − 1.01−.07−.00−.04.14−.01−.1219. Betweenness centrality i as of t − 1.09.37.23.29.07.07.32−.07110. Eigenvector centrality as of t − 1.05.04.33.09.07.04.00.07.09111. Last week (1 if day t is in the last week of funding)−.05.14.02.15−.05.05.61.03.01.00112. Cum. no. of communal words in updates by t − 1.02.02.26.01.03.00−.00.01.00.09−.00 2 Notes: We take logarithms of all variables, which are not proportions. For variables that can take zero values, we take the logarithm of the variable added to.001. All variables pertain to the focal idea. Coefficients with p < .05 are in boldface.We use lags of all covariates because information about the focal day is not updated in real time and is unavailable until the following day.[11] We present the correlation matrix of all variables in Table 4; most correlations are less than.3, allaying multicollinearity's ill effects. Empirical strategyWe first discuss how our work is different from the peer effects literature and then explain our identification strategy. The prototypical problem in the marketing literature on the identification of peer effects (e.g., [40]) is to estimate the likelihood of agent A adopting a product (e.g., buying an online game) under the knowledge that agent B (a self-identified ""friend"" or influencer) has already adopted that same product. The herding literature has conclusively documented positive peer effects across various consumer contexts (e.g., [57]; [66]). If there are positive effects due to the size of the crowd or the number of peers, that would be equivalent to herding, not our study's primary focus. In other words, our main objective is not to estimate how backer A will fund the focal idea if another backer B has previously funded it. We account for herding in our model by incorporating the prior number of backers of the focal idea as a control. Instead, our objective is to study the effect of affiliation, which is formed when two backers back an idea that is not the focal idea. Affiliation arises in collaborative contexts (e.g., board interlocks, product development teams) rather than common product purchases. We note that this is a key difference of our article from other contexts. In addition, our interest is less in modeling agent behavior (e.g., an individual's rating of a product in [57]) than in modeling product success (i.e., funding success of an idea). We next address three main issues that could confound identifying the causal effect of affiliation on the focal idea's funding. Correlated unobservablesIdea-specific characteristics that are not observable to the researcher could be correlated with our affiliation measure and affect the focal idea's funding. Perhaps highly affiliated backers are attracted to ideas with high (or low) unobserved quality. The inability to control for quality dimensions might induce an upward (or downward) bias in our estimate of the effect of affiliation. Following [40] and [57], we incorporate idea-specific fixed effects. These effectively control for all idea-specific factors that might be correlated with affiliation. Next, there could be time-varying factors across the funding window that might be correlated with affiliation and funding. For example, affiliation and funding are both likely to be low in the first few days of funding. We control for all day-specific trends by incorporating day fixed effects. Finally, the presence of idea-specific time-varying factors cannot be ruled out. We control for funding received by the focal idea on day t − 1. As mentioned previously, this approach enables us to control those time-varying idea-specific unobservables, which may be serially correlated, and to attenuate serial correlation among the residuals. SimultaneityIn the context of peer influence, simultaneity implies that not only can the influencer influence the focal individual but the focal individual could also affect the influencer's actions, leading to an upward bias in the estimate of peer effects. In our context, affiliations formed on a focal day may affect the focal idea's funding. Simultaneously, the focal idea's funding on a focal day also affects affiliation formation on that day. Following recent literature (e.g., [45]), we use the lagged measures of affiliation in the model. While affiliation before the focal day can affect funding on the focal day, the reverse is not possible. Endogenous group formation (or homophily)Backers with similar preferences may be more likely to behave similarly. In such a scenario, the effect of prior affiliation on subsequent funding of the focal idea might manifest these common preferences. The literature on consumer peer effects has used consumer-specific fixed effects to deal with this. However, crowdfunding is different from consumer contexts in that while consumers buy (and evaluate) several products, backers typically fund very few ideas on a platform.Moreover, unlike crowdfunding, consumer contexts generally focus on the individual more than collective action ([50]). So, backer-specific fixed effects are econometrically infeasible to estimate for both the researcher and the platform. Instead, we first include the cumulative number of backers and several other network measures as controls. Next, we note that controlling for lagged funding of the idea also controls backer characteristics that have affected funding before the focal day.Finally, we include an instrument for affiliation. If our measure of affiliation is correlated with the error term in Equation 1, its coefficient could be biased. In our primary analysis, we use an observed instrument to estimate a two-stage least-squares instrumental variable regression model. As [49], p. 4) mentions, the ideal solution for endogeneity is to conduct an experiment where the endogenous variable is uncorrelated with the construction's dependent variable. Therefore, we ran controlled experiments, which we explain subsequently, where participants were randomly assigned to different affiliation levels, creating exogenous variation.For the primary instrumental variable approach, we follow recent research (e.g., [20]; [51]) that uses instruments based on agent behavior in categories (or firms) different from the focal category (or a firm). Following this approach, we use the mean (across ideas) of affiliations on day t − 1 of all ideas in our Kickstarter data, which are in a category different from that of the focal idea as the primary instrument for Affilit − 1 in Kickstarter. For example, for an observation about an idea on movies on December 22, this instrument is the mean of affiliations on December 22 of all ideas in our data that are not in the movies category. This instrument is correlated with Affilit − 1 (correlation = .16).Conceptually, this instrument is appealing because of the interdependencies across different parts of the global affiliation network on Kickstarter (i.e., the affiliation network across all ideas seeking funding concurrently), thus satisfying the relevance criterion. However, because most backers only back one idea (i.e., affiliation is sparse), the mean affiliation across ideas in other categories is very unlikely to be related to the unobserved component of the focal idea's funding outcome in Equation 1 providing the basis for identification. Further, a category-level measure of affiliation should remain unaffected by idea-level factors, especially if the idea is from a different category. A category-level measure should not correlate strongly with idea-day-level idiosyncratic shocks from another category, thus meeting the exclusion criterion. The first-stage equation is specified as log(Affilit–1)=λ0+λ1IVit–1+λ2log(CumBackersit–1)+λ3log(CumUpdatesit–1)+λ4log(yit–1)+λ5PropGoalit–1+λ6PropDurationit–1+λ7LastWeekit–1+λ8Networkit–1+λ9log(CommunalUpdatesit–1)+δit–1. Graph( 2)The R2 for the first stage regression without the instrument (i.e., assuming that λ1 = 0) is.365 and with the instrument is.385, showing that the instrument's addition improves the in-sample model fit. The estimate of λ1 is.42 (p < .01). The corresponding F-statistic for the F-test of excluded instruments is 879.83, far exceeding the threshold value of 10 ([55], p. 522). The large value of the Anderson–Rubin statistic (F( 1, 28,300) = 298.41) rejects the null hypothesis that the instrument is weak. We show in robustness analyses that the estimates are consistent across the use of alternative instruments. We also instrument for the interaction of affiliation and the number of communal words contained in the updates (CommunalUpdatesit − 1). Following [44], the instrument for this interaction variable is the interaction of the instrument for affiliation and CommunalUpdatesit − 1. We do not instrument for the interaction of affiliation and the number of Facebook shares, because the Durbin–Wu–Hausman test of the hypothesis that this regressor is exogenous could not be rejected (χ2 = .055, p > .1). Furthermore, the sharing activity of a specific idea on a social media platform other than Kickstarter is conceptually independent of its funding outcome on Kickstarter. ResultsFirst, we present the parameter estimates of the instrumental variable regression models estimated on the Kickstarter data and then discuss robustness checks. We present estimates of five models, with and without instruments, and the sequential addition of interactions in Table 5. M1–M4 do not have interaction effects, and while M1 ignores endogeneity, M2, M3, and M4 correct for it and show that the results are robust to different instruments. The results from the full model specified in Equation 1 are reported in M5, which we discuss next.GraphTable 5. Coefficient Estimates of the Fixed-Effects Regression Model of Daily Funding of Ideas on Kickstarter. VariableM1M2M3M4M5 (Final Model)Backer affiliation of idea i by day t − 1 (Affilit − 1)−.04***(.01)−.86***(.06)−1.88***(.41)−.80***(.06)−.87***(.06)Cumulative number of backers funding idea i by day t − 1 (CumBackersit − 1).15**(.06)1.92***(.15)4.13***(.90)1.79***(.15)2.02***(.16)Cumulative number of updates by creator of idea i by day t − 1 (CumUpdatesit− 1)−.05**(.02)−.06**(.03)−.07*(.04)−.06**(.03)−.05*(.03)Amount of funding of idea i (in $) on day t − 1−.03(.02).01(.02).07**(.03).01(.02).01(.02)Proportion of funding goal of idea i achieved by day t − 1−.09(.06)−.23***(.07)−.39**(.10)−.22**(.07)−.17**(.07)Proportion of funding duration of idea i completed by day t − 1−.27(.99)1.12(1.10)2.84*(1.65)1.02(1.09).63(1.10)Closeness centrality of idea i as of day t − 12.54(3.12)2.43(3.54)8.46(5.30)1.99(3.48)2.71(3.54)Betweenness centrality of idea i as of day t − 1−.02**(.01)−.02**(.01)−.03*(.02)−.02**(.01)−.02*(.01)Eigenvector centrality of idea i as of day t − 1−.74(.82)−4.39*(2.33)−8.95*(4.66)−4.13*(2.21)−4.40*(2.67)Last week (1 if day t is in the last week of funding of idea i, 0 otherwise).25(.21).13(.24).03(.35).14(.24).09(.25)Cumulative number of communal words in updates by creator of idea i by day t − 1 (CommunalUpdatesit − 1)−.04(.03)−.02(.03)−.01(.04)−.02(.03)−.29*(.15)Interactions of Backer AffiliationsAffilit − 1 × CommunalUpdatesit − 1−7.68**(3.80)Affilit − 1 × Number of Facebook shares of idea i−.006***(.001)Fixed effects for each idea iYesYesYesYesYesFixed effects for each day tYesYesYesYesYesInstrument for Affilit − 1NoYesConstraOtherYes 3 *p < .10.4 **p < .05.5 ***p < .01.6 Notes:""Constra"" refers to [ 7] measure of constraint of the focal idea. ""Other"" instrument refers to the instrument constructed from Indiegogo data. Full model resultsWe find that affiliation among backers has a consistent negative effect on the funding of ideas on Kickstarter (β = −.87, p < .01). This effect persists despite the inclusion of idea-specific fixed effects, daily fixed effects, controlling for lagged funding, and the prior number of backers of the idea. We corroborate extant findings on herding (e.g., [66]) and additionally show that affiliation plays a key role and that its effect is negative.Concerning the moderators, the creator's engagement measured as using communal words in updates further strengthens the negative effect of affiliation, perhaps because of a heightened licensing effect (β = −7.68, p < .01). For backer engagement, we find the negative effect of affiliation is stronger as backer engagement, measured as the number of Facebook shares of the idea by backers, increases (β = −.006, p < .01). One explanation of this is that while individuals share on Facebook for various motives, the primary motivation is prosocial, and others seeing the shares likely see them as such, strengthening the vicarious moral licensing effect ([29]).Concerning control variables, the greater the number of backers of an idea before the focal day, a measure of herding, the more funding the idea will attract on the focal day (β = 2.02, p < .01). This indicates that the total number of backers for an idea may act as a signal of its quality or potential worthiness, a finding that is consistent with prior research (e.g., [34]). The current research replicates this effect and demonstrates that social structure influences behavior beyond the herding effect. Moreover, this theory supports our contention that affiliation, measured by cobacking, drives the negative effect, not herding. We also find that the total number of updates posted by the creator has a negative effect on crowdfunding success (β = −.05, p < .10), although this effect is not significant across all model specifications. The effect of the proportion of the funding goal which was achieved on the previous day is negative (β = −.17, p < .05), perhaps suggesting a preference to fund underfunded ideas. For network centrality measures, we find that betweenness (β = −.02, p < .05) and eigenvector centrality (β = −4.40, p < .05) have a negative effect on funding. The negative effects of these second-order network measures, compared with the positive effect of number of backers (proxy for first-order network effect), highlight the complexity in flow of information on the network and are consistent with findings from prior studies (e.g., [35]). This is perhaps because these measures indicate the backers' ability to identify and fund salient opportunities, or access to information from their overall networks about idea quality based on indirect ties across the whole network, not just direct ties. Thus, the effects also highlight the importance of distinguishing direct and indirect aspects of how networks operate in community contexts.To ensure that outliers are not driving our results, we estimate the main model (M5) after dropping the top 10th percentile of observations (which have affiliation values greater than 7), yielding a significant and negative estimate of the affiliation coefficient (β = −.87, p < .01). We find a similar negative effect in models estimated on various subsets of the data. To investigate if specific categories of ideas drive our results, we estimate the model separately for each category's ideas. We find a negative effect of affiliation for 11 out of 12 categories, with the most negative effect of affiliation in the ideas from the photography and technology categories. Our estimate of affiliation's effect is negative but not statistically significant for the ""dance"" category, which accounts for just 21 out of 2,021 ideas in our data. Robustness analysesWe conducted several robustness analyses. First, we estimated the model on Indiegogo data; the results are quite consistent (see Web Appendix F). Second, we estimated the model on Kickstarter data using three alternative sets of instruments: discrete latent instrumental variables, an instrument constructed using affiliation from another platform, and a network-based instrument (see Web Appendix G). Third, we estimated probit, logit, and Tobit models of funding success and checked the robustness of our results to two alternative measures of affiliation (Web Appendix H). All analyses show that our results are robust. Next, we report three experiments in which we probe the effect of affiliation, the underlying process, and a moderating factor to further validate our empirical model. Collection and Analysis of Experimental DataIn the first experiment, we demonstrate the negative effect of affiliation in a controlled experimental setting. In the second experiment, we validate vicarious moral licensing as an underlying mechanism and rule out uniqueness as one potential alternative explanation. In the third experiment, we show how the idea's description might moderate the effect of affiliation. Experiment 1We conducted Experiment 1 on MTurk with 200 North American residents[12] (Mage = 35.26 years; 49.8% women; 42.6% have previously funded a crowdfunding idea). We presented participants with two ideas seeking funding (both real ideas from Kickstarter; see Web Appendix I). First, participants saw a screenshot of a website created by a graphic designer to look like an idea page on a real crowdfunding platform (e.g., [ 8]; [65]).Consistent with prior research, participants were given money beyond study payment, creating an incentive-compatible dependent measure ([21]; [39]). Participants were told, ""As part of this study, you will receive a $2 bonus. You can use some or all of this money to fund this project."" They were then asked how much they would give toward the idea on a nine-point scale with dollar amounts in $.25 intervals, ranging from $0 to $2.00. If participants chose ""$0"" and opted to keep the full bonus, they were then forwarded to the end of the survey and were paid the original MTurk fee as well as the $2 bonus. If participants used any of their bonus to fund the first idea, they were included in our primary analyses. Ninety-three participants opted not to fund the first idea, leaving us with 107 participants. Four participants were removed who indicated that they had a child affected by autism, the focus of one of the two ideas, and were inclined toward funding but would opt to put the money toward helping their child. All participants completed the dependent measures. Two participants were removed for spending less than a second on the manipulation, leaving us with 101 participants. Participants then saw a screenshot of a second website designed to look like an idea on a crowdfunding platform (for details, see Web Appendix I).The screenshot included idea information and a list of recent backers shown on the screen's right side. Participants in the high-affiliation condition saw a high overlap in the number of backers across the two ideas. Participants in the control-affiliation condition saw the same number of backers, but the names on the two lists did not overlap. A manipulation check confirmed the effectiveness of the manipulation. All participants who funded the first idea were told that they would receive an additional $2 bonus to keep or use to fund the second idea. Their decision on a nine-point scale ranging from $0 to $2.00 in $.25 intervals served as the outcome. At the end of the study, participants were given the money that they chose to keep as a bonus, and the remainder (i.e., what they chose to fund each of the ideas) was put toward each crowdfunding idea. Finally, participants responded to a set of demographic measures (e.g., age, gender, whether they had previously funded an idea on a crowdfunding platform). A one-way analysis of variance showed a significant effect of affiliation on the funding of the second idea (F( 1, 99) = 4.05, p < .05). As we expected, those in the high-affiliation condition funded less than those in the control-affiliation condition (Mhigh = 4.27, SD = 2.27 vs. Mcontrol = 5.27, SD = 2.70). Of the $2 bonus, those in the high-affiliation condition chose to fund $.82 toward the focal idea, while those in the control-affiliation condition chose to fund $1.07, on average.The first experiment confirmed the negative effect of affiliation in the lab setting, validating our primary empirical finding that affiliation negatively affects crowdfunding success. Experiment 2In the second experiment, we measured two potential mediators in an attempt to document ""a"" mediating process (i.e., the mediating process given our stimuli and procedures) as opposed to ""the"" mediating process (i.e., a single mediating process that is operative across all crowdfunding contexts; e.g., [ 6]). We propose vicarious moral licensing as a mechanism for the negative impact of affiliation on funding and test need for uniqueness as an alternative mechanism ([59]).We conducted the study on MTurk with 228 North American residents (Mage = 39.57 years; 54.4% women; 38.2% had previously funded an idea on an online crowdfunding platform). All participants spent adequate time on the manipulation. Three participants did not complete the dependent measures, resulting in an effective sample of 225 participants. Participants were told to imagine that they had $50 and were asked to choose one idea to fund from a set of four real ideas seeking funding on Kickstarter and across categories (e.g., technology, nonprofits, arts/film); details appear in Web Appendix I. After this decision, they read about a second idea that they were told is seeking funding. Those in the high-affiliation condition were told that many of the backers who funded the first idea they chose also funded the focal idea. Those in the control affiliation condition were provided no information about other backers' funding decisions. A pretest confirmed the effectiveness of the manipulation (see Web Appendix I). Next, participants responded to two items to capture vicarious moral licensing (""Based on the funding behavior of cobackers, I do not feel the need to fund [focal idea]"" and ""Based on the funding behavior of cobackers, I do not feel obligated to fund [focal idea]""; M = 4.10, SD = 1.48; r = .72) and two items to capture uniqueness (""If I funded [focal idea], my decision to fund would say a lot about me as a unique individual"" and ""If I funded [focal idea], it would help me stand out from the crowd""; M = 3.68, SD = 1.45; r = .81).Next, we asked participants how much money they would pledge toward funding the subsequent focal idea (range: $0–$5,000, the total needed to hit the focal idea's funding goal). Consistent with prior research and our empirical model, we log-transformed funding ([36]). Finally, participants completed demographic questions.As expected, we found a negative effect of affiliation on funding (F( 1, 223) = 4.29, p < .04) such that those in the high-affiliation condition reported a lower funding amount than those in the control condition (Mhigh = 3.17, SD = 2.49 vs. Mcontrol = 3.82, SD = 2.24) or in raw numbers (Mhigh = $256.96, SD = $674.40 vs. Mcontrol = $339.88, SD = $875.90). A one-way analysis of variance showed a significant effect of affiliation on the licensing measure (F( 1, 223) = 3.89, p = .05). As we expected, those in the high-affiliation condition agreed more with the licensing measure, indicating less need to fund than those in the control condition (Mhigh = 4.29, SD = 1.57 vs. Mcontrol = 3.90, SD = 1.37). However, there was no significant effect of affiliation on uniqueness (Mhigh = 3.56, SD = 1.52 vs. Mcontrol = 3.80, SD = 1.36; F( 1, 223) = 1.53, p = .22). We then assessed the indirect effects of the two mediators on funding. The results indicate that licensing was a significant mediator (95% confidence interval does not include 0: [−.4423, −.0003]), but uniqueness was not (95% confidence interval: [−.5479,.1142]).In this experiment, we replicated the negative effect of affiliation and uncovered vicarious moral licensing as an underlying mechanism. Although we did not find an effect of affiliation on uniqueness in this study, we note that uniqueness may operate more strongly for some ideas and some individuals, providing an interesting avenue for future research on crowdfunding ([59]). Experiment 3In Experiment 3, we explored the role of a moderator: how the creator describes the idea. We theorized that the negative effect of affiliation occurs in a crowdfunding context, at least partly due to its communal nature and how the ideas are presented to potential backers. We conducted the third experiment on MTurk with 206 North American residents (Mage = 38.81 years; 46.1% women; 42.2% have previously funded an idea on an online crowdfunding platform). All participants completed the dependent measures. Three participants who spent less than one second reading the manipulation were removed, resulting in N = 203. We manipulated two factors between participants: ( 1) affiliation (high vs. control) and ( 2) idea description (more vs. less communal).As in Experiment 2, participants read about an idea currently seeking funding on Kickstarter and were told to imagine that they had funded this idea (see Web Appendix I). We used the same manipulation of affiliation as in Experiment 2. Those in the high-affiliation condition were told that many backers who funded the first idea they chose also funded the focal idea. Those in the control-affiliation condition were not provided any information about other backers' funding decisions. Participants then read about diveLIVE, a technology that allows divers to talk underwater while streaming live video to the internet. diveLIVE, the focal idea, was described as more or less communal with small changes (e.g., ""Let's learn about the oceans"" vs. ""This product uses technology to take videos of the oceans"").Next, participants indicated how much money they would pledge toward diveLIVE, the focal idea (range: $0–$20,000, the total needed to hit the focal idea's funding goal). Consistent with prior research, our empirical model, and Experiment 2, we log-transformed funding ([36]) for analysis but provide results in raw numbers for ease of interpretation. Finally, participants completed demographic questions.We found evidence for a main effect of idea description (F( 1, 199) = 13.86, p < .01) consistent with prior research, which finds that ideas described as more communal tend to be more successful than those described as an investment opportunity ([ 3]). More importantly, we found an interaction between the two manipulated factors (F( 1, 199) = 5.84, p < .02). As we expected, when the idea was described as more communal, those in the high-affiliation condition reported lower funding than those in the control-affiliation condition (Mhigh = $2,155.32, SD = $3,998.08 vs. Mcontrol = $4,073.04, SD = $5,316.08; t(199) = 2.09, p < .04). When the idea was described as less communal, there was no effect of affiliation on funding (Mhigh = $2,572.33, SD = $4,933.01 vs. Mcontrol = $1,868.68, SD = $4,039.03; t(199) = −1.34, p = .18; see Figure W9 in Web Appendix I). The third experiment established that the negative effect of affiliation is stronger when creator's use more communal words in the description of the idea. Validating Moderation with Observational DataAs discussed previously, we find a negative moderating effect of the number of communal words in updates posted by creators. To validate the third experiment with converging evidence, we returned to our secondary data to examine how the number of communal words in the idea description influenced the relationship between affiliation and funding behavior across thousands of crowdfunding ideas (e.g., [42]). This would establish how the use of communal words in creator's updates as well as in the idea's description would influence the effect of backer affiliation and highlight the importance of the communal mechanism. We used the same text dictionary that we created for coding communal words in updates and coded the description of every idea in our sample. The median number of communal words in an idea description is 3 (M = 6.1). We then created two subsets of our data based on a median split of the number of communal words used in describing the idea. We estimated the model separately on each subset and find that the coefficient of affiliation is less negative for ideas described using three or fewer communal words (M = −.92, SE = .09) than for ideas described using four or more communal words (M = −1.25, SE = .15). Replacing the number of communal words in this analysis with the ratio of the number of communal words to the total number of words does not affect this result, nor does splitting the data on the basis of the average number of communal words instead of the median. Finally, the effect of affiliation is less negative for ideas with no communal words than for ideas with at least one communal word. This provides real-world evidence for the role of idea description on the relationship between affiliation and funding behavior, validating our theory and experimental evidence.In summary, these findings further support our reasoning that the negative effect of affiliation is driven, at least in part, by the communal nature of crowdfunding and the prosocial mindset that it prompts ([50]). When an idea is described as more communal, these prosocial goals are exacerbated, leading potential backers to feel that they do not need to fund the idea because these affiliated others are funding it (e.g., [37]). However, when an idea is described as less communal, this effect is mitigated. Next, we discuss our results and develop implications for theory and practice. DiscussionWe establish a negative effect of affiliation on the crowdfunding success of ideas using a large empirical study and then validating the effect through experiments. We provide preliminary insights into the role of vicarious moral licensing as the underlying mechanism for this effect and investigate the moderating role of creator and backer engagement. The licensing effect and its role in reducing backers' perceived obligation to fund ideas could make backers less likely to fund or fund with less money if they opt to fund, both of which could explain the negative effect at the idea level. We begin with a focus on the novel contribution of our finding concerning affiliation, discuss the economic implications of our results, and identify the primary contributions of our research and how it paves the way for future research.The negative effect of affiliation among backers in crowdfunding is distinct from and in addition to the positive effect of herding due to the crowd's size shown in prior research (e.g., [66]). We establish an inherent tension between the positive effect of crowd size and the negative effect of backer affiliation in crowdfunding. Thus, we show that, in addition to relying on crowd size, backers make inferences based on the behavior of affiliated others in a crowdfunding context. A 10% daily increase in number of backers leads to an additional 20.2% in funding or an increase of US$83/day (i.e., the herding effect). In contrast, a 10% daily increase in backer affiliation leads to an 8.7% decrease in funding or a decrease of US$36/day, offsetting the increase due to number of backers by 43%. Our results concerning affiliation are both statistically and economically meaningful and highlight the need to recognize the tension between increasing the number of backers and limiting the ill effects of affiliation.Interestingly, Kickstarter stopped disclosing the prior backers' list on an idea's page as of the time of writing this article. This policy change is consistent with our results. If backer identities remain unknown, potential backers cannot infer affiliation, and therefore ideas cannot be negatively impacted by backer affiliation. Other crowdfunding platforms should reevaluate disclosure policies about past backers of an idea or perhaps reconsider whom they show at the top of their backer lists.So how might creators mitigate the negative effects of affiliation? The moderation effects from our results provide actionable insights for creators seeking crowdfunding from potential backers and considering what platforms to pursue. Our results concerning the interaction between affiliation and creator engagement show that creators can subdue the negative effects of affiliation by carefully crafting the idea description and updates, avoiding communal language.Further, while it appears that encouraging backers to share the idea on social media might be counterproductive because it strengthens affiliation's negative effect, the impact is small and should not be a major concern. The change in the marginal effect of affiliation as sharing by backers increases is small, indicating that change in backers' engagement, while statistically significant, does not have a meaningful effect on crowdfunding. Doubling the number of Facebook shares from its mean of 79 to 148 strengthens the negative effect of affiliation by.42% and translates to a decline of US$1.72/day.We developed recommendations for creators and examples of best practices from our data set (see Table 6). For example, creators should focus on the idea's inherent purpose and objective value in its description and avoid using too much communal language (e.g., cooperate, partner, support) in the idea description and updates. Overall, we recommend that platforms educate creators on how best to structure communication with backers and guide creators in meeting their goals. Backers could perhaps learn to interpret such updates better and use the information provided by the backer to qualify what they infer from the community.GraphTable 6. Actionable Outcomes for Managers Recommendations for Idea Descriptions and Updates. FindingRecommendations for CreatorsExamples from Kickstarter DataInteraction between affiliation and communal words in idea descriptionFocus on the idea's inherent purpose as opposed to a focus on community.The Drone PocketIdea Description: ""The world's first multicopter that'spowerful enough to carry a high-quality action camera and folds up smaller than a 7 in tablet.""Key technology features outlined prominently on idea's home page.Total Amount Raised: $929,212Pegasus Touch Laser SLA 3D PrinterIdea page includes recent press articles with links that highlight idea's featuresTotal Amount Raised: $819,535Use noncommunal words (e.g., ""you"" vs. ""we"") in idea description.The Floyd Leg""The Floyd Leg gives you the framework to take ownership of your furniture by allowing you to create a table from any flat surface"" (emphasis added)Total Amount Raised: $256,273Avoid thanking backers too much in idea description, as it can make the project appear needy.""First off, I want to say thanks for checking out of project. Every single person that takes the time to look at our project means the world to us.""Do not describe idea with overemphasis on communal language (e.g., ""support,"" ""team"").""As we approach Thanksgiving, I continue to be thankful for the patience and support that the unsung backers have shown with our team.""Interaction between affiliation and communal words in updatesDo not show too much appreciation via updates for funding as it is progressing.""Thanks to all of you who pledged for this campaign. We really appreciate your continued support.""Minimize communal language (e.g., partner) in updates.""Your first duty as partners with us on this project; should you choose to accept..."" Our results about the mechanism provide insights on how platforms and creators should engage with backers. Research has shown that licensing is a nonconscious effect and can be mitigated by making individuals aware of their behavior ([27]). Particularly in this type of vicarious moral licensing, highlighting individuals' uniqueness and independent identity may also mitigate the negative effect of affiliation on funding ([30]; [37]; [43]). If creators expect high overlap among backers, they could describe their ideas using less communal language, thereby lowering the licensing effect. Our results suggest that vicarious licensing might overwhelm other relevant idea information, potentially leading to suboptimal backer decisions. In line with our findings, backers might, in some cases, pay more attention to signals from affiliated others rather than from the whole crowd.For crowdfunding platforms, our findings provide a rationale for why there might be room for new crowdfunding platforms to thrive and grow. Although several crowdfunding platforms have flourished in the past decade, Kickstarter, Indiegogo, and GoFundMe have arguably dominated the market. Other once-popular platforms, such as Sellaband and PledgeMusic, have failed. Large platforms with millions of backers might pose high entry barriers to new entrants. However, our findings point to one source of competitive advantage for newer platforms: negative affiliation effects are more likely to occur in well-established platforms with large backer communities. Strategically building diverse and unaffiliated communities of backers might confer a competitive advantage to new platforms. Our results show that this can be achieved by expanding the number of categories of ideas, as affiliation's negative effect may be mitigated as backers of ideas across different categories may be less likely to coback ideas. The failure of category-specific platforms such as Sellaband (music), and the relative success of platforms hosting diverse ideas, such as Kickstarter, provides support for this reasoning. Second, platforms allocating marketing resources across existing and new backers (e.g., allocating social media spending across established markets such as Los Angeles and new markets such as Lima) could perhaps view our results as a reason to divert resources away from backer-dense markets. Third, platforms that provide backer information may also want to use algorithms that promote unaffiliated (vs. affiliated) backers, for example, by highlighting first-time backers. Finally, drawing on our results about creator engagement, we recommend that platforms educate creators on how to design better backer communication.Insights from our study are relevant to other types of crowdsourcing platforms as well. For example, participants on LEGO's Ideas, which focuses on ideation, and SeedInvest, which helps raise equity, could mitigate the negative effects of affiliation, for example, by describing initiatives as less communal and by posting updates with less communal language. Our findings are also applicable to crowdfunding contests (e.g., [ 9]; [25]), where participants could be encouraged to vote across categories to reduce coparticipation and help them break away from the adverse effects of groupthink.We highlight several areas of inquiry for future research. Reward structures could impact the role of affiliation in crowdfunding and thus merit attention (e.g., [56]). Fake reviews have been investigated in the online context (e.g., [67]), and it would be interesting to explore the veracity of idea descriptions and creator updates. In addition to affiliation, which we study, other network characteristics such as clans and core–periphery structures ([60]) could explain the nature of information flow across affiliation structures.As interest in crowdfunding increases, interesting research questions continue to emerge. We believe that our research explores important questions concerning crowdfunding that involve backer affiliation and community structure, and we hope to lay the foundation for future studies in the domain. Supplemental Materialsj-pdf-1-jmx-10.1177_00222429211031814 - Supplemental material for Do Backer Affiliations Help or Hurt Crowdfunding Success?Supplemental material, sj-pdf-1-jmx-10.1177_00222429211031814 for Do Backer Affiliations Help or Hurt Crowdfunding Success? by Kelly B. Herd, Girish Mallapragada, and Vishal Narayan in Journal of Marketing  "
7,"Examining Why and When Market Share Drives Firm Profit Many firms use market share to set marketing goals and monitor performance. Recent meta-analytic research reveals the average economic impact of market share performance and identifies some factors affecting its value. However, empirical understanding of why any market share–profit relationship exists and varies is limited. The authors simultaneously examine the three primary theoretical mechanisms linking firm market share with profit. On average, they find that most of the variance in market share's positive effect on firm profit is explained by market power and quality signaling, with little support for operating efficiency as a mechanism. They find a similar explanatory role of the three mechanisms in conditions where market share negatively predicts profit (for niche firms and those ""buying"" market share). Using these mechanism insights, the authors show that the value of market share differs in predictable ways between firms and across industries, providing new understanding of when managers may usefully set market share goals. The authors also provide new insights into how market share should be measured for goal setting and performance monitoring. They show that revenue market share is a predictor of firm profit while unit market share is not, and that relative measures of revenue market share can provide greater predictive power.Keywords: market share; quality; efficiency; market power; niche; firm profit; revenue share; unit shareMany firms use market share to set goals and monitor marketing performance, and market share is also widely used in research examining marketing's performance impact ([24]; [40]). [20] recent meta-analytic study (hereinafter, E-H 2018) reports a significant positive relationship between a firm's market share and its economic performance and identifies contingencies affecting this relationship. However, while the literature suggests several reasons market share may drive firm performance, few empirical studies have directly examined any (and none more than one) of these mechanisms. Thus, little is known about the underlying ""why"" of mechanism(s) linking firms' market share and economic performance and how they may both explain previously identified moderators and facilitate identification of additional moderators of this important relationship. In addition, when understanding of the mechanisms linking market share with firm performance suggests that it is economically valuable to measure market share for goal setting and performance monitoring purposes, managers currently have no empirical insights into how to do so.These knowledge gaps are important because understanding why market share is linked to firms' future profit can provide new insights into when and where market share is most likely to be valuable. While many firms use market share as a marketing performance metric, our research identifies new ways for managers to assess when this is most appropriate—and when it may not be. Because market share is such a common marketing goal, this is also important in delineating the role that marketing plays in determining firm performance and in understanding contingencies that may affect this role. Exploring the predictive value of alternative measures of market share, we also provide important new insights into how market share goals should be set and performance assessed via different market share measurement options in terms of unit versus revenue market share and absolute versus relative market share.In addressing these key questions, this study offers several contributions. First, we provide the first direct empirical assessment of the three primary causal mechanisms that have been theorized to link market share with firm profit: market power, operating efficiency, and quality signaling. Using direct measures, we examine each of these three mechanisms simultaneously and show that both market power and quality signaling are key mechanisms linking market share with firm profit. On average, we find little evidence of theorized economies of scale and learning benefits of market share, but we identify conditions under which such efficiency benefits do exist. We find no support for a fourth theorized mechanism linking market share negatively with profit as a result of a strong competitor orientation. However, we do find support for the same three mechanisms in conditions under which the market share–firm profit relationship is negative—for niche firms and when a firm ""buys"" market share. Overall, these findings provide important new empirical insights into market share's value-creating role.Second, using these new causal mechanism insights, we explore the consistency of the market share–profit relationship across different types of marketplaces and firms where the relative value of market share via the three mechanisms may be expected to vary. We show that the market share–profit relationship varies across industries and firms, and that the different causal mechanisms identified provide high explanatory power for such variations; thus, all three theories from which the hypothesized mechanisms arise can be ""correct."" In addition, this insight provides an empirically supported way for managers to identify when setting market share goals and monitoring market share performance may be more or less valuable. In contrast, we find that using indirect contingencies to try to infer the mechanisms linking market share with performance relationship often does not align with the directly observed mechanism effects, further indicating the value of direct measures in understanding the ""why"" mechanisms involved.Third, we extend recent meta-analytic insights regarding the nature of the relationship between market share and firms' economic performance by using direct measures of the three most widely cited mechanisms: measures of both revenue and unit market share and different market share benchmarks, firm size controls to isolate the benefits of market share versus firm scale, and different econometric approaches to address panel data and endogeneity estimation concerns. These aspects of our study enable us to provide several new insights. For example, we show that for most firms, economies of scale arise from firm size and not firm market share. They also allow us to identify which market share metrics are most predictive of profit for different types of firms and the economic value of increasing market share on these metrics. This is useful new knowledge for managers because it provides new insights into how market share should be measured in goal setting and performance monitoring as well as the scale of profit benefits that may be expected from any gain in a firm's market share.The article is organized as follows. First, we develop a conceptual framework and hypothesize relationships involving the three key mechanisms by which market share may be linked with firms' future profit. Next, we use the three mechanisms to identify three conditions under which the market share–profit relationship may be expected to be stronger versus weaker. We then describe the data set assembled and analysis approaches used to test the hypotheses and discuss the results. Having shown that the three mechanisms collectively mediate the market share–profit relationship, we then assess whether this remains true even under conditions when the market share–profit relationship is negative. Next, having shown that managers can use knowledge of the three mechanisms to identify when market share is likely to be economically valuable for their firm, we assess how managers may best measure market share. Finally, we assess the implications of our study for theory and practice and identify new questions for future research suggested by our findings. Conceptual Framework and HypothesesMuch of the theorizing regarding market share and firm performance in economics and management concerns related but distinct phenomena such as firm size and market concentration. We focus only on relationships that directly pertain to firm market share and the mechanisms underlying its economic value. As a result, we center our market share conceptualization on revenue market share—units sold × realized price (i.e., sales revenue) divided by total market sales revenue. In doing so, we conceptualize and measure the ""market"" as comprising firms selling similar product/service offerings. However, we also examine unit market share—units sold divided by total market unit sales—as well as several different operationalizations of revenue market share in robustness checks and post hoc analyses. Market Share and Firm Economic PerformanceThe marketing literature generally views market share as an indicator of the success of a firm's efforts to compete in a product-marketplace (e.g., [13]; [63]). From this perspective, market share is an outcome of a firm's marketing efforts including its advertising and promotion, product/service offering quality and price, channel and customer relationships, and selling activities ([24]). All of these are evaluated relative to those of other suppliers by customers (channel members and end users) when they consider and select offerings, which is what conceptually distinguishes a firm's market share (how the firm's sales compare with those of the total market) from its sales revenue (the number of units sold × price). Importantly, this means that (unlike sales revenue) market share is not a component variable in any indicators of firm economic performance,[ 6] so there is no synthetic (or ""hard-wired"") market share–firm economic performance relationship.Historically, the empirical literature provided conflicting and equivocal answers concerning the ""main effect"" relationship between firms' market share and their economic performance (e.g., [11]; [36]; [37]). However, the recent E-H (2018) meta-analysis using more sophisticated methodological approaches has provided new insight on this question, showing a generally positive effect of market share on firm economic performance. We corroborate this in our data and focus our hypothesizing on why this relationship exists and how this ""why"" understanding may help explain and predict differences in the strength of the relationship across firms and industries. Mechanisms Through Which Market Share May Impact ProfitWhile several explanations have been independently proffered for why a firm with higher market share may enjoy superior economic performance, three mechanisms are much more widely discussed than others. As Figure 1 shows, we focus our theorizing on these mechanisms and consider how each may link a firm's market share with its profit.Graph: Figure 1. Conceptual framework. Market powerThe first proposed mechanism by which market share may be linked with firm profit is via market power (i.e., the firm's ability to influence the price of its product/service offerings by exercising control over demand, supply, or both; e.g., [10]; [59]). Industrial organization theory posits that firms enjoy superior profit when they are able to charge higher prices than rivals, which is determined by the availability of alternatives to customers and firms' ability to create and/or control resources that give them stronger market positions (e.g., [57]). Market share may be a resource that provides a firm with the opportunity for greater market power over both ""upstream"" suppliers and ""downstream"" channels and customers and thereby control prices in several ways.For upstream suppliers, buyer firms with higher end-user market share are more attractive, which may allow them to negotiate lower prices and/or higher-quality inputs from their suppliers ([ 9]). For example, Apple's smartphone market share allows it to both charge app developers for selling their products and enforce strict quality controls on the apps it sells. It may also increase supplier willingness to cooperate with others in the buyer's supply network to further lower the buyer's input costs and improve input quality ([28]). For downstream channels, higher–market share firms are more attractive upstream partners because they generate end-user demand for more and/or higher-value products. They may also attract larger customer numbers and/or more frequent interactions for channels to engage in cross-selling. This may enable higher–market share firms to negotiate better list prices than rivals in downstream channels and to benefit from greater channel cooperation (e.g., preferred shelf-space, merchandizing support). For example, PepsiCo's snacks division leverages its leading market share position to obtain preferential shelf and display access in many U.S. retail chains. The input and go-to-market cost and quality benefits of higher–market share firms should allow them to provide better value offerings, which may thus allow them to charge higher prices to end users (as in the case with Apple) and/or enjoy higher profit margins on each unit sold (e.g., Walmart). Thus, H1:  The positive effect of market share on firm profit is mediated by the firm's market power. Operating efficiencyThe second theorized mechanism by which a firm's market share may lead to profit is via increasing the firm's operating efficiency (e.g., [17]). Disputing market power arguments, the ""Chicago school"" in economics argues that market share is an outcome of firm efficiency that allows a firm to sell quality-equivalent offerings at lower prices than rivals, attracting greater demand (e.g., [14]; [50]). Following this logic, strategic management scholars propose that higher market share may also allow firms to further increase their efficiency in a recursive relationship with lowering firm costs via learning effects (e.g., [ 1]; [29]). Much of this logic is framed in terms of a firm's position on the production ""experience curve"" as a function of the volume of units sold, with greater experience allowing production-related learning and lower production costs (e.g., [30]). Thus, firms selling a greater number of units produce more and learn how to do so more efficiently. For example, Tesla has used its greater accumulated experience in producing electric vehicles (EVs) to lower its costs compared with rivals.Conceptually, this may also be possible via market share impacting the number of interactions a firm has with suppliers, channels, and customers, enhancing opportunities for higher–market share firms to learn and use knowledge gained to improve their supply-and-demand chains ([55]). For example, Tesla has used its greater EV sales to learn how to drive improvements in battery designs and configurations from suppliers as well as to optimize its own software to increase EV range. More interactions also increase the likelihood that suppliers, channels, and customers will trust higher–market share firms, increasing information sharing, lowering coordination costs, and enhancing cooperation in changes designed to enhance the firm's supply-and-demand chains ([16]; [27]). This should enable higher–market share firms to lower costs and enhance supply-and-demand chain quality and reliability, allowing superior value offerings for customers and/or greater margins. Thus, H2:  The positive effect of market share on firm profit is mediated by the firm's operating efficiency. Quality signalingThe third mechanism by which market share may enhance firm profit is by signaling unobserved quality. Information economics theory posits that customers' limited evaluative knowledge often makes it difficult for them to observe ""true"" product/service quality (e.g., [38]; [41]). Empirical studies also show that customers are often unable to accurately (or confidently) evaluate an offering's quality prior to making purchase decisions, and they frequently rely on indirect cues (e.g., [47]; [61]). Market share may signal quality by increasing the credibility of firm claims and thereby lowering customer perceived risk ([21]; [34]). Customers may also infer that ""everyone can't be wrong"" in choosing the offerings of a high–market share firm (e.g., [18]). For example, Toyota campaigns have touted that its products are ""#1 for a Reason."" Thus, to the extent that market share signals higher quality, it should increase future demand and reduce customer churn. It may also lower the firm's costs relative to rivals, because alternative ways to signal quality (e.g., advertising) may be more costly.Market share may also signal quality to suppliers and channel members. Firms that are perceived to be producing high-quality offerings may be viewed by suppliers as not just attractive buyers, in terms of their own demand, but also as potentially providing a halo image spillover benefit. Similar to customers viewing them as having ""too much to lose"" to provide inferior offerings, supplier choices made by high–market share firms may be viewed as being based on ensuring high quality and reliable inputs to protect their reputation and market position. For example, Apple's suppliers are frequently identified as such in business press reports. This could also apply to channel partners where selling offerings that are perceived as higher quality can provide a halo effect making the channel member more attractive to other suppliers and end-user customers (e.g., [42]). All of these arguments suggest the following: H3:  The positive effect of market share on firm profit is mediated by the firm's perceived quality. Using Mechanism Insights to Predict Where Market Share Is ValuablePrior research suggests that the value of market share varies across industries (e.g., [ 4]), indicating that setting market share goals may be more beneficial for some firms than others. To explore this, E-H's (2018) meta-analysis examines the sample characteristics most commonly reported in prior studies and reports that market share is more valuable in business-to-customer (B2C) markets and in markets with medium market concentration, whereas it is less valuable in the banking industry. While offering initial useful insight to managers, these boundary conditions are limited in number and scope—and the ""why"" mechanisms involved are unobserved. Robust empirical understanding of the mechanisms using direct assessments should allow additional boundary conditions to be identified and provide empirically verified principles for managers to distinguish when they should and should not care about market share.To provide an initial assessment of the predictive value of our mechanism results and offer new insights for managers, we next examine the extent to which the market share–profit relationship varies under conditions in which each of the three mechanism in turn may be expected a priori to be more versus less important. For each mechanism, we identify a condition expected to be particularly impactful on that particular market share–profit pathway. However, in our analyses we also allow for the possibility that each of the conditions we identify may affect the strength of all three mechanisms linking market share with profit. First, in terms of market power we examine industries characterized by higher customer switching costs, where firms are more easily able to retain customers. Firms should benefit more from the market power provided by market share when switching costs are high because they are better placed to increase prices without fear of customers switching ([23]; [58]; [60]).Second, in terms of the value of operating efficiency in explaining the market share–profit relationship, the literature suggests that cost-reducing learning effects are more likely earlier in the life of a firm (e.g., [66]). For example, ""experience effect"" studies of the value of a firm's cumulative doubling of output show that this is more likely to occur early in a firm's existence (e.g., [31]). In addition, learning effects require changing and adapting firms' processes—which tend to become more rigid over time (e.g., [54]). Thus, younger firms are less knowledgeable in their operations and less ""set in their ways,"" providing incentives to seek out the learning opportunities presented by market share and the ability to exploit the efficiency-enhancing knowledge gained via process changes.Third, to explore conditions where the quality-signaling value of market share may be stronger, we examine differences between ""service-dominant"" and ""goods-dominant"" industries.[ 7] A key difference between these markets is the greater intangibility of service offerings, which creates more quality uncertainty for customers ([68]). Under such conditions, customers are more likely to use cues such as market share as indicators of the quality of a firm's offerings (e.g., [12]). Interestingly, this prediction is the opposite of E-H (2018), who reason that physical goods manufacturers may benefit more from efficiency, and that this may be more important in driving profit than any dampening of the quality-signaling effect of market share in physical goods-focused markets. We explore this reasoning empirically when we directly examine the three mechanisms underpinning the market share–profit relationship.We therefore hypothesize the following: H4:  The effect of market share on firm profit via market power is stronger in marketplaces with higher switching costs. H5:  The effect of market share on firm profit via efficiency is stronger for younger firms. H6:  The effect of market share on firm profit via perceived quality is stronger for firms selling service- versus product-dominant offerings. Methodology DataWe combine secondary data from a variety of sources. From Compustat, we obtained data to construct measures of market power and operating efficiency, firm economic performance indicators, firm-specific controls, and a set of industry and competitive context control variables. Equitrend provided data on the perceived quality of firms' offerings. To calculate measures of unit market share, we use unit sales data from the Global Market Information Database (GMID). We assembled our initial data set by merging data from Compustat and GMID. To test the mediation hypotheses, we also require data from Equitrend, for which our access covers only the years 2000–2013. Because each data source has distinct firm and year coverage, the compiled data set used to confirm the main effect of market share on firm profit and test the hypothesized mediation effects contains 3,058 firm-year observations from 244 individual firms, operating in 126 North American Industry Classification System (NAICS) four-digit industries, 2000 through 2013. The average firm in this sample has $13.81 billion in assets and has been operating for 45 years. Table 1 shows summary statistics and correlations for the main variables in our sample and additional details are contained in Web Appendix 1. To test H4–H6, we also required American Customer Satisfaction Index (ACSI) data (to measure switching costs), which reduced our sample for testing these three hypotheses to 2,629 firm-year observations from 207 firms (2000–2013).[ 8]GraphTable 1. Descriptive Statistics and Correlations (N = 3,058). VariablesMeanSD1234567891011121314 1. Firm Profit ($M)839.101,104.101.00 2. Market Share (%)6.859.59.141.00 3. Sales Revenue ($M)3,907.085,747.18.77.141.00 4. Market Power (%)30.7910.90.23.13.091.00 5. Firm Efficiency (Index)50.099.06.10.08.38.231.00 6. Perceived Quality (Index)65.3516.95.18.07.06.35−.051.00 7. Firm Size ($B)13.8162.89.39.16.69.17.03.221.00 8. Market Growth ($M)122.23523.18.10.02.03.14−.04−.10.301.00 9. Advertising ($M)53.92255.75.44.34.36.07−.01.01.31.031.0010. R&D ($M)64.92366.32.40.30.22.15−.02−.07.32.05.481.0011. Service Indicator (0/1).19.39−.02−.10−.15−.03.18−.05.08−.04−.08−.051.0012. Switching Costs (Index)−.011.11.33−.14.06.16.07.02.18.14.19.07.111.0013. Firm Age (Years)45.3041.30.21.05.09.14.10.21.18−.08.49.41−.08.341.0014. Niche Focus (Index)2.438.10.02−.15−.02.12.06.05−.07−.06.06.13.12.01−.061.00 1 Notes: All descriptive statistics are for the ""raw"" (i.e., untransformed) variables. Correlations with an absolute value larger than.046 are significant at p < .01, and those greater than.035 are significant at p < .05. Hypothesis Testing Variable MeasurementThe Appendix contains definitions and operationalization details of all variables described next. Market shareMarket share is the percentage of a market's total sales garnered by a firm over a specified time period ([25]). The market may consist of all suppliers selling products/services with the same characteristics, or those that are thought of similarly by customers and are purchased for the same use. We follow [35] to compute a measure of market share using a set of competitors and market definitions derived from business descriptions in ﬁrm 10-Ks. This allows market definitions to be dynamic, where a firm may move in and out of any given market depending on whether its offerings changed over time and thus compete with a different set of firms.To compute market share, we divide the total sales of each firm by the aggregate sales for that market for that year, where the market is dynamically defined as described previously using data from all 22,076 firms in Compustat for the 2000–2013 period. In defining markets, we note that each firm has a similarity/competition score with respect to any other firm (i.e., all possible dualities are computed) in the Compustat database. In line with [35], the number of competitors can be defined using a threshold of similarity scores and/or specified number of nearest neighbors (e.g., 50 or 20). We combine the two approaches and specify 50 as the largest number of neighbors, while also imposing a minimum threshold limit. Thus, our market definition comprises a maximum of 50 firms per industry, while allowing for fewer firms, to maintain a minimum level of similarity among competitors in the same market.[ 9]To assess the robustness of the findings using this dynamic measure of market share, we also use a more static approach, defining markets via each firm's primary NAICS designation using the four-digit level that researchers suggest most closely represents the real ""competed"" market (e.g., [44]). To calculate this, we first collect the total revenue-by-industry data that comprise gross domestic product (i.e., total expenditures on products and services) for all four-digit NAICS industries from the U.S. Bureau of Economic Analysis, which allows us to account for the sales of firms that are private, small, or otherwise not available in Compustat. We then divide the total sales of each firm by the gross domestic product value for that four-digit NAICS industry for that year. Firm market shares are computed from their revenues in their primary NAICS markets. Firm profitWe use net income as our primary measure of firm profit, obtained from Compustat. We use this indicator of absolute firm profit (while controlling for asset size in our model) because economic theories of the value of market share assume that maximizing the amount of profit—not the efficiency with which profit is generated, which is what ""return on asset"" (or investment) relative profit measures capture—is a firm's superordinate performance objective. Market powerWe use profit elasticity relative to the industry average (similar to [ 8]) to indicate firm-level market power. This is calculated by estimating regressions of firms' profit (net income) on their total variable costs for each industry as follows: ln(πit)=α+βln(tvcit)+εit, Graphwhere π is firm profit and tvc is the firm's total variable cost (Cost of Goods Sold + Selling, General and Administrative Expenses) for firm i at time t. Both profit and variable costs are scaled by firm size (total assets). Because profit and costs are natural log transformed, the β from this regression captures the average profit elasticity within the industry, with less negative βs indicating the average ability of firms within the industry to mark up prices when costs rise and thus exercise market power (e.g., [39]). Firm-specific residuals measure each firm's margins relative to its industry's average, providing an indicator of firm's market power ([ 8]). Positive residuals (equivalent to less negative elasticities) indicate greater market power, and negative residuals (i.e., more negative elasticities) indicate weaker market power. Web Appendix 2a indicates favorable face validity for this measure. Firm efficiencyFrom an economic theory viewpoint, this concerns producing goods and services in ways that optimize the combination of inputs to produce maximum output at the minimum cost ([ 5]). To operationalize productive (in)efficiency, we use a stochastic frontier estimation approach. Following [ 5], we use operating expense as the input and total sales as the output. In stochastic frontier estimation, the firm in the industry with the lowest input requirements to produce a given set of outputs forms the efficiency frontier and the closeness of a firm's inputs-to-outputs to this frontier determines its relative (to the industry's most efficient firm) efficiency. Web Appendix 2b provides evidence of strong face validity for this measure. Perceived qualityWe use the perceived quality measure of brands from the Equitrend database, which comprises consumer ratings on an 11-point perceived quality scale. For multibrand firms, we take the mean perceived quality of all brands owned by the firm.[10] Face validity assessments for this measure (see Web Appendix 2c) provide strong support for the measure. Switching costsWe use ACSI data and follow [53] to construct an industry-level measure of switching costs as the ""excess loyalty"" displayed by customers to suppliers using the residual of regressing each industry's customers' loyalty onto its customers' satisfaction, controlling for time fixed effects (FEs). This measure has been shown to have strong face validity ([53]), and we also find evidence of this (Web Appendix 3). Service- (vs. product-) dominant industriesService- (vs. product-) dominant industries is a dummy variable identifying firms operating in nonbanking (banks have idiosyncratic characteristics we later explore) service-focused industries using Fama–French industry definitions ([22]). Firm ageFirm age is the number of years since the firm's founding using information from annual reports and websites. Control variablesIn addition to firm and year FEs used to control for unobserved heterogeneity, we employ several firm- and industry-level covariates in our analyses, including firm size, operationalized as the logarithm of each firm's total assets to account for scale economies not captured by market share, and the firm's advertising and research-and-development (R&D) expenditures to control for firm-level heterogeneity. We also control for market growth that may affect the profit outcomes of market share ([56]), captured as the year-to-year change in total market sales.The Appendix and Web Appendix 1 summarize descriptive statistics for all variables used in our analyses. To enable log-log specification and interpretation in our analyses and reduce deviations from normality present in several of our measures (market share, firm profit, market power, firm efficiency, perceived quality, advertising expense, R&D expense, and market growth), we applied log transformations to our data.[11] Model SpecificationWe empirically test the hypothesized relationships using a fixed-effects autoregressive (FE-AR) estimation approach ([65]) for several reasons. First, we are using panel data, and the Hausman test indicates that an FE correction is needed to address unobserved heterogeneity and separate between time-variant and -invariant firm-specific errors. Second, several of our measures are longitudinally persistent, raising concerns about serial correlation—the AR correction of the errors addresses any potential bias to the estimates. The modified Durbin–Watson and Baltagi–Wu LBI tests indicate that an AR1 correction is appropriate. In addition, we control for heteroskedasticity using cluster-adjusted robust standard errors at the firm level. Finally, we estimate our hypothesis-testing models using generalized least squares (GLS), because OLS are statistically inefficient and may result in biased inference in the presence of serially correlated residuals.We first verify the average positive relationship between market share and profit (E-H 2018) and estimate the total effect using the following model specification: Profiti,t+1=α0+α1MarketSharei,t+α8FirmSizei,t+α9Advertisingi,t+α10R&Di,t+α11MarketGrowthi,t+YearFEs+ζi+εi,t+1, Graph( 1)where i stands for firm and t for time (year), ζi is a time-invariant firm FE, and εi, t + 1 is the random error representing all unobserved influences on future profit, modeled as an AR1 process such that εi, t + 1 = ρεi, t + ηi, t + 1 and where |ρ|<1 and ηi, t + 1 is an independent and identically distributed (i.i.d) error. Market Share, Firm Size, Advertising, R&D, and Market Growth are as described previously, and Year FEs are mutually exclusive year dummies. Lagged regressors are used to alleviate concerns due to simultaneity and reverse causality (i.e., future profit should not impact past market share).Having selected an appropriate estimation approach given the nature of our data, we next deal with potential endogeneity concerns with respect to omitted variables—of which reverse causality and simultaneity are special cases ([65]). We examine the potential for the presence and effect of such endogeneity concerns using a Gaussian copula correction to Equation 1 and assess the presence and effect of any endogeneity (including potential selection bias introduced by the various data sets on which we draw for our measures) via a likelihood ratio test of whether there is a significant difference between the uncorrected set of parameter estimates and the endogeneity-corrected set ([65]).[12] Once we show that potential endogeneity issues are not material, we empirically test H1–H3 using an identical FE-AR approach by estimating the following equations: Profiti,t+1=α0+α1MarketSharei,t+α2MarketPoweri,t+α3FirmEfficiencyi,t+α4PerceivedQualityi,t+α5SwitchingCostsi,t+α6ServicesDummyi,t+α7FirmAgei,t+α8FirmSizei,t+α9Advertisingi,t+α10RDi,t+α11MarketGrowthi,t+YearFEs+ζi+εi,t+1, Graph(2\rm a) MarketPoweri,t+1=β0+β1MarketSharei,t+β5SwitchingCostsi,t+β6ServicesDummyi,t+β7FirmAgei,t+β8FirmSizei,t+β9Advertisingi,t+β10RDi,t+β11MarketGrowthi,t+YearFEs+τi+ξi,t+1, Graph(2\rm b) FirmEfficiencyi,t+1=γ0+γ1MarketSharei,t+γ5SwitchingCostsi,t+γ6ServicesDummyi,t+γ7FirmAgei,t+γ8FirmSizei,t+γ9Advertisingi,t+γ10RDi,t+γ11MarketGrowthi,t+YearFEs+μi+ςi,t+1, Graph(2\rm c) PerceivedQualityi,t+1=θ0+θ1MarketSharei,t+θ5SwitchingCostsi,t+θ6ServicesDummyi,t+θ7FirmAgei,t+θ8FirmSizei,t+θ9Advertisingi,t+θ10RDi,t+θ11MarketGrowthi,t+YearFEs+νi+φi,t+1, Graph(2\rm d)where Market Power, Firm Efficiency, Perceived Quality, Switching Costs, Services Dummy, and Firm Age are as described in the variable measurement section, and all other variables and subscripts follow Equation 1. Finally, we empirically test H4–H6 by estimating the moderated-mediation contingencies and include interactions between Market Sharei,t and Switching Costsi,t, Services Dummyi,t, and Firm Agei,t in Equations 2a–2d. To estimate the relative effects of the three hypothesized mediation mechanisms (market power, firm efficiency, and quality signaling) and three moderated-mediation contingencies (switching costs, firm age, and services), we follow [51] using [64] approach to augment the FE-AR estimation. Results and Discussion Main Effect of Market Share on Firm ProfitPrior to testing the hypothesized mechanisms, we first verify the main effect results indicated in the E-H (2018) meta-analysis in our sample using several variants of the model specification detailed in Equation 1. We begin by estimating a model with FEs and cluster-adjusted robust standard errors that includes only the covariates as predictors (M1), to which we then add market share (M2), allowing us to verify the main effect of market share on firm profit and reveal its incremental predictive power. We also estimate this same model using an FE-AR error correction and cluster-adjusted robust standard errors (M3) to demonstrate the stability of the estimates across the different statistical corrections proposed. In M4 we examine whether the reported estimates suffer from endogeneity bias by including a Gaussian copula for the Market Share variable as a control function to empirically correct endogeneity bias. The likelihood ratio test for joint parameter differences ([65]) indicates that the endogeneity-corrected estimates in M4 are not statistically different from those in M3.As Table 2 shows, the estimates are consistent across all four models, demonstrating the robustness of the effect of market share on firm profit. In addition, while the Gaussian copula estimate in M4 is significant (.048, p < .05) indicating the presence of some omitted variable endogeneity, the likelihood ratio test indicates no significant difference in the market share parameter estimates between M3 (β = .137) and M4 (β = .159). This supports the use of an FE-AR( 1) (i.e., model specification M3) estimation approach and confirms that any remaining bias is modest and does not substantively impact the estimates. In a robustness check, we also replaced the dynamic market share measure with a four-digit NAICS alternative and again confirmed the main effect (Web Appendix 5). Finally, we further verified that endogeneity bias does not unduly influence our findings using a difference-in-differences version of Equation 1 comparing the market share–profit relationship for firms in industries that experience an exogenous demand shock (exit of bankrupt firms) with those that do not. The results (Web Appendix 6) again confirm the main effect findings.GraphTable 2. Main Effect of Market Share on Firm Profit. Models and Dependent VariablesM1M2M3M4Independent VariablesProfit(t + 1)Profit(t + 1)Profit(t + 1)Profit(t + 1)Main Effect Market Share(t).153**.137**.159**(.053)(.038)(.052)Controls Firm Size(t).228*.208***.521***.291***(.113)(.067)(.045)(.051) Advertising(t).234***.130**.073***.098*(.061)(.045)(.023)(.044) R&D(t).061*.044.066***.042***(.027)(.025)(.014)(.010) Market Growth(t).020.012.002.029*(.017)(.020)(.002)(.013) Market Share(t)COPULA.048*(.021)Specification Tests  Wald χ2125.32198.12188.36115.92 R2.57.59.58.59 Rho_AR.40.43 2 *p < .05.3 **p < .01.4 ***p < .001.5 Notes: All model specifications estimated using 3,058 firm-year observations. M1/M2: GLS estimation, FEs and cluster-adjusted robust standard errors. M3/M4/M5: GLS estimation, FEs with AR errors and cluster-adjusted robust standard errors. Z-test difference in share coefficients between M3 (.137) and M4 (.159) = .64 (p > .05).Collectively, these analyses verify the main effect results in E-H (2018) that, on average, firm market share positively predicts future firm profit—and the effect sizes reported on Table 2 are both consistent and aligned with the average elasticity of.132 reported by E-H (2018), further enhancing confidence in our findings. Table 2 results also show the suitability of the FE-AR error correction and cluster-adjusted robust standard errors GLS estimation approach (model specification M3), which we employ in the hypothesis-testing analyses. Hypothesized Mechanism (Mediator) ResultsAs Table 3 shows, in testing H1–H3 we find support for both market power in M1a (.230, p < .001) and quality signaling (.141, p < .05) in M1c as mechanisms linking market share with firm profit. However, while M2 confirms that firm efficiency predicts firm profit (.129, p < .001), M1b reveals that a firm's efficiency is not predicted by its market share (.024, p > .1). Thus, on average we find no evidence supporting efficiency as a mechanism linking firm market share and profit in our sample. Overall, these results provide support for H1 and H3 but not for H2. As M2 shows, all three of the mechanism variables are significant predictors of firm profit, and the main effect of market share becomes insignificant (.031, p > .10) in the presence of these three variables. To examine the relative strength of the mediator role played by the three mechanism variables in explaining the market share–profit relationship, we follow [64] approach. This reveals that the three mechanisms collectively explain 77.37% of the total effect of market share on firm profit, with 63.21% of this flowing through market power, 33.96% via perceived quality, and 2.83% through firm efficiency.GraphTable 3. Mechanism for Market Share Effect on Firm Profit. Models and Dependent VariablesM1aM1bM1cM2Independent VariablesPower(t + 1)Efficiency(t + 1)Quality(t + 1)Profit(t + 1)Direct Effect Market Share(t).230***.024.141*.031(.081)(.016)(.065)(.018)Indirect Effect Market Power(t).302***(.042) Firm Efficiency(t).129***(.029) Perceived Quality(t).274***(.061)Controls Firm Size(t).029*.027***.039***.210***(.013)(.006)(.008)(.029) Advertising(t).020.021.022*.090*(.023)(.020)(.010)(.043) R&D(t).032**.013***.028**.023***(.011)(.002)(.011)(.005) Market Growth(t).012.007.012.008(.019)(.009)(.010)(.007)Specification Tests −Log-likelihood2,810.17 R2.16.18.10.68 6 *p < .05.7 **p < .01.8 ***p < .001.9 Notes: 3,058 firm-year observations covering 244 firms for the 2000–2013 period (Equitrend available 2000–2013). Total effect (from Table 2: M3).137 (100.00%) minus direct effect (from M1a).031 (22.63%) = indirect effect of.106 (77.37%). Indirect effect via ( 1) Power = .067 (63.21%); ( 2) Quality = .036 (33.96%); and ( 3) Efficiency = .003 (2.83%).To check the robustness of the mechanism results, we conducted four additional analyses. First, to check for any potential scale effect of absolute sales revenue beyond firm size, we reestimated our model using market share ranks and adding firm sales revenue as a separate control. The estimates replicated the hypothesis-testing results (Web Appendix 7). Second, to check for any potential biasing effect of firm orientation to market share ([43]) we used text analysis of 10-K reports to construct an annual measure of each firm's market share focus based on the number of times ""market share"" is mentioned relative to the total number of words. When this is added to our model, we find that the results remain essentially unchanged (Web Appendix 8). Third, to ensure that results are robust to alternative firm performance measures, we replaced net profit in turn with return on assets and Tobin's q as dependent variables. As shown in Web Appendices 9 and 10, we replicate the hypothesis-testing results. Fourth, we also checked that a firm's competitor orientation—a potential fourth mechanism linking market share (negatively) with firm profit ([ 3])—does not explain additional variance in the market share–profit relationship. Using 10-K reports and [ 7] text-based measure, we computed the competitor orientation of each firm in our sample and included this in our model. As Web Appendix 11 shows, we find that while competitor orientation predicts firm market share, it does not materially affect the market share–profit relationship. Hypothesized Moderating Condition ResultsHaving demonstrated the robustness of the hypothesized mechanism results, we next examine whether the market share–profit relationship may be stronger in industry and firm conditions in which each of the three mechanism variables in turn may be expected a priori to be more versus less important as captured in H4–H6. The results are summarized in Table 4, with M1 showing that firms in industries with higher customer switching costs are more profitable (.137, p < .05), and M2 supporting H4 by confirming that market share is more valuable in such industries (.087, p < .001) via its stronger effect on market power (.157, p < .05). In addition, M4c reveals that firms also gain stronger perceived quality benefits from market share in industries with higher switching costs (.203, p < .05), suggesting that some of the switching costs we observe are due to customers continuing to choose a provider because of positive relational bonds that may influence both customers and others' perceptions of the quality of such firms' offerings.GraphTable 4. Main Effect and Mechanisms for Market Share Effect on Firm Profit in Hypothesized Moderators. Model Specifications (M) and Dependent VariablesM1M2M3aM3bM3cM3dM4aM4bM4cM4dIndependent VariablesProfit(t + 1)Profit(t + 1)Power(t + 1)Efficiency(t + 1)Quality(t + 1)Profit(t + 1)Power(t + 1)Efficiency(t + 1)Quality(t + 1)Profit(t + 1)Direct Effects Market Share(t).118***.114***.105*.031.278**.017.136***.028.149***.030Indirect Effects Market Power(t).210***.223*** Firm Efficiency(t).075**.083*** Perceived Quality(t).169***.163***Moderators Switching Costs(t).137*.149*.093*.005.051*.013.107.015.077*.027 Firm Age(t).178.208−.002.013.006.015*.034−.031.004.019 Services Dummy(t)−.058*−.059*.017−.033*.008−.004.093.509***.028−.006Interaction Effects Market Share(t) × Switching Costs(t).087***.157*.017.203*.033 Market Share(t) × Firm Age(t)−.069***−.048−.109***−.092*−.043 Market Share(t) × Services Dummy(t).056***−.006.148***.012.020Controls Firm Size(t).514***.534***.025***.031***.046***.028***.030***.083*.041***.046*** Advertising(t).278***.281***.008.022.006.011.007.010.039.039*** R&D(t).274***.272***.039***.010.059*.034***.029***.012.062***.031*** Market Growth(t).014.015*.011.008***.002.004.011.017.012.003Specification Tests Wald χ2303.11358.07 −Log-likelihood2,489.312,913.87 R2.50.52.24.21.22.69.25.29.26.70 10 *p < .05.11 **p < .01.12 ***p < .001.13 Notes: 2,629 firm-year observations covering 207 firms for the 2000–2013 period (sample size due to ACSI data availability).The interactions reported for M2 also show that market share is generally less valuable for older firms (−.069, p < .001), and consistent with H5, the mechanism estimates in M4b provide strong evidence supporting the expected effect of market share on firm efficiency being weaker for older firms (−.109, p < .001). This is aligned with our rationale that efficiency-enhancing learning effects associated with market share accrue mainly to firms that are earlier in their development. M4c estimates also reveal that older firms benefit less from market share via quality signaling (−.092, p < .05). We reason that older firms that have been in the marketplace for longer are likely to be better known and also that firm age may indicate a firm's stability and lower risk, which reduce the signaling value of its market share.In terms of services-dominant firms, the significant positive estimate in M2 for the services × market share interaction (.056, p < .001) indicates that service firms benefit more from market share. However, our mechanism estimates in M4c show that this is not a result of the expected strengthening of the quality-signaling benefit of market share (.012, p > .10) posited in H6 but rather, as shown in M4b, that service firms benefit more from the efficiency-enhancing effect of market share (.148, p < .001).[13] Because controlling for scale effects via firm size isolates the efficiency-enhancing learning effects of market share, this finding suggests that market share provides a greater opportunity for service firms to learn how to operate more efficiently and to use this knowledge to change their operations to do so. We reason that this may be because the greater direct customer interactions from higher market share are more valuable in helping service firms learn how to efficiently deal with customer heterogeneity, and that applying what is learned may also be less capital-intensive for service firms (vs. manufacturers). Additional Analyses of Hypothesis-Testing EffectsTo provide additional insight into how the hypothesized moderators affect the profit value of market share via the three mechanisms, we examined these effects in an additional analysis (Table 5). Of the.086 total effect (elasticity) of market share on profit when the moderator variables are included in the model,.056 is indirect (65% of the total) via the three mechanisms, with 62% of this flowing through market power, 6% through firm efficiency, and 32% via perceived quality. Consistent with the H4 testing results (Table 4), the effect of market share on firm profit is strengthened by switching costs, with the total effect amplified by.287 for each unit increase in switching costs, of which.195 is indirect via market power (50.9%), firm efficiency (2.5%), and perceived quality (46.6%). These direct and indirect effects of switching costs on market share's effect on firm profit are proportionately lower (higher) at lower (higher) levels of switching costs (i.e., ± one standard deviation around average switching costs) with the indirect effects flowing through the three mechanisms in very similar percentages.GraphTable 5. Indirect Effects for Market Share Effect on Firm Profit in Hypothesized Moderators. Market Share–Profit EffectsIndirect Effect MechanismsModerator Variable ConditionsTotal EffectDirect Effect% of TotalIndirect Effect% of TotalPowerEfficiencyQualityOverall.086*.03034.9%.056*65.1%62.0%6.0%32.0%Switching costs.287***.092*32.1%.195***67.9%50.9%2.5%46.6% +1 SD.345***.111*32.2%.234***67.8%51.2%2.4%46.4% −1 SD.218***.073*33.5%.145***66.5%51.1%2.4%46.5%Service dominant.032*.02062.5%.01237.5%41.0%21.0%38.0%Product dominant−.032*−.01031.2%−.02268.8%54.0%3.0%43.0%Firm age−.136***−.01410.3%−.122***89.7%12.1%45.5%42.4% +1 SD−.170***−.01810.6%−.152***89.4%17.1%40.2%42.7% −1 SD−.081*.011−13.6%−.092*113.6%2.7%56.8%40.5% 14 *p < .05.15 **p < .01.16 ***p < .001.17 Notes: 2,629 firm-year observations covering 207 firms for the 2000–2013 period (sample size due to ACSI data availability).Consistent with H5 testing results (Table 4), the total effect of market share on firm profit is also amplified for service-dominant firms by an extra.032, of which.012 is indirect (38% of the total) and flows through market power (41.0%), firm efficiency (21.0%), and perceived quality (38.0%). Meanwhile, for product-dominant firms, the total effect is reduced by −.032, of which −.022 is indirect, with 54.0% flowing through market power, 3.0% through firm efficiency, and the remaining 43.0% via perceived quality.Finally, in line with H6 testing results (Table 4), Table 5 shows the effect of market share on profit is weakened by firm age with each additional year reducing the total effect of market share on profit by −.136, of which −.122 is indirect (90% of the total) and flows through market power (12.1%), firm efficiency (45.5%), and perceived quality (42.4%). As we expected, the total effect of firm age on the market share–profit relationship is more pronounced for very high (old) versus very low (young) age levels, with a marked increase in the indirect effect flowing through firm efficiency (from 40.2% to 56.8%) and decrease in that flowing through market power (17.1% to 2.7%) in the case of very young firms. This is consistent with our Table 4 hypothesis testing results revealing stronger efficiency gains with market share for younger firms. Market Share–Profit Mechanisms When Market Share Negatively Impacts Firm ProfitAligned with E-H's (2018) finding that 82% of market share–performance elasticities in prior research are positive (82% of the same elasticities in our sample are also positive), our hypotheses are framed in terms of a net positive performance effect of market share. However, conceptual arguments concerning potential negative outcomes of market share have also been proposed (e.g., E-H 2018; [34]). Drawing on our theorizing, we expect that the three mechanisms we identify should empirically capture any negative and positive effects of market share. For example, any associated diseconomies of scale will reduce a firm's efficiency while a reduction in perceived exclusivity will affect the quality-signaling value of market share. To empirically verify this expectation, we identify two conditions under which market share's positive benefits may be outweighed by negative consequences, such that larger market share might reduce firm profit and reestimate the mediation effects of the market power, firm efficiency, and quality-signaling mechanism in these conditions. Niche firmsOne condition in which market share may negatively predict profit concerns firms with a strategic focus on serving a smaller segment of a market, usually a group of customers with a distinct set of needs and requirements (e.g., [49]). For example, Louboutin specializes in high-fashion stiletto shoes. By serving distinctive needs, niche-focused firms make money by occupying positions in a segment of a broader market in which competition is more limited (e.g., [19]). As a result, they may not serve enough customers to gain market power benefits from market share, and their specialist positioning may diminish any quality-signaling benefit. They are also unlikely to gain from any learning effects in production. However, niche-focused firms with higher overall market shares are likely to have achieved this by selling to customers beyond their original niche ([62]). This may negatively impact the firm's profitability by reducing its original niche appeal via a negative effect on perceived quality (e.g., [34]) and also attract more competition (e.g., [32]). These downsides may outweigh any potential market power and/or firm efficiency benefits of having a larger market share. Firms buying market shareAnother circumstance when market share may negatively impact profit is when firms ""buy"" market share by lowering prices relative to rivals. This is analogous to findings in the sales promotion literature that price promotions often produce negative returns (e.g., [33]). In this circumstance, any market share gain via greater market power and the ability to charge higher prices is not only relinquished but reversed. In addition, because there is a price-perceived quality heuristic among customers in many markets (e.g., [52]), charging lower prices may offset any quality-signaling benefit of higher market share, and the net result on perceived quality could be negative. Our previous results suggest that in most circumstances, these negative market power and quality-signaling effects are likely to outweigh any firm efficiency gains via learning produced by increasing market share. Empirical test of the two conditionsTo assess the robustness of our mechanism results under conditions when the market share–profit relationship may be negative, we first identified firms that are likely pursuing a niche strategy by combining a new text measure indicator of the degree to which a firm has a niche strategic emphasis (for details, see Web Appendices 4a and 4b) with the number of brands they market (both firms with both a high niche-focus in their product-market coverage strategy and those that offer only a single brand are likely to be niche firms). The face validity assessments in Web Appendices 4a and 4b support this identification logic. Second, to identify firms that may be ""buying"" market share, we created a dummy variable indicator for firm-years in which a firm both reduced its average prices (computed using GMID data) and experienced a positive market share change.We then reestimated our market share–profit models from Table 3 with the addition of the new niche firm measure and buying share dummy indicator, along with their respective interactions with market share. As Table 6 shows, model M1 shows that higher market share reduces profit for niche firms (−.115, p < .05). As we expected, M2c reveals that this is a result of a strong negative effect of market share via perceived quality (−.062, p < .001). M1 also shows that the effect of market share on firm profit is significantly lower for firms ""buying"" market share (−.036, p < .001).[14] The mechanism results indicate that this is caused by a significant reversal in both the market power (M2a: −.047, p < .001) and firm efficiency (M2b: −.033, p < .001) effects of market share and a reduction of the perceived quality mechanism to insignificance (M2c: −.022, p > .1). These findings suggest that any supplier input cost benefits of greater market power from market share are more than offset by lowering downstream prices to ""buy"" the market share. In addition, consistent with the well-known ""bullwhip"" effect, rapid increases in short-term demand resulting from lowering price seems to disrupt the efficient production and delivery of these firms' products and services. Overall, the Table 6 results provide support for the robustness of the three mechanism variables in mediating the relationship between firm market share and profit, even in the relatively rare conditions under which the relationship is negative.GraphTable 6. Moderating Effect and Mechanism When We Include Conditions in Which Market Share May Have a Negative Effect on Profit. Model Specifications and Dependent VariablesM1M2aM2bM2cM2dProfit(t + 1)Power(t + 1)Efficiency(t + 1)Quality(t + 1)Profit(t + 1)Direct Effect Market Share(t).058***.091***.034.108***.033Indirect Effect Market Power(t).218*** Firm Efficiency(t).095*** Perceived Quality(t).179***Moderators Switching Costs(t).149*.118.021.081***.041 Services Dummy(t)−.042*.088.510***.027−.010 Firm Age(t).193.037−.036.008.021 Niche Focus Firms(t).078***−.025***.027.119**.180* Buying Share Dummy(t).016.024−.032*−.009−.026Prior Moderator Effects Share(t) × Switching Costs(t).050*.162*.022.200*.037 Share(t) × Services Dummy(t).063***−.011.166***.018.019 Share(t) × Firm Age(t)−.053***−.055−.113***−.078−.009Proposed Negative Moderators Share × Niche Focus Firms(t)−.115**−.016−.001−.062***−.010 Share × Buying Share Dummy(t)−.036***−.047***−.033***−.022−.036Controls Firm Size(t).490***.035***.086*.039***.049*** ADV(t).233***.008.010.018.041* R&D(t).241***.031***.015.055***.059*** Market Growth(t).022***.023.018.015.012Specification Tests −Log-likelihood3,104.92 R2.55.30.39.20.72 18 *p < .05.19 **p < .01.20 ***p < .001.21 Notes: 2,629 firm-year observations covering 207 firms for the 2000–2013 period (sample size due to ACSI data availability). For Niche Firms, indirect effect = 58%, of which Power = 21%; Efficiency = 0%; and Quality = 79%. For Firms Buying Share, Indirect Effect = 33%, of which Power = 56%, Efficiency = 22%, and Quality = 22%. Comparison with E-H's (2018) Indirect Moderator InferencesHaving provided robust evidence to support the three mechanisms, to offer additional insight on the utility of the direct measures of the three mechanisms employed, we also examined how the results compare with previous indirect inferences regarding these mechanisms drawn from observable moderators of the market share–profit relationship. To accomplish this, we first replicated E-H's (2018) measures as well as main effect and substantive moderator results (banking services, concentration, and B2C). We then examined the mechanisms explaining the effect of these moderators of the market share–profit relationship in our sample, and the results are revealing (Web Appendix 12). For example, we find that while E-H's theorizing focuses on quality signaling, the reason for the stronger market share–profit relationship in B2C industries is a significant strengthening of all three mechanisms relative to business-to-business (B2B) industries (market power:.143, p < .001; efficiency:.044, p < .05; quality:.082, p < .05). In addition, we find that while banks are in general more profitable (.426, p < .01) and have greater market power (.042, p < .05), this is in spite of—not due to—their market share (−.087, p < .05). In fact, results reveal that market share reduces banks' profitability by lowering their efficiency (−.410, p < .001). We also find a direct moderating effect for concentration (.109, p < .05), whereas E-H found a nonlinear effect, and we observe that this is via increasing the market power benefit of market share (.110, p < .01). These results show that using moderators to indirectly infer the three mechanisms underlying the market share–profit relationship often does not do a good job of isolating these mechanisms. This reinforces the value of direct empirical understanding of the mechanisms linking market share with firm profit in predicting when market share is more valuable and thus when managers should set market share goals. When Its Value Is Indicated, How Should Managers Measure Market Share?The new empirical understanding of the mechanisms linking market share with firm profit revealed in our analyses can help managers evaluate when market share may be a valuable goal. When its value is indicated, a manager's next task is to decide how to measure market share for goal setting and performance monitoring. To provide insights on this question, we examined two key market share measure design choices facing managers. First, ""share of what?,"" in terms of unit sales volume or sales revenue, should be used in computing market share ([ 6]). Managers use both types of indicators to track market share, and both rank among the most popular measures of marketing performance in practice (e.g., https://marketbusinessnews.com/financial-glossary/market-share/). The second is ""relative to what?,"" in terms of whether and how the firm's market share is benchmarked—as an absolute value (% of total market sales) or relative to others in the market (the market share leader or the top three players). Revenue versus unit shareTo provide insights on the first question, we replicated model M3 in Table 2 and replaced the sales revenue market share with unit sales volume market share using the same dynamic market definition. As we show in Table 7, in contrast to revenue market share (M2:.151, p < .05), unit market share (M1:.009, p > .1) does not predict firm profit. This result is robust to all of the same checks performed on our revenue market share main effect testing analyses and also to using benchmarked (vs. absolute) values of unit market share. Post hoc analysis of the mechanisms associated with unit share (Web Appendix 13) reveal that although it has a small positive effect on both market power and firm efficiency (consistent with the learning effect logic that market share is a proxy for number of units produced), this is insufficient to overcome the significant negative relationship with quality signaling. We reason that the weaker effect of unit (vs. revenue) market share on market power is a result of unit market share ignoring prices charged to customers (a downstream indicator of market power). The negative quality-signaling effect of unit market share is consistent with both ignoring price (which is often a quality cue for customers) and the notion that ubiquity reduces perceived exclusivity (e.g., [34]). These results show that when the presence of the three mechanisms indicates market share's value, managers should set market share goals and monitor performance in terms of revenue market share.GraphTable 7. Market Share–Profit Relationship Using Alternative Market Share Measures and Benchmarks. Market Share Measure, Model, Benchmark, and Dependent VariableUnit Market ShareRevenue Market ShareRevenue Market ShareRevenue Market ShareM1M2M3M4Independent VariablesAbsoluteAbsoluteRelative to Market LeaderRelative to Top 3Profit(t + 1)Profit(t + 1)Profit(t + 1)Profit(t + 1)Main Effect Market Share(t).009.151*.222***.392***Controls Firm Size(t).201***.270***.213***.243*** Advertising(t).081**.121***.121***.123*** R&D(t).033.024.033.030 Market Growth(t).001.001.004.006Specification Tests Wald χ2115.23188.91210.81167.81 R2.18.59.52.52 22 *p < .05.23 **p < .01.24 ***p < .001.25 Notes: 3,058 firm-year observations covering 244 firms for the 2000–2013 period, except for model specification M1, which is estimated using 2,214 firm-year observations covering 235 firms for the period 2004–2013 (due to GMID data availability). In a subsequent robustness check, model specifications M2 through M4 were reestimated using the same 2,214 firm-year observations, and estimates remain identical. Absolute versus relative shareIn terms of the ""relative to what?"" question, in Table 7 we compared the market share–profit estimates of the absolute value of market share used in the main effect testing (M2) and two different relative market share benchmark operationalizations: relative to the market share leader (M3) and relative to the combined market share of the top three market share firms (M4).[15] The results indicate that benchmarked measures of firm market share provide stronger predictive power (of future profit) (M3:.222, p < .001; M4:.392, p < .001, respectively) than using absolute market share (M2:.151, p < .05). Subsequent analysis of the three mechanisms show that this is a result of the relative market share measures ""dialing up"" the market share–market power link (Web Appendix 14). This is likely due to such ""relative to others in the same industry"" measures capturing some of the industry-level market concentration power that our previous analysis showed increased the market share–market power relationship in terms of both switching costs (which are higher when markets have fewer equivalent players) and average market share (as an indicator of market concentration in the E-H [2018] replication analyses). Implications Implications for TheoryThis study offers several new insights into theories of firm behavior and performance. First, economic theory assumes that market share predicts firm profit but offers different reasons for why this relationship exists. We provide the first simultaneous test of three mechanisms proffered in competing economic theories for this relationship and show that in combination, they explain the vast majority of the variance in the market share–profit relationship. This suggests that individual single-theory lens explanations of the mechanisms linking market share with profit are incomplete, and all three mechanisms can provide higher (or lower) explanatory power under different conditions. While, on average, market power provides the highest level, and firm efficiency the lowest level, of explanatory power, we also identify conditions under which the reverse is true (e.g., for young firms). Thus, none of the three theories from which the hypothesized mechanisms arise is ""correct"" or ""incorrect,"" but market power and quality signaling generally explain more of the variance in the market share–profit relationship across firms and industries.Second, our results offer new insights into efficiency-enhancing experience-based ""learning effects"" identified in strategic management theorizing ([ 2]). Management scholars have used this logic to explain why market share (a proxy for the number of times a firm may have produced a value offering) may be positively related to firm profit (e.g., [29]). We find that while firm efficiency is valuable (predicts profit), on average it is explained mainly by a firm's size rather than its market share. This suggests that for most firms, scale economies are more important in driving profit than economies of learning. However, for young firms, we find that market share delivers significant efficiency benefits above and beyond those associated with size, and we also find significant efficiency benefits from market share among service businesses. This suggests that ""learning by doing"" effects occur where organizational routines are less set and when firms can use experience gained to update and change their processes with lower investments.Third, we find support for information economics theorizing on the value of signals of unobservable firm quality. While prior research has explored market share's role in consumer evaluations of quality ([34]), we provide the first empirical evidence that market share generally signals firm quality and thereby increases firm profit. The negative effects on perceived quality we observe when using unit (vs. revenue) market share also suggest that price combines with market share in signaling quality to customers. In addition, we find that market share's positive quality-signal effect depends on previously unidentified industry and firm conditions (stronger for younger firms, in B2C markets, and for those with switching costs).For researchers, our study also has broader implications. Not least, it clearly shows the effect that sampling can have on the findings and inferences drawn in firm-level empirical research. We find wide variance in both the main market share–profit relationship and in the specific mechanisms accounting for the relationship across industries. Thus, samples made up of a single industry, or an industry dominated by certain types of firms, would lead to very different results and widely varying inferences being drawn as to which theory may be supported in empirical tests. This is unlikely to be unique to the market share phenomenon we examine. In addition, our study also reveals the desirability of directly observing (or at least finding direct indicators of) mechanisms believed to underlie relationships of interest. In particular, our results highlight the need for researchers to be careful about using indirect contingencies to infer such unobserved mechanisms when there may be more than one mechanism involved. Implications for PracticeThis study also provides new insights for managers regarding how market share should be measured. Although unit (volume) market share is widely used in practice to set marketing goals and monitor performance (e.g., auto and motorcycle manufacturers, many consumer packaged goods companies), our results reveal that it is not predictive of firm profit, whereas revenue (value) market share is. We also find that in terms of predicting profit, relative (to others) measures of revenue market share can be superior to absolute measures. Post hoc analyses suggest that such relative measures can enhance the market power value of the observed market share, and that benchmarking a firm's market share relative to the top three market share firms versus the market share leader offers a stronger predictor of future profit. This is aligned with the intuition that benchmarking against others provides an indicator of both the firm's market share and the concentration present in the marketplace, which we show interact significantly in predicting firm performance.To provide finer-grained managerial insights, we also examined ( 1) which measures of market share were the strongest predictors of future profit for different types of firms to help managers select the most appropriate market share metrics for goal setting and performance monitoring and ( 2) the average profit value of a 1% increase in the average firms' market share for different types of businesses to give managers a calibration of the dollar-value benefits that may be expected when evaluating costs associated with share building strategies. Given our sample size, we are somewhat limited in how fine-grained we can be in these analyses without running into power issues. We therefore split our sample in a managerially meaningful way by identifying firms on the basis of whether they serve primarily consumer or business customers and whether their value offerings are mainly product- versus service-based. As shown in Table 8, the results vary across the four cells, with B2C product firm and B2B service firm profit being most strongly predicted by absolute revenue market share, whereas for B2C service and B2B product firms, it is revenue share relative to the top three market share players. The one-year profit increases associated with a 1% improvement in the average firm's market share vary across the four cells from a low of just over $1 million to almost $6 million. These findings have clear and important implications for managers setting market share goals and monitoring market share performance in their firms and offer a useful dollar benefit scale calibration for managers with respect to the potential payoffs they may expect from investments in market share–building strategies.GraphTable 8. Managerial Matrix: Metrics. ProductsServicesB2CStrongest market share–profit predictorAbsolute revenue shareRelative to top three revenue shareMean firm market share6.80%7.19%Profit value of 1% increase in mean market shareFrom 6.80% to 6.87%:.121% (p < .001) × $840 million = $1.02 millionFrom 7.19% to 7.26%:.704% (p < .001) × $840 million = $5.9 millionObservations1,910 firm/year observations (136 firms)484 firm/year observations (52 firms)B2BStrongest market share–profit predictorRelative to top 3 revenue shareAbsolute revenue shareMean firm market share6.68%7.31%Profit value of 1% increase in mean market shareFrom 6.68% to 6.75%:.309% (p < .001) × $840 million = $2.6 millionFrom 7.31% to 7.38%:.146% (p < .01) × $840 million = $1.2 millionObservations322 firm/year observations (32 firms)342 firm/year observations (24 firms) 26 Notes: Unit share is not predictive of firm profit in any one of the four cells. Reported elasticities estimated via a model specification equivalent to M3 in Table 2, with the noted strongest market share predictor measure as a regressor and using the observations specific to each of the Product/Services and B2C/B2B cells. Profit increase $ values are for a 1% increase in the mean firm's market share in each cell (e.g., 7.310% to 7.383%) not an increase of 1 point of total market share (e.g., from 7.310% to 8.310%). Because we estimate log-log models, the estimated coefficients in each condition can be interpreted as market share–profit elasticities (%) which can be converted to a dollar profit value by multiplying them by the mean profit in our sample (i.e., $840 million).In terms of where managers would be advised to pursue market share to a greater or lesser degree, our results provide several new insights (Table 9). For younger firms and for nonbanking services firms, it may make sense to set market share goals and monitor performance. It may also be more beneficial for firms operating in marketplaces with high levels of quality uncertainty and those with higher switching costs. However, it may make less sense for banks and firms in industries in which pricing power is low and/or quality is relatively certain. Older firms may also find market share to be of less value as a marketing goal and performance metric. Firms pursuing a niche strategy would be well advised to either ignore market share or ensure that they assess it only within their selected niche market definition. Finally, we show that, while relatively rare, ""buying share"" is not a profitable move.GraphTable 9. Managerial Matrix: Contingency Effects on Share-Profit Mechanisms. Relative Mechanism ImportanceContingencyMarket PowerFirm EfficiencyPerceived QualitySwitching costs (high)+n.s.+Service (vs. product)n.s.+n.s.Firm age (older)n.s.−−Concentration (more)+n.s.n.s.B2C (vs. B2B)+++Banking (vs. others)n.s.−n.s. 27 Notes: n.s. = not significant. This table summarizes analyses reported in Table 4 and Web Appendix 12, with mechanism importance indicated relative to the average displayed by all firms in our sample. Implications for PolicyFor policy makers, this study provides new insights with respect to when market share may lead to market power and potential abuse that requires regulation. Importantly, our results show that firm profits from market share result from quality signaling and learning-based efficiencies as well as market power. Thus, policy makers need to be careful not to directly equate market share and market power; we show that while they are often related, they are far from synonymous. Rather, our results suggest that regulatory authorities can be less concerned by a firm's market share in marketplaces where customer quality uncertainty is significant and where efficiency-enhancing learning benefits from market share may exist (e.g., young firms, service firms). In such conditions, market share could enhance rather than harm consumer welfare by reducing consumer–firm information asymmetry and potentially lowering costs. Limitations and Future ResearchThis study has some limitations that should be taken into account when considering the findings. First, because we require public data to explore our research questions, our sample is naturally skewed toward larger firms. While we include small, nonpublic firms in the definition of the total market sales used in constructing the robustness check NAICS measure of market share, we are unable to include such firms' individual market shares in the hypothesis testing because these firms' sales data are private. Although we have a wide range of market shares in our sample (with a low of less than 1%, a high of 77%, and a mean of less than 7%), and no evidence of range restriction in our key variables, researchers with access to private firm data could test the generalizability of this study's findings to firms with much smaller market shares.Second, our data are focused on firms with U.S. listings. However, including studies covering broader geographies and longer time period data, E-H (2018) suggest that the market share–profit relationship is weaker in recent times in Western Europe than the United States, so future research in other regions is required to examine how the mechanisms we identify may differ across geographies. Third, our study examines market share at a firm level. However, market shares may also be computed at other levels (e.g., brand or geographic market level). A post hoc analysis of monobrand firms in our sample suggests that the same market share–profit main effect and mechanism relationships hold (Web Appendix 15); however, research is required to confirm this.Our study also reveals several new avenues for theoretically interesting and managerially relevant research. First, we find that the vast majority of market share's effect on profit is mediated through its effects on firm market power, perceived quality, and efficiency. This suggests that new theorizing regarding why market share is valuable may be of limited value. However, in light of our findings, new research on the details of how each of the three mechanisms works is clearly required. For example, what is the relative effect of market power on upstream versus downstream parties, and how much is contributed by cost reductions versus pricing versus coordination benefits? Similarly, what types and levels of quality uncertainty create conditions that lead to market share's value in signaling quality? How much of market share's signal value is to upstream versus downstream parties?Second, this study reveals market power, quality signaling, and operating efficiency as the mechanisms linking market share with firm profit. Because market share is a market-based outcome of firms' marketing efforts, this raises the interesting possibility that these three mechanisms may also mediate the relationship between other marketing-related phenomena and firm performance. For example, are market-based assets such as brand equity and customer relationships also linked to firm profit via the same three mechanisms? Are there also other mechanisms that may be available to such market-based assets but not to market share?Third, given that market share is more or less valuable under different market and firm conditions—and that buying share is both rare and ineffective—does it also matter how firms create and leverage market share? For example, are market shares more or less valuable to firms pursuing low-cost business strategies versus those pursuing differentiated advantages? Are the three mechanisms linking market share and profit the same for these different strategies, or are some mechanisms more important to one strategy than another? Addressing these questions would provide important new insights for both managers and researchers. Appendix: Measure DetailsGraph Appendix: Measure Details VariablesMeasurement DetailsData Source/LiteratureFirm ProfitNet income of the firm (Item NI).CompustatMarket Share (Revenue)Percentage of an industry or market's total sales garnered by a particular firm over a specified time period. Markets are defined through text analysis of similarity between product-market descriptions within 10-Ks. Sales for each firm obtained from Compustat.SEC, CompustatHoberg and Phillips (2010)Market Share (Units)Units sold by each firm were obtained directly using the GMID (Euromonitor) database. Market definition for firms with unit share data calculated as for revenue share.GMIDMarket Power (Power)Operationalized based on a profit elasticity measure following Boone (2008), estimated by regressing (at the industry level) firms' profit (Item NI) on their total costs (Items COGS and XSGA). Firm-specific residuals are used to calibrate each firm's margins relative to industry average, providing a firm-level indicator of market power.CompustatBoone (2008)Firm Efficiency (Efficiency)Concerns producing goods and services with the optimal combination of inputs to produce maximum output at the minimum cost. We use a stochastic frontier estimation approach with operating expense (Item XOPR) as the input and total sales (Item SALE) as the output.CompustatBauer, Berger, and Humphrey (1993)Perceived Quality (Quality)Measured using customer perceived quality ratings of the firm's brand(s) from Equitrend database.EquitrendMorgan and Rego (2009)Switching CostsThese are perceived costs associated by the firm's customers with moving to an alternative supplier. We calibrate these costs as the degree to which customers exhibit loyalty to a firm that cannot be explained by the level of satisfaction delivered by the firm's offerings. Using ACSI data, we estimate customer-level loyalty as a latent factor comprising variables capturing customers' repurchase intentions and price sensitivity. Satisfaction is the ACSI measure detailed previously. We estimate switching costs for each firm/year as the residual of regressing each firm's customers' loyalty onto its customers' satisfaction, controlling for industry and time.Loyalty(it) = β0 + β1 × Satisfaction(it) + ID(it) + YD(it) + ε(it), where ID(it) are industry and YD(it) year dummies. ε(it) is the residual of this regression and is used as our estimate of switching costs, which are firm- and year-specific.ACSI (firm/year-level aggregation of individual-level respondent survey response data).Rego, Morgan, and Fornell (2013)Niche-Focused Strategy (Niche)Text analysis employing a new dictionary utilizing an inductive word search with exemplar niche firms. The analysis is then performed using a bag-of-words approach where each firm gets a score corresponding to the ratio of niche-related words and total words in each firm 10-K. To ensure that we were isolating the types of niche firms where market share was expected to be negatively associated with profit, suggested in the theorizing (i.e., those pursuing a single niche in a market vs. those targeting several different segments with different offerings), we then identified mono- versus multibrand firms by multiplying the niche-focus score for each firm by the dummy variable (1 for monobrand firms, 0 for multibrand firms).New measureService-Dominant Markets (Services)Dummy variable identifying service firms/ industries using Fama–French NAICS industries.Fama and French (2008)Firm AgeNumber of years of operation of the firm since incorporation, obtained from the firm's annual reports and websites.Industry ConcentrationIndustry-level average market share.Edeling and Himme (2018)B2C versus B2B FirmsDummy variable capturing whether the firm caters mainly to business customers. Each firm was coded manually by three coders who used information on categorization from secondary sources such as Hoover's. Reliability was >85%.Services (Banking)Dummy variable capturing whether a firm belongs to the banking sector (SIC Code 602).CompustatCompetitor OrientationText analysis of 10-K reports following dictionaries on competitor orientation (as a part of Market Orientation) developed in prior literature (Zachary et al. 2011).SECZachary et al. (2011)ControlsFirm SizeThe firm's reported total assets (Item AT).CompustatMarket Growth Annualchange in cumulative industry sales (Item SALE).CompustatR&D ExpenseFirm's reported expenditures on Research and Development (Item XRD).CompustatAdvertising ExpenseFirm's reported expenditures on Advertising (Item XAD)CompustatRobustness Check VariablesROAThe ratio of current year income before extraordinary items (Item IB) to the firm's previous year total assets (Item AT).CompustatTobin's qRatio of the firm's market value to the replacement cost of physical and intangible capital of the firmWe measure the firm's market value as the market value of outstanding equity (Items PRCC_F × CSHO), plus the book value of debt (Items DLTT + DLC), minus the firm's current assets (Item ACT). The firm's replacement cost of physical capital is measured as the book value of property, plant, and equipment (Item PPEGT). Intangible capital is estimated as the sum of the firm's knowledge capital (the capitalized value of firm R&D expenditures) and organizational capital (a fraction of the capitalized value of firm SGA expenditures) following Peters and Taylor (2017).Peters and Taylor (2017)Alternate Market PowerOperationalized based on Lerner Index as profit margin relative to price. Average variable costs are used as a proxy for marginal costs, operationalized using total variable costs divided by sales (Items XOPR and SALE). Average price was estimated dividing sales revenues (Item SALE) by unit sales (obtained from GMID database).GMID, CompustatBoone (2008)Market Share FocusBased on text analysis of 10-K reports, estimated as the ratio of the number of times ""market share"" is reported relative to the total number of words in the annual 10-K report.New measurePerceived QualityMeasured via average annual perceived quality ratings of the firm's brand(s) from the Brand Asset Valuator database.Brand Asset Valuator Mizik and Jacobson (2005)Perceived QualityMeasured using average annual firm quality ratings from Fortune's World's Most Admired Companies database.AMACCretu and Brodie (2007) 28 Notes: SEC = Securities & Exchange Commission; SGA = selling and general administrative.  "
7,"Examining Why and When Market Share Drives Firm Profit Many firms use market share to set marketing goals and monitor performance. Recent meta-analytic research reveals the average economic impact of market share performance and identifies some factors affecting its value. However, empirical understanding of why any market share–profit relationship exists and varies is limited. The authors simultaneously examine the three primary theoretical mechanisms linking firm market share with profit. On average, they find that most of the variance in market share's positive effect on firm profit is explained by market power and quality signaling, with little support for operating efficiency as a mechanism. They find a similar explanatory role of the three mechanisms in conditions where market share negatively predicts profit (for niche firms and those ""buying"" market share). Using these mechanism insights, the authors show that the value of market share differs in predictable ways between firms and across industries, providing new understanding of when managers may usefully set market share goals. The authors also provide new insights into how market share should be measured for goal setting and performance monitoring. They show that revenue market share is a predictor of firm profit while unit market share is not, and that relative measures of revenue market share can provide greater predictive power.Keywords: market share; quality; efficiency; market power; niche; firm profit; revenue share; unit shareMany firms use market share to set goals and monitor marketing performance, and market share is also widely used in research examining marketing's performance impact ([24]; [40]). [20] recent meta-analytic study (hereinafter, E-H 2018) reports a significant positive relationship between a firm's market share and its economic performance and identifies contingencies affecting this relationship. However, while the literature suggests several reasons market share may drive firm performance, few empirical studies have directly examined any (and none more than one) of these mechanisms. Thus, little is known about the underlying ""why"" of mechanism(s) linking firms' market share and economic performance and how they may both explain previously identified moderators and facilitate identification of additional moderators of this important relationship. In addition, when understanding of the mechanisms linking market share with firm performance suggests that it is economically valuable to measure market share for goal setting and performance monitoring purposes, managers currently have no empirical insights into how to do so.These knowledge gaps are important because understanding why market share is linked to firms' future profit can provide new insights into when and where market share is most likely to be valuable. While many firms use market share as a marketing performance metric, our research identifies new ways for managers to assess when this is most appropriate—and when it may not be. Because market share is such a common marketing goal, this is also important in delineating the role that marketing plays in determining firm performance and in understanding contingencies that may affect this role. Exploring the predictive value of alternative measures of market share, we also provide important new insights into how market share goals should be set and performance assessed via different market share measurement options in terms of unit versus revenue market share and absolute versus relative market share.In addressing these key questions, this study offers several contributions. First, we provide the first direct empirical assessment of the three primary causal mechanisms that have been theorized to link market share with firm profit: market power, operating efficiency, and quality signaling. Using direct measures, we examine each of these three mechanisms simultaneously and show that both market power and quality signaling are key mechanisms linking market share with firm profit. On average, we find little evidence of theorized economies of scale and learning benefits of market share, but we identify conditions under which such efficiency benefits do exist. We find no support for a fourth theorized mechanism linking market share negatively with profit as a result of a strong competitor orientation. However, we do find support for the same three mechanisms in conditions under which the market share–firm profit relationship is negative—for niche firms and when a firm ""buys"" market share. Overall, these findings provide important new empirical insights into market share's value-creating role.Second, using these new causal mechanism insights, we explore the consistency of the market share–profit relationship across different types of marketplaces and firms where the relative value of market share via the three mechanisms may be expected to vary. We show that the market share–profit relationship varies across industries and firms, and that the different causal mechanisms identified provide high explanatory power for such variations; thus, all three theories from which the hypothesized mechanisms arise can be ""correct."" In addition, this insight provides an empirically supported way for managers to identify when setting market share goals and monitoring market share performance may be more or less valuable. In contrast, we find that using indirect contingencies to try to infer the mechanisms linking market share with performance relationship often does not align with the directly observed mechanism effects, further indicating the value of direct measures in understanding the ""why"" mechanisms involved.Third, we extend recent meta-analytic insights regarding the nature of the relationship between market share and firms' economic performance by using direct measures of the three most widely cited mechanisms: measures of both revenue and unit market share and different market share benchmarks, firm size controls to isolate the benefits of market share versus firm scale, and different econometric approaches to address panel data and endogeneity estimation concerns. These aspects of our study enable us to provide several new insights. For example, we show that for most firms, economies of scale arise from firm size and not firm market share. They also allow us to identify which market share metrics are most predictive of profit for different types of firms and the economic value of increasing market share on these metrics. This is useful new knowledge for managers because it provides new insights into how market share should be measured in goal setting and performance monitoring as well as the scale of profit benefits that may be expected from any gain in a firm's market share.The article is organized as follows. First, we develop a conceptual framework and hypothesize relationships involving the three key mechanisms by which market share may be linked with firms' future profit. Next, we use the three mechanisms to identify three conditions under which the market share–profit relationship may be expected to be stronger versus weaker. We then describe the data set assembled and analysis approaches used to test the hypotheses and discuss the results. Having shown that the three mechanisms collectively mediate the market share–profit relationship, we then assess whether this remains true even under conditions when the market share–profit relationship is negative. Next, having shown that managers can use knowledge of the three mechanisms to identify when market share is likely to be economically valuable for their firm, we assess how managers may best measure market share. Finally, we assess the implications of our study for theory and practice and identify new questions for future research suggested by our findings. Conceptual Framework and HypothesesMuch of the theorizing regarding market share and firm performance in economics and management concerns related but distinct phenomena such as firm size and market concentration. We focus only on relationships that directly pertain to firm market share and the mechanisms underlying its economic value. As a result, we center our market share conceptualization on revenue market share—units sold × realized price (i.e., sales revenue) divided by total market sales revenue. In doing so, we conceptualize and measure the ""market"" as comprising firms selling similar product/service offerings. However, we also examine unit market share—units sold divided by total market unit sales—as well as several different operationalizations of revenue market share in robustness checks and post hoc analyses. Market Share and Firm Economic PerformanceThe marketing literature generally views market share as an indicator of the success of a firm's efforts to compete in a product-marketplace (e.g., [13]; [63]). From this perspective, market share is an outcome of a firm's marketing efforts including its advertising and promotion, product/service offering quality and price, channel and customer relationships, and selling activities ([24]). All of these are evaluated relative to those of other suppliers by customers (channel members and end users) when they consider and select offerings, which is what conceptually distinguishes a firm's market share (how the firm's sales compare with those of the total market) from its sales revenue (the number of units sold × price). Importantly, this means that (unlike sales revenue) market share is not a component variable in any indicators of firm economic performance,[ 6] so there is no synthetic (or ""hard-wired"") market share–firm economic performance relationship.Historically, the empirical literature provided conflicting and equivocal answers concerning the ""main effect"" relationship between firms' market share and their economic performance (e.g., [11]; [36]; [37]). However, the recent E-H (2018) meta-analysis using more sophisticated methodological approaches has provided new insight on this question, showing a generally positive effect of market share on firm economic performance. We corroborate this in our data and focus our hypothesizing on why this relationship exists and how this ""why"" understanding may help explain and predict differences in the strength of the relationship across firms and industries. Mechanisms Through Which Market Share May Impact ProfitWhile several explanations have been independently proffered for why a firm with higher market share may enjoy superior economic performance, three mechanisms are much more widely discussed than others. As Figure 1 shows, we focus our theorizing on these mechanisms and consider how each may link a firm's market share with its profit.Graph: Figure 1. Conceptual framework. Market powerThe first proposed mechanism by which market share may be linked with firm profit is via market power (i.e., the firm's ability to influence the price of its product/service offerings by exercising control over demand, supply, or both; e.g., [10]; [59]). Industrial organization theory posits that firms enjoy superior profit when they are able to charge higher prices than rivals, which is determined by the availability of alternatives to customers and firms' ability to create and/or control resources that give them stronger market positions (e.g., [57]). Market share may be a resource that provides a firm with the opportunity for greater market power over both ""upstream"" suppliers and ""downstream"" channels and customers and thereby control prices in several ways.For upstream suppliers, buyer firms with higher end-user market share are more attractive, which may allow them to negotiate lower prices and/or higher-quality inputs from their suppliers ([ 9]). For example, Apple's smartphone market share allows it to both charge app developers for selling their products and enforce strict quality controls on the apps it sells. It may also increase supplier willingness to cooperate with others in the buyer's supply network to further lower the buyer's input costs and improve input quality ([28]). For downstream channels, higher–market share firms are more attractive upstream partners because they generate end-user demand for more and/or higher-value products. They may also attract larger customer numbers and/or more frequent interactions for channels to engage in cross-selling. This may enable higher–market share firms to negotiate better list prices than rivals in downstream channels and to benefit from greater channel cooperation (e.g., preferred shelf-space, merchandizing support). For example, PepsiCo's snacks division leverages its leading market share position to obtain preferential shelf and display access in many U.S. retail chains. The input and go-to-market cost and quality benefits of higher–market share firms should allow them to provide better value offerings, which may thus allow them to charge higher prices to end users (as in the case with Apple) and/or enjoy higher profit margins on each unit sold (e.g., Walmart). Thus, H1:  The positive effect of market share on firm profit is mediated by the firm's market power. Operating efficiencyThe second theorized mechanism by which a firm's market share may lead to profit is via increasing the firm's operating efficiency (e.g., [17]). Disputing market power arguments, the ""Chicago school"" in economics argues that market share is an outcome of firm efficiency that allows a firm to sell quality-equivalent offerings at lower prices than rivals, attracting greater demand (e.g., [14]; [50]). Following this logic, strategic management scholars propose that higher market share may also allow firms to further increase their efficiency in a recursive relationship with lowering firm costs via learning effects (e.g., [ 1]; [29]). Much of this logic is framed in terms of a firm's position on the production ""experience curve"" as a function of the volume of units sold, with greater experience allowing production-related learning and lower production costs (e.g., [30]). Thus, firms selling a greater number of units produce more and learn how to do so more efficiently. For example, Tesla has used its greater accumulated experience in producing electric vehicles (EVs) to lower its costs compared with rivals.Conceptually, this may also be possible via market share impacting the number of interactions a firm has with suppliers, channels, and customers, enhancing opportunities for higher–market share firms to learn and use knowledge gained to improve their supply-and-demand chains ([55]). For example, Tesla has used its greater EV sales to learn how to drive improvements in battery designs and configurations from suppliers as well as to optimize its own software to increase EV range. More interactions also increase the likelihood that suppliers, channels, and customers will trust higher–market share firms, increasing information sharing, lowering coordination costs, and enhancing cooperation in changes designed to enhance the firm's supply-and-demand chains ([16]; [27]). This should enable higher–market share firms to lower costs and enhance supply-and-demand chain quality and reliability, allowing superior value offerings for customers and/or greater margins. Thus, H2:  The positive effect of market share on firm profit is mediated by the firm's operating efficiency. Quality signalingThe third mechanism by which market share may enhance firm profit is by signaling unobserved quality. Information economics theory posits that customers' limited evaluative knowledge often makes it difficult for them to observe ""true"" product/service quality (e.g., [38]; [41]). Empirical studies also show that customers are often unable to accurately (or confidently) evaluate an offering's quality prior to making purchase decisions, and they frequently rely on indirect cues (e.g., [47]; [61]). Market share may signal quality by increasing the credibility of firm claims and thereby lowering customer perceived risk ([21]; [34]). Customers may also infer that ""everyone can't be wrong"" in choosing the offerings of a high–market share firm (e.g., [18]). For example, Toyota campaigns have touted that its products are ""#1 for a Reason."" Thus, to the extent that market share signals higher quality, it should increase future demand and reduce customer churn. It may also lower the firm's costs relative to rivals, because alternative ways to signal quality (e.g., advertising) may be more costly.Market share may also signal quality to suppliers and channel members. Firms that are perceived to be producing high-quality offerings may be viewed by suppliers as not just attractive buyers, in terms of their own demand, but also as potentially providing a halo image spillover benefit. Similar to customers viewing them as having ""too much to lose"" to provide inferior offerings, supplier choices made by high–market share firms may be viewed as being based on ensuring high quality and reliable inputs to protect their reputation and market position. For example, Apple's suppliers are frequently identified as such in business press reports. This could also apply to channel partners where selling offerings that are perceived as higher quality can provide a halo effect making the channel member more attractive to other suppliers and end-user customers (e.g., [42]). All of these arguments suggest the following: H3:  The positive effect of market share on firm profit is mediated by the firm's perceived quality. Using Mechanism Insights to Predict Where Market Share Is ValuablePrior research suggests that the value of market share varies across industries (e.g., [ 4]), indicating that setting market share goals may be more beneficial for some firms than others. To explore this, E-H's (2018) meta-analysis examines the sample characteristics most commonly reported in prior studies and reports that market share is more valuable in business-to-customer (B2C) markets and in markets with medium market concentration, whereas it is less valuable in the banking industry. While offering initial useful insight to managers, these boundary conditions are limited in number and scope—and the ""why"" mechanisms involved are unobserved. Robust empirical understanding of the mechanisms using direct assessments should allow additional boundary conditions to be identified and provide empirically verified principles for managers to distinguish when they should and should not care about market share.To provide an initial assessment of the predictive value of our mechanism results and offer new insights for managers, we next examine the extent to which the market share–profit relationship varies under conditions in which each of the three mechanism in turn may be expected a priori to be more versus less important. For each mechanism, we identify a condition expected to be particularly impactful on that particular market share–profit pathway. However, in our analyses we also allow for the possibility that each of the conditions we identify may affect the strength of all three mechanisms linking market share with profit. First, in terms of market power we examine industries characterized by higher customer switching costs, where firms are more easily able to retain customers. Firms should benefit more from the market power provided by market share when switching costs are high because they are better placed to increase prices without fear of customers switching ([23]; [58]; [60]).Second, in terms of the value of operating efficiency in explaining the market share–profit relationship, the literature suggests that cost-reducing learning effects are more likely earlier in the life of a firm (e.g., [66]). For example, ""experience effect"" studies of the value of a firm's cumulative doubling of output show that this is more likely to occur early in a firm's existence (e.g., [31]). In addition, learning effects require changing and adapting firms' processes—which tend to become more rigid over time (e.g., [54]). Thus, younger firms are less knowledgeable in their operations and less ""set in their ways,"" providing incentives to seek out the learning opportunities presented by market share and the ability to exploit the efficiency-enhancing knowledge gained via process changes.Third, to explore conditions where the quality-signaling value of market share may be stronger, we examine differences between ""service-dominant"" and ""goods-dominant"" industries.[ 7] A key difference between these markets is the greater intangibility of service offerings, which creates more quality uncertainty for customers ([68]). Under such conditions, customers are more likely to use cues such as market share as indicators of the quality of a firm's offerings (e.g., [12]). Interestingly, this prediction is the opposite of E-H (2018), who reason that physical goods manufacturers may benefit more from efficiency, and that this may be more important in driving profit than any dampening of the quality-signaling effect of market share in physical goods-focused markets. We explore this reasoning empirically when we directly examine the three mechanisms underpinning the market share–profit relationship.We therefore hypothesize the following: H4:  The effect of market share on firm profit via market power is stronger in marketplaces with higher switching costs. H5:  The effect of market share on firm profit via efficiency is stronger for younger firms. H6:  The effect of market share on firm profit via perceived quality is stronger for firms selling service- versus product-dominant offerings. Methodology DataWe combine secondary data from a variety of sources. From Compustat, we obtained data to construct measures of market power and operating efficiency, firm economic performance indicators, firm-specific controls, and a set of industry and competitive context control variables. Equitrend provided data on the perceived quality of firms' offerings. To calculate measures of unit market share, we use unit sales data from the Global Market Information Database (GMID). We assembled our initial data set by merging data from Compustat and GMID. To test the mediation hypotheses, we also require data from Equitrend, for which our access covers only the years 2000–2013. Because each data source has distinct firm and year coverage, the compiled data set used to confirm the main effect of market share on firm profit and test the hypothesized mediation effects contains 3,058 firm-year observations from 244 individual firms, operating in 126 North American Industry Classification System (NAICS) four-digit industries, 2000 through 2013. The average firm in this sample has $13.81 billion in assets and has been operating for 45 years. Table 1 shows summary statistics and correlations for the main variables in our sample and additional details are contained in Web Appendix 1. To test H4–H6, we also required American Customer Satisfaction Index (ACSI) data (to measure switching costs), which reduced our sample for testing these three hypotheses to 2,629 firm-year observations from 207 firms (2000–2013).[ 8]GraphTable 1. Descriptive Statistics and Correlations (N = 3,058). VariablesMeanSD1234567891011121314 1. Firm Profit ($M)839.101,104.101.00 2. Market Share (%)6.859.59.141.00 3. Sales Revenue ($M)3,907.085,747.18.77.141.00 4. Market Power (%)30.7910.90.23.13.091.00 5. Firm Efficiency (Index)50.099.06.10.08.38.231.00 6. Perceived Quality (Index)65.3516.95.18.07.06.35−.051.00 7. Firm Size ($B)13.8162.89.39.16.69.17.03.221.00 8. Market Growth ($M)122.23523.18.10.02.03.14−.04−.10.301.00 9. Advertising ($M)53.92255.75.44.34.36.07−.01.01.31.031.0010. R&D ($M)64.92366.32.40.30.22.15−.02−.07.32.05.481.0011. Service Indicator (0/1).19.39−.02−.10−.15−.03.18−.05.08−.04−.08−.051.0012. Switching Costs (Index)−.011.11.33−.14.06.16.07.02.18.14.19.07.111.0013. Firm Age (Years)45.3041.30.21.05.09.14.10.21.18−.08.49.41−.08.341.0014. Niche Focus (Index)2.438.10.02−.15−.02.12.06.05−.07−.06.06.13.12.01−.061.00 1 Notes: All descriptive statistics are for the ""raw"" (i.e., untransformed) variables. Correlations with an absolute value larger than.046 are significant at p < .01, and those greater than.035 are significant at p < .05. Hypothesis Testing Variable MeasurementThe Appendix contains definitions and operationalization details of all variables described next. Market shareMarket share is the percentage of a market's total sales garnered by a firm over a specified time period ([25]). The market may consist of all suppliers selling products/services with the same characteristics, or those that are thought of similarly by customers and are purchased for the same use. We follow [35] to compute a measure of market share using a set of competitors and market definitions derived from business descriptions in ﬁrm 10-Ks. This allows market definitions to be dynamic, where a firm may move in and out of any given market depending on whether its offerings changed over time and thus compete with a different set of firms.To compute market share, we divide the total sales of each firm by the aggregate sales for that market for that year, where the market is dynamically defined as described previously using data from all 22,076 firms in Compustat for the 2000–2013 period. In defining markets, we note that each firm has a similarity/competition score with respect to any other firm (i.e., all possible dualities are computed) in the Compustat database. In line with [35], the number of competitors can be defined using a threshold of similarity scores and/or specified number of nearest neighbors (e.g., 50 or 20). We combine the two approaches and specify 50 as the largest number of neighbors, while also imposing a minimum threshold limit. Thus, our market definition comprises a maximum of 50 firms per industry, while allowing for fewer firms, to maintain a minimum level of similarity among competitors in the same market.[ 9]To assess the robustness of the findings using this dynamic measure of market share, we also use a more static approach, defining markets via each firm's primary NAICS designation using the four-digit level that researchers suggest most closely represents the real ""competed"" market (e.g., [44]). To calculate this, we first collect the total revenue-by-industry data that comprise gross domestic product (i.e., total expenditures on products and services) for all four-digit NAICS industries from the U.S. Bureau of Economic Analysis, which allows us to account for the sales of firms that are private, small, or otherwise not available in Compustat. We then divide the total sales of each firm by the gross domestic product value for that four-digit NAICS industry for that year. Firm market shares are computed from their revenues in their primary NAICS markets. Firm profitWe use net income as our primary measure of firm profit, obtained from Compustat. We use this indicator of absolute firm profit (while controlling for asset size in our model) because economic theories of the value of market share assume that maximizing the amount of profit—not the efficiency with which profit is generated, which is what ""return on asset"" (or investment) relative profit measures capture—is a firm's superordinate performance objective. Market powerWe use profit elasticity relative to the industry average (similar to [ 8]) to indicate firm-level market power. This is calculated by estimating regressions of firms' profit (net income) on their total variable costs for each industry as follows: ln(πit)=α+βln(tvcit)+εit, Graphwhere π is firm profit and tvc is the firm's total variable cost (Cost of Goods Sold + Selling, General and Administrative Expenses) for firm i at time t. Both profit and variable costs are scaled by firm size (total assets). Because profit and costs are natural log transformed, the β from this regression captures the average profit elasticity within the industry, with less negative βs indicating the average ability of firms within the industry to mark up prices when costs rise and thus exercise market power (e.g., [39]). Firm-specific residuals measure each firm's margins relative to its industry's average, providing an indicator of firm's market power ([ 8]). Positive residuals (equivalent to less negative elasticities) indicate greater market power, and negative residuals (i.e., more negative elasticities) indicate weaker market power. Web Appendix 2a indicates favorable face validity for this measure. Firm efficiencyFrom an economic theory viewpoint, this concerns producing goods and services in ways that optimize the combination of inputs to produce maximum output at the minimum cost ([ 5]). To operationalize productive (in)efficiency, we use a stochastic frontier estimation approach. Following [ 5], we use operating expense as the input and total sales as the output. In stochastic frontier estimation, the firm in the industry with the lowest input requirements to produce a given set of outputs forms the efficiency frontier and the closeness of a firm's inputs-to-outputs to this frontier determines its relative (to the industry's most efficient firm) efficiency. Web Appendix 2b provides evidence of strong face validity for this measure. Perceived qualityWe use the perceived quality measure of brands from the Equitrend database, which comprises consumer ratings on an 11-point perceived quality scale. For multibrand firms, we take the mean perceived quality of all brands owned by the firm.[10] Face validity assessments for this measure (see Web Appendix 2c) provide strong support for the measure. Switching costsWe use ACSI data and follow [53] to construct an industry-level measure of switching costs as the ""excess loyalty"" displayed by customers to suppliers using the residual of regressing each industry's customers' loyalty onto its customers' satisfaction, controlling for time fixed effects (FEs). This measure has been shown to have strong face validity ([53]), and we also find evidence of this (Web Appendix 3). Service- (vs. product-) dominant industriesService- (vs. product-) dominant industries is a dummy variable identifying firms operating in nonbanking (banks have idiosyncratic characteristics we later explore) service-focused industries using Fama–French industry definitions ([22]). Firm ageFirm age is the number of years since the firm's founding using information from annual reports and websites. Control variablesIn addition to firm and year FEs used to control for unobserved heterogeneity, we employ several firm- and industry-level covariates in our analyses, including firm size, operationalized as the logarithm of each firm's total assets to account for scale economies not captured by market share, and the firm's advertising and research-and-development (R&D) expenditures to control for firm-level heterogeneity. We also control for market growth that may affect the profit outcomes of market share ([56]), captured as the year-to-year change in total market sales.The Appendix and Web Appendix 1 summarize descriptive statistics for all variables used in our analyses. To enable log-log specification and interpretation in our analyses and reduce deviations from normality present in several of our measures (market share, firm profit, market power, firm efficiency, perceived quality, advertising expense, R&D expense, and market growth), we applied log transformations to our data.[11] Model SpecificationWe empirically test the hypothesized relationships using a fixed-effects autoregressive (FE-AR) estimation approach ([65]) for several reasons. First, we are using panel data, and the Hausman test indicates that an FE correction is needed to address unobserved heterogeneity and separate between time-variant and -invariant firm-specific errors. Second, several of our measures are longitudinally persistent, raising concerns about serial correlation—the AR correction of the errors addresses any potential bias to the estimates. The modified Durbin–Watson and Baltagi–Wu LBI tests indicate that an AR1 correction is appropriate. In addition, we control for heteroskedasticity using cluster-adjusted robust standard errors at the firm level. Finally, we estimate our hypothesis-testing models using generalized least squares (GLS), because OLS are statistically inefficient and may result in biased inference in the presence of serially correlated residuals.We first verify the average positive relationship between market share and profit (E-H 2018) and estimate the total effect using the following model specification: Profiti,t+1=α0+α1MarketSharei,t+α8FirmSizei,t+α9Advertisingi,t+α10R&Di,t+α11MarketGrowthi,t+YearFEs+ζi+εi,t+1, Graph( 1)where i stands for firm and t for time (year), ζi is a time-invariant firm FE, and εi, t + 1 is the random error representing all unobserved influences on future profit, modeled as an AR1 process such that εi, t + 1 = ρεi, t + ηi, t + 1 and where |ρ|<1 and ηi, t + 1 is an independent and identically distributed (i.i.d) error. Market Share, Firm Size, Advertising, R&D, and Market Growth are as described previously, and Year FEs are mutually exclusive year dummies. Lagged regressors are used to alleviate concerns due to simultaneity and reverse causality (i.e., future profit should not impact past market share).Having selected an appropriate estimation approach given the nature of our data, we next deal with potential endogeneity concerns with respect to omitted variables—of which reverse causality and simultaneity are special cases ([65]). We examine the potential for the presence and effect of such endogeneity concerns using a Gaussian copula correction to Equation 1 and assess the presence and effect of any endogeneity (including potential selection bias introduced by the various data sets on which we draw for our measures) via a likelihood ratio test of whether there is a significant difference between the uncorrected set of parameter estimates and the endogeneity-corrected set ([65]).[12] Once we show that potential endogeneity issues are not material, we empirically test H1–H3 using an identical FE-AR approach by estimating the following equations: Profiti,t+1=α0+α1MarketSharei,t+α2MarketPoweri,t+α3FirmEfficiencyi,t+α4PerceivedQualityi,t+α5SwitchingCostsi,t+α6ServicesDummyi,t+α7FirmAgei,t+α8FirmSizei,t+α9Advertisingi,t+α10RDi,t+α11MarketGrowthi,t+YearFEs+ζi+εi,t+1, Graph(2\rm a) MarketPoweri,t+1=β0+β1MarketSharei,t+β5SwitchingCostsi,t+β6ServicesDummyi,t+β7FirmAgei,t+β8FirmSizei,t+β9Advertisingi,t+β10RDi,t+β11MarketGrowthi,t+YearFEs+τi+ξi,t+1, Graph(2\rm b) FirmEfficiencyi,t+1=γ0+γ1MarketSharei,t+γ5SwitchingCostsi,t+γ6ServicesDummyi,t+γ7FirmAgei,t+γ8FirmSizei,t+γ9Advertisingi,t+γ10RDi,t+γ11MarketGrowthi,t+YearFEs+μi+ςi,t+1, Graph(2\rm c) PerceivedQualityi,t+1=θ0+θ1MarketSharei,t+θ5SwitchingCostsi,t+θ6ServicesDummyi,t+θ7FirmAgei,t+θ8FirmSizei,t+θ9Advertisingi,t+θ10RDi,t+θ11MarketGrowthi,t+YearFEs+νi+φi,t+1, Graph(2\rm d)where Market Power, Firm Efficiency, Perceived Quality, Switching Costs, Services Dummy, and Firm Age are as described in the variable measurement section, and all other variables and subscripts follow Equation 1. Finally, we empirically test H4–H6 by estimating the moderated-mediation contingencies and include interactions between Market Sharei,t and Switching Costsi,t, Services Dummyi,t, and Firm Agei,t in Equations 2a–2d. To estimate the relative effects of the three hypothesized mediation mechanisms (market power, firm efficiency, and quality signaling) and three moderated-mediation contingencies (switching costs, firm age, and services), we follow [51] using [64] approach to augment the FE-AR estimation. Results and Discussion Main Effect of Market Share on Firm ProfitPrior to testing the hypothesized mechanisms, we first verify the main effect results indicated in the E-H (2018) meta-analysis in our sample using several variants of the model specification detailed in Equation 1. We begin by estimating a model with FEs and cluster-adjusted robust standard errors that includes only the covariates as predictors (M1), to which we then add market share (M2), allowing us to verify the main effect of market share on firm profit and reveal its incremental predictive power. We also estimate this same model using an FE-AR error correction and cluster-adjusted robust standard errors (M3) to demonstrate the stability of the estimates across the different statistical corrections proposed. In M4 we examine whether the reported estimates suffer from endogeneity bias by including a Gaussian copula for the Market Share variable as a control function to empirically correct endogeneity bias. The likelihood ratio test for joint parameter differences ([65]) indicates that the endogeneity-corrected estimates in M4 are not statistically different from those in M3.As Table 2 shows, the estimates are consistent across all four models, demonstrating the robustness of the effect of market share on firm profit. In addition, while the Gaussian copula estimate in M4 is significant (.048, p < .05) indicating the presence of some omitted variable endogeneity, the likelihood ratio test indicates no significant difference in the market share parameter estimates between M3 (β = .137) and M4 (β = .159). This supports the use of an FE-AR( 1) (i.e., model specification M3) estimation approach and confirms that any remaining bias is modest and does not substantively impact the estimates. In a robustness check, we also replaced the dynamic market share measure with a four-digit NAICS alternative and again confirmed the main effect (Web Appendix 5). Finally, we further verified that endogeneity bias does not unduly influence our findings using a difference-in-differences version of Equation 1 comparing the market share–profit relationship for firms in industries that experience an exogenous demand shock (exit of bankrupt firms) with those that do not. The results (Web Appendix 6) again confirm the main effect findings.GraphTable 2. Main Effect of Market Share on Firm Profit. Models and Dependent VariablesM1M2M3M4Independent VariablesProfit(t + 1)Profit(t + 1)Profit(t + 1)Profit(t + 1)Main Effect Market Share(t).153**.137**.159**(.053)(.038)(.052)Controls Firm Size(t).228*.208***.521***.291***(.113)(.067)(.045)(.051) Advertising(t).234***.130**.073***.098*(.061)(.045)(.023)(.044) R&D(t).061*.044.066***.042***(.027)(.025)(.014)(.010) Market Growth(t).020.012.002.029*(.017)(.020)(.002)(.013) Market Share(t)COPULA.048*(.021)Specification Tests  Wald χ2125.32198.12188.36115.92 R2.57.59.58.59 Rho_AR.40.43 2 *p < .05.3 **p < .01.4 ***p < .001.5 Notes: All model specifications estimated using 3,058 firm-year observations. M1/M2: GLS estimation, FEs and cluster-adjusted robust standard errors. M3/M4/M5: GLS estimation, FEs with AR errors and cluster-adjusted robust standard errors. Z-test difference in share coefficients between M3 (.137) and M4 (.159) = .64 (p > .05).Collectively, these analyses verify the main effect results in E-H (2018) that, on average, firm market share positively predicts future firm profit—and the effect sizes reported on Table 2 are both consistent and aligned with the average elasticity of.132 reported by E-H (2018), further enhancing confidence in our findings. Table 2 results also show the suitability of the FE-AR error correction and cluster-adjusted robust standard errors GLS estimation approach (model specification M3), which we employ in the hypothesis-testing analyses. Hypothesized Mechanism (Mediator) ResultsAs Table 3 shows, in testing H1–H3 we find support for both market power in M1a (.230, p < .001) and quality signaling (.141, p < .05) in M1c as mechanisms linking market share with firm profit. However, while M2 confirms that firm efficiency predicts firm profit (.129, p < .001), M1b reveals that a firm's efficiency is not predicted by its market share (.024, p > .1). Thus, on average we find no evidence supporting efficiency as a mechanism linking firm market share and profit in our sample. Overall, these results provide support for H1 and H3 but not for H2. As M2 shows, all three of the mechanism variables are significant predictors of firm profit, and the main effect of market share becomes insignificant (.031, p > .10) in the presence of these three variables. To examine the relative strength of the mediator role played by the three mechanism variables in explaining the market share–profit relationship, we follow [64] approach. This reveals that the three mechanisms collectively explain 77.37% of the total effect of market share on firm profit, with 63.21% of this flowing through market power, 33.96% via perceived quality, and 2.83% through firm efficiency.GraphTable 3. Mechanism for Market Share Effect on Firm Profit. Models and Dependent VariablesM1aM1bM1cM2Independent VariablesPower(t + 1)Efficiency(t + 1)Quality(t + 1)Profit(t + 1)Direct Effect Market Share(t).230***.024.141*.031(.081)(.016)(.065)(.018)Indirect Effect Market Power(t).302***(.042) Firm Efficiency(t).129***(.029) Perceived Quality(t).274***(.061)Controls Firm Size(t).029*.027***.039***.210***(.013)(.006)(.008)(.029) Advertising(t).020.021.022*.090*(.023)(.020)(.010)(.043) R&D(t).032**.013***.028**.023***(.011)(.002)(.011)(.005) Market Growth(t).012.007.012.008(.019)(.009)(.010)(.007)Specification Tests −Log-likelihood2,810.17 R2.16.18.10.68 6 *p < .05.7 **p < .01.8 ***p < .001.9 Notes: 3,058 firm-year observations covering 244 firms for the 2000–2013 period (Equitrend available 2000–2013). Total effect (from Table 2: M3).137 (100.00%) minus direct effect (from M1a).031 (22.63%) = indirect effect of.106 (77.37%). Indirect effect via ( 1) Power = .067 (63.21%); ( 2) Quality = .036 (33.96%); and ( 3) Efficiency = .003 (2.83%).To check the robustness of the mechanism results, we conducted four additional analyses. First, to check for any potential scale effect of absolute sales revenue beyond firm size, we reestimated our model using market share ranks and adding firm sales revenue as a separate control. The estimates replicated the hypothesis-testing results (Web Appendix 7). Second, to check for any potential biasing effect of firm orientation to market share ([43]) we used text analysis of 10-K reports to construct an annual measure of each firm's market share focus based on the number of times ""market share"" is mentioned relative to the total number of words. When this is added to our model, we find that the results remain essentially unchanged (Web Appendix 8). Third, to ensure that results are robust to alternative firm performance measures, we replaced net profit in turn with return on assets and Tobin's q as dependent variables. As shown in Web Appendices 9 and 10, we replicate the hypothesis-testing results. Fourth, we also checked that a firm's competitor orientation—a potential fourth mechanism linking market share (negatively) with firm profit ([ 3])—does not explain additional variance in the market share–profit relationship. Using 10-K reports and [ 7] text-based measure, we computed the competitor orientation of each firm in our sample and included this in our model. As Web Appendix 11 shows, we find that while competitor orientation predicts firm market share, it does not materially affect the market share–profit relationship. Hypothesized Moderating Condition ResultsHaving demonstrated the robustness of the hypothesized mechanism results, we next examine whether the market share–profit relationship may be stronger in industry and firm conditions in which each of the three mechanism variables in turn may be expected a priori to be more versus less important as captured in H4–H6. The results are summarized in Table 4, with M1 showing that firms in industries with higher customer switching costs are more profitable (.137, p < .05), and M2 supporting H4 by confirming that market share is more valuable in such industries (.087, p < .001) via its stronger effect on market power (.157, p < .05). In addition, M4c reveals that firms also gain stronger perceived quality benefits from market share in industries with higher switching costs (.203, p < .05), suggesting that some of the switching costs we observe are due to customers continuing to choose a provider because of positive relational bonds that may influence both customers and others' perceptions of the quality of such firms' offerings.GraphTable 4. Main Effect and Mechanisms for Market Share Effect on Firm Profit in Hypothesized Moderators. Model Specifications (M) and Dependent VariablesM1M2M3aM3bM3cM3dM4aM4bM4cM4dIndependent VariablesProfit(t + 1)Profit(t + 1)Power(t + 1)Efficiency(t + 1)Quality(t + 1)Profit(t + 1)Power(t + 1)Efficiency(t + 1)Quality(t + 1)Profit(t + 1)Direct Effects Market Share(t).118***.114***.105*.031.278**.017.136***.028.149***.030Indirect Effects Market Power(t).210***.223*** Firm Efficiency(t).075**.083*** Perceived Quality(t).169***.163***Moderators Switching Costs(t).137*.149*.093*.005.051*.013.107.015.077*.027 Firm Age(t).178.208−.002.013.006.015*.034−.031.004.019 Services Dummy(t)−.058*−.059*.017−.033*.008−.004.093.509***.028−.006Interaction Effects Market Share(t) × Switching Costs(t).087***.157*.017.203*.033 Market Share(t) × Firm Age(t)−.069***−.048−.109***−.092*−.043 Market Share(t) × Services Dummy(t).056***−.006.148***.012.020Controls Firm Size(t).514***.534***.025***.031***.046***.028***.030***.083*.041***.046*** Advertising(t).278***.281***.008.022.006.011.007.010.039.039*** R&D(t).274***.272***.039***.010.059*.034***.029***.012.062***.031*** Market Growth(t).014.015*.011.008***.002.004.011.017.012.003Specification Tests Wald χ2303.11358.07 −Log-likelihood2,489.312,913.87 R2.50.52.24.21.22.69.25.29.26.70 10 *p < .05.11 **p < .01.12 ***p < .001.13 Notes: 2,629 firm-year observations covering 207 firms for the 2000–2013 period (sample size due to ACSI data availability).The interactions reported for M2 also show that market share is generally less valuable for older firms (−.069, p < .001), and consistent with H5, the mechanism estimates in M4b provide strong evidence supporting the expected effect of market share on firm efficiency being weaker for older firms (−.109, p < .001). This is aligned with our rationale that efficiency-enhancing learning effects associated with market share accrue mainly to firms that are earlier in their development. M4c estimates also reveal that older firms benefit less from market share via quality signaling (−.092, p < .05). We reason that older firms that have been in the marketplace for longer are likely to be better known and also that firm age may indicate a firm's stability and lower risk, which reduce the signaling value of its market share.In terms of services-dominant firms, the significant positive estimate in M2 for the services × market share interaction (.056, p < .001) indicates that service firms benefit more from market share. However, our mechanism estimates in M4c show that this is not a result of the expected strengthening of the quality-signaling benefit of market share (.012, p > .10) posited in H6 but rather, as shown in M4b, that service firms benefit more from the efficiency-enhancing effect of market share (.148, p < .001).[13] Because controlling for scale effects via firm size isolates the efficiency-enhancing learning effects of market share, this finding suggests that market share provides a greater opportunity for service firms to learn how to operate more efficiently and to use this knowledge to change their operations to do so. We reason that this may be because the greater direct customer interactions from higher market share are more valuable in helping service firms learn how to efficiently deal with customer heterogeneity, and that applying what is learned may also be less capital-intensive for service firms (vs. manufacturers). Additional Analyses of Hypothesis-Testing EffectsTo provide additional insight into how the hypothesized moderators affect the profit value of market share via the three mechanisms, we examined these effects in an additional analysis (Table 5). Of the.086 total effect (elasticity) of market share on profit when the moderator variables are included in the model,.056 is indirect (65% of the total) via the three mechanisms, with 62% of this flowing through market power, 6% through firm efficiency, and 32% via perceived quality. Consistent with the H4 testing results (Table 4), the effect of market share on firm profit is strengthened by switching costs, with the total effect amplified by.287 for each unit increase in switching costs, of which.195 is indirect via market power (50.9%), firm efficiency (2.5%), and perceived quality (46.6%). These direct and indirect effects of switching costs on market share's effect on firm profit are proportionately lower (higher) at lower (higher) levels of switching costs (i.e., ± one standard deviation around average switching costs) with the indirect effects flowing through the three mechanisms in very similar percentages.GraphTable 5. Indirect Effects for Market Share Effect on Firm Profit in Hypothesized Moderators. Market Share–Profit EffectsIndirect Effect MechanismsModerator Variable ConditionsTotal EffectDirect Effect% of TotalIndirect Effect% of TotalPowerEfficiencyQualityOverall.086*.03034.9%.056*65.1%62.0%6.0%32.0%Switching costs.287***.092*32.1%.195***67.9%50.9%2.5%46.6% +1 SD.345***.111*32.2%.234***67.8%51.2%2.4%46.4% −1 SD.218***.073*33.5%.145***66.5%51.1%2.4%46.5%Service dominant.032*.02062.5%.01237.5%41.0%21.0%38.0%Product dominant−.032*−.01031.2%−.02268.8%54.0%3.0%43.0%Firm age−.136***−.01410.3%−.122***89.7%12.1%45.5%42.4% +1 SD−.170***−.01810.6%−.152***89.4%17.1%40.2%42.7% −1 SD−.081*.011−13.6%−.092*113.6%2.7%56.8%40.5% 14 *p < .05.15 **p < .01.16 ***p < .001.17 Notes: 2,629 firm-year observations covering 207 firms for the 2000–2013 period (sample size due to ACSI data availability).Consistent with H5 testing results (Table 4), the total effect of market share on firm profit is also amplified for service-dominant firms by an extra.032, of which.012 is indirect (38% of the total) and flows through market power (41.0%), firm efficiency (21.0%), and perceived quality (38.0%). Meanwhile, for product-dominant firms, the total effect is reduced by −.032, of which −.022 is indirect, with 54.0% flowing through market power, 3.0% through firm efficiency, and the remaining 43.0% via perceived quality.Finally, in line with H6 testing results (Table 4), Table 5 shows the effect of market share on profit is weakened by firm age with each additional year reducing the total effect of market share on profit by −.136, of which −.122 is indirect (90% of the total) and flows through market power (12.1%), firm efficiency (45.5%), and perceived quality (42.4%). As we expected, the total effect of firm age on the market share–profit relationship is more pronounced for very high (old) versus very low (young) age levels, with a marked increase in the indirect effect flowing through firm efficiency (from 40.2% to 56.8%) and decrease in that flowing through market power (17.1% to 2.7%) in the case of very young firms. This is consistent with our Table 4 hypothesis testing results revealing stronger efficiency gains with market share for younger firms. Market Share–Profit Mechanisms When Market Share Negatively Impacts Firm ProfitAligned with E-H's (2018) finding that 82% of market share–performance elasticities in prior research are positive (82% of the same elasticities in our sample are also positive), our hypotheses are framed in terms of a net positive performance effect of market share. However, conceptual arguments concerning potential negative outcomes of market share have also been proposed (e.g., E-H 2018; [34]). Drawing on our theorizing, we expect that the three mechanisms we identify should empirically capture any negative and positive effects of market share. For example, any associated diseconomies of scale will reduce a firm's efficiency while a reduction in perceived exclusivity will affect the quality-signaling value of market share. To empirically verify this expectation, we identify two conditions under which market share's positive benefits may be outweighed by negative consequences, such that larger market share might reduce firm profit and reestimate the mediation effects of the market power, firm efficiency, and quality-signaling mechanism in these conditions. Niche firmsOne condition in which market share may negatively predict profit concerns firms with a strategic focus on serving a smaller segment of a market, usually a group of customers with a distinct set of needs and requirements (e.g., [49]). For example, Louboutin specializes in high-fashion stiletto shoes. By serving distinctive needs, niche-focused firms make money by occupying positions in a segment of a broader market in which competition is more limited (e.g., [19]). As a result, they may not serve enough customers to gain market power benefits from market share, and their specialist positioning may diminish any quality-signaling benefit. They are also unlikely to gain from any learning effects in production. However, niche-focused firms with higher overall market shares are likely to have achieved this by selling to customers beyond their original niche ([62]). This may negatively impact the firm's profitability by reducing its original niche appeal via a negative effect on perceived quality (e.g., [34]) and also attract more competition (e.g., [32]). These downsides may outweigh any potential market power and/or firm efficiency benefits of having a larger market share. Firms buying market shareAnother circumstance when market share may negatively impact profit is when firms ""buy"" market share by lowering prices relative to rivals. This is analogous to findings in the sales promotion literature that price promotions often produce negative returns (e.g., [33]). In this circumstance, any market share gain via greater market power and the ability to charge higher prices is not only relinquished but reversed. In addition, because there is a price-perceived quality heuristic among customers in many markets (e.g., [52]), charging lower prices may offset any quality-signaling benefit of higher market share, and the net result on perceived quality could be negative. Our previous results suggest that in most circumstances, these negative market power and quality-signaling effects are likely to outweigh any firm efficiency gains via learning produced by increasing market share. Empirical test of the two conditionsTo assess the robustness of our mechanism results under conditions when the market share–profit relationship may be negative, we first identified firms that are likely pursuing a niche strategy by combining a new text measure indicator of the degree to which a firm has a niche strategic emphasis (for details, see Web Appendices 4a and 4b) with the number of brands they market (both firms with both a high niche-focus in their product-market coverage strategy and those that offer only a single brand are likely to be niche firms). The face validity assessments in Web Appendices 4a and 4b support this identification logic. Second, to identify firms that may be ""buying"" market share, we created a dummy variable indicator for firm-years in which a firm both reduced its average prices (computed using GMID data) and experienced a positive market share change.We then reestimated our market share–profit models from Table 3 with the addition of the new niche firm measure and buying share dummy indicator, along with their respective interactions with market share. As Table 6 shows, model M1 shows that higher market share reduces profit for niche firms (−.115, p < .05). As we expected, M2c reveals that this is a result of a strong negative effect of market share via perceived quality (−.062, p < .001). M1 also shows that the effect of market share on firm profit is significantly lower for firms ""buying"" market share (−.036, p < .001).[14] The mechanism results indicate that this is caused by a significant reversal in both the market power (M2a: −.047, p < .001) and firm efficiency (M2b: −.033, p < .001) effects of market share and a reduction of the perceived quality mechanism to insignificance (M2c: −.022, p > .1). These findings suggest that any supplier input cost benefits of greater market power from market share are more than offset by lowering downstream prices to ""buy"" the market share. In addition, consistent with the well-known ""bullwhip"" effect, rapid increases in short-term demand resulting from lowering price seems to disrupt the efficient production and delivery of these firms' products and services. Overall, the Table 6 results provide support for the robustness of the three mechanism variables in mediating the relationship between firm market share and profit, even in the relatively rare conditions under which the relationship is negative.GraphTable 6. Moderating Effect and Mechanism When We Include Conditions in Which Market Share May Have a Negative Effect on Profit. Model Specifications and Dependent VariablesM1M2aM2bM2cM2dProfit(t + 1)Power(t + 1)Efficiency(t + 1)Quality(t + 1)Profit(t + 1)Direct Effect Market Share(t).058***.091***.034.108***.033Indirect Effect Market Power(t).218*** Firm Efficiency(t).095*** Perceived Quality(t).179***Moderators Switching Costs(t).149*.118.021.081***.041 Services Dummy(t)−.042*.088.510***.027−.010 Firm Age(t).193.037−.036.008.021 Niche Focus Firms(t).078***−.025***.027.119**.180* Buying Share Dummy(t).016.024−.032*−.009−.026Prior Moderator Effects Share(t) × Switching Costs(t).050*.162*.022.200*.037 Share(t) × Services Dummy(t).063***−.011.166***.018.019 Share(t) × Firm Age(t)−.053***−.055−.113***−.078−.009Proposed Negative Moderators Share × Niche Focus Firms(t)−.115**−.016−.001−.062***−.010 Share × Buying Share Dummy(t)−.036***−.047***−.033***−.022−.036Controls Firm Size(t).490***.035***.086*.039***.049*** ADV(t).233***.008.010.018.041* R&D(t).241***.031***.015.055***.059*** Market Growth(t).022***.023.018.015.012Specification Tests −Log-likelihood3,104.92 R2.55.30.39.20.72 18 *p < .05.19 **p < .01.20 ***p < .001.21 Notes: 2,629 firm-year observations covering 207 firms for the 2000–2013 period (sample size due to ACSI data availability). For Niche Firms, indirect effect = 58%, of which Power = 21%; Efficiency = 0%; and Quality = 79%. For Firms Buying Share, Indirect Effect = 33%, of which Power = 56%, Efficiency = 22%, and Quality = 22%. Comparison with E-H's (2018) Indirect Moderator InferencesHaving provided robust evidence to support the three mechanisms, to offer additional insight on the utility of the direct measures of the three mechanisms employed, we also examined how the results compare with previous indirect inferences regarding these mechanisms drawn from observable moderators of the market share–profit relationship. To accomplish this, we first replicated E-H's (2018) measures as well as main effect and substantive moderator results (banking services, concentration, and B2C). We then examined the mechanisms explaining the effect of these moderators of the market share–profit relationship in our sample, and the results are revealing (Web Appendix 12). For example, we find that while E-H's theorizing focuses on quality signaling, the reason for the stronger market share–profit relationship in B2C industries is a significant strengthening of all three mechanisms relative to business-to-business (B2B) industries (market power:.143, p < .001; efficiency:.044, p < .05; quality:.082, p < .05). In addition, we find that while banks are in general more profitable (.426, p < .01) and have greater market power (.042, p < .05), this is in spite of—not due to—their market share (−.087, p < .05). In fact, results reveal that market share reduces banks' profitability by lowering their efficiency (−.410, p < .001). We also find a direct moderating effect for concentration (.109, p < .05), whereas E-H found a nonlinear effect, and we observe that this is via increasing the market power benefit of market share (.110, p < .01). These results show that using moderators to indirectly infer the three mechanisms underlying the market share–profit relationship often does not do a good job of isolating these mechanisms. This reinforces the value of direct empirical understanding of the mechanisms linking market share with firm profit in predicting when market share is more valuable and thus when managers should set market share goals. When Its Value Is Indicated, How Should Managers Measure Market Share?The new empirical understanding of the mechanisms linking market share with firm profit revealed in our analyses can help managers evaluate when market share may be a valuable goal. When its value is indicated, a manager's next task is to decide how to measure market share for goal setting and performance monitoring. To provide insights on this question, we examined two key market share measure design choices facing managers. First, ""share of what?,"" in terms of unit sales volume or sales revenue, should be used in computing market share ([ 6]). Managers use both types of indicators to track market share, and both rank among the most popular measures of marketing performance in practice (e.g., https://marketbusinessnews.com/financial-glossary/market-share/). The second is ""relative to what?,"" in terms of whether and how the firm's market share is benchmarked—as an absolute value (% of total market sales) or relative to others in the market (the market share leader or the top three players). Revenue versus unit shareTo provide insights on the first question, we replicated model M3 in Table 2 and replaced the sales revenue market share with unit sales volume market share using the same dynamic market definition. As we show in Table 7, in contrast to revenue market share (M2:.151, p < .05), unit market share (M1:.009, p > .1) does not predict firm profit. This result is robust to all of the same checks performed on our revenue market share main effect testing analyses and also to using benchmarked (vs. absolute) values of unit market share. Post hoc analysis of the mechanisms associated with unit share (Web Appendix 13) reveal that although it has a small positive effect on both market power and firm efficiency (consistent with the learning effect logic that market share is a proxy for number of units produced), this is insufficient to overcome the significant negative relationship with quality signaling. We reason that the weaker effect of unit (vs. revenue) market share on market power is a result of unit market share ignoring prices charged to customers (a downstream indicator of market power). The negative quality-signaling effect of unit market share is consistent with both ignoring price (which is often a quality cue for customers) and the notion that ubiquity reduces perceived exclusivity (e.g., [34]). These results show that when the presence of the three mechanisms indicates market share's value, managers should set market share goals and monitor performance in terms of revenue market share.GraphTable 7. Market Share–Profit Relationship Using Alternative Market Share Measures and Benchmarks. Market Share Measure, Model, Benchmark, and Dependent VariableUnit Market ShareRevenue Market ShareRevenue Market ShareRevenue Market ShareM1M2M3M4Independent VariablesAbsoluteAbsoluteRelative to Market LeaderRelative to Top 3Profit(t + 1)Profit(t + 1)Profit(t + 1)Profit(t + 1)Main Effect Market Share(t).009.151*.222***.392***Controls Firm Size(t).201***.270***.213***.243*** Advertising(t).081**.121***.121***.123*** R&D(t).033.024.033.030 Market Growth(t).001.001.004.006Specification Tests Wald χ2115.23188.91210.81167.81 R2.18.59.52.52 22 *p < .05.23 **p < .01.24 ***p < .001.25 Notes: 3,058 firm-year observations covering 244 firms for the 2000–2013 period, except for model specification M1, which is estimated using 2,214 firm-year observations covering 235 firms for the period 2004–2013 (due to GMID data availability). In a subsequent robustness check, model specifications M2 through M4 were reestimated using the same 2,214 firm-year observations, and estimates remain identical. Absolute versus relative shareIn terms of the ""relative to what?"" question, in Table 7 we compared the market share–profit estimates of the absolute value of market share used in the main effect testing (M2) and two different relative market share benchmark operationalizations: relative to the market share leader (M3) and relative to the combined market share of the top three market share firms (M4).[15] The results indicate that benchmarked measures of firm market share provide stronger predictive power (of future profit) (M3:.222, p < .001; M4:.392, p < .001, respectively) than using absolute market share (M2:.151, p < .05). Subsequent analysis of the three mechanisms show that this is a result of the relative market share measures ""dialing up"" the market share–market power link (Web Appendix 14). This is likely due to such ""relative to others in the same industry"" measures capturing some of the industry-level market concentration power that our previous analysis showed increased the market share–market power relationship in terms of both switching costs (which are higher when markets have fewer equivalent players) and average market share (as an indicator of market concentration in the E-H [2018] replication analyses). Implications Implications for TheoryThis study offers several new insights into theories of firm behavior and performance. First, economic theory assumes that market share predicts firm profit but offers different reasons for why this relationship exists. We provide the first simultaneous test of three mechanisms proffered in competing economic theories for this relationship and show that in combination, they explain the vast majority of the variance in the market share–profit relationship. This suggests that individual single-theory lens explanations of the mechanisms linking market share with profit are incomplete, and all three mechanisms can provide higher (or lower) explanatory power under different conditions. While, on average, market power provides the highest level, and firm efficiency the lowest level, of explanatory power, we also identify conditions under which the reverse is true (e.g., for young firms). Thus, none of the three theories from which the hypothesized mechanisms arise is ""correct"" or ""incorrect,"" but market power and quality signaling generally explain more of the variance in the market share–profit relationship across firms and industries.Second, our results offer new insights into efficiency-enhancing experience-based ""learning effects"" identified in strategic management theorizing ([ 2]). Management scholars have used this logic to explain why market share (a proxy for the number of times a firm may have produced a value offering) may be positively related to firm profit (e.g., [29]). We find that while firm efficiency is valuable (predicts profit), on average it is explained mainly by a firm's size rather than its market share. This suggests that for most firms, scale economies are more important in driving profit than economies of learning. However, for young firms, we find that market share delivers significant efficiency benefits above and beyond those associated with size, and we also find significant efficiency benefits from market share among service businesses. This suggests that ""learning by doing"" effects occur where organizational routines are less set and when firms can use experience gained to update and change their processes with lower investments.Third, we find support for information economics theorizing on the value of signals of unobservable firm quality. While prior research has explored market share's role in consumer evaluations of quality ([34]), we provide the first empirical evidence that market share generally signals firm quality and thereby increases firm profit. The negative effects on perceived quality we observe when using unit (vs. revenue) market share also suggest that price combines with market share in signaling quality to customers. In addition, we find that market share's positive quality-signal effect depends on previously unidentified industry and firm conditions (stronger for younger firms, in B2C markets, and for those with switching costs).For researchers, our study also has broader implications. Not least, it clearly shows the effect that sampling can have on the findings and inferences drawn in firm-level empirical research. We find wide variance in both the main market share–profit relationship and in the specific mechanisms accounting for the relationship across industries. Thus, samples made up of a single industry, or an industry dominated by certain types of firms, would lead to very different results and widely varying inferences being drawn as to which theory may be supported in empirical tests. This is unlikely to be unique to the market share phenomenon we examine. In addition, our study also reveals the desirability of directly observing (or at least finding direct indicators of) mechanisms believed to underlie relationships of interest. In particular, our results highlight the need for researchers to be careful about using indirect contingencies to infer such unobserved mechanisms when there may be more than one mechanism involved. Implications for PracticeThis study also provides new insights for managers regarding how market share should be measured. Although unit (volume) market share is widely used in practice to set marketing goals and monitor performance (e.g., auto and motorcycle manufacturers, many consumer packaged goods companies), our results reveal that it is not predictive of firm profit, whereas revenue (value) market share is. We also find that in terms of predicting profit, relative (to others) measures of revenue market share can be superior to absolute measures. Post hoc analyses suggest that such relative measures can enhance the market power value of the observed market share, and that benchmarking a firm's market share relative to the top three market share firms versus the market share leader offers a stronger predictor of future profit. This is aligned with the intuition that benchmarking against others provides an indicator of both the firm's market share and the concentration present in the marketplace, which we show interact significantly in predicting firm performance.To provide finer-grained managerial insights, we also examined ( 1) which measures of market share were the strongest predictors of future profit for different types of firms to help managers select the most appropriate market share metrics for goal setting and performance monitoring and ( 2) the average profit value of a 1% increase in the average firms' market share for different types of businesses to give managers a calibration of the dollar-value benefits that may be expected when evaluating costs associated with share building strategies. Given our sample size, we are somewhat limited in how fine-grained we can be in these analyses without running into power issues. We therefore split our sample in a managerially meaningful way by identifying firms on the basis of whether they serve primarily consumer or business customers and whether their value offerings are mainly product- versus service-based. As shown in Table 8, the results vary across the four cells, with B2C product firm and B2B service firm profit being most strongly predicted by absolute revenue market share, whereas for B2C service and B2B product firms, it is revenue share relative to the top three market share players. The one-year profit increases associated with a 1% improvement in the average firm's market share vary across the four cells from a low of just over $1 million to almost $6 million. These findings have clear and important implications for managers setting market share goals and monitoring market share performance in their firms and offer a useful dollar benefit scale calibration for managers with respect to the potential payoffs they may expect from investments in market share–building strategies.GraphTable 8. Managerial Matrix: Metrics. ProductsServicesB2CStrongest market share–profit predictorAbsolute revenue shareRelative to top three revenue shareMean firm market share6.80%7.19%Profit value of 1% increase in mean market shareFrom 6.80% to 6.87%:.121% (p < .001) × $840 million = $1.02 millionFrom 7.19% to 7.26%:.704% (p < .001) × $840 million = $5.9 millionObservations1,910 firm/year observations (136 firms)484 firm/year observations (52 firms)B2BStrongest market share–profit predictorRelative to top 3 revenue shareAbsolute revenue shareMean firm market share6.68%7.31%Profit value of 1% increase in mean market shareFrom 6.68% to 6.75%:.309% (p < .001) × $840 million = $2.6 millionFrom 7.31% to 7.38%:.146% (p < .01) × $840 million = $1.2 millionObservations322 firm/year observations (32 firms)342 firm/year observations (24 firms) 26 Notes: Unit share is not predictive of firm profit in any one of the four cells. Reported elasticities estimated via a model specification equivalent to M3 in Table 2, with the noted strongest market share predictor measure as a regressor and using the observations specific to each of the Product/Services and B2C/B2B cells. Profit increase $ values are for a 1% increase in the mean firm's market share in each cell (e.g., 7.310% to 7.383%) not an increase of 1 point of total market share (e.g., from 7.310% to 8.310%). Because we estimate log-log models, the estimated coefficients in each condition can be interpreted as market share–profit elasticities (%) which can be converted to a dollar profit value by multiplying them by the mean profit in our sample (i.e., $840 million).In terms of where managers would be advised to pursue market share to a greater or lesser degree, our results provide several new insights (Table 9). For younger firms and for nonbanking services firms, it may make sense to set market share goals and monitor performance. It may also be more beneficial for firms operating in marketplaces with high levels of quality uncertainty and those with higher switching costs. However, it may make less sense for banks and firms in industries in which pricing power is low and/or quality is relatively certain. Older firms may also find market share to be of less value as a marketing goal and performance metric. Firms pursuing a niche strategy would be well advised to either ignore market share or ensure that they assess it only within their selected niche market definition. Finally, we show that, while relatively rare, ""buying share"" is not a profitable move.GraphTable 9. Managerial Matrix: Contingency Effects on Share-Profit Mechanisms. Relative Mechanism ImportanceContingencyMarket PowerFirm EfficiencyPerceived QualitySwitching costs (high)+n.s.+Service (vs. product)n.s.+n.s.Firm age (older)n.s.−−Concentration (more)+n.s.n.s.B2C (vs. B2B)+++Banking (vs. others)n.s.−n.s. 27 Notes: n.s. = not significant. This table summarizes analyses reported in Table 4 and Web Appendix 12, with mechanism importance indicated relative to the average displayed by all firms in our sample. Implications for PolicyFor policy makers, this study provides new insights with respect to when market share may lead to market power and potential abuse that requires regulation. Importantly, our results show that firm profits from market share result from quality signaling and learning-based efficiencies as well as market power. Thus, policy makers need to be careful not to directly equate market share and market power; we show that while they are often related, they are far from synonymous. Rather, our results suggest that regulatory authorities can be less concerned by a firm's market share in marketplaces where customer quality uncertainty is significant and where efficiency-enhancing learning benefits from market share may exist (e.g., young firms, service firms). In such conditions, market share could enhance rather than harm consumer welfare by reducing consumer–firm information asymmetry and potentially lowering costs. Limitations and Future ResearchThis study has some limitations that should be taken into account when considering the findings. First, because we require public data to explore our research questions, our sample is naturally skewed toward larger firms. While we include small, nonpublic firms in the definition of the total market sales used in constructing the robustness check NAICS measure of market share, we are unable to include such firms' individual market shares in the hypothesis testing because these firms' sales data are private. Although we have a wide range of market shares in our sample (with a low of less than 1%, a high of 77%, and a mean of less than 7%), and no evidence of range restriction in our key variables, researchers with access to private firm data could test the generalizability of this study's findings to firms with much smaller market shares.Second, our data are focused on firms with U.S. listings. However, including studies covering broader geographies and longer time period data, E-H (2018) suggest that the market share–profit relationship is weaker in recent times in Western Europe than the United States, so future research in other regions is required to examine how the mechanisms we identify may differ across geographies. Third, our study examines market share at a firm level. However, market shares may also be computed at other levels (e.g., brand or geographic market level). A post hoc analysis of monobrand firms in our sample suggests that the same market share–profit main effect and mechanism relationships hold (Web Appendix 15); however, research is required to confirm this.Our study also reveals several new avenues for theoretically interesting and managerially relevant research. First, we find that the vast majority of market share's effect on profit is mediated through its effects on firm market power, perceived quality, and efficiency. This suggests that new theorizing regarding why market share is valuable may be of limited value. However, in light of our findings, new research on the details of how each of the three mechanisms works is clearly required. For example, what is the relative effect of market power on upstream versus downstream parties, and how much is contributed by cost reductions versus pricing versus coordination benefits? Similarly, what types and levels of quality uncertainty create conditions that lead to market share's value in signaling quality? How much of market share's signal value is to upstream versus downstream parties?Second, this study reveals market power, quality signaling, and operating efficiency as the mechanisms linking market share with firm profit. Because market share is a market-based outcome of firms' marketing efforts, this raises the interesting possibility that these three mechanisms may also mediate the relationship between other marketing-related phenomena and firm performance. For example, are market-based assets such as brand equity and customer relationships also linked to firm profit via the same three mechanisms? Are there also other mechanisms that may be available to such market-based assets but not to market share?Third, given that market share is more or less valuable under different market and firm conditions—and that buying share is both rare and ineffective—does it also matter how firms create and leverage market share? For example, are market shares more or less valuable to firms pursuing low-cost business strategies versus those pursuing differentiated advantages? Are the three mechanisms linking market share and profit the same for these different strategies, or are some mechanisms more important to one strategy than another? Addressing these questions would provide important new insights for both managers and researchers. Appendix: Measure DetailsGraph Appendix: Measure Details VariablesMeasurement DetailsData Source/LiteratureFirm ProfitNet income of the firm (Item NI).CompustatMarket Share (Revenue)Percentage of an industry or market's total sales garnered by a particular firm over a specified time period. Markets are defined through text analysis of similarity between product-market descriptions within 10-Ks. Sales for each firm obtained from Compustat.SEC, CompustatHoberg and Phillips (2010)Market Share (Units)Units sold by each firm were obtained directly using the GMID (Euromonitor) database. Market definition for firms with unit share data calculated as for revenue share.GMIDMarket Power (Power)Operationalized based on a profit elasticity measure following Boone (2008), estimated by regressing (at the industry level) firms' profit (Item NI) on their total costs (Items COGS and XSGA). Firm-specific residuals are used to calibrate each firm's margins relative to industry average, providing a firm-level indicator of market power.CompustatBoone (2008)Firm Efficiency (Efficiency)Concerns producing goods and services with the optimal combination of inputs to produce maximum output at the minimum cost. We use a stochastic frontier estimation approach with operating expense (Item XOPR) as the input and total sales (Item SALE) as the output.CompustatBauer, Berger, and Humphrey (1993)Perceived Quality (Quality)Measured using customer perceived quality ratings of the firm's brand(s) from Equitrend database.EquitrendMorgan and Rego (2009)Switching CostsThese are perceived costs associated by the firm's customers with moving to an alternative supplier. We calibrate these costs as the degree to which customers exhibit loyalty to a firm that cannot be explained by the level of satisfaction delivered by the firm's offerings. Using ACSI data, we estimate customer-level loyalty as a latent factor comprising variables capturing customers' repurchase intentions and price sensitivity. Satisfaction is the ACSI measure detailed previously. We estimate switching costs for each firm/year as the residual of regressing each firm's customers' loyalty onto its customers' satisfaction, controlling for industry and time.Loyalty(it) = β0 + β1 × Satisfaction(it) + ID(it) + YD(it) + ε(it), where ID(it) are industry and YD(it) year dummies. ε(it) is the residual of this regression and is used as our estimate of switching costs, which are firm- and year-specific.ACSI (firm/year-level aggregation of individual-level respondent survey response data).Rego, Morgan, and Fornell (2013)Niche-Focused Strategy (Niche)Text analysis employing a new dictionary utilizing an inductive word search with exemplar niche firms. The analysis is then performed using a bag-of-words approach where each firm gets a score corresponding to the ratio of niche-related words and total words in each firm 10-K. To ensure that we were isolating the types of niche firms where market share was expected to be negatively associated with profit, suggested in the theorizing (i.e., those pursuing a single niche in a market vs. those targeting several different segments with different offerings), we then identified mono- versus multibrand firms by multiplying the niche-focus score for each firm by the dummy variable (1 for monobrand firms, 0 for multibrand firms).New measureService-Dominant Markets (Services)Dummy variable identifying service firms/ industries using Fama–French NAICS industries.Fama and French (2008)Firm AgeNumber of years of operation of the firm since incorporation, obtained from the firm's annual reports and websites.Industry ConcentrationIndustry-level average market share.Edeling and Himme (2018)B2C versus B2B FirmsDummy variable capturing whether the firm caters mainly to business customers. Each firm was coded manually by three coders who used information on categorization from secondary sources such as Hoover's. Reliability was >85%.Services (Banking)Dummy variable capturing whether a firm belongs to the banking sector (SIC Code 602).CompustatCompetitor OrientationText analysis of 10-K reports following dictionaries on competitor orientation (as a part of Market Orientation) developed in prior literature (Zachary et al. 2011).SECZachary et al. (2011)ControlsFirm SizeThe firm's reported total assets (Item AT).CompustatMarket Growth Annualchange in cumulative industry sales (Item SALE).CompustatR&D ExpenseFirm's reported expenditures on Research and Development (Item XRD).CompustatAdvertising ExpenseFirm's reported expenditures on Advertising (Item XAD)CompustatRobustness Check VariablesROAThe ratio of current year income before extraordinary items (Item IB) to the firm's previous year total assets (Item AT).CompustatTobin's qRatio of the firm's market value to the replacement cost of physical and intangible capital of the firmWe measure the firm's market value as the market value of outstanding equity (Items PRCC_F × CSHO), plus the book value of debt (Items DLTT + DLC), minus the firm's current assets (Item ACT). The firm's replacement cost of physical capital is measured as the book value of property, plant, and equipment (Item PPEGT). Intangible capital is estimated as the sum of the firm's knowledge capital (the capitalized value of firm R&D expenditures) and organizational capital (a fraction of the capitalized value of firm SGA expenditures) following Peters and Taylor (2017).Peters and Taylor (2017)Alternate Market PowerOperationalized based on Lerner Index as profit margin relative to price. Average variable costs are used as a proxy for marginal costs, operationalized using total variable costs divided by sales (Items XOPR and SALE). Average price was estimated dividing sales revenues (Item SALE) by unit sales (obtained from GMID database).GMID, CompustatBoone (2008)Market Share FocusBased on text analysis of 10-K reports, estimated as the ratio of the number of times ""market share"" is reported relative to the total number of words in the annual 10-K report.New measurePerceived QualityMeasured via average annual perceived quality ratings of the firm's brand(s) from the Brand Asset Valuator database.Brand Asset Valuator Mizik and Jacobson (2005)Perceived QualityMeasured using average annual firm quality ratings from Fortune's World's Most Admired Companies database.AMACCretu and Brodie (2007) 28 Notes: SEC = Securities & Exchange Commission; SGA = selling and general administrative.  "
8,"Fields of Gold: Scraping Web Data for Marketing Insights Marketing scholars increasingly use web scraping and application programming interfaces (APIs) to collect data from the internet. Yet, despite the widespread use of such web data, the idiosyncratic and sometimes insidious challenges in its collection have received limited attention. How can researchers ensure that the data sets generated via web scraping and APIs are valid? While existing resources emphasize technical details of extracting web data, the authors propose a novel methodological framework focused on enhancing its validity. In particular, the framework highlights how addressing validity concerns requires the joint consideration of idiosyncratic technical and legal/ethical questions along the three stages of collecting web data: selecting data sources, designing the data collection, and extracting the data. The authors further review more than 300 articles using web data published in the top five marketing journals and offer a typology of how web data have advanced marketing thought. The article concludes with directions for future research to identify promising web data sources and embrace novel approaches for using web data to capture and describe evolving marketplace realities.Keywords: web scraping; application programming interface; API; crawling; validity; user-generated content; social media; big dataThe accelerating digitization of social and commercial life has created an unprecedented number of digital traces of consumer and firm behavior. Every minute, users worldwide conduct 5.7 million searches on Google, make 6 million commercial transactions, and share 65,000 photos on Instagram ([76]). The resulting web data—enormous in size, diverse in form, and often publicly accessible on the internet—is a potential goldmine for marketing scholars who want to quantify consumption, gain insights on firm behavior, and track social activities difficult or costly to observe otherwise. The importance of web data for marketing research is reflected in a growing number of impactful publications across all methodological traditions, including consumer culture theory, consumer psychology, empirical modeling, and marketing strategy.Researchers can use web scraping and application programming interfaces (APIs) to efficiently collect web data at scale. Web scraping is the process of developing software to automatically collect information displayed in a web browser. For example, researchers can scrape Amazon's website to construct data sets of online consumer reviews. Because many websites and web apps are publicly accessible, data sets can be generated without involving data providers. In contrast, some data providers also offer APIs for programmatic access to their internal databases. For example, scholars can apply for academic research access to retrieve data from the Twitter API. Researchers can also access a wide range of algorithms via APIs. For instance, Google offers advanced image and video recognition through its Cloud Vision API (for additional examples and explanations, see Table W1 in Web Appendix A).Data extracted from the internet, at first sight, might resemble other organically generated data sets that address related research questions (e.g., a firm's clickstream data). Yet, collecting web data for academic use in a highly automated manner may prompt a set of novel and sometimes insidious validity challenges. Validity concerns may arise from, among others, ( 1) failing to capture contextual information in a rapidly changing environment (e.g., updates to the website's data-generating process), ( 2) not sufficiently aligning the psychological processes of interest with the frequency of data extraction on review platforms (e.g., the collected information does not capture the time when the behavior occurred), ( 3) overlooking the influence of algorithmic interference on e-commerce websites (e.g., the effect of personalization algorithms on information display), or ( 4) failing to retain raw website or API data necessary for construct validation, sampling, and analysis.Against this background, this article makes three interlinked contributions. First, we develop a methodological framework that highlights how addressing validity concerns arising from web scraping and APIs requires the joint consideration of idiosyncratic technical and legal/ethical concerns. Within marketing, guidance exists for collecting web data in the consumer culture theory research tradition, particularly using netnography (e.g., [46], [47]). A handful of articles address selected challenges that occur during the automatic extraction of web data (e.g., sampling; [39]). Outside of marketing, tutorials and books primarily focus on technical details for the automatic extraction of web data (see Table W2 in Web Appendix B). Yet, neither these resources nor methodological articles in other disciplines (e.g., [19]; [52]) address the broad spectrum of validity concerns arising from the automatic collection of web data for academic use. It is this void that our methodological framework fills. In discussing the methodological framework, we offer a stylized marketing example for illustration and provide recommendations for addressing challenges researchers encounter during the collection of web data via web scraping and APIs.Second, despite the use of web data in marketing for two decades, no systematic review reflects on how it has and could advance marketing thought. Importantly, understanding the richness and versatility of web data is invaluable for scholars curious about integrating it into their research programs. To offer these insights, we have systematically reviewed more than 300 articles in the top five marketing journals across two decades that have used web data. We leverage our coding to reveal which web sources have been considered and how data have been extracted. The resulting typology of web data may spark the imagination of researchers interested in generating new marketing insights from web data.Finally, we use our methodological framework and typology to unearth new and underexploited ""fields of gold"" associated with web data. We seek to demystify the use of web scraping and APIs and thereby facilitate broader adoption of web data across the marketing discipline. Our future research section highlights novel and creative avenues of using web data that include exploring underutilized sources, creating rich multisource data sets, and fully exploiting the potential of APIs beyond data extraction. We particularly highlight the value of web scraping and APIs for research streams that have not yet embraced them at scale.In what follows, we provide an overview of the use of web data in marketing and document four pathways through which web data have advanced marketing thought. We then introduce our methodological framework to help researchers make sensible design decisions when automatically extracting web data. We conclude with directions for future research. Using Web Data to Advance Marketing ThoughtAcross the top five marketing journals, marketing researchers increasingly use information available on the internet. For example, the share of web data–based publications has more than tripled in the last decade, from about 4% in 2010 to 15% in 2020 (see the thick line in Figure 1). The growing use of web data has been fueled by its increased accessibility and associated time and cost savings. Most of the 313 identified web data–based articles rely on web scraping (59%); APIs are used much more sparingly (12%), and some articles combine web scraping and APIs (9%). The remaining articles—especially netnographic work—use web data but tend to extract it manually (20%). The median annual citation count of articles using web data is 7.55, compared with 3.90 for publications not using web data.Graph: Figure 1. Increased use of web data in marketing (2001–2020).Some of the earliest uses of web data in marketing can be attributed to the development of netnography to study online communities (e.g., [46], [45]). Subsequently, the first quantitative marketing scholars extracted web data at scale (e.g., [27]). Today, all subfields—including marketing strategy and consumer behavior—have embraced web data.Online word of mouth and social media are the most prominent domains of inquiry using web scraping (see Table W3 in Web Appendix C). The most widely used data source in academic marketing research is Amazon (38 articles). Other prevalent sources are Twitter (30), IMDb (24), Facebook, and Google Trends (both 22; see Table W4 in Web Appendix C).Via a comprehensive literature review, we next identify the four central pathways through which web data facilitate the creation of new knowledge in marketing. Studying New PhenomenaWeb data can boost the field's relevance by enabling marketing scholars to study novel phenomena. For example, initial work using web data focused on novel online phenomena that emerged at the beginning of this century, such as online conversations ([27]) and the impact of consumer reviews on sales ([14]). Web data are well suited to provide fertile grounds for inductive research to develop novel theories about emerging marketing phenomena (e.g., brand public; [ 4]).Gathering data via web scraping or APIs often decreases the time between the occurrence of a marketplace phenomenon and the availability of data for academic research. This inherent timeliness of web data continues to be an essential lever for marketing scholars to advance our understanding of emerging substantive topics such as the sharing economy (e.g., Airbnb; [93]), access-based business models (e.g., Spotify; [15]), and fake online content (e.g., [ 3]). More generally, web data enable researchers to weigh in on contemporary issues before any ""conventional"" data sets become available, such as measuring the effect of pandemic lockdown policies on consumption ([74]). Boosting Ecological ValueWeb data can create knowledge by allowing researchers to move closer to marketing's ""natural habitat"" ([83]). Some of the most used web sources contain commercial outcome variables relevant to marketing stakeholders and are difficult or costly to collect otherwise. Examples are sales (e.g., The-Numbers.com), sales ranks (e.g., Amazon), online searches (e.g., Google Trends), and donations (e.g., contributions to a Kiva project).As web data can be collected unobtrusively, they can effectively complement more controlled data collection methods. Using web data, researchers can demonstrate that focal psychological processes occur outside the confines of a controlled laboratory environment and stylized experimental stimuli ([64]). Consider, for instance, the controversy around the decoy effect ([37])—one of the most prominent context effects in consumer behavior. Using experiments, [23] questioned the robustness and practical relevance of the decoy effect. In response, [90] built a panel data set from an online diamond market using web data. Their work not only shows that the decoy effect emerges in a high-stakes setting but also, more importantly, reaffirms its practical significance by quantifying its profit implications for the diamond retailer.Another benefit of using web data to boost ecological value is that they can often be collected without the data provider's direct involvement. Thereby, researchers can limit the interference of data suppliers or collaborating firms to ensure that the societal relevance of a particular research question is given precedence over business objectives (e.g., firms might be unwilling to share data about the tracking tools they use on websites; [81]). Further, using web data, researchers can ensure the publication of research findings, regardless of how palatable they are to the organizations that are being studied. Facilitating Methodological AdvancementAs much of the data produced by consumers and firms is inherently unstructured, extracting insights can be challenging ([88]). Thus, marketing researchers have leveraged web data for developing methods that deal with and extract insights from different types of unstructured data, such as textual, image, and video data. For instance, web data have fueled the rapid improvement of automated text analysis (see [ 6]) and the large-scale analysis of image and video content (e.g., [55]; [57]).The availability of network data on the internet (e.g., friend or product networks), along with outcome variables (e.g., posts, likes, sales ranks), has further enabled the use and advancement of methods for analyzing networks (e.g., [67]). Given their wealth and richness, web data have also stimulated the development of novel methods that can complement or replace traditional marketing research methods (e.g., using user-generated content to construct accurate multidimensional scaling maps of brands; [65]). Improving MeasurementWeb data can advance marketing knowledge by allowing researchers to measure constructs more precisely and obtain more valid inferences. For example, the collection of adequate control variables is often difficult. To capture seasonality in purchase patterns across a wide range of geographical markets and calendar years, researchers have used APIs to construct continuous (vs. dichotomous) variables that accurately reflect national holidays (e.g., HolidayAPI; [16]). Web data also allow researchers to efficiently operationalize new measures at scale, such as weather conditions based on the location of users' IP addresses (e.g., Weather Underground; [53]).Relative to non-web data sources, researchers can collect data on the behavior of many consumers and firms at higher frequencies ([ 1]). Such data enhance statistical power, enable identification of causal effects, and facilitate the examination of theoretically relevant variation within individuals over time (e.g., how various psychological distances shape review content for the same consumer; [35]) or how effects unfold over time (e.g., the impact of video elements on virality over time; [77]). SummaryTable 1 presents a typology of the four central pathways through which web data have advanced marketing thought. The typology highlights web data-based studies that investigate key marketing constructs across different entities, from consumers to organizations and other marketing stakeholders. For example, [80] explored a new phenomenon (tweeting), focusing on consumers (i.e., their motivation to tweet). These pathways for knowledge creation from web data are not mutually exclusive. Combining different pathways might be particularly promising for making breakthrough contributions.GraphTable 1. How to Create Knowledge Using Web Data: A Typology. Effect on ...Primary Pathways of Knowledge Creation Using Web DataPathway 1:Studying New PhenomenaPathway 2:BoostingEcological ValuePathway 3:Facilitating Methodological AdvancementPathway 4: Improving MeasurementConsumers(e.g., social media use, consumer learning)Toubia and Stephen (2013) test the motivations of users to contribute content to social media.Sridhar and Srinivasan (2012) explore peer effects in evaluating online product reviews.Huang (2019) studies how picture quality improves due to consumer learning.Huang et al. (2016) exploit within-user variation to measure how psychological distances interact.Organizations(e.g., sales and profits of firms, donations to nonprofits)Chevalier and Mayzlin (2006) demonstrate the impact of online reviews on book sales.Wu and Cosguner (2020) probe the prevalence and profit implications of decoy effects.Netzer et al. (2012) mine user-generated content to identify market structures.Datta et al. (2022) gather national holidays across 14 countries and 11 years to capture seasonality.Other marketing stakeholders(e.g., market reaction of investors, public health outcomes)Hermosilla, Gutiérrez-Navratil, and Prieto-Rodríguez (2018) examine how consumers' aesthetic preferences create biases in firms' hiring decisions.Blaseg, Schulze, and Skiera (2020) examine whether consumers are protected against false price advertising claims on Kickstarter.Tirunillai and Tellis (2012) develop novel online metrics based on user-generated content to predict stock returns.Kim and KC (2020) explore the effect of ads for erectile dysfunction drugs on birth rates. 1 Notes: The table highlights illustrative and diverse examples of web data–based studies and corresponding outcome variables, cross-tabulated by four pathways through which web data have advanced marketing thought (the columns) and three of the most studied actors in marketing research (the rows).Next, we introduce our methodological framework, which outlines an approach for making design decisions that enhance the validity of web data collected via web scraping and APIs. Researchers interested in learning more about the technical details of automatically extracting web data can consult our curation of technical tutorials in Web Appendix B or the digital companion to this article (available at https://web-scraping.org), which features a searchable database of all marketing articles in the top five marketing journals using web data. Methodological Framework for Collecting Web DataIn automatically collecting web data using web scraping and APIs, researchers make seemingly innocuous design decisions. However, as we will show, these decisions often involve trade-offs about research validity, technical feasibility, and legal/ethical risks[ 5] that are not always apparent. How researchers resolve these trade-offs shapes the credibility of research findings by enhancing or undermining statistical conclusion validity, internal validity, construct validity, and external validity ([73]).We develop a methodological framework to provide guidance for the automatic collection of web data using web scraping and APIs. Figure 2 offers a stylized view of this process involving three key stages—source selection, collection design, and data extraction. Researchers typically start with a broad set of potential data sources and eliminate some of them as a function of three key considerations—validity, technical feasibility, and legal/ethical risks. These three considerations appear in the corners of an inverted pyramid, with validity at the bottom to underscore its importance. Given the difficulty in projecting the exact characteristics of the final data set before it is collected, researchers often revisit these considerations as they design, prototype, and refine their data collection. Failure to resolve technical or legal/ethical issues might mean that web data cannot inform the research question meaningfully.Graph: Figure 2. Methodological framework for collecting web data.Our framework deliberately focuses on collecting web data rather than its subsequent analysis. Analyzing web data involves many familiar methodological challenges encountered with organically generated data (e.g., cleaning to remove erroneous data or create measures, selecting observations, addressing endogeneity). However, approaches for the valid collection of web data are not yet documented nor commonplace in marketing research.The methodological framework—designed to guide the automatic extraction of web data at scale—is agnostic to research paradigms. It is applicable to both deductive (i.e., identifying compelling web data to test hypotheses) and inductive (i.e., observing interesting irregularities in web data to identify novel marketing concepts and/or novel relationships between constructs) approaches to theory building. We next highlight the idiosyncratic challenges encountered when collecting web data and summarize solutions to these challenges in Tables 2–4. For expositional clarity, we focus on web scraping in our text.[ 6] To illustrate the key challenges encountered in designing the data collection, we gradually introduce a stylized marketing example involving the collection of book reviews from Amazon.GraphTable 2. Challenges and Solutions in Selecting Web Data Sources. Challenge #1.1: Exploring the Universe of Potential Web SourcesReason for importanceAs web sources vastly differ in quality, stability, and retrievability, researchers might be tempted to consider dominant or familiar platforms only. A thorough exploration of the data universe permits compelling theory testing and identifying novel, emerging marketing phenomena that may be difficult to notice otherwise.Solutions and best practicesAssume the perspective of different stakeholders (e.g., consumers, analysts, managers) during the search processBrowse through public API directories (e.g., ProgrammableWeb, GitHub)Broaden geographic search criteria (e.g., non-Western)Identify adjacent data sources (e.g., using Google Trend's ""related search queries"")Expand search to nonprimary data providers (i.e., aggregators, databases)Carefully vet the provider's description of relevant metricsDetermine the conditions necessary to access data (e.g., requirement to log in on a website, creating an API key, subscribing to an API, possibility to signal academic status/scientific use)Verify whether it is possible to opt out of firm-administered experiments or whether the site is accessible without cookiesUse the website or make some initial API requests to assess information availability (in the case of APIs, assess which authentication procedure is necessary to obtain data)Challenge #1.2: Considering Alternatives to Web ScrapingReason for importanceBecause web scraping is the most popular extraction method for web data, researchers may overlook alternative ways to extract data. APIs provide a documented and authorized way to obtain web data for many sources. Some sources also provide readily available data sets. Using such alternatives leads to time savings and minimizes exposure to legal risk.Solutions and best practicesExpand search by explicitly including terms such as ""API"" or ""data set download""Explore whether the source or third parties (e.g., public data platforms, researchers) offer data sets for download and assess their terms of useIf a data source provides an API and a website, understand the differences in what data could be retrieved from them (e.g., by screening the API documentation) and how well the API can be accessed (e.g., using packages)Use stable versions of an API, and subscribe to a source's API support updatesChallenge #1.3: Mapping the Data ContextReason for importanceWeb data are usually not accompanied by extensive documentation. Identifying potentially relevant contextual information early on is essential for the relevance and validity of the research.Solutions and best practicesScreen blogs, press releases, a source's software ""changelogs,"" or use Google's reverse search to identify important (technical) developmentsBuild an initial understanding of the presence of algorithms by visiting the source with different devices at different times or by inspecting the site's source codeUnderstand changes to the data-generating process (e.g., by studying changes over time using archive.org)Inspect the robots.txt file and assess how the source requires users to agree to their terms of service (e.g., preferrable ""browsewrap"" vs. less preferable ""clickwrap"" agreements)Scrutinize popularity, legitimacy, and business model of data sources (e.g., by using firm reports, stock filings, news, and social media, other data providers like Statista)Explore forums where users of the source talk about the source (e.g., Reddit)Assess whether the data links to other data sets (e.g., by spotting common IDs)Map out ""worst-case"" scenarios for research objectives in the case that the data source changes (e.g., discontinuation of an API, removal of a website) GraphTable 3. Challenges and Solutions in Designing Web Data Collections. Challenge #2.1: Which Information to Extract from Which Pages?Validity Challenges [V]Which information is necessary to justify construct operationalization and allow analysis?Which metadata might enhance internal and external validity?Is information subject to algorithmic biases or missing data?Are there significant changes to the data-generating process?Legal/Ethical Challenges [L]Is all of the required information publicly accessible, or is a login required?Does the data contain personal or sensitive information, and can subjects be identified?Is there a sufficient scientific justification for using the data?How large is the overlap between the research objective and the original intent of subjects disclosing the data?Technical Challenges [T]Is all information extractable?Are there any limits to iterating through pages or endpoints?Does the extraction software obtain information reliably?Solutions and Best PracticesExplore different types of pages to detect unique vs. identical information [V]Explore whether alternative ways to browse/navigate the site (e.g., URLs, clicking, scrolling, logging into the site) provides different or reveals new information [T]Explore how extraction methods (e.g., ""headless"" HTTP requests vs. simulated browsing, different user agents, screen width, login status, use of different packages) affect information display [V, T]Assess the accuracy of timestamps (e.g., time zones) [V]Save screenshots of pages that describe the calculation of metrics [V]Explore (temporarily available) information in the source code of a website using the browser's ""inspect"" tools [V]Assess the presence of technical roadblocks (e.g., captchas) [T]Assess how data was generated historically at the source (e.g., via archive.org) [V]Explore limits to iterating through pages [T]Obtain information from various sources to reduce dependency on data provider [L]If possible, opt out of firm-administered experiments or block cookies; alternatively, identify relevant metadata that can be used to control for the presence of algorithms [V]Challenge #2.2: How to Sample?Validity Challenges [V]Is the sample size sufficient to effectively inform the research question?To which population does the sample generalize?Is the sampling frame corresponding to the research objective (e.g., randomness)?How prevalent is panel attrition?Legal/Ethical Challenges [L]Does the data represent an excessive portion relative to all data available?Can the data be obtained in similar forms elsewhere, or is the research question only answerable with the targeted data?Are some of the sampled units (potentially) vulnerable?Technical Challenges [T]Is the required sample size technically feasible?Can external information (e.g., IDs) be consistently matched to the data?Solutions and Best PracticesAssess characteristics of the population (e.g., using secondary sources) [V]Explore options to sample directly from the source (e.g., from different pages, randomization through filtering/searching, obtaining usernames from forums, see also Neuendorf 2017 and Humphreys and Wang 2018) [V]Choose lists or pages that are not affected by algorithmic influence [V]Refresh sample (or use multiple types of sampled units) to assess the stability of sample and counterbalance panel attrition [V]Discard units from the sample to prevent data collection from subjects falling under prohibitive national and supranational legislation (e.g., GDPR) [L]Explore external sources to inform the sampling frame [V], or facilitate linkage [T]Assess the efficiency of different navigation paths and their impact on sample size [T]Pseudo-anonymize or discard sensitive or personal information [L]Ensure that no excessive amount of data (e.g., data on all users) is collected (absolute volume, relative volume) [L]Reexamine alternative sources to improve justification of data extraction [L]Challenge #2.3: At Which Frequency to Extract the Data?Validity Challenges [V]Is the extraction frequency in sync with the studied phenomena?Is the refresh rate of the source sufficient?Is the data (thought to be archival) really archival?Is the information consistently available across all periods of interest?Does the order and frequency in which information is retrieved induce bias?Legal/Ethical Challenges [L]Does the extraction frequency pose an excessive load on the source?Does collecting more data at higher frequencies make the data more sensitive?Technical Challenges [T]Does the desired extraction frequency pose new technical hurdles?How can the stability of data collection be guaranteed, and different collection batches be distinguished?Solutions and Best PracticesExplore the gains in collecting data multiple times rather than once (e.g., in a ""live"" data collection) [V]Adhere to best practices in setting the extracting frequency (e.g., five requests per second for APIs, one request per two seconds for web scraping) [L, T]Experiment with technical parameters (e.g., number of computers) to balance technically feasible sample size and desired frequency of data extraction [T]Formulate, test, and refine data source theory (Landers et al. 2016) [V]Reinspect the robots.txt file to avoid exceeding retrieval limits for selected pages [T]Consider randomizing extraction order for sampled units over time [V]Consider (cost) implications for storage and computation time [T]Consider getting in touch with the data provider if the targeted data set is infeasible to extract via web scraping or APIs [T, L]Devise a schedule for the automatic extraction of the data (e.g., using Windows Task Manager or Cron) [T, V]Challenge #2.4: How to Process the Data During the Collection?Validity Challenges [V]Could erroneous processing lead to unexpected data loss?Could there be any significant scientific value in retaining the raw data?Legal/Ethical Challenges [L]Is the collected data in conflict with prohibitive laws (e.g., GDPR)?Is the collected data sufficiently secured from unauthorized access?Is anonymization or pseudonymization required?Technical Challenges [T]Which storage facilities to use to accommodate the expected data (size, location, format, encoding)Is normalization necessary?Solutions and Best PracticesRetain raw data (e.g., HTML pages, JSON responses) whenever possible [V, T]Always parse some minimal amount of data (e.g., timestamps) to facilitate monitoring checks in real-time [V, T]Remove sensitive and personal information on the fly; if personal or sensitive information is strictly required to meet the research objective, consider pseudo-anonymizing (potentially via third parties) [L]Verify data storage during collection meets legal requirements for potentially sensitive or personal data [L]Ensure proper encoding of (non-English) characters, retain correct digit separators and correct data format GraphTable 4. Challenges and Solutions in Extracting Web Data. Challenge #3.1: Improving PerformanceReason for importanceIn scaling up the data collection, researchers might encounter new technical issues. For example, the data collection could stop unexpectedly or proceed much slower than anticipated.Solutions and best practicesWhen scraping, use stable selectors (e.g., tags, classes, attributes, styles associated with particular information) and make only selective use of error handlingWhen using APIs, choose a stable and supported versionAttempt to reparse data from stored raw data if the extraction failedCheck for traces of being banned/blocked/slowed down by the website (e.g., by scanning the content that was retrieved)Notify data sources about potential bandwidth issues with APIsUpdate the technically feasible retrieval limit, and recalculate desired sample size, extraction frequency, etc.Verify that computing resources are appropriate and reliable (e.g., scale up or down servers, verify that database runs optimally)Move data to a remote (and more scalable) file storage or databaseConsider potential benefits from using cloud computing (e.g., for extended, uninterrupted data collection) vs. benefits from local setups (e.g., due to security or privacy concerns)Budget the expected costs of API subscriptions, cloud computing and data storage and transferChallenge #3.2: Monitoring Data QualityReason for importanceMonitoring is critical to be timely alerted to data quality issues. Setting up a monitoring system allows researchers to intervene before discarding data altogether.Solutions and best practicesLog each web request (i.e., URL call), along with response status codes, timestamps of when the collection was started, and when the request was madeSave raw data (i.e., source code of HTML websites), along with the parsed data for triangulationVerify whether the raw data was correctly parsed (e.g., for a sample of information, compare raw data and parsed data)Check file sizes or the number of observations at regular intervalsSet up a monitoring tool to timely alert you to any future issues (e.g., based on the number of files retrieved or requests made, file sizes retrieved, time the collection last ran, budget spent)Automatically generate reports on data quality (e.g., using RMarkdown)Record issue(s) in a logbook (e.g., in the documentation); especially if considered critical for data qualityChallenge #3.3: Documenting DataReason for importanceResearchers are responsible for documenting the data set they produce from web data. Building documentation during the collection is important to guarantee accuracy and completeness, which facilitates use, reuse, and replicability.Solutions and best practicesMaintain a logbook in which to note important events (e.g., when the collection broke down and why)Start writing the documentation in the early stages of collecting the data, and make use of templates (e.g., Datasheets for Datasets; Gebru et al. 2020)Keep and organize copies of relevant files (e.g., screenshots of the website at the time of data extraction, the API documentation, details on variable operationalization with summary statistics, information about the context)Have a plan for long-term archival storage (e.g., re3data.org, The Dataverse Project, Zenodo), anonymization (e.g., generating synthetic versions of sensitive data), and consider which license to use for the data (e.g., Creative Commons)  Data Source SelectionA critical first step in the use of web data is selecting the data source(s). We examine three challenges faced by researchers in this selection process. First, it is essential that researchers explore the universe of potential sources (challenge #1.1). Second, researchers need to consider the range of possible extraction methods (challenge #1.2). Third, it is crucial to map the context in which the data are generated (challenge #1.3). Table 2 summarizes our recommendations for tackling these challenges. Exploring the Universe of Potential Sources (Challenge #1.1)In the absence of conventional gatekeepers (e.g., data providers), researchers can select from countless web data sources. For example, there are 2.1 million online retailers in the United States alone ([20]). Further, websites and APIs differ greatly in scope (e.g., number of users), data quality (e.g., consistency), and retrievability (e.g., extraction limits). Even within the same product category, data sources differ vastly. For example, Amazon reports a book's sales rank (an aggregate outcome metric for product sales), whereas Goodreads reports users' reading behavior (an individual outcome metric for consumers' usage intensity).Faced with a vast universe of potential sources, researchers may be tempted to focus on familiar platforms only ([89]). For instance, Amazon is the most used web data source in marketing (see Table W4 in Web Appendix C). Amazon might be a relevant source to extract book reviews, given its broad assortment and user base. Yet, in other cases, researchers might miss opportunities for identifying novel, emerging marketing phenomena or conduct more compelling theory testing without a thorough exploration of potential sources. Researchers can avoid the pitfalls of defaulting to dominant sources by actively considering a broad spectrum of websites and APIs, ranging from highly popular (e.g., Amazon) to less popular sources (e.g., Goodreads), from primary data providers (e.g., YouTube) to data aggregators (e.g., Social Blade), and from platforms with global reach (e.g., Twitter) to more regional ones (e.g., Taringa!). Another strategy to move beyond familiar sources is to adopt alternative perspectives. For instance, researchers can consider websites or APIs that are used by consumers, analysts, or managers. API directories at GitHub or programmableweb.com can facilitate identifying potentially relevant APIs.A broad exploration of web data sources may lead researchers to discover sources that may be more permissive for (academic) data extraction or less likely to trigger ethical concerns. For example, websites that do not require logging into the site to reveal information are typically more scraping-friendly than sites that first require registering a user account. In the case of Amazon, researchers can obtain most information without logging in and do not have to explicitly provide their agreement to the website's Terms of Service. To reduce legal (e.g., breaches of contract, as researchers have provided explicit agreements to the terms of service) and ethical (e.g., website users may consider their data private) risks, researchers should refrain from creating fake accounts to access information requiring a login. By explicitly declaring their academic status (e.g., when registering at the site using the institutional email address), researchers might be able to diminish their exposure to legal risk.When exploring web sources, researchers need to examine whether theoretical constructs can be operationalized in a valid manner ([91]). A healthy level of skepticism is warranted when using idiosyncratic metrics from APIs or websites. For example, researchers might be interested in scraping the price tier of restaurants from Yelp. Yet, it is not entirely clear how Yelp computes this metric from individual consumer ratings.To determine when to stop exploring sources, researchers need to assess to what extent the selected source(s) improve(s) on alternatives. One way to justify selecting a single web source is the presence of unique features. For example, a researcher studying how observers react to humor in reviews might prefer Yelp to alternative platforms as it is the only source featuring ""funny"" votes (e.g., [60]). At other times, researchers may be indifferent between potential sources and can draw from multiple sources to boost the generalizability of their findings (e.g., tweets and restaurant reviews; [61]). Collecting data from multiple sources is often useful because even similar types of information (e.g., consumer comments) may affect marketing outcomes differently, depending on source characteristics (e.g., forums vs. microblogs; [72]). Data aggregators—some of which offer authorized data access via APIs—facilitate the collection of such multisource data. Considering Alternatives to Web Scraping (Challenge #1.2)The popularity of web scraping may lead to the conclusion that it should be preferred over other methods. However, some web sources offer access to data via APIs ([12]). In general, extracting data via APIs is more scalable and less likely to invoke the same level of legal risks compared with web scraping. Although some sources offer unconstrained APIs that do not require authentication, others require (paid) subscriptions and authentication procedures. Some sources, such as Twitter, have recently started offering APIs for academic research. In the case of Amazon, an API offering access to consumer reviews is currently not available.In addition to APIs, numerous other options exist for researchers to obtain web data. For example, some data providers (e.g., Yelp, IMDb), public data platforms (e.g., Kaggle, The Dataverse Project), and researchers (e.g., [59]) provide documented web data sets that can readily be used for academic research. There are many potential use cases of such data sets, but less than 5% of all web data–based articles in marketing used such data sets. To avoid the pitfall of defaulting to web scraping for data extraction, researchers can expand their search by explicitly including terms such as ""API"" or ""data set download"" in their search queries. Mapping the Data Context (Challenge #1.3)Relative to other frequently used archival sources in marketing, web data entail large and often undocumented complexities. Thus, it is critical that researchers map the data context, which involves identifying relevant contextual developments that may undermine the validity of the research if gone unnoticed.First, mapping the data context may reveal changes in the underlying data structure. For example, a major change in a platform's user interface may affect subsequent consumer behavior. Second, mapping the data context enables researchers to identify relevant pieces of information for collection together with the focal web data. For example, researchers may discover an external website (e.g., Statista) that offers information about the composition of a focal data source's user base. If stored, such data could eventually be used to detect changes in the composition of the user base or verify the representativeness of the extracted data. Third, mapping the data context may reveal unknown information, potentially allowing researchers to discover novel research opportunities. For example, researchers may use the (unexpected) launch of a new recommendation system at a music streaming service as a natural experiment to investigate the impact of recommendations on music consumption.To understand and map the contextual complexity of web data, researchers can immerse themselves in the ecosystem surrounding the focal source by signing up and using the source, tracking press releases, social media, and scanning the competitive environment. Helpful tools include a search engine's advanced search features, newsletters, and alerts on leading business and technology magazines (e.g., TechCrunch.com, WSJ.com, FT.com). The website's source code may also hold valuable information about potentially relevant environmental changes. Sometimes, researchers may also detect the presence of algorithms on the site that may threaten the validity of the collected data. For example, Amazon's product pages personalize information based on which preceding products were viewed—even without users logging into the site. Designing the Data CollectionAfter narrowing down potential sources, researchers decide which information to extract from them (challenge #2.1), how to sample (challenge #2.2), at which frequency to extract the information (challenge #2.3), and how to process the information during the collection (challenge #2.4). Table 3 summarizes these challenges and corresponding solutions. Which Information to Extract? (Challenge #2.1)In the absence of any ""downloadable"" data set, the first challenge lies in deciding which information to extract from a source. Researchers begin by browsing the web page to identify from which pages to extract which information. In our Amazon example, some of the most commonly used pages are product pages (e.g., [14]) and review pages (e.g., [84]). Generally, pages such as those from e-commerce platforms contain information from the company's database, offering researchers the opportunity to capture some of the information available at a company. Collecting such data involves iterating through a set of related pages (i.e., browsing through many product pages and corresponding review pages in our Amazon example) and saving the data as it becomes visible.As the goal of websites like Amazon is rarely the provision of data sets for academic research, it is often necessary to combine information from different pages (e.g., book descriptions from the product page and ratings from the review page). It is particularly difficult to recognize the subtleties of available information, which makes the decision from which pages to extract information challenging. For example, researchers interested in building a data set of book reviews would find total ratings both on the product and review page, but only the review page reveals all product reviews.[ 7] Yet, neither the product nor the review pages contain all the biographical information available on a reviewer's profile page. Widely exploring a website or API is necessary for identifying information relevant for subsequent analysis (e.g., construct operationalization). The amount and type of information also often vary (e.g., depending on screen width or whether the user is logged in). In this phase, researchers should assess the degree to which the information could be considered personal or sensitive under different regulatory regimes (e.g., the European Union's General Data Protection Regulation [GDPR]), which may require planning measures such as pseudo-anonymization of reviewer names. Researchers may also reassess whether all information needs to be captured to meet the research objective. Suppose reviewer names are strictly necessary (e.g., because they allow for matching different sources). In that case, researchers can explore whether the targeted web data source offers ways to exclude subjects governed by prohibitive privacy regulations (e.g., by using filters).An important threat to internal validity in any study involving web data is algorithmic interference (e.g., [91]). The (visual) design of websites that facilitates usability can undermine the validity of the collected data if gone unnoticed and unaddressed. Especially when deciding which information to extract, it is important to reexamine the website or API for the presence of algorithms. For example, the order in which the researchers in our example visited the website while designing their data extraction could affect which related books are displayed on the product pages on Amazon. Other algorithms that often affect the display of data on websites are sorting algorithms (e.g., by popularity or mixed with sponsored search results) and filtering algorithms (e.g., showing subsets of the data). Algorithmic interference is often hard to detect without being sensitive to it. To account for potential algorithmic interference, the researcher might extract variables as part of an algorithm's more extensive set of input variables, which offers opportunities to control for them in the empirical analysis (e.g., the order in which books were extracted in our Amazon example).Researchers also need to establish the intertemporal stability of available information. Because the web is constantly evolving, the information on a page might not have been generated via the same process over time, undermining the internal validity of the data. Some changes to sources are drastic enough to alter how the data were created in the first place, introducing measurement error ([87]). For example, Amazon shifted to a positive-only evaluation of reviews by removing the ""not helpful"" vote button in 2018, and it no longer displays ""not helpful"" counts next to reviews ([28]). This change might have impacted review content (e.g., users writing shorter review texts). Yet, researchers collecting data today cannot find any traces of these ""not helpful"" votes. A tool for examining changes to relevant information on a website is the Wayback Machine (archive.org). Researchers can use this tool to retroactively inspect websites over time (e.g., [58]) or submit their own website links for archiving.Finally, collecting metadata that ""annotates"" the data collection enhances internal and external validity (e.g., storing the timestamp of data extraction, whether an API request was completed successfully, or the IP address from which the data request was made). Such metadata can be used not only for diagnostic purposes but also to link the extracted web data to other data sets. For example, in our Amazon example, the collected data could be linked to other data using IP-based geolocations (e.g., linking geolocation and web search data; [86]) or timestamps (e.g., linking reviews to stock prices; [78]). How to Sample? (Challenge #2.2)A second challenge in designing the data extraction lies in deciding how to sample from the data source. In particular, in the absence of access to the data source's entire database, it is difficult or impossible to draw a random sample from the population (e.g., all products) available at the data source. Instead, researchers need to devise their own sampling frame to reveal the units they want to sample from the website ([66]). For example, researchers could scan the site for an index of all products that could inform their sampling. In our example studying reviews at Amazon, multiple such indexes may be available. Should products be sampled from the bestseller page for books (so-called exposure-based populations; [66]) or instead from the category page for books (i.e., availability-based populations)? Choices like this result in different data and may even invalidate inferences, as sampling frames might inadvertently induce systematic bias ([39]).One common validity challenge in choosing how to sample is determining how many units (e.g., books) are sufficient to inform the research question. From a validity standpoint, it would be ideal to collect information on the entire population (e.g., all books available at Amazon). However, Amazon does not have an obvious page to extract all books. Imagine that a research team wanted to collect information about all marketing books sold on Amazon. The bestseller page, for example, lists only the top 100 bestsellers. By manually changing pagination parameters in the URL, the top 400 bestsellers can be revealed. Yet, this list of 400 books neither constitutes the entire population nor represents a random sample of marketing books sold at Amazon. Alternatively, when starting from the product overview pages, these pages list an imprecise number of books (e.g., ""over 60,000""), which can only be viewed up to page 50. With each result page featuring 24 organic search results, this approach would produce 1,600 books per category at best. Thus, researchers need to consider other ways to identify more books on Amazon, such as searching for books using various keywords. To expand the number of sampled units, researchers could collect data multiple times, use other keywords, or tweak search parameters to reveal more data by requesting narrower subsets from the database (e.g., only books published during a specific month).Even if a list of the population (e.g., all books) could be retrieved, it may be infeasible to extract data within a reasonable time frame. While sample size requirements are mostly concerned with a researcher's inferential goals (e.g., [50]), few articles make the resource constraints that affect collecting web data explicit (e.g., [68]). For example, with web data, a study's sample size critically depends on technical parameters such as the number of computers used for data extraction or the number of pages that need to be visited. We illustrate how to calculate the technically feasible sample size in Web Appendix F, which may effectively complement traditional sample size calculations commonplace in marketing.As a result of these complications, researchers often restrict their sample size. One way to motivate a compelling sampling frame is to use external sources that can be linked to the web data. For instance, the New York Times or Publishers Weekly bestseller lists might be a starting point for sampling books ([14]). An alternative approach focuses on internal data available at the source itself. Researchers may have to allocate substantial time to identify ways to sample from the focal source. Sometimes, starting the data collection from a page unrelated to the focal pages of interest might facilitate collecting a more representative sample (e.g., by reducing geographical biases; [86]). For example, on Amazon, researchers could first sample reviewers and associated demographic information (available at the user profile of reviewers) and subsequently retrieve data on all reviewed products. Similar to how researchers build network data from an initial set of products or users, the sampling units retrieved from an initial set of pages can be considered seeds. In choosing seeds, researchers should be cautious about drawing from vulnerable populations (e.g., minors) or infringing on prohibitive privacy regulations. At Which Frequency to Extract Information? (Challenge #2.3)Web data are nonstatic, as they change often or might disappear altogether. Therefore, researchers need to consider at which frequency to extract information. This decision encompasses whether to collect data once or multiple times and when to run (and potentially schedule) the data extraction. Consideration of the frequency and schedule is challenging but required to ensure the intertemporal stability of measurement, which is critical for internal and construct validity.From a technical and legal perspective, it is most desirable to extract data only once. Single extractions are less likely to represent a burden on the firm's servers, and the extracted data often only represent a limited snapshot of the entire database, reducing the risks of copyright infringement. Further, such data may be more likely to respect users' ""right to be forgotten,"" which is part of the privacy laws in some jurisdictions. Yet, single data extraction might raise several validity issues that can easily go unnoticed. For instance, in our example, researchers extracting book reviews once from Amazon will not be able to identify whether any of the archival information has changed. Only when extracting data multiple times can researchers systematically notice changes on the site, which may lead to the identification of ""fake"" reviews that have been removed by the platform (e.g., [29]). More generally, researchers can compare information over time to detect whether data that initially appeared to be archival is truly archival (i.e., does not change over time).Another concern is that a single extraction may not produce a data set that adequately maps onto the focal processes of interest. For example, suppose researchers in our example want to examine whether a review by a so-called ""Top 1000 Reviewer"" leads to more subsequent reviews from other users. However, the researcher merely observes that the reviewer is a top reviewer at the time of data extraction. This does not necessarily imply that this user had the same status when the review was first posted and thus was most likely to affect subsequent reviewing behavior of other users. Formulating and testing the essential assumptions about the data, including the relation between the time of data extraction and the focal (psychological) processes, is thus critical. The formulation of such assumptions is called a ""data source theory"" ([52]). Testing and refining the data source theory helps take proactive steps to enhance internal and construct validity. In the preceding example, it would thus be necessary to collect data from these review pages closer to the original posting date, ensuring that reviewers classified as ""Top Reviewers"" had that status when their reviews became visible.When extracting data more than once, automatic scheduling can help ensure consistency and contribute to validity. Scheduling is beneficial if the required information is only available in real-time. For example, sales ranks at Amazon are updated hourly for popular products, and historical sales ranks cannot be retrieved. Suppose researchers in our example were interested in studying the sales performance of books over time. In that case, they could repeatedly extract the books' sales ranks from the product pages at Amazon. Sometimes fixed intervals enhance validity (e.g., every Monday, 8 a.m.). In other circumstances (e.g., when collecting data from many pages), it may be better to vary the starting time or weekday of the data extraction.Another decision is whether to set an end date for the data extraction. Collecting data over extended periods offers the potential for researchers to build a programmatic stream of research and stumble into unexpected natural experiments (e.g., [13]). Especially for longitudinal data collections, continuing the data collection while the project is in the review process brings numerous benefits, such as the ability to update the data (e.g., a longer time frame, new measures). Yet, concerns about technical feasibility (e.g., storage requirements, continued availability of data source) grow as the data extraction horizon extends. Similarly, from an ethical perspective, the longer the data extraction, the greater the likelihood of potentially identifying individuals via triangulation. Next to ethics, long-term data collection also places a heavier load on servers, potentially increasing exposure to legal risks. How to Process the Information During the Extraction? (Challenge #2.4)As a final step in designing the data extraction, researchers decide how to process the information while it is collected. Any kind of web data collection requires a minimal degree of processing, given that the information is available in a computer's memory (e.g., in the browser or the software processing the API output) and still needs to be stored in files or databases. Thus, this processing step occurs before data sets are cleaned or analyzed.When deciding on how to process information during the extraction, researchers must balance potential efficiency gains from molding raw web data into readily usable data sets with the potential threats to validity due to ""on-the-fly"" processing. For example, in our Amazon example, researchers may be tempted to remove seemingly unnecessary information (e.g., image links in reviews), apply text processing (e.g., removing characters used as separators), or force specific information (e.g., prices) to be stored in a strictly numeric format. Such on-the-fly processing promises to produce essential efficiency gains, as the data set resulting from the extraction could directly be analyzed. However, because on-the-fly processing decisions are usually made after the inspection of only a limited number of pages in early prototypes of the data collection, it is difficult to guarantee their correctness. For example, using our example, what if the initial screening revealed only pictures posted in a review, while the extensive data collection revealed the need to capture video files? Given this and related challenges, keeping the raw data (such as the source code of websites, API output, or any media files loaded at the time of data extraction) is ideal from a validity perspective. For example, even if the data collection breaks, researchers could still process and use the information after debugging their extraction code. Retaining the raw data can also help reduce Type 1 errors by increasing transparency about researchers' degree of freedom in collecting and processing the web data. Yet, retaining the raw data prompts significant concerns about the technical feasibility and ethical risks. From a technical standpoint, storing the raw data might require databases to retain their original structure and facilitate processing, especially for projects involving many raw data files collected over extended periods. Keeping all raw data might raise questions regarding the right to store the raw data—especially if it is not (pseudo-) anonymized before storage.Finally, retaining the raw data allows researchers to refine their extraction design at later project stages. For example, a researcher might have collected Amazon reviews in 2018—around the time of the removal of the ""not helpful"" voting feature. Although extracting ""not helpful"" votes was not part of the original extraction design, researchers would be able to use the raw web data to examine the effect of the removal of these ""not helpful"" votes. Collecting the DataAfter source selection and designing the data collection, researchers gradually transition to turning their small-scale prototype into stable extraction software. In so doing, researchers face three challenges. First, researchers may need to improve the performance of their extraction software when operating it automatically at scale (challenge #3.1). Second, they may need to implement monitoring checks to be alerted to any issues arising during extended data collections (challenge #3.2). Third, researchers should compile information important for documenting the final data set (challenge #3.3). Table 4 contains a summary of solutions and best practices to these challenges. How to Improve the Performance of the Data Extraction? (Challenge #3.1)In scaling up their data extraction, researchers may notice that the extraction software frequently breaks across a larger number of pages or runs significantly slower than expected. Such technical challenges, if unaddressed, have the potential to undermine research validity (e.g., missing data, not meeting sample size requirements). A practical solution to preempt these and similar challenges involves capturing the focal information in different ways and storing raw data—especially in the early stages of data collection and for more ambitious, large-scale web data collection projects. To track whether the extraction targets are met, researchers can log the (timestamped) URLs of scraped pages and visualize the performance of the extraction software over an extended period. The resulting ""effective"" extraction frequency can then be used in recomputing the technically feasible sample size (see Web Appendix F). Novel web scraping services promise to handle technical difficulties efficiently (e.g., ScrapingBee, Zyte). How to Monitor Data Quality During the Extraction? (Challenge #3.2)As a next step, researchers consider which metadata can help them diagnose issues with the data collection in real-time. Especially when websites constantly change, monitoring the health of web scrapers can be a tedious task. Researchers should consider performance at a higher level (e.g., the file sizes of extracted raw data) and lower level (i.e., the accuracy of the information in resulting data files) to assess whether the collection is proceeding as expected. When collecting over long periods, automatic reporting can greatly facilitate monitoring. Finally, alerts (e.g., via email or mobile) can help researchers detect predefined data issues quickly. How to Document the Data During and After the Extraction? (Challenge #3.3)During the data extraction, researchers need to record relevant information about the data in real-time. This is an essential step in building documentation, enabling future data usage by the researcher(s) who collected the data and other scholars. Even after the data extraction has ended, researchers can continuously refine the documentation as they become familiar with the characteristics of the data (e.g., variables that were erroneously captured, missing values).Accurate and comprehensive documentation is particularly critical given that collecting web data tends to involve repeated iterations between discovery (and often troubleshooting) and confirmation (i.e., subsequent analyses that are outside the scope of our framework). Designing web data extractions requires a different mindset compared with experiments or archival research. Unlike running experiments, the extraction design for collecting web data may be in flux, even when the collection is already running. Relative to traditional archival research in which data sets are sufficiently annotated, researchers are in charge of accurately recalling details about the data collection. Such details encompass information about the data composition (e.g., sampled units), extraction process (e.g., annotated code, detected errors during the collection), and processing details (e.g., applied cleaning steps). The template of [24] provides a useful starting point for building the documentation for a data set collected via web scraping or APIs. Given that contextual changes are inevitable (see challenge #1.3), documenting the source's institutional background (e.g., screenshots, corporate blog posts, API documentation) is crucial. Future Research Opportunities with Web DataAn unprecedented gold rush of web data has enriched the marketing discipline for two decades—over 300 published articles provide countless examples of impactful marketing insights using web data. With the ever-increasing digitization of social and commercial life, it is hard to imagine that the heyday of this gold rush might subside any time soon. Yet, are marketing's currently productive mines the only or the most promising sources of marketing insights in the future? Which novel approaches and technologies are necessary to capture and describe evolving marketplace realities?To identify directions for future research, we have reviewed more than 300 articles to provide a snapshot of the current state of web data in marketing. We use these insights to inform the subsequent discussion, which we organize along the four pathways through which web data can advance marketing thought (as summarized in Table 1). We supplement our discussion with key elements from our methodological framework (see Figure 2) and inspiring use cases from other disciplines. Direction 1: Identify New Web Data SourcesNext, we discuss how researchers can use source selection to branch out to new or underutilized sources for studying emerging substantive topics. We also highlight how researchers can design more complex, longitudinal, and multisource web data sets to reveal otherwise invisible phenomena. Draw from underutilized sourcesOur review reveals that marketing research draws from a somewhat concentrated list of web sources (see Table W4 in Web Appendix C). We encourage researchers to focus on underused or niche sources that have received limited or no attention in marketing. Web data are often prized, as they allow for collecting ""consequential dependent variables from the 'real world'"" ([40], p. 357). Identifying new sources or novel consequential variables constitutes a promising avenue for discovering emerging phenomena.Consider, for example, the twilight state of the nascent legal cannabis industry in the United States. While more states are legalizing cannabis for medical and recreational use, the market value of the legal U.S. cannabis industry was still less than a third of the illegal market in 2020 (i.e., $20 billion vs. $66 billion; [22]). Using surveys, media coverage, and in-depth interviews, marketing scholars have begun to explore how such legalized markets emerge and seek legitimacy ([38]). Sociologists and organizational scholars, in turn, have already used web data to compile intriguing data sets from sources such as Weedmaps. Using these data, they examine, for example, how existing medical cannabis dispensaries have repositioned themselves after the entry of recreational dispensaries ([34]) or how consumers deal with potential stigma transfer (Khessina, Reis, and Cameron Verhaal 2021). By leveraging similar web data, marketing researchers could explore intriguing marketing questions. For instance, how should brands position themselves (e.g., brand personalities, emphasis on product vs. service), depending on the strength of categorical stigma? What are the potential public health and welfare implications of the increasing competition among cannabis dispensaries or their growing social media activities?In addition to being attuned to work in other disciplines, a low-tech route for source exploration is provided by Similarweb, which allows researchers to browse website rankings by region or category. Given the broad accessibility of web sources worldwide, the dominance of Northern American and European data sources is surprising. Not a single article focuses exclusively on African web sources, and only a handful of articles use some African data (e.g., [49]). Possible starting points for branching out into these underexplored marketplaces could be popular websites such as Nairaland.com (online community), bidorbuy.co.za (auction platform), and Jumia.com.ng (e-commerce). Build unique and rich data sets by drawing from multiple sourcesMost published marketing articles use web data gathered from a single source. Only very few articles collect data from a large number of web sources (i.e., 50 or more web sources). Following the lead of these articles, we encourage marketing researchers to envision unique data sets compiled from many and diverse sources. For example, in economics, [10] collected online and offline prices for individual goods sold by 56 large multichannel retailers in ten countries (i.e., United States, United Kingdom, Argentina, Australia, Brazil, Canada, China, Germany, Japan, and South Africa) between 2014 and 2016. This ""Billion Prices Project"" (bpp.mit.edu; [11]) exemplifies how creative and ambitious data collection from diverse web sources can fuel entire research programs. Especially if sufficiently documented, such web data are poised to unearth new fields of gold for the marketing discipline. Rediscover frequently used sourcesAs researchers decide which information to extract (see challenge #2.1), they may overlook novel information on sources they already know. Therefore, refocusing on different information may also reveal how to study novel phenomena on frequently used sources. Adopting a ""discovery mode"" may reveal that phenomena of high societal relevance such as gender or racial issues are occurring at frequently used sources such as TripAdvisor ([69]), Kickstarter ([92]), and DonorsChoose ([ 2]). For example, in entrepreneurship, [92] scraped Kickstarter information to examine whether male African American founders are less successful in crowdfunding. Researchers in marketing, in turn, could build on these and similar ideas to explore whether biases exist in other online market exchanges. Alter the extraction frequencyAnother promising lever for exploring emerging phenomena is the extraction frequency (challenge #2.3). In most articles, the data were extracted once (e.g., on a single occasion). Extracting data once is sufficient for many research objectives, such as demonstrating the prevalence of a phenomenon in the marketplace (e.g., [79]). Yet, researchers can also uncover novel marketing phenomena by creatively envisioning web data sets that only reveal the phenomenon if the information is extracted multiple times. For example, [29] leverage the observation that Amazon removed certain reviews to study the market for ""fake"" reviews. Specifically, they combine repeatedly web-scraped data from Amazon with hand-coded data from large private groups on Facebook used to solicit fake reviews to examine the short- and long-term impact of such rating manipulations. This example illustrates that data imperfections (e.g., data modifications discovered when mapping the data context, see challenge #1.3) can be opportunities to pose novel research questions rather than merely nuisances that warrant correction. Direction 2: Harvest the Versatility of Web Data to Boost Ecological ValueAs a second direction for knowledge discovery, web data are often used to increase the ecological value of marketing research by complementing carefully controlled experiments. Triangulating findings generated via different methods is fruitful. Yet, there are many other underutilized avenues for how researchers can select and extract web data to infuse ecological validity into experiments and other types of marketing studies. Infuse ecological validity into experimental stimuliBy carefully selecting websites and APIs, researchers can enhance the ecological validity of their experiments (e.g., through more realistic or diverse stimuli and measures). This enormous potential has hardly been realized in marketing, particularly at scale (for a creative smaller-scale application, see [62]). Social psychologists demonstrate the full potential of such an approach. Consider, for example, [33], who scraped 87 real-world profiles of doctors (including their fitness habits) from the website of a health insurance provider. These profiles served as the foundation for a novel stimulus-sampling paradigm wherein participants in experiments were presented with randomly selected subsets (i.e., five fitness-focused and five non-fitness-focused profiles). In doing so, the authors first ground the phenomenon in the field (i.e., that doctors signal their fitness habits) and then use stimuli created from real profiles to demonstrate that overweight and obese individuals are less likely to choose fitness-focused doctors for their own care. Such triangulation and the creation of larger and more representative samples of naturalistic stimuli enhance the replicability and generalizability of experimental effects ([42]). The experimental paradigms in core marketing topics (e.g., branding, advertising, pricing) and methods (e.g., lab experiments, conjoint studies) could benefit from similar applications to mimic real marketplaces. For instance, branding or advertising researchers might develop stimuli based on data extracted from sources like crowdfunding platforms or Bing's Image Search API (e.g., brand logos, ads, and slogans). Run self-administered field experiments via APIsWhile field experiments continue to be prized for their realism and high ecological value ([83]), very few published marketing articles use APIs to run field experiments (e.g., [51]; [80]). There are many untapped opportunities to run field experiments administered by researchers rather than cooperating partners (e.g., firms or charities). Using APIs to run field experiments gives researchers more control over the design and debriefing processes and allows for monitoring of granular participant behavior over longer periods. Thus, web data–based field experiments potentially produce more precise effect sizes and allow researchers to capture long-term effects ([26]). In such experiments, researchers might randomly assign users to different treatments, such as adding (vs. not adding) followers on Twitter ([80]) or assigning (vs. not assigning) Reddit's Gold Awards to user posts ([ 9]). By gathering high-frequency data via APIs, researchers can analyze how experimental treatments influence outcomes such as posting or the creativity of user-generated content. Alternatively, APIs can be leveraged to infuse realism into experiments, as embodied in [56], who developed ""Hoogle,"" a mock search engine that relies on APIs offered by Google but only displays organic search results that are not altered based on previous user queries. We foresee many more creative future applications of web data to facilitate such field experimentation. Direction 3: Adopt New Metrics and Methods for Generating Marketing InsightsA core topic in marketing research is to develop marketing metrics that can guide managerial decision making. Traditionally, many metrics have been based on offline information and established data providers (e.g., [21]). Given the continued growth and diversification of web data, it is tempting for marketing managers to focus more on web data for managing firm growth and profitability. Yet, deciding which information to select and extract for marketing insight is challenging (see challenge #2.1). More research is needed to help managers avoid succumbing to the streetlight effect (i.e., an ""overreliance on readily available data due to ease of measurement and application, irrespective of their growth objective""; [18], pp. 164–65). But, how can researchers get started? Explore which web sources provide cheaper, faster, or better marketing metricsOver the last decade, scholars have begun to explore which types of web data could proxy or improve on existing core marketing metrics. For example, managers may use search data extracted from Google to spot trends in the relative importance of their firm's product attributes, which is more cost effective than traditional methods ([17]). Mining Twitter data provides cheaper, real-time, and more actionable measures and insights about brand reputation than existing survey-based metrics like the Brand Asset Valuator data from the advertising agency VMLY&R ([71]). Yet, in other circumstances, readily and cheaply available web data might not be a good substitute for more expensive or established proprietary data sources to uncover market structure ([70]).An exciting direction for future research is to explore what web data sources should be selected or combined to generate marketing insights that fuel firm growth. For instance, many novel metrics rely on textual data ([ 6]). This focus limits applications to markets using the same language employed by the original method (i.e., mostly English). Future research could explore what other types of web data might enable the creation of metrics and insights that allow real-time monitoring and managing diverse global markets. What insights can managers draw from differences and commonalities between the volume of different kinds of internet searches available via Google Trends (e.g., web search vs. image search vs. Google Shopping vs. YouTube search)? Alternatively, what insights about consumer preferences (or any other stakeholder) can be extracted from short videos posted on platforms such as TikTok? Operate API-based microservicesA fascinating opportunity arises from providing microservices via APIs to marketing stakeholders. This means that researchers not only use APIs to retrieve data but can also operate their own APIs to examine real marketplaces (e.g., using rplumber.io in R). Researchers in data science, for example, offer firms a framework for testing multiarmed bandit policies via APIs while at the same time gathering field experimental data ([48]). Marketing researchers could use similar API-powered microservices to study emerging topics such as recommendation systems (and resulting biases) or tap into a firm's customer relationship management system to validate new customer churn models. At a small scale, researcher-powered APIs could lower the entry barriers for firms to experiment with novel algorithms that have not yet been implemented in major software packages.The provision of APIs provides access to novel types of data, while also increasing the timeliness and ecological value of such data. For example, consider the differences between web data collected by a web scraper and the underlying clickstream data stored in the company's database. The website may merely show aggregate statistics about the number of reviews posted. At the same time, the underlying clickstream data also feature information on every website visit (e.g., time, IP address). As with self-administered APIs, researchers define which information a company should submit (e.g., as input to a recommendation algorithm). Thus, researchers can gain access to unique firm data that are otherwise difficult to obtain. For example, large-scale studies with image and video data are still scarce in marketing. Offering image and video analysis as microservices may generate knowledge discovery for new image sources, such as GIFs used in social media (e.g., Giphy). Direction 4: Exploit Efficiency Gains to Improve MeasurementWeb data also have advanced marketing by improving measurement by efficiently collecting diverse variables. Therefore, as a fourth direction, we discuss how web data can improve measurement across the discipline, particularly by rejuvenating interest in core marketing topics (e.g., market orientation, advertising; for an overview of these topics, see [41]). Relatedly, researchers can also leverage APIs to effectively integrate algorithms for processing unstructured data at scale into empirical analyses ([88]). Leverage web sources to describe diverse online and offline behaviorsMost marketing articles gather web data to describe and examine behavior occurring online. As documented in Table W4 in Web Appendix C, many of the used sources in marketing are focused on online consumer behaviors, such as e-commerce websites (e.g., Amazon), online reviewing platforms (e.g., Yelp), social media sites (e.g., Twitter), and search engines (e.g., Google Trends). Relatively less research has focused on firm behavior online. Yet, by doing so, researchers could explore many core marketing constructs (e.g., service orientation, sustainability). For example, researchers could systematically collect information available on the websites of many firms to analyze which organizational factors influence how firms signal their service orientation (e.g., employees' digital presence; [30]) or environmental credentials (e.g., the B Corporations certification; [25]) to customers and other stakeholders.We encourage marketing researchers who have not yet used web data in their research to consider websites and APIs as valuable, rich, and timely sources to exploit the increased digitization of all forms of behaviors—not only online behavior. A recent example of bringing web data into an established ""offline"" research stream is [32], who scraped the annual reports of more than 8,000 firms from AnnualReports.com between 1998 and 2016. Web sources contain historical information about periods, even long before the web in its current form existed (e.g., 1998 in this case). The authors subsequently use these reports to develop a novel text-based measure of marketing excellence derived from firm letters to shareholders. Many other untapped online sources (e.g., job posting platforms) offer new insights into how firms communicate their marketing capabilities to external stakeholders beyond consumers, such as prospective employees, social activists, and investors.Particularly for the marketing–finance interface, the web features many understudied forms of investor-facing communication that are ripe for collection at scale. For example, which type of marketing topics besides marketing excellence (e.g., marketing capabilities, brand positioning, pricing) should top management emphasize to investors to increase firm valuation during investor relations presentations, investor days, or earnings calls? Researchers could also examine the relative importance of the content versus the delivery (e.g., the tone of the speaker on the recording of an investor day; see [85]). Such multimodal data can also benefit the inferences made in established research streams. Embrace APIs for better measurementAPIs offer many opportunities for improving measurement—some of which are unexpected. For example, consumer researchers planning to run longitudinal studies might consider APIs for automating processes for managing participants at scale, thereby reducing the operating costs (and potentially boosting sample size). The Amazon Mechanical Turk API and the various Prolific Academic APIs (e.g., Study API) are good starting points for running multiwave studies.APIs also enable much more than just retrieving data. For example, to reduce validity concerns in long-term data collection, researchers can use the Pushover API (https://pushover.net/api) to send monitoring alerts to their smartphones. The API of Amazon Web Services allows for the orchestration of virtual computing infrastructure (e.g., to capture data from different countries). Another fruitful avenue in which APIs are currently underused in marketing is facilitating stimuli selection. For example, a classic area of inquiry in marketing is how (background) music affects product and brand perceptions and choices (e.g., [ 8]). In 2022, background music is quite different (e.g., self-chosen, more diverse use cases). Researchers could use the Spotify Web API to select stimuli from millions of mood, sleep, or study playlists, thereby discovering perfect ""lookalikes"" that only differ on one focal attribute (e.g., tempo) but not on other acoustic attributes available at the API (e.g., valence, loudness). Even in this simple example, there might be a substantive interest in better understanding the effect of new background music on consumption choices, especially given the shift to working and studying from home. Concluding ThoughtsWeb data have unearthed many fields of gold in marketing. However, extracting data for generating relevant and valid research insights is challenging. Our article highlights validity concerns that require the joint consideration of idiosyncratic technical, legal, and ethical questions. We introduce a novel methodological framework (Figure 2), offer practical solutions (Tables 2–4), and outline directions for future research to enable researchers to create impactful and credible marketing knowledge. While our focus is primarily on authors, our work also spotlights crucial validity concerns to scholars reviewing web data–based research and practitioners interested in deriving accurate and actionable marketing insights from web data.We hope that our work encourages marketing scholars to integrate web data into their research programs. While web data often provide compelling answers to the question, ""Assuming that this hypothesis is true, in what ways does it manifest in the world?"" ([ 5], p. 1455), this does not imply that web data are relevant for all research projects. Web data sources tend to feature a large N (i.e., many users) with many V (i.e., different pieces of information for potential variables) observable over a large T (i.e., many observations over extended periods of time and at a very granular level; [ 1]). Yet, collecting web data via web scraping or APIs provides limited information about the browsing behavior of individuals on the website that led to the creation of the data in the first place. Significant synergies exist by enriching clickstream stream data capturing such browsing processes with web data retrieved from web scraping and APIs (e.g., [54]).Our work aims to bridge entrenched training silos (e.g., between quantitative marketing and consumer behavior). We encourage scholars to further integrate and leverage existing best practices with regard to the collection and analysis of web data (e.g., preregistration, addressing endogeneity). There is significant untapped potential for collaborations across methodological traditions to explore and exploit new fields of gold. Collecting valid web data can enable marketing as a discipline to enhance its relevance and assert intellectual leadership on important emerging substantive topics that are also increasingly studied in fields such as computer science, information systems, and management science ([63]).We would be remiss not to mention the nonmonetary costs of collecting web data via web scraping and APIs. While browsing the web is (mainly) free, researchers should not assume that collecting web data is costless. The prototype of a data collection can be ready and running in a matter of hours. Yet, researchers will often find out that the data collection does not work entirely as intended or encounter some of the challenges discussed in our methodological framework. Just like with any other method, the devil is in the details.Web data democratize data access and make our discipline more inclusive for scholars who would otherwise find it difficult to obtain access to data. To further reduce entry barriers, it would be helpful to create incentives (e.g., journal space) for rich web data sets and their documentation, like the Billion Prices Project ([11]). Similarly, authors can make their algorithms or data available for other researchers by sharing code publicly or deploying API-based microservices that can increase their methods' adoption and offer unique opportunities for field experimentation. In summary, web data present a golden opportunity to examine important marketing questions, now and in the future. Supplemental Materialsj-pdf-1-jmx-10.1177_00222429221100750 - Supplemental material for Fields of Gold: Scraping Web Data for Marketing InsightsSupplemental material, sj-pdf-1-jmx-10.1177_00222429221100750 for Fields of Gold: Scraping Web Data for Marketing Insights by Johannes Boegershausen, Hannes Datta, Abhishek Borah and Andrew T. Stephen in Journal of Marketing  "
8,"Fields of Gold: Scraping Web Data for Marketing Insights Marketing scholars increasingly use web scraping and application programming interfaces (APIs) to collect data from the internet. Yet, despite the widespread use of such web data, the idiosyncratic and sometimes insidious challenges in its collection have received limited attention. How can researchers ensure that the data sets generated via web scraping and APIs are valid? While existing resources emphasize technical details of extracting web data, the authors propose a novel methodological framework focused on enhancing its validity. In particular, the framework highlights how addressing validity concerns requires the joint consideration of idiosyncratic technical and legal/ethical questions along the three stages of collecting web data: selecting data sources, designing the data collection, and extracting the data. The authors further review more than 300 articles using web data published in the top five marketing journals and offer a typology of how web data have advanced marketing thought. The article concludes with directions for future research to identify promising web data sources and embrace novel approaches for using web data to capture and describe evolving marketplace realities.Keywords: web scraping; application programming interface; API; crawling; validity; user-generated content; social media; big dataThe accelerating digitization of social and commercial life has created an unprecedented number of digital traces of consumer and firm behavior. Every minute, users worldwide conduct 5.7 million searches on Google, make 6 million commercial transactions, and share 65,000 photos on Instagram ([76]). The resulting web data—enormous in size, diverse in form, and often publicly accessible on the internet—is a potential goldmine for marketing scholars who want to quantify consumption, gain insights on firm behavior, and track social activities difficult or costly to observe otherwise. The importance of web data for marketing research is reflected in a growing number of impactful publications across all methodological traditions, including consumer culture theory, consumer psychology, empirical modeling, and marketing strategy.Researchers can use web scraping and application programming interfaces (APIs) to efficiently collect web data at scale. Web scraping is the process of developing software to automatically collect information displayed in a web browser. For example, researchers can scrape Amazon's website to construct data sets of online consumer reviews. Because many websites and web apps are publicly accessible, data sets can be generated without involving data providers. In contrast, some data providers also offer APIs for programmatic access to their internal databases. For example, scholars can apply for academic research access to retrieve data from the Twitter API. Researchers can also access a wide range of algorithms via APIs. For instance, Google offers advanced image and video recognition through its Cloud Vision API (for additional examples and explanations, see Table W1 in Web Appendix A).Data extracted from the internet, at first sight, might resemble other organically generated data sets that address related research questions (e.g., a firm's clickstream data). Yet, collecting web data for academic use in a highly automated manner may prompt a set of novel and sometimes insidious validity challenges. Validity concerns may arise from, among others, ( 1) failing to capture contextual information in a rapidly changing environment (e.g., updates to the website's data-generating process), ( 2) not sufficiently aligning the psychological processes of interest with the frequency of data extraction on review platforms (e.g., the collected information does not capture the time when the behavior occurred), ( 3) overlooking the influence of algorithmic interference on e-commerce websites (e.g., the effect of personalization algorithms on information display), or ( 4) failing to retain raw website or API data necessary for construct validation, sampling, and analysis.Against this background, this article makes three interlinked contributions. First, we develop a methodological framework that highlights how addressing validity concerns arising from web scraping and APIs requires the joint consideration of idiosyncratic technical and legal/ethical concerns. Within marketing, guidance exists for collecting web data in the consumer culture theory research tradition, particularly using netnography (e.g., [46], [47]). A handful of articles address selected challenges that occur during the automatic extraction of web data (e.g., sampling; [39]). Outside of marketing, tutorials and books primarily focus on technical details for the automatic extraction of web data (see Table W2 in Web Appendix B). Yet, neither these resources nor methodological articles in other disciplines (e.g., [19]; [52]) address the broad spectrum of validity concerns arising from the automatic collection of web data for academic use. It is this void that our methodological framework fills. In discussing the methodological framework, we offer a stylized marketing example for illustration and provide recommendations for addressing challenges researchers encounter during the collection of web data via web scraping and APIs.Second, despite the use of web data in marketing for two decades, no systematic review reflects on how it has and could advance marketing thought. Importantly, understanding the richness and versatility of web data is invaluable for scholars curious about integrating it into their research programs. To offer these insights, we have systematically reviewed more than 300 articles in the top five marketing journals across two decades that have used web data. We leverage our coding to reveal which web sources have been considered and how data have been extracted. The resulting typology of web data may spark the imagination of researchers interested in generating new marketing insights from web data.Finally, we use our methodological framework and typology to unearth new and underexploited ""fields of gold"" associated with web data. We seek to demystify the use of web scraping and APIs and thereby facilitate broader adoption of web data across the marketing discipline. Our future research section highlights novel and creative avenues of using web data that include exploring underutilized sources, creating rich multisource data sets, and fully exploiting the potential of APIs beyond data extraction. We particularly highlight the value of web scraping and APIs for research streams that have not yet embraced them at scale.In what follows, we provide an overview of the use of web data in marketing and document four pathways through which web data have advanced marketing thought. We then introduce our methodological framework to help researchers make sensible design decisions when automatically extracting web data. We conclude with directions for future research. Using Web Data to Advance Marketing ThoughtAcross the top five marketing journals, marketing researchers increasingly use information available on the internet. For example, the share of web data–based publications has more than tripled in the last decade, from about 4% in 2010 to 15% in 2020 (see the thick line in Figure 1). The growing use of web data has been fueled by its increased accessibility and associated time and cost savings. Most of the 313 identified web data–based articles rely on web scraping (59%); APIs are used much more sparingly (12%), and some articles combine web scraping and APIs (9%). The remaining articles—especially netnographic work—use web data but tend to extract it manually (20%). The median annual citation count of articles using web data is 7.55, compared with 3.90 for publications not using web data.Graph: Figure 1. Increased use of web data in marketing (2001–2020).Some of the earliest uses of web data in marketing can be attributed to the development of netnography to study online communities (e.g., [46], [45]). Subsequently, the first quantitative marketing scholars extracted web data at scale (e.g., [27]). Today, all subfields—including marketing strategy and consumer behavior—have embraced web data.Online word of mouth and social media are the most prominent domains of inquiry using web scraping (see Table W3 in Web Appendix C). The most widely used data source in academic marketing research is Amazon (38 articles). Other prevalent sources are Twitter (30), IMDb (24), Facebook, and Google Trends (both 22; see Table W4 in Web Appendix C).Via a comprehensive literature review, we next identify the four central pathways through which web data facilitate the creation of new knowledge in marketing. Studying New PhenomenaWeb data can boost the field's relevance by enabling marketing scholars to study novel phenomena. For example, initial work using web data focused on novel online phenomena that emerged at the beginning of this century, such as online conversations ([27]) and the impact of consumer reviews on sales ([14]). Web data are well suited to provide fertile grounds for inductive research to develop novel theories about emerging marketing phenomena (e.g., brand public; [ 4]).Gathering data via web scraping or APIs often decreases the time between the occurrence of a marketplace phenomenon and the availability of data for academic research. This inherent timeliness of web data continues to be an essential lever for marketing scholars to advance our understanding of emerging substantive topics such as the sharing economy (e.g., Airbnb; [93]), access-based business models (e.g., Spotify; [15]), and fake online content (e.g., [ 3]). More generally, web data enable researchers to weigh in on contemporary issues before any ""conventional"" data sets become available, such as measuring the effect of pandemic lockdown policies on consumption ([74]). Boosting Ecological ValueWeb data can create knowledge by allowing researchers to move closer to marketing's ""natural habitat"" ([83]). Some of the most used web sources contain commercial outcome variables relevant to marketing stakeholders and are difficult or costly to collect otherwise. Examples are sales (e.g., The-Numbers.com), sales ranks (e.g., Amazon), online searches (e.g., Google Trends), and donations (e.g., contributions to a Kiva project).As web data can be collected unobtrusively, they can effectively complement more controlled data collection methods. Using web data, researchers can demonstrate that focal psychological processes occur outside the confines of a controlled laboratory environment and stylized experimental stimuli ([64]). Consider, for instance, the controversy around the decoy effect ([37])—one of the most prominent context effects in consumer behavior. Using experiments, [23] questioned the robustness and practical relevance of the decoy effect. In response, [90] built a panel data set from an online diamond market using web data. Their work not only shows that the decoy effect emerges in a high-stakes setting but also, more importantly, reaffirms its practical significance by quantifying its profit implications for the diamond retailer.Another benefit of using web data to boost ecological value is that they can often be collected without the data provider's direct involvement. Thereby, researchers can limit the interference of data suppliers or collaborating firms to ensure that the societal relevance of a particular research question is given precedence over business objectives (e.g., firms might be unwilling to share data about the tracking tools they use on websites; [81]). Further, using web data, researchers can ensure the publication of research findings, regardless of how palatable they are to the organizations that are being studied. Facilitating Methodological AdvancementAs much of the data produced by consumers and firms is inherently unstructured, extracting insights can be challenging ([88]). Thus, marketing researchers have leveraged web data for developing methods that deal with and extract insights from different types of unstructured data, such as textual, image, and video data. For instance, web data have fueled the rapid improvement of automated text analysis (see [ 6]) and the large-scale analysis of image and video content (e.g., [55]; [57]).The availability of network data on the internet (e.g., friend or product networks), along with outcome variables (e.g., posts, likes, sales ranks), has further enabled the use and advancement of methods for analyzing networks (e.g., [67]). Given their wealth and richness, web data have also stimulated the development of novel methods that can complement or replace traditional marketing research methods (e.g., using user-generated content to construct accurate multidimensional scaling maps of brands; [65]). Improving MeasurementWeb data can advance marketing knowledge by allowing researchers to measure constructs more precisely and obtain more valid inferences. For example, the collection of adequate control variables is often difficult. To capture seasonality in purchase patterns across a wide range of geographical markets and calendar years, researchers have used APIs to construct continuous (vs. dichotomous) variables that accurately reflect national holidays (e.g., HolidayAPI; [16]). Web data also allow researchers to efficiently operationalize new measures at scale, such as weather conditions based on the location of users' IP addresses (e.g., Weather Underground; [53]).Relative to non-web data sources, researchers can collect data on the behavior of many consumers and firms at higher frequencies ([ 1]). Such data enhance statistical power, enable identification of causal effects, and facilitate the examination of theoretically relevant variation within individuals over time (e.g., how various psychological distances shape review content for the same consumer; [35]) or how effects unfold over time (e.g., the impact of video elements on virality over time; [77]). SummaryTable 1 presents a typology of the four central pathways through which web data have advanced marketing thought. The typology highlights web data-based studies that investigate key marketing constructs across different entities, from consumers to organizations and other marketing stakeholders. For example, [80] explored a new phenomenon (tweeting), focusing on consumers (i.e., their motivation to tweet). These pathways for knowledge creation from web data are not mutually exclusive. Combining different pathways might be particularly promising for making breakthrough contributions.GraphTable 1. How to Create Knowledge Using Web Data: A Typology. Effect on ...Primary Pathways of Knowledge Creation Using Web DataPathway 1:Studying New PhenomenaPathway 2:BoostingEcological ValuePathway 3:Facilitating Methodological AdvancementPathway 4: Improving MeasurementConsumers(e.g., social media use, consumer learning)Toubia and Stephen (2013) test the motivations of users to contribute content to social media.Sridhar and Srinivasan (2012) explore peer effects in evaluating online product reviews.Huang (2019) studies how picture quality improves due to consumer learning.Huang et al. (2016) exploit within-user variation to measure how psychological distances interact.Organizations(e.g., sales and profits of firms, donations to nonprofits)Chevalier and Mayzlin (2006) demonstrate the impact of online reviews on book sales.Wu and Cosguner (2020) probe the prevalence and profit implications of decoy effects.Netzer et al. (2012) mine user-generated content to identify market structures.Datta et al. (2022) gather national holidays across 14 countries and 11 years to capture seasonality.Other marketing stakeholders(e.g., market reaction of investors, public health outcomes)Hermosilla, Gutiérrez-Navratil, and Prieto-Rodríguez (2018) examine how consumers' aesthetic preferences create biases in firms' hiring decisions.Blaseg, Schulze, and Skiera (2020) examine whether consumers are protected against false price advertising claims on Kickstarter.Tirunillai and Tellis (2012) develop novel online metrics based on user-generated content to predict stock returns.Kim and KC (2020) explore the effect of ads for erectile dysfunction drugs on birth rates. 1 Notes: The table highlights illustrative and diverse examples of web data–based studies and corresponding outcome variables, cross-tabulated by four pathways through which web data have advanced marketing thought (the columns) and three of the most studied actors in marketing research (the rows).Next, we introduce our methodological framework, which outlines an approach for making design decisions that enhance the validity of web data collected via web scraping and APIs. Researchers interested in learning more about the technical details of automatically extracting web data can consult our curation of technical tutorials in Web Appendix B or the digital companion to this article (available at https://web-scraping.org), which features a searchable database of all marketing articles in the top five marketing journals using web data. Methodological Framework for Collecting Web DataIn automatically collecting web data using web scraping and APIs, researchers make seemingly innocuous design decisions. However, as we will show, these decisions often involve trade-offs about research validity, technical feasibility, and legal/ethical risks[ 5] that are not always apparent. How researchers resolve these trade-offs shapes the credibility of research findings by enhancing or undermining statistical conclusion validity, internal validity, construct validity, and external validity ([73]).We develop a methodological framework to provide guidance for the automatic collection of web data using web scraping and APIs. Figure 2 offers a stylized view of this process involving three key stages—source selection, collection design, and data extraction. Researchers typically start with a broad set of potential data sources and eliminate some of them as a function of three key considerations—validity, technical feasibility, and legal/ethical risks. These three considerations appear in the corners of an inverted pyramid, with validity at the bottom to underscore its importance. Given the difficulty in projecting the exact characteristics of the final data set before it is collected, researchers often revisit these considerations as they design, prototype, and refine their data collection. Failure to resolve technical or legal/ethical issues might mean that web data cannot inform the research question meaningfully.Graph: Figure 2. Methodological framework for collecting web data.Our framework deliberately focuses on collecting web data rather than its subsequent analysis. Analyzing web data involves many familiar methodological challenges encountered with organically generated data (e.g., cleaning to remove erroneous data or create measures, selecting observations, addressing endogeneity). However, approaches for the valid collection of web data are not yet documented nor commonplace in marketing research.The methodological framework—designed to guide the automatic extraction of web data at scale—is agnostic to research paradigms. It is applicable to both deductive (i.e., identifying compelling web data to test hypotheses) and inductive (i.e., observing interesting irregularities in web data to identify novel marketing concepts and/or novel relationships between constructs) approaches to theory building. We next highlight the idiosyncratic challenges encountered when collecting web data and summarize solutions to these challenges in Tables 2–4. For expositional clarity, we focus on web scraping in our text.[ 6] To illustrate the key challenges encountered in designing the data collection, we gradually introduce a stylized marketing example involving the collection of book reviews from Amazon.GraphTable 2. Challenges and Solutions in Selecting Web Data Sources. Challenge #1.1: Exploring the Universe of Potential Web SourcesReason for importanceAs web sources vastly differ in quality, stability, and retrievability, researchers might be tempted to consider dominant or familiar platforms only. A thorough exploration of the data universe permits compelling theory testing and identifying novel, emerging marketing phenomena that may be difficult to notice otherwise.Solutions and best practicesAssume the perspective of different stakeholders (e.g., consumers, analysts, managers) during the search processBrowse through public API directories (e.g., ProgrammableWeb, GitHub)Broaden geographic search criteria (e.g., non-Western)Identify adjacent data sources (e.g., using Google Trend's ""related search queries"")Expand search to nonprimary data providers (i.e., aggregators, databases)Carefully vet the provider's description of relevant metricsDetermine the conditions necessary to access data (e.g., requirement to log in on a website, creating an API key, subscribing to an API, possibility to signal academic status/scientific use)Verify whether it is possible to opt out of firm-administered experiments or whether the site is accessible without cookiesUse the website or make some initial API requests to assess information availability (in the case of APIs, assess which authentication procedure is necessary to obtain data)Challenge #1.2: Considering Alternatives to Web ScrapingReason for importanceBecause web scraping is the most popular extraction method for web data, researchers may overlook alternative ways to extract data. APIs provide a documented and authorized way to obtain web data for many sources. Some sources also provide readily available data sets. Using such alternatives leads to time savings and minimizes exposure to legal risk.Solutions and best practicesExpand search by explicitly including terms such as ""API"" or ""data set download""Explore whether the source or third parties (e.g., public data platforms, researchers) offer data sets for download and assess their terms of useIf a data source provides an API and a website, understand the differences in what data could be retrieved from them (e.g., by screening the API documentation) and how well the API can be accessed (e.g., using packages)Use stable versions of an API, and subscribe to a source's API support updatesChallenge #1.3: Mapping the Data ContextReason for importanceWeb data are usually not accompanied by extensive documentation. Identifying potentially relevant contextual information early on is essential for the relevance and validity of the research.Solutions and best practicesScreen blogs, press releases, a source's software ""changelogs,"" or use Google's reverse search to identify important (technical) developmentsBuild an initial understanding of the presence of algorithms by visiting the source with different devices at different times or by inspecting the site's source codeUnderstand changes to the data-generating process (e.g., by studying changes over time using archive.org)Inspect the robots.txt file and assess how the source requires users to agree to their terms of service (e.g., preferrable ""browsewrap"" vs. less preferable ""clickwrap"" agreements)Scrutinize popularity, legitimacy, and business model of data sources (e.g., by using firm reports, stock filings, news, and social media, other data providers like Statista)Explore forums where users of the source talk about the source (e.g., Reddit)Assess whether the data links to other data sets (e.g., by spotting common IDs)Map out ""worst-case"" scenarios for research objectives in the case that the data source changes (e.g., discontinuation of an API, removal of a website) GraphTable 3. Challenges and Solutions in Designing Web Data Collections. Challenge #2.1: Which Information to Extract from Which Pages?Validity Challenges [V]Which information is necessary to justify construct operationalization and allow analysis?Which metadata might enhance internal and external validity?Is information subject to algorithmic biases or missing data?Are there significant changes to the data-generating process?Legal/Ethical Challenges [L]Is all of the required information publicly accessible, or is a login required?Does the data contain personal or sensitive information, and can subjects be identified?Is there a sufficient scientific justification for using the data?How large is the overlap between the research objective and the original intent of subjects disclosing the data?Technical Challenges [T]Is all information extractable?Are there any limits to iterating through pages or endpoints?Does the extraction software obtain information reliably?Solutions and Best PracticesExplore different types of pages to detect unique vs. identical information [V]Explore whether alternative ways to browse/navigate the site (e.g., URLs, clicking, scrolling, logging into the site) provides different or reveals new information [T]Explore how extraction methods (e.g., ""headless"" HTTP requests vs. simulated browsing, different user agents, screen width, login status, use of different packages) affect information display [V, T]Assess the accuracy of timestamps (e.g., time zones) [V]Save screenshots of pages that describe the calculation of metrics [V]Explore (temporarily available) information in the source code of a website using the browser's ""inspect"" tools [V]Assess the presence of technical roadblocks (e.g., captchas) [T]Assess how data was generated historically at the source (e.g., via archive.org) [V]Explore limits to iterating through pages [T]Obtain information from various sources to reduce dependency on data provider [L]If possible, opt out of firm-administered experiments or block cookies; alternatively, identify relevant metadata that can be used to control for the presence of algorithms [V]Challenge #2.2: How to Sample?Validity Challenges [V]Is the sample size sufficient to effectively inform the research question?To which population does the sample generalize?Is the sampling frame corresponding to the research objective (e.g., randomness)?How prevalent is panel attrition?Legal/Ethical Challenges [L]Does the data represent an excessive portion relative to all data available?Can the data be obtained in similar forms elsewhere, or is the research question only answerable with the targeted data?Are some of the sampled units (potentially) vulnerable?Technical Challenges [T]Is the required sample size technically feasible?Can external information (e.g., IDs) be consistently matched to the data?Solutions and Best PracticesAssess characteristics of the population (e.g., using secondary sources) [V]Explore options to sample directly from the source (e.g., from different pages, randomization through filtering/searching, obtaining usernames from forums, see also Neuendorf 2017 and Humphreys and Wang 2018) [V]Choose lists or pages that are not affected by algorithmic influence [V]Refresh sample (or use multiple types of sampled units) to assess the stability of sample and counterbalance panel attrition [V]Discard units from the sample to prevent data collection from subjects falling under prohibitive national and supranational legislation (e.g., GDPR) [L]Explore external sources to inform the sampling frame [V], or facilitate linkage [T]Assess the efficiency of different navigation paths and their impact on sample size [T]Pseudo-anonymize or discard sensitive or personal information [L]Ensure that no excessive amount of data (e.g., data on all users) is collected (absolute volume, relative volume) [L]Reexamine alternative sources to improve justification of data extraction [L]Challenge #2.3: At Which Frequency to Extract the Data?Validity Challenges [V]Is the extraction frequency in sync with the studied phenomena?Is the refresh rate of the source sufficient?Is the data (thought to be archival) really archival?Is the information consistently available across all periods of interest?Does the order and frequency in which information is retrieved induce bias?Legal/Ethical Challenges [L]Does the extraction frequency pose an excessive load on the source?Does collecting more data at higher frequencies make the data more sensitive?Technical Challenges [T]Does the desired extraction frequency pose new technical hurdles?How can the stability of data collection be guaranteed, and different collection batches be distinguished?Solutions and Best PracticesExplore the gains in collecting data multiple times rather than once (e.g., in a ""live"" data collection) [V]Adhere to best practices in setting the extracting frequency (e.g., five requests per second for APIs, one request per two seconds for web scraping) [L, T]Experiment with technical parameters (e.g., number of computers) to balance technically feasible sample size and desired frequency of data extraction [T]Formulate, test, and refine data source theory (Landers et al. 2016) [V]Reinspect the robots.txt file to avoid exceeding retrieval limits for selected pages [T]Consider randomizing extraction order for sampled units over time [V]Consider (cost) implications for storage and computation time [T]Consider getting in touch with the data provider if the targeted data set is infeasible to extract via web scraping or APIs [T, L]Devise a schedule for the automatic extraction of the data (e.g., using Windows Task Manager or Cron) [T, V]Challenge #2.4: How to Process the Data During the Collection?Validity Challenges [V]Could erroneous processing lead to unexpected data loss?Could there be any significant scientific value in retaining the raw data?Legal/Ethical Challenges [L]Is the collected data in conflict with prohibitive laws (e.g., GDPR)?Is the collected data sufficiently secured from unauthorized access?Is anonymization or pseudonymization required?Technical Challenges [T]Which storage facilities to use to accommodate the expected data (size, location, format, encoding)Is normalization necessary?Solutions and Best PracticesRetain raw data (e.g., HTML pages, JSON responses) whenever possible [V, T]Always parse some minimal amount of data (e.g., timestamps) to facilitate monitoring checks in real-time [V, T]Remove sensitive and personal information on the fly; if personal or sensitive information is strictly required to meet the research objective, consider pseudo-anonymizing (potentially via third parties) [L]Verify data storage during collection meets legal requirements for potentially sensitive or personal data [L]Ensure proper encoding of (non-English) characters, retain correct digit separators and correct data format GraphTable 4. Challenges and Solutions in Extracting Web Data. Challenge #3.1: Improving PerformanceReason for importanceIn scaling up the data collection, researchers might encounter new technical issues. For example, the data collection could stop unexpectedly or proceed much slower than anticipated.Solutions and best practicesWhen scraping, use stable selectors (e.g., tags, classes, attributes, styles associated with particular information) and make only selective use of error handlingWhen using APIs, choose a stable and supported versionAttempt to reparse data from stored raw data if the extraction failedCheck for traces of being banned/blocked/slowed down by the website (e.g., by scanning the content that was retrieved)Notify data sources about potential bandwidth issues with APIsUpdate the technically feasible retrieval limit, and recalculate desired sample size, extraction frequency, etc.Verify that computing resources are appropriate and reliable (e.g., scale up or down servers, verify that database runs optimally)Move data to a remote (and more scalable) file storage or databaseConsider potential benefits from using cloud computing (e.g., for extended, uninterrupted data collection) vs. benefits from local setups (e.g., due to security or privacy concerns)Budget the expected costs of API subscriptions, cloud computing and data storage and transferChallenge #3.2: Monitoring Data QualityReason for importanceMonitoring is critical to be timely alerted to data quality issues. Setting up a monitoring system allows researchers to intervene before discarding data altogether.Solutions and best practicesLog each web request (i.e., URL call), along with response status codes, timestamps of when the collection was started, and when the request was madeSave raw data (i.e., source code of HTML websites), along with the parsed data for triangulationVerify whether the raw data was correctly parsed (e.g., for a sample of information, compare raw data and parsed data)Check file sizes or the number of observations at regular intervalsSet up a monitoring tool to timely alert you to any future issues (e.g., based on the number of files retrieved or requests made, file sizes retrieved, time the collection last ran, budget spent)Automatically generate reports on data quality (e.g., using RMarkdown)Record issue(s) in a logbook (e.g., in the documentation); especially if considered critical for data qualityChallenge #3.3: Documenting DataReason for importanceResearchers are responsible for documenting the data set they produce from web data. Building documentation during the collection is important to guarantee accuracy and completeness, which facilitates use, reuse, and replicability.Solutions and best practicesMaintain a logbook in which to note important events (e.g., when the collection broke down and why)Start writing the documentation in the early stages of collecting the data, and make use of templates (e.g., Datasheets for Datasets; Gebru et al. 2020)Keep and organize copies of relevant files (e.g., screenshots of the website at the time of data extraction, the API documentation, details on variable operationalization with summary statistics, information about the context)Have a plan for long-term archival storage (e.g., re3data.org, The Dataverse Project, Zenodo), anonymization (e.g., generating synthetic versions of sensitive data), and consider which license to use for the data (e.g., Creative Commons)  Data Source SelectionA critical first step in the use of web data is selecting the data source(s). We examine three challenges faced by researchers in this selection process. First, it is essential that researchers explore the universe of potential sources (challenge #1.1). Second, researchers need to consider the range of possible extraction methods (challenge #1.2). Third, it is crucial to map the context in which the data are generated (challenge #1.3). Table 2 summarizes our recommendations for tackling these challenges. Exploring the Universe of Potential Sources (Challenge #1.1)In the absence of conventional gatekeepers (e.g., data providers), researchers can select from countless web data sources. For example, there are 2.1 million online retailers in the United States alone ([20]). Further, websites and APIs differ greatly in scope (e.g., number of users), data quality (e.g., consistency), and retrievability (e.g., extraction limits). Even within the same product category, data sources differ vastly. For example, Amazon reports a book's sales rank (an aggregate outcome metric for product sales), whereas Goodreads reports users' reading behavior (an individual outcome metric for consumers' usage intensity).Faced with a vast universe of potential sources, researchers may be tempted to focus on familiar platforms only ([89]). For instance, Amazon is the most used web data source in marketing (see Table W4 in Web Appendix C). Amazon might be a relevant source to extract book reviews, given its broad assortment and user base. Yet, in other cases, researchers might miss opportunities for identifying novel, emerging marketing phenomena or conduct more compelling theory testing without a thorough exploration of potential sources. Researchers can avoid the pitfalls of defaulting to dominant sources by actively considering a broad spectrum of websites and APIs, ranging from highly popular (e.g., Amazon) to less popular sources (e.g., Goodreads), from primary data providers (e.g., YouTube) to data aggregators (e.g., Social Blade), and from platforms with global reach (e.g., Twitter) to more regional ones (e.g., Taringa!). Another strategy to move beyond familiar sources is to adopt alternative perspectives. For instance, researchers can consider websites or APIs that are used by consumers, analysts, or managers. API directories at GitHub or programmableweb.com can facilitate identifying potentially relevant APIs.A broad exploration of web data sources may lead researchers to discover sources that may be more permissive for (academic) data extraction or less likely to trigger ethical concerns. For example, websites that do not require logging into the site to reveal information are typically more scraping-friendly than sites that first require registering a user account. In the case of Amazon, researchers can obtain most information without logging in and do not have to explicitly provide their agreement to the website's Terms of Service. To reduce legal (e.g., breaches of contract, as researchers have provided explicit agreements to the terms of service) and ethical (e.g., website users may consider their data private) risks, researchers should refrain from creating fake accounts to access information requiring a login. By explicitly declaring their academic status (e.g., when registering at the site using the institutional email address), researchers might be able to diminish their exposure to legal risk.When exploring web sources, researchers need to examine whether theoretical constructs can be operationalized in a valid manner ([91]). A healthy level of skepticism is warranted when using idiosyncratic metrics from APIs or websites. For example, researchers might be interested in scraping the price tier of restaurants from Yelp. Yet, it is not entirely clear how Yelp computes this metric from individual consumer ratings.To determine when to stop exploring sources, researchers need to assess to what extent the selected source(s) improve(s) on alternatives. One way to justify selecting a single web source is the presence of unique features. For example, a researcher studying how observers react to humor in reviews might prefer Yelp to alternative platforms as it is the only source featuring ""funny"" votes (e.g., [60]). At other times, researchers may be indifferent between potential sources and can draw from multiple sources to boost the generalizability of their findings (e.g., tweets and restaurant reviews; [61]). Collecting data from multiple sources is often useful because even similar types of information (e.g., consumer comments) may affect marketing outcomes differently, depending on source characteristics (e.g., forums vs. microblogs; [72]). Data aggregators—some of which offer authorized data access via APIs—facilitate the collection of such multisource data. Considering Alternatives to Web Scraping (Challenge #1.2)The popularity of web scraping may lead to the conclusion that it should be preferred over other methods. However, some web sources offer access to data via APIs ([12]). In general, extracting data via APIs is more scalable and less likely to invoke the same level of legal risks compared with web scraping. Although some sources offer unconstrained APIs that do not require authentication, others require (paid) subscriptions and authentication procedures. Some sources, such as Twitter, have recently started offering APIs for academic research. In the case of Amazon, an API offering access to consumer reviews is currently not available.In addition to APIs, numerous other options exist for researchers to obtain web data. For example, some data providers (e.g., Yelp, IMDb), public data platforms (e.g., Kaggle, The Dataverse Project), and researchers (e.g., [59]) provide documented web data sets that can readily be used for academic research. There are many potential use cases of such data sets, but less than 5% of all web data–based articles in marketing used such data sets. To avoid the pitfall of defaulting to web scraping for data extraction, researchers can expand their search by explicitly including terms such as ""API"" or ""data set download"" in their search queries. Mapping the Data Context (Challenge #1.3)Relative to other frequently used archival sources in marketing, web data entail large and often undocumented complexities. Thus, it is critical that researchers map the data context, which involves identifying relevant contextual developments that may undermine the validity of the research if gone unnoticed.First, mapping the data context may reveal changes in the underlying data structure. For example, a major change in a platform's user interface may affect subsequent consumer behavior. Second, mapping the data context enables researchers to identify relevant pieces of information for collection together with the focal web data. For example, researchers may discover an external website (e.g., Statista) that offers information about the composition of a focal data source's user base. If stored, such data could eventually be used to detect changes in the composition of the user base or verify the representativeness of the extracted data. Third, mapping the data context may reveal unknown information, potentially allowing researchers to discover novel research opportunities. For example, researchers may use the (unexpected) launch of a new recommendation system at a music streaming service as a natural experiment to investigate the impact of recommendations on music consumption.To understand and map the contextual complexity of web data, researchers can immerse themselves in the ecosystem surrounding the focal source by signing up and using the source, tracking press releases, social media, and scanning the competitive environment. Helpful tools include a search engine's advanced search features, newsletters, and alerts on leading business and technology magazines (e.g., TechCrunch.com, WSJ.com, FT.com). The website's source code may also hold valuable information about potentially relevant environmental changes. Sometimes, researchers may also detect the presence of algorithms on the site that may threaten the validity of the collected data. For example, Amazon's product pages personalize information based on which preceding products were viewed—even without users logging into the site. Designing the Data CollectionAfter narrowing down potential sources, researchers decide which information to extract from them (challenge #2.1), how to sample (challenge #2.2), at which frequency to extract the information (challenge #2.3), and how to process the information during the collection (challenge #2.4). Table 3 summarizes these challenges and corresponding solutions. Which Information to Extract? (Challenge #2.1)In the absence of any ""downloadable"" data set, the first challenge lies in deciding which information to extract from a source. Researchers begin by browsing the web page to identify from which pages to extract which information. In our Amazon example, some of the most commonly used pages are product pages (e.g., [14]) and review pages (e.g., [84]). Generally, pages such as those from e-commerce platforms contain information from the company's database, offering researchers the opportunity to capture some of the information available at a company. Collecting such data involves iterating through a set of related pages (i.e., browsing through many product pages and corresponding review pages in our Amazon example) and saving the data as it becomes visible.As the goal of websites like Amazon is rarely the provision of data sets for academic research, it is often necessary to combine information from different pages (e.g., book descriptions from the product page and ratings from the review page). It is particularly difficult to recognize the subtleties of available information, which makes the decision from which pages to extract information challenging. For example, researchers interested in building a data set of book reviews would find total ratings both on the product and review page, but only the review page reveals all product reviews.[ 7] Yet, neither the product nor the review pages contain all the biographical information available on a reviewer's profile page. Widely exploring a website or API is necessary for identifying information relevant for subsequent analysis (e.g., construct operationalization). The amount and type of information also often vary (e.g., depending on screen width or whether the user is logged in). In this phase, researchers should assess the degree to which the information could be considered personal or sensitive under different regulatory regimes (e.g., the European Union's General Data Protection Regulation [GDPR]), which may require planning measures such as pseudo-anonymization of reviewer names. Researchers may also reassess whether all information needs to be captured to meet the research objective. Suppose reviewer names are strictly necessary (e.g., because they allow for matching different sources). In that case, researchers can explore whether the targeted web data source offers ways to exclude subjects governed by prohibitive privacy regulations (e.g., by using filters).An important threat to internal validity in any study involving web data is algorithmic interference (e.g., [91]). The (visual) design of websites that facilitates usability can undermine the validity of the collected data if gone unnoticed and unaddressed. Especially when deciding which information to extract, it is important to reexamine the website or API for the presence of algorithms. For example, the order in which the researchers in our example visited the website while designing their data extraction could affect which related books are displayed on the product pages on Amazon. Other algorithms that often affect the display of data on websites are sorting algorithms (e.g., by popularity or mixed with sponsored search results) and filtering algorithms (e.g., showing subsets of the data). Algorithmic interference is often hard to detect without being sensitive to it. To account for potential algorithmic interference, the researcher might extract variables as part of an algorithm's more extensive set of input variables, which offers opportunities to control for them in the empirical analysis (e.g., the order in which books were extracted in our Amazon example).Researchers also need to establish the intertemporal stability of available information. Because the web is constantly evolving, the information on a page might not have been generated via the same process over time, undermining the internal validity of the data. Some changes to sources are drastic enough to alter how the data were created in the first place, introducing measurement error ([87]). For example, Amazon shifted to a positive-only evaluation of reviews by removing the ""not helpful"" vote button in 2018, and it no longer displays ""not helpful"" counts next to reviews ([28]). This change might have impacted review content (e.g., users writing shorter review texts). Yet, researchers collecting data today cannot find any traces of these ""not helpful"" votes. A tool for examining changes to relevant information on a website is the Wayback Machine (archive.org). Researchers can use this tool to retroactively inspect websites over time (e.g., [58]) or submit their own website links for archiving.Finally, collecting metadata that ""annotates"" the data collection enhances internal and external validity (e.g., storing the timestamp of data extraction, whether an API request was completed successfully, or the IP address from which the data request was made). Such metadata can be used not only for diagnostic purposes but also to link the extracted web data to other data sets. For example, in our Amazon example, the collected data could be linked to other data using IP-based geolocations (e.g., linking geolocation and web search data; [86]) or timestamps (e.g., linking reviews to stock prices; [78]). How to Sample? (Challenge #2.2)A second challenge in designing the data extraction lies in deciding how to sample from the data source. In particular, in the absence of access to the data source's entire database, it is difficult or impossible to draw a random sample from the population (e.g., all products) available at the data source. Instead, researchers need to devise their own sampling frame to reveal the units they want to sample from the website ([66]). For example, researchers could scan the site for an index of all products that could inform their sampling. In our example studying reviews at Amazon, multiple such indexes may be available. Should products be sampled from the bestseller page for books (so-called exposure-based populations; [66]) or instead from the category page for books (i.e., availability-based populations)? Choices like this result in different data and may even invalidate inferences, as sampling frames might inadvertently induce systematic bias ([39]).One common validity challenge in choosing how to sample is determining how many units (e.g., books) are sufficient to inform the research question. From a validity standpoint, it would be ideal to collect information on the entire population (e.g., all books available at Amazon). However, Amazon does not have an obvious page to extract all books. Imagine that a research team wanted to collect information about all marketing books sold on Amazon. The bestseller page, for example, lists only the top 100 bestsellers. By manually changing pagination parameters in the URL, the top 400 bestsellers can be revealed. Yet, this list of 400 books neither constitutes the entire population nor represents a random sample of marketing books sold at Amazon. Alternatively, when starting from the product overview pages, these pages list an imprecise number of books (e.g., ""over 60,000""), which can only be viewed up to page 50. With each result page featuring 24 organic search results, this approach would produce 1,600 books per category at best. Thus, researchers need to consider other ways to identify more books on Amazon, such as searching for books using various keywords. To expand the number of sampled units, researchers could collect data multiple times, use other keywords, or tweak search parameters to reveal more data by requesting narrower subsets from the database (e.g., only books published during a specific month).Even if a list of the population (e.g., all books) could be retrieved, it may be infeasible to extract data within a reasonable time frame. While sample size requirements are mostly concerned with a researcher's inferential goals (e.g., [50]), few articles make the resource constraints that affect collecting web data explicit (e.g., [68]). For example, with web data, a study's sample size critically depends on technical parameters such as the number of computers used for data extraction or the number of pages that need to be visited. We illustrate how to calculate the technically feasible sample size in Web Appendix F, which may effectively complement traditional sample size calculations commonplace in marketing.As a result of these complications, researchers often restrict their sample size. One way to motivate a compelling sampling frame is to use external sources that can be linked to the web data. For instance, the New York Times or Publishers Weekly bestseller lists might be a starting point for sampling books ([14]). An alternative approach focuses on internal data available at the source itself. Researchers may have to allocate substantial time to identify ways to sample from the focal source. Sometimes, starting the data collection from a page unrelated to the focal pages of interest might facilitate collecting a more representative sample (e.g., by reducing geographical biases; [86]). For example, on Amazon, researchers could first sample reviewers and associated demographic information (available at the user profile of reviewers) and subsequently retrieve data on all reviewed products. Similar to how researchers build network data from an initial set of products or users, the sampling units retrieved from an initial set of pages can be considered seeds. In choosing seeds, researchers should be cautious about drawing from vulnerable populations (e.g., minors) or infringing on prohibitive privacy regulations. At Which Frequency to Extract Information? (Challenge #2.3)Web data are nonstatic, as they change often or might disappear altogether. Therefore, researchers need to consider at which frequency to extract information. This decision encompasses whether to collect data once or multiple times and when to run (and potentially schedule) the data extraction. Consideration of the frequency and schedule is challenging but required to ensure the intertemporal stability of measurement, which is critical for internal and construct validity.From a technical and legal perspective, it is most desirable to extract data only once. Single extractions are less likely to represent a burden on the firm's servers, and the extracted data often only represent a limited snapshot of the entire database, reducing the risks of copyright infringement. Further, such data may be more likely to respect users' ""right to be forgotten,"" which is part of the privacy laws in some jurisdictions. Yet, single data extraction might raise several validity issues that can easily go unnoticed. For instance, in our example, researchers extracting book reviews once from Amazon will not be able to identify whether any of the archival information has changed. Only when extracting data multiple times can researchers systematically notice changes on the site, which may lead to the identification of ""fake"" reviews that have been removed by the platform (e.g., [29]). More generally, researchers can compare information over time to detect whether data that initially appeared to be archival is truly archival (i.e., does not change over time).Another concern is that a single extraction may not produce a data set that adequately maps onto the focal processes of interest. For example, suppose researchers in our example want to examine whether a review by a so-called ""Top 1000 Reviewer"" leads to more subsequent reviews from other users. However, the researcher merely observes that the reviewer is a top reviewer at the time of data extraction. This does not necessarily imply that this user had the same status when the review was first posted and thus was most likely to affect subsequent reviewing behavior of other users. Formulating and testing the essential assumptions about the data, including the relation between the time of data extraction and the focal (psychological) processes, is thus critical. The formulation of such assumptions is called a ""data source theory"" ([52]). Testing and refining the data source theory helps take proactive steps to enhance internal and construct validity. In the preceding example, it would thus be necessary to collect data from these review pages closer to the original posting date, ensuring that reviewers classified as ""Top Reviewers"" had that status when their reviews became visible.When extracting data more than once, automatic scheduling can help ensure consistency and contribute to validity. Scheduling is beneficial if the required information is only available in real-time. For example, sales ranks at Amazon are updated hourly for popular products, and historical sales ranks cannot be retrieved. Suppose researchers in our example were interested in studying the sales performance of books over time. In that case, they could repeatedly extract the books' sales ranks from the product pages at Amazon. Sometimes fixed intervals enhance validity (e.g., every Monday, 8 a.m.). In other circumstances (e.g., when collecting data from many pages), it may be better to vary the starting time or weekday of the data extraction.Another decision is whether to set an end date for the data extraction. Collecting data over extended periods offers the potential for researchers to build a programmatic stream of research and stumble into unexpected natural experiments (e.g., [13]). Especially for longitudinal data collections, continuing the data collection while the project is in the review process brings numerous benefits, such as the ability to update the data (e.g., a longer time frame, new measures). Yet, concerns about technical feasibility (e.g., storage requirements, continued availability of data source) grow as the data extraction horizon extends. Similarly, from an ethical perspective, the longer the data extraction, the greater the likelihood of potentially identifying individuals via triangulation. Next to ethics, long-term data collection also places a heavier load on servers, potentially increasing exposure to legal risks. How to Process the Information During the Extraction? (Challenge #2.4)As a final step in designing the data extraction, researchers decide how to process the information while it is collected. Any kind of web data collection requires a minimal degree of processing, given that the information is available in a computer's memory (e.g., in the browser or the software processing the API output) and still needs to be stored in files or databases. Thus, this processing step occurs before data sets are cleaned or analyzed.When deciding on how to process information during the extraction, researchers must balance potential efficiency gains from molding raw web data into readily usable data sets with the potential threats to validity due to ""on-the-fly"" processing. For example, in our Amazon example, researchers may be tempted to remove seemingly unnecessary information (e.g., image links in reviews), apply text processing (e.g., removing characters used as separators), or force specific information (e.g., prices) to be stored in a strictly numeric format. Such on-the-fly processing promises to produce essential efficiency gains, as the data set resulting from the extraction could directly be analyzed. However, because on-the-fly processing decisions are usually made after the inspection of only a limited number of pages in early prototypes of the data collection, it is difficult to guarantee their correctness. For example, using our example, what if the initial screening revealed only pictures posted in a review, while the extensive data collection revealed the need to capture video files? Given this and related challenges, keeping the raw data (such as the source code of websites, API output, or any media files loaded at the time of data extraction) is ideal from a validity perspective. For example, even if the data collection breaks, researchers could still process and use the information after debugging their extraction code. Retaining the raw data can also help reduce Type 1 errors by increasing transparency about researchers' degree of freedom in collecting and processing the web data. Yet, retaining the raw data prompts significant concerns about the technical feasibility and ethical risks. From a technical standpoint, storing the raw data might require databases to retain their original structure and facilitate processing, especially for projects involving many raw data files collected over extended periods. Keeping all raw data might raise questions regarding the right to store the raw data—especially if it is not (pseudo-) anonymized before storage.Finally, retaining the raw data allows researchers to refine their extraction design at later project stages. For example, a researcher might have collected Amazon reviews in 2018—around the time of the removal of the ""not helpful"" voting feature. Although extracting ""not helpful"" votes was not part of the original extraction design, researchers would be able to use the raw web data to examine the effect of the removal of these ""not helpful"" votes. Collecting the DataAfter source selection and designing the data collection, researchers gradually transition to turning their small-scale prototype into stable extraction software. In so doing, researchers face three challenges. First, researchers may need to improve the performance of their extraction software when operating it automatically at scale (challenge #3.1). Second, they may need to implement monitoring checks to be alerted to any issues arising during extended data collections (challenge #3.2). Third, researchers should compile information important for documenting the final data set (challenge #3.3). Table 4 contains a summary of solutions and best practices to these challenges. How to Improve the Performance of the Data Extraction? (Challenge #3.1)In scaling up their data extraction, researchers may notice that the extraction software frequently breaks across a larger number of pages or runs significantly slower than expected. Such technical challenges, if unaddressed, have the potential to undermine research validity (e.g., missing data, not meeting sample size requirements). A practical solution to preempt these and similar challenges involves capturing the focal information in different ways and storing raw data—especially in the early stages of data collection and for more ambitious, large-scale web data collection projects. To track whether the extraction targets are met, researchers can log the (timestamped) URLs of scraped pages and visualize the performance of the extraction software over an extended period. The resulting ""effective"" extraction frequency can then be used in recomputing the technically feasible sample size (see Web Appendix F). Novel web scraping services promise to handle technical difficulties efficiently (e.g., ScrapingBee, Zyte). How to Monitor Data Quality During the Extraction? (Challenge #3.2)As a next step, researchers consider which metadata can help them diagnose issues with the data collection in real-time. Especially when websites constantly change, monitoring the health of web scrapers can be a tedious task. Researchers should consider performance at a higher level (e.g., the file sizes of extracted raw data) and lower level (i.e., the accuracy of the information in resulting data files) to assess whether the collection is proceeding as expected. When collecting over long periods, automatic reporting can greatly facilitate monitoring. Finally, alerts (e.g., via email or mobile) can help researchers detect predefined data issues quickly. How to Document the Data During and After the Extraction? (Challenge #3.3)During the data extraction, researchers need to record relevant information about the data in real-time. This is an essential step in building documentation, enabling future data usage by the researcher(s) who collected the data and other scholars. Even after the data extraction has ended, researchers can continuously refine the documentation as they become familiar with the characteristics of the data (e.g., variables that were erroneously captured, missing values).Accurate and comprehensive documentation is particularly critical given that collecting web data tends to involve repeated iterations between discovery (and often troubleshooting) and confirmation (i.e., subsequent analyses that are outside the scope of our framework). Designing web data extractions requires a different mindset compared with experiments or archival research. Unlike running experiments, the extraction design for collecting web data may be in flux, even when the collection is already running. Relative to traditional archival research in which data sets are sufficiently annotated, researchers are in charge of accurately recalling details about the data collection. Such details encompass information about the data composition (e.g., sampled units), extraction process (e.g., annotated code, detected errors during the collection), and processing details (e.g., applied cleaning steps). The template of [24] provides a useful starting point for building the documentation for a data set collected via web scraping or APIs. Given that contextual changes are inevitable (see challenge #1.3), documenting the source's institutional background (e.g., screenshots, corporate blog posts, API documentation) is crucial. Future Research Opportunities with Web DataAn unprecedented gold rush of web data has enriched the marketing discipline for two decades—over 300 published articles provide countless examples of impactful marketing insights using web data. With the ever-increasing digitization of social and commercial life, it is hard to imagine that the heyday of this gold rush might subside any time soon. Yet, are marketing's currently productive mines the only or the most promising sources of marketing insights in the future? Which novel approaches and technologies are necessary to capture and describe evolving marketplace realities?To identify directions for future research, we have reviewed more than 300 articles to provide a snapshot of the current state of web data in marketing. We use these insights to inform the subsequent discussion, which we organize along the four pathways through which web data can advance marketing thought (as summarized in Table 1). We supplement our discussion with key elements from our methodological framework (see Figure 2) and inspiring use cases from other disciplines. Direction 1: Identify New Web Data SourcesNext, we discuss how researchers can use source selection to branch out to new or underutilized sources for studying emerging substantive topics. We also highlight how researchers can design more complex, longitudinal, and multisource web data sets to reveal otherwise invisible phenomena. Draw from underutilized sourcesOur review reveals that marketing research draws from a somewhat concentrated list of web sources (see Table W4 in Web Appendix C). We encourage researchers to focus on underused or niche sources that have received limited or no attention in marketing. Web data are often prized, as they allow for collecting ""consequential dependent variables from the 'real world'"" ([40], p. 357). Identifying new sources or novel consequential variables constitutes a promising avenue for discovering emerging phenomena.Consider, for example, the twilight state of the nascent legal cannabis industry in the United States. While more states are legalizing cannabis for medical and recreational use, the market value of the legal U.S. cannabis industry was still less than a third of the illegal market in 2020 (i.e., $20 billion vs. $66 billion; [22]). Using surveys, media coverage, and in-depth interviews, marketing scholars have begun to explore how such legalized markets emerge and seek legitimacy ([38]). Sociologists and organizational scholars, in turn, have already used web data to compile intriguing data sets from sources such as Weedmaps. Using these data, they examine, for example, how existing medical cannabis dispensaries have repositioned themselves after the entry of recreational dispensaries ([34]) or how consumers deal with potential stigma transfer (Khessina, Reis, and Cameron Verhaal 2021). By leveraging similar web data, marketing researchers could explore intriguing marketing questions. For instance, how should brands position themselves (e.g., brand personalities, emphasis on product vs. service), depending on the strength of categorical stigma? What are the potential public health and welfare implications of the increasing competition among cannabis dispensaries or their growing social media activities?In addition to being attuned to work in other disciplines, a low-tech route for source exploration is provided by Similarweb, which allows researchers to browse website rankings by region or category. Given the broad accessibility of web sources worldwide, the dominance of Northern American and European data sources is surprising. Not a single article focuses exclusively on African web sources, and only a handful of articles use some African data (e.g., [49]). Possible starting points for branching out into these underexplored marketplaces could be popular websites such as Nairaland.com (online community), bidorbuy.co.za (auction platform), and Jumia.com.ng (e-commerce). Build unique and rich data sets by drawing from multiple sourcesMost published marketing articles use web data gathered from a single source. Only very few articles collect data from a large number of web sources (i.e., 50 or more web sources). Following the lead of these articles, we encourage marketing researchers to envision unique data sets compiled from many and diverse sources. For example, in economics, [10] collected online and offline prices for individual goods sold by 56 large multichannel retailers in ten countries (i.e., United States, United Kingdom, Argentina, Australia, Brazil, Canada, China, Germany, Japan, and South Africa) between 2014 and 2016. This ""Billion Prices Project"" (bpp.mit.edu; [11]) exemplifies how creative and ambitious data collection from diverse web sources can fuel entire research programs. Especially if sufficiently documented, such web data are poised to unearth new fields of gold for the marketing discipline. Rediscover frequently used sourcesAs researchers decide which information to extract (see challenge #2.1), they may overlook novel information on sources they already know. Therefore, refocusing on different information may also reveal how to study novel phenomena on frequently used sources. Adopting a ""discovery mode"" may reveal that phenomena of high societal relevance such as gender or racial issues are occurring at frequently used sources such as TripAdvisor ([69]), Kickstarter ([92]), and DonorsChoose ([ 2]). For example, in entrepreneurship, [92] scraped Kickstarter information to examine whether male African American founders are less successful in crowdfunding. Researchers in marketing, in turn, could build on these and similar ideas to explore whether biases exist in other online market exchanges. Alter the extraction frequencyAnother promising lever for exploring emerging phenomena is the extraction frequency (challenge #2.3). In most articles, the data were extracted once (e.g., on a single occasion). Extracting data once is sufficient for many research objectives, such as demonstrating the prevalence of a phenomenon in the marketplace (e.g., [79]). Yet, researchers can also uncover novel marketing phenomena by creatively envisioning web data sets that only reveal the phenomenon if the information is extracted multiple times. For example, [29] leverage the observation that Amazon removed certain reviews to study the market for ""fake"" reviews. Specifically, they combine repeatedly web-scraped data from Amazon with hand-coded data from large private groups on Facebook used to solicit fake reviews to examine the short- and long-term impact of such rating manipulations. This example illustrates that data imperfections (e.g., data modifications discovered when mapping the data context, see challenge #1.3) can be opportunities to pose novel research questions rather than merely nuisances that warrant correction. Direction 2: Harvest the Versatility of Web Data to Boost Ecological ValueAs a second direction for knowledge discovery, web data are often used to increase the ecological value of marketing research by complementing carefully controlled experiments. Triangulating findings generated via different methods is fruitful. Yet, there are many other underutilized avenues for how researchers can select and extract web data to infuse ecological validity into experiments and other types of marketing studies. Infuse ecological validity into experimental stimuliBy carefully selecting websites and APIs, researchers can enhance the ecological validity of their experiments (e.g., through more realistic or diverse stimuli and measures). This enormous potential has hardly been realized in marketing, particularly at scale (for a creative smaller-scale application, see [62]). Social psychologists demonstrate the full potential of such an approach. Consider, for example, [33], who scraped 87 real-world profiles of doctors (including their fitness habits) from the website of a health insurance provider. These profiles served as the foundation for a novel stimulus-sampling paradigm wherein participants in experiments were presented with randomly selected subsets (i.e., five fitness-focused and five non-fitness-focused profiles). In doing so, the authors first ground the phenomenon in the field (i.e., that doctors signal their fitness habits) and then use stimuli created from real profiles to demonstrate that overweight and obese individuals are less likely to choose fitness-focused doctors for their own care. Such triangulation and the creation of larger and more representative samples of naturalistic stimuli enhance the replicability and generalizability of experimental effects ([42]). The experimental paradigms in core marketing topics (e.g., branding, advertising, pricing) and methods (e.g., lab experiments, conjoint studies) could benefit from similar applications to mimic real marketplaces. For instance, branding or advertising researchers might develop stimuli based on data extracted from sources like crowdfunding platforms or Bing's Image Search API (e.g., brand logos, ads, and slogans). Run self-administered field experiments via APIsWhile field experiments continue to be prized for their realism and high ecological value ([83]), very few published marketing articles use APIs to run field experiments (e.g., [51]; [80]). There are many untapped opportunities to run field experiments administered by researchers rather than cooperating partners (e.g., firms or charities). Using APIs to run field experiments gives researchers more control over the design and debriefing processes and allows for monitoring of granular participant behavior over longer periods. Thus, web data–based field experiments potentially produce more precise effect sizes and allow researchers to capture long-term effects ([26]). In such experiments, researchers might randomly assign users to different treatments, such as adding (vs. not adding) followers on Twitter ([80]) or assigning (vs. not assigning) Reddit's Gold Awards to user posts ([ 9]). By gathering high-frequency data via APIs, researchers can analyze how experimental treatments influence outcomes such as posting or the creativity of user-generated content. Alternatively, APIs can be leveraged to infuse realism into experiments, as embodied in [56], who developed ""Hoogle,"" a mock search engine that relies on APIs offered by Google but only displays organic search results that are not altered based on previous user queries. We foresee many more creative future applications of web data to facilitate such field experimentation. Direction 3: Adopt New Metrics and Methods for Generating Marketing InsightsA core topic in marketing research is to develop marketing metrics that can guide managerial decision making. Traditionally, many metrics have been based on offline information and established data providers (e.g., [21]). Given the continued growth and diversification of web data, it is tempting for marketing managers to focus more on web data for managing firm growth and profitability. Yet, deciding which information to select and extract for marketing insight is challenging (see challenge #2.1). More research is needed to help managers avoid succumbing to the streetlight effect (i.e., an ""overreliance on readily available data due to ease of measurement and application, irrespective of their growth objective""; [18], pp. 164–65). But, how can researchers get started? Explore which web sources provide cheaper, faster, or better marketing metricsOver the last decade, scholars have begun to explore which types of web data could proxy or improve on existing core marketing metrics. For example, managers may use search data extracted from Google to spot trends in the relative importance of their firm's product attributes, which is more cost effective than traditional methods ([17]). Mining Twitter data provides cheaper, real-time, and more actionable measures and insights about brand reputation than existing survey-based metrics like the Brand Asset Valuator data from the advertising agency VMLY&R ([71]). Yet, in other circumstances, readily and cheaply available web data might not be a good substitute for more expensive or established proprietary data sources to uncover market structure ([70]).An exciting direction for future research is to explore what web data sources should be selected or combined to generate marketing insights that fuel firm growth. For instance, many novel metrics rely on textual data ([ 6]). This focus limits applications to markets using the same language employed by the original method (i.e., mostly English). Future research could explore what other types of web data might enable the creation of metrics and insights that allow real-time monitoring and managing diverse global markets. What insights can managers draw from differences and commonalities between the volume of different kinds of internet searches available via Google Trends (e.g., web search vs. image search vs. Google Shopping vs. YouTube search)? Alternatively, what insights about consumer preferences (or any other stakeholder) can be extracted from short videos posted on platforms such as TikTok? Operate API-based microservicesA fascinating opportunity arises from providing microservices via APIs to marketing stakeholders. This means that researchers not only use APIs to retrieve data but can also operate their own APIs to examine real marketplaces (e.g., using rplumber.io in R). Researchers in data science, for example, offer firms a framework for testing multiarmed bandit policies via APIs while at the same time gathering field experimental data ([48]). Marketing researchers could use similar API-powered microservices to study emerging topics such as recommendation systems (and resulting biases) or tap into a firm's customer relationship management system to validate new customer churn models. At a small scale, researcher-powered APIs could lower the entry barriers for firms to experiment with novel algorithms that have not yet been implemented in major software packages.The provision of APIs provides access to novel types of data, while also increasing the timeliness and ecological value of such data. For example, consider the differences between web data collected by a web scraper and the underlying clickstream data stored in the company's database. The website may merely show aggregate statistics about the number of reviews posted. At the same time, the underlying clickstream data also feature information on every website visit (e.g., time, IP address). As with self-administered APIs, researchers define which information a company should submit (e.g., as input to a recommendation algorithm). Thus, researchers can gain access to unique firm data that are otherwise difficult to obtain. For example, large-scale studies with image and video data are still scarce in marketing. Offering image and video analysis as microservices may generate knowledge discovery for new image sources, such as GIFs used in social media (e.g., Giphy). Direction 4: Exploit Efficiency Gains to Improve MeasurementWeb data also have advanced marketing by improving measurement by efficiently collecting diverse variables. Therefore, as a fourth direction, we discuss how web data can improve measurement across the discipline, particularly by rejuvenating interest in core marketing topics (e.g., market orientation, advertising; for an overview of these topics, see [41]). Relatedly, researchers can also leverage APIs to effectively integrate algorithms for processing unstructured data at scale into empirical analyses ([88]). Leverage web sources to describe diverse online and offline behaviorsMost marketing articles gather web data to describe and examine behavior occurring online. As documented in Table W4 in Web Appendix C, many of the used sources in marketing are focused on online consumer behaviors, such as e-commerce websites (e.g., Amazon), online reviewing platforms (e.g., Yelp), social media sites (e.g., Twitter), and search engines (e.g., Google Trends). Relatively less research has focused on firm behavior online. Yet, by doing so, researchers could explore many core marketing constructs (e.g., service orientation, sustainability). For example, researchers could systematically collect information available on the websites of many firms to analyze which organizational factors influence how firms signal their service orientation (e.g., employees' digital presence; [30]) or environmental credentials (e.g., the B Corporations certification; [25]) to customers and other stakeholders.We encourage marketing researchers who have not yet used web data in their research to consider websites and APIs as valuable, rich, and timely sources to exploit the increased digitization of all forms of behaviors—not only online behavior. A recent example of bringing web data into an established ""offline"" research stream is [32], who scraped the annual reports of more than 8,000 firms from AnnualReports.com between 1998 and 2016. Web sources contain historical information about periods, even long before the web in its current form existed (e.g., 1998 in this case). The authors subsequently use these reports to develop a novel text-based measure of marketing excellence derived from firm letters to shareholders. Many other untapped online sources (e.g., job posting platforms) offer new insights into how firms communicate their marketing capabilities to external stakeholders beyond consumers, such as prospective employees, social activists, and investors.Particularly for the marketing–finance interface, the web features many understudied forms of investor-facing communication that are ripe for collection at scale. For example, which type of marketing topics besides marketing excellence (e.g., marketing capabilities, brand positioning, pricing) should top management emphasize to investors to increase firm valuation during investor relations presentations, investor days, or earnings calls? Researchers could also examine the relative importance of the content versus the delivery (e.g., the tone of the speaker on the recording of an investor day; see [85]). Such multimodal data can also benefit the inferences made in established research streams. Embrace APIs for better measurementAPIs offer many opportunities for improving measurement—some of which are unexpected. For example, consumer researchers planning to run longitudinal studies might consider APIs for automating processes for managing participants at scale, thereby reducing the operating costs (and potentially boosting sample size). The Amazon Mechanical Turk API and the various Prolific Academic APIs (e.g., Study API) are good starting points for running multiwave studies.APIs also enable much more than just retrieving data. For example, to reduce validity concerns in long-term data collection, researchers can use the Pushover API (https://pushover.net/api) to send monitoring alerts to their smartphones. The API of Amazon Web Services allows for the orchestration of virtual computing infrastructure (e.g., to capture data from different countries). Another fruitful avenue in which APIs are currently underused in marketing is facilitating stimuli selection. For example, a classic area of inquiry in marketing is how (background) music affects product and brand perceptions and choices (e.g., [ 8]). In 2022, background music is quite different (e.g., self-chosen, more diverse use cases). Researchers could use the Spotify Web API to select stimuli from millions of mood, sleep, or study playlists, thereby discovering perfect ""lookalikes"" that only differ on one focal attribute (e.g., tempo) but not on other acoustic attributes available at the API (e.g., valence, loudness). Even in this simple example, there might be a substantive interest in better understanding the effect of new background music on consumption choices, especially given the shift to working and studying from home. Concluding ThoughtsWeb data have unearthed many fields of gold in marketing. However, extracting data for generating relevant and valid research insights is challenging. Our article highlights validity concerns that require the joint consideration of idiosyncratic technical, legal, and ethical questions. We introduce a novel methodological framework (Figure 2), offer practical solutions (Tables 2–4), and outline directions for future research to enable researchers to create impactful and credible marketing knowledge. While our focus is primarily on authors, our work also spotlights crucial validity concerns to scholars reviewing web data–based research and practitioners interested in deriving accurate and actionable marketing insights from web data.We hope that our work encourages marketing scholars to integrate web data into their research programs. While web data often provide compelling answers to the question, ""Assuming that this hypothesis is true, in what ways does it manifest in the world?"" ([ 5], p. 1455), this does not imply that web data are relevant for all research projects. Web data sources tend to feature a large N (i.e., many users) with many V (i.e., different pieces of information for potential variables) observable over a large T (i.e., many observations over extended periods of time and at a very granular level; [ 1]). Yet, collecting web data via web scraping or APIs provides limited information about the browsing behavior of individuals on the website that led to the creation of the data in the first place. Significant synergies exist by enriching clickstream stream data capturing such browsing processes with web data retrieved from web scraping and APIs (e.g., [54]).Our work aims to bridge entrenched training silos (e.g., between quantitative marketing and consumer behavior). We encourage scholars to further integrate and leverage existing best practices with regard to the collection and analysis of web data (e.g., preregistration, addressing endogeneity). There is significant untapped potential for collaborations across methodological traditions to explore and exploit new fields of gold. Collecting valid web data can enable marketing as a discipline to enhance its relevance and assert intellectual leadership on important emerging substantive topics that are also increasingly studied in fields such as computer science, information systems, and management science ([63]).We would be remiss not to mention the nonmonetary costs of collecting web data via web scraping and APIs. While browsing the web is (mainly) free, researchers should not assume that collecting web data is costless. The prototype of a data collection can be ready and running in a matter of hours. Yet, researchers will often find out that the data collection does not work entirely as intended or encounter some of the challenges discussed in our methodological framework. Just like with any other method, the devil is in the details.Web data democratize data access and make our discipline more inclusive for scholars who would otherwise find it difficult to obtain access to data. To further reduce entry barriers, it would be helpful to create incentives (e.g., journal space) for rich web data sets and their documentation, like the Billion Prices Project ([11]). Similarly, authors can make their algorithms or data available for other researchers by sharing code publicly or deploying API-based microservices that can increase their methods' adoption and offer unique opportunities for field experimentation. In summary, web data present a golden opportunity to examine important marketing questions, now and in the future. Supplemental Materialsj-pdf-1-jmx-10.1177_00222429221100750 - Supplemental material for Fields of Gold: Scraping Web Data for Marketing InsightsSupplemental material, sj-pdf-1-jmx-10.1177_00222429221100750 for Fields of Gold: Scraping Web Data for Marketing Insights by Johannes Boegershausen, Hannes Datta, Abhishek Borah and Andrew T. Stephen in Journal of Marketing  "
9,"GMO Labeling Policy and Consumer Choice Most scientists claim that genetically modified organisms (GMOs) in foods are safe for human consumption and offer societal benefits such as better nutritional content. However, many consumers remain skeptical about their safety. Against this backdrop of diverging views, the authors investigate the impact of different GMO labeling policy regimes on the products consumers choose. Guided by the literature on negativity bias, structural alignment theory, and message presentation, and based on findings from four experiments, the authors show that consumer demand for GM foods depends on the labeling regime policy makers adopt. Both absence-focused (""non-GMO"") and presence-focused (""contains GMO"") labeling regimes reduce the market share of GM foods, with the reduction being greater in the latter case. GMO labels reduce the importance consumers place on price and enhance their willingness to pay for non-GM products. Results indicate that specific label design choices policy makers implement (in the form of color and style) also affect consumer responses to GM labeling. Consumer attitudes toward GMOs moderate this effect—consumers with neutral attitudes toward GMOs are influenced most significantly by the label design.Keywords: GMO; food claim; voluntary labeling; mandatory labeling; public policy and marketingAlthough the use of genetically modified (GM) foods has become widespread across the world, scientific and public opinions diverge about their safety. Most scientists agree that GM foods are invaluable because they offer increased nutritional content, a higher yield per acre, and a better shelf life ([51]). They also agree that GM foods are as safe for humans and the environment as non-GM foods ([18]). Yet some scientists disagree ([29]), citing concerns about possible long-term effects of GM foods on human health and the environment ([10]). On the demand side, many consumers question the safety of GM foods and their scientific promise ([30]). Indeed, a 2013 New York Times poll showed that 75% of Americans expressed concern about genetically modified organisms (GMOs) in their food, and most worried about their potential health effects. It is important to note here that the baseline consumer knowledge on this issue is low ([64]). The most extreme opponents of GM foods think they have the most knowledge about the issue, but research shows that their scientific literacy is low ([21]).The opposing views that firms and consumers have about GM foods create a fundamental tension in how such foods should be labeled, which is the central focus of this research. On the one hand, consumers and advocacy groups believe that GM foods are potentially risky; therefore, policy makers must mandate GMO labeling. In a mandatory labeling regime, food manufacturers are required to include labels such as ""contains GMO"" when their foods are GM. The most commonly used argument in support of such labeling is consumers' right to know. On the other hand, food manufacturers rely on scientific evidence to claim that GM foods are as safe as conventionally grown foods. As a result, they argue that mandatory labeling arbitrarily singles out GMO technology for specific attention and misleads consumers into thinking that they should be concerned about consuming GM foods ([60]). Therefore, food manufacturers support a voluntary GMO labeling policy, where firms have the freedom to use a ""non-GMO"" label when appropriate.The discordant views about the safety of GM foods between firms and consumers, as well as the demands for GMO labeling by consumer advocacy groups (e.g., the Non-GMO Project) create a substantial challenge for policy makers in their efforts to develop a GMO labeling policy. As a result, GMO labels vary a great deal around the world (see Figure 1). For example, the United States allows firms to display non-GMO labels on their products if they wish. Brazil, the world's second-largest GM producer after the United States, adopted a mandatory GM label that features a black T inside a yellow triangle. The letter T stands for the Portuguese word transgenicos (""transgenics""), and the symbol resembles a caution sign indicating an upcoming T-junction ([ 6]). Similar logos have been adopted by South American countries such as Bolivia and Uruguay.Graph: Figure 1. GMO labels.In light of the diverse GMO policy regimes that currently exist, an important prerequisite for carefully constructing a GMO labeling policy is a theory-based understanding of whether and how consumers shift their choices under the different GMO labeling regimes. The intention behind a labeling policy that requires the disclosure of a GMO ingredient as a horizontally differentiated attribute is that it simply allows consumers to make choices that reflect their taste differences. However, an externality of such a policy may be that it leads consumers to treat GMO ingredients as a vertically differentiated attribute, signaling that non-GM foods are of a higher quality than GM foods.To investigate the product quality–related implications empirically, we examine how the different GMO labeling policy regimes impact consumers' choice and their willingness to pay for non-GM products as well as the market shares of GM and non-GM products. Guided by the policy question of mandatory versus voluntary labeling for GM foods, we investigate the substantial impact of different GMO policy regimes on choices consumers make. More specifically, the purpose of our research is to answer the following research questions that have significant GMO labeling policy implications. Using the theory-driven terminology adopted by [ 1], in the remainder of the article we refer to the mandatory labeling regime as ""presence-focused"" (contains GMO) and the voluntary labeling regime as ""absence-focused"" (non-GMO). Does the labeling policy (absence-focused vs. presence-focused) affect a consumer's choice of GM products? Does the labeling policy (absence-focused vs. presence-focused) affect other aspects of the consumer's choice process, such as their price sensitivity and willingness to shop in a category? Is the consumer's choice impacted according to whether the GMO related information disclosure is complete (presence-focused and absence-focused) or partial (presence-focused or absence-focused)? Complete GMO information disclosure occurs when policy makers mandate presence-focused labeling and firms that produce non-GM products display an absence-focused label. Partial disclosure occurs when either presence or absence labels are present. Do consumers behave differently depending on the GM label's presentation format (e.g., color, theme)? Which consumers are most likely to alter choices because of the label format?To answer these key policy-related questions, we combine insights from the social psychology literature with rigorous consumer choice models to make novel predictions about the effect of GMO labeling changes on consumers' demand for GM products. We develop our theory based on the literature on negativity bias (e.g., [31]), structural alignment theory (e.g., [25]), and message presentation (e.g., [33]). We use choice experiments grounded in microeconomic theory ([40]) and a hierarchical Bayes model to test our hypotheses.Across four studies involving 3,913 respondents, we study the impact of different GMO labeling regimes on demand for GM products. Our first between-subjects experiment (Study 1) examines whether consumer choice depends on the GMO labeling regime (i.e., no labeling, absence-focused, presence-focused, or both labeling conditions). Study 2 investigates how the GMO labeling regime may impact the importance consumers place on product price and product category. Study 3 shows how the alignability of GMO information, whether partial information or full information is disclosed, affects consumer choice. Finally, Study 4 investigates how the signal used in a GMO label (e.g., color) can impact consumer demand for GM foods and reveals which market segment is most likely to be affected by the signal used.Our findings have substantive implications for two key stakeholder groups: policy makers and food manufacturers. By quantifying the effects of various GMO labeling regimes, we offer policy makers guidance on the impact of each labeling system on consumer demand. Absence-focused policies result in the smallest change in demand for GM products compared with a regime with no GM labels. Presence-focused labeling policies can substantially alter demand for GM products, and the signal contained in the GMO logo (e.g., color) also plays a critical role in consumers' perceptions of GM products. Both policy regimes create incentives for firms to expand their offerings to include more non-GM products for the market segment that prefers such products and is willing to pay more for them. The critical question for policy makers here is whether they wish to incentivize such firm behavior.For food manufacturers, our research reveals that GM labels add an important product feature for consumers to evaluate. Such labels create vertical differentiation for many consumers by signaling that non-GM products are better than GM products. They draw attention away from factors such as price—making it less important—and allow firms to charge a premium for non-GM products. The GM label can also drive some consumers away from a category (e.g., from crackers to another non-GM snack). All of the aforementioned effects are amplified when moving from an absence-focused to presence-focused regime. Background GMOs: What Science SaysThe [63], p. 1) defines GMOs as ""organisms (i.e., plants, animals, or microorganisms) in which the genetic material (DNA) has been altered in a way that does not occur naturally by mating and/or natural recombination."" Proponents of GM crops argue that they increase yield, lower food prices, reduce damage to crops after the harvest, make crops tolerant of stresses such as cold and heat, help fight malnourishment, and reduce reliance on chemical pesticides ([51]). Most scientists claim that there is no substantiated evidence that genetic crop modification makes foods less safe. For example, the National Academies of Sciences and Medicine ([46]) reported that food from GM crops is no more dangerous than food produced by conventional agriculture. More than 150 Nobel laureates in areas such as chemistry, physics, and medicine signed an open letter in 2016 to endorse the safety of GM foods, noting that ""opposition based on emotion and dogma contradicted by data must be stopped"" (Support Precision Agriculture 2016).Although the dominant view among scientific organizations is that GMOs do not harm human health, this view is not ubiquitous. In one review article, [36] noted that a group of scientists believe that each GM product should be tested over long periods for possible side effects. The author reviewed 26 animal feeding studies that identified adverse effects or animal health uncertainties, leading him to conclude that ""putative consensus about the inherent safety of transgenic crops is premature"" (p. 909). A joint statement by a group of researchers ([29]) claimed that no consensus on GM food safety exists. They indicated that a conflict of interest exists in many reported studies supporting GM food because biotechnology companies often fund this research ([15]). They further noted that no epidemiological studies have examined the effects of GM food consumption on humans. They concluded that it is necessary to test the effect of GM foods on humans and over longer periods. GMOs: What Consumers ThinkSeveral studies have documented consumers' lack of knowledge about GMOs, as noted by [64] in their review. These studies also document an overall negative attitude toward GMOs among consumers. Such negative attitudes could be driven by negative press associated with occurrences such as GM crops causing a decline in monarch butterflies, which a recent article refutes ([ 7]). The primary concerns are that growing and consuming GM crops may cause health problems and allergic reactions.Research has shown that the most extreme opponents of GM foods know the least about GMOs but think that they know the most ([21]). People's misplaced confidence stemming from the mismatch between what they think they know about science and what they actually know ([45]) polarizes attitudes even more ([22]).The controversy around GM foods also relates to the growing literature on science denial ([54]) that identifies social mechanisms as the basis for extreme confidence in beliefs that contradict scientific consensus ([33]). Specifically, many people have insufficient information to establish their own opinions on new technologies and scientific developments ([20]) and instead accept the opinions of people they trust ([54]). Well-known examples of science denial include vaccine safety, global warming and climate change, the rise in antibiotic resistance, and the safety of GM foods. GMOs: What Policy Makers DoGMO labeling policy in the United States was absence-focused when GM foods were first released in 1994. Some food companies use third-party verification, such as the Non-GMO Project (https://www.nongmoproject.org), to highlight the non-GMO aspect of their products. However, various consumer groups and nongovernmental organizations have argued for presence-focused labeling based on consumers' right to know what is in their food. They contend that the potential harm of GM foods needs to be made explicit.Over the years, political pressure to introduce presence-focused GMO labeling in the United States has grown. In July 2016, U.S. Congress passed a bill requiring the U.S. Department of Agriculture to establish a national disclosure standard for GMOs. The new policy has a two-year phase-in period that began in January 2020. The proposed label under this policy has a nature-friendly theme on a green or black-and-white background and uses the term ""bioengineered (BE)."" Dozens of nations around the world have enacted presence-focused GMO labels based on the percentage of GMOs in ingredients or how the seed was developed. The GMO percentage thresholds vary among countries that have regulations. For example, the European Union (EU) and United Kingdom set this limit at.9%, whereas Australia set it at 1% ([31]).Policy makers in many countries are uncertain whether GM foods are safe, and their labeling rules are based on such a perspective. For example, the EU has adopted the precautionary principle (European Commission [19]) for GMO labeling. This principle is often cited in cases of scientific uncertainty and the possibility of irreversible damage. It states that ""where there are threats of serious or irreversible damage, lack of full scientific certainty shall not be used as a reason for postponing cost-effective measures to prevent environmental degradation"" ([49], Principle 15).Not surprisingly, GMO labeling policies are controversial. In a compelling counterargument to EU policies, [59] acknowledged the rationality behind the precautionary principle but cautioned that rigid regulatory controls based on the idea of ""possible risk"" can paralyze progress. He explained that, for GMOs, the precautionary principle results in substitute risks because it interferes with the promise of mitigating hunger and disease in developing countries by using foods such as golden rice, which is bioengineered to be rich in vitamin A.In the United States, debate over the recently adopted logo is intense because it appears to signal the government's positive attitude toward GM foods. In Brazil, opponents of the current mandatory presence-focused GM logos accuse the country's policy makers of scaremongering. In Canada, which currently has no GMO labeling legislation, petitions have been circulated supporting mandatory presence-focused GMO labeling; in a survey of adults living in Canada, 90% of the respondents expressed support for mandatory GM labeling ([56]). PredictionsIn this section, we develop predictions on how GMO labeling affects different aspects of consumer choice, including preference for GM foods, price sensitivity, and product category purchase. We also outline how these predicted effects are moderated by the GMO label format. Figure 2 summarizes our main hypotheses.Graph: Figure 2. Overview of predictions. GMO Labeling and Consumer ChoiceIn an absence-focused labeling policy, manufacturers may use a ""non-GMO"" label when appropriate. In a world in which many consumers have negative attitudes toward GM products, non-GMO producers often use absence-focused claims (e.g., a TV ad for Triscuit[ 5]). Such claims are in line with those that highlight the absence of negatives—namely, no preservatives, no artificial colors, no chemicals, and so on. These nature-based claims that remove a negative strongly affect consumers' inferences about product's taste and healthfulness ([ 1]), even when they are irrelevant to the actual quality. In contrast, in a presence-focused GMO labeling policy, regulators mandate that GM-food products display a ""contains GMO"" label. For many consumers who have negative attitudes toward GM products, this information signals potential risk.The absence- and presence-focused labeling policies outlined thus far may impact consumers' evaluations of GM foods via two separate mechanisms—information valence and information source. With regard to information valence, it is well known that people place greater weight on negative information than positive information. This negativity bias ([50]) is at the core of how consumers may evaluate a GMO label. From an evolutionary perspective, this bias occurs because we have a greater chance of surviving and thriving if we pay greater attention to negative information; negative events are more consequential than positive ones. Some argue that negative information is more informative because it is rarer ([24]), attracting more attention and thus being more ""diagnostic or informative"" ([53]). Previous research has documented negativity bias in a variety of contexts. For example, negative attributes are more diagnostic of product quality than positive ones ([28]), and negative reviews have a stronger effect on purchase decisions ([ 9]).In addition, absence- and presence-focused labeling differs by their information source. In presence-focused GMO labeling, a regulatory body mandates the display of GMO labels, whereas in absence-focused labeling, this decision is voluntary and made by the firm. The perceived credibility of a message's source can affect the recipient's cognitive response ([58]). For trusted information sources, consumers accept a message without undertaking an extensive assessment of its content ([23]). Evidence suggests that consumers trust public sources (e.g., a government) more than private ones (e.g., a firm). For example, [17] found that advertising from a government source (the Federal Trade Commission) is more credible than that from a firm. Similar results have been noted for safety hazard information ([38]), environmental information ([11]), and forest-product certification seals ([48]). Given the differences in information valence and source between the two labeling regimes discussed thus far, we propose the following hypothesis: H1:  Presence-focused GMO labeling makes consumers more sensitive to the GMO attribute when choosing a product than absence-focused GMO labeling. GMO Labeling and Price SensitivityPrevious studies have shown that negative information is more diagnostic and, as a result, attracts more attention ([53]). In line with the argument that attention is a scarce resource, [12] demonstrated that greater attention to a previously unconsidered attribute reduces the relative importance of other attributes. Extending these theoretical findings to our research context, greater attention to GMO ingredients likely diverts consumer attention away from other product-related information, such as price. Negative presence-focused GMO labeling should reduce consumers' focus on price information more compared with absence-focused GMO labeling. Thus, H2:  Presence-focused GMO labeling makes consumers less price sensitive than absence-focused labeling. GMO Labeling and Product Category PurchaseChoice deferral is a means of mitigating the negativity generated in uncertain or difficult choice contexts. Previous research shows that this negative feeling in such contexts increases the likelihood that consumers will defer their decision ([39]). Deferral occurs when no single option dominates a choice set or when consumers face difficult trade-offs between product attributes ([13]).In our context, consider a Brand A, which contains GMOs, and a Brand B, which does not. Assume that a consumer prefers Brand A but prefers non-GMO ingredients. In the absence-focused condition, this consumer will choose between Brand A, not knowing whether it is a GM product, and Brand B, fully aware that it is a non-GM product because of the ""non-GMO"" label. Conversely, in the presence-focused condition, the same consumer will choose between Brand A, fully aware that it is a GM product because of the ""contains GMO"" label and Brand B, with no GMO-related knowledge. The trade-off between brand name and GM ingredients may be more difficult in the presence-focused condition because the consumer has to analyze the costs and benefits of a brand they prefer (Brand A) and an attribute they do not (GMO). This increased task difficulty may enhance the likelihood of choice deferral.[42] show that consumers tend to view a government's default option as an implicitly recommended course of action. In their studies, when the government uses the ""organ donor"" default, most participants inferred that ( 1) the policy makers were willing to be donors and ( 2) people ought to be donors. In the GMO labeling context, a GMO label mandated by a regulatory body may send a negative signal that consumers should avoid a product with GMO ingredients. Growing concerns and perceived risks associated with GMOs could increase customers' uncertainty about brand quality, thus leading to choice deferral. Therefore, H3:  Presence-focused GMO labeling makes consumers less likely to purchase the relevant product category than absence-focused GMO labeling. GMO Labeling Alignment and Consumer ChoiceThe first three hypotheses focus on scenarios where the product alternatives available apply either an absence-focused label or a presence-focused label. However, when a government mandates presence-focused labeling, firms that produce non-GM products may be free to use absence-focused labeling, as is the case in the United States today. Because many consumers view non-GM products favorably, firms offering non-GM products have a strong incentive to include such information on their product packaging to differentiate themselves from firms offering GM products. Therefore, when a mandatory GMO labeling policy is implemented in the marketplace, it is plausible that most—if not all—products will display either GMO or non-GMO labels. We use structural alignment theory ([35]; [55]; [65]) to discuss the impact of partial or complete GMO-related information on consumer choice and how they drive our predictions.Consider the following example involving two marinara sauce brands, A and B. Brand A is sold at $2.00, without providing any information on GMO attributes; Brand B is sold at $2.50 and includes a non-GMO label. In this example, the price is alignable information because the attribute is present in both options. In contrast, under either the absence-focused or presence-focused labeling, GMO-related information is only available for Brand B, making it nonalignable. The structural alignment literature suggests that consumers pay more attention to alignable attributes ([25]) and put greater weight on them ([55]).Consistent with this discussion, when both types of GMO labels are included (i.e., ""non-GMO"" and ""contains GMO""), consumers will give greater weight to the GMO attribute. According to the arguments used previously for H2, giving greater weight to the GMO attribute would ( 1) further reduce the weight consumers give to price information and ( 2) make them more reluctant to purchase a product in the category. Formally, H4:  Compared with a situation where only presence-focused (""contains GMO"") labels are displayed, when both absence-focused (""non-GMO"") and presence-focused (""contains GMO"") labels are displayed, consumers become (a) more sensitive to GMOs, (b) less sensitive to price, and (c) less likely to purchase in the product category. GMO Labeling Format and Consumer ChoiceA regulatory body's choice of GMO label reveals its beliefs or attitudes about GMOs and is, therefore, a critical policy decision—consumers tend to view a government's default option as an implicit recommended course of action ([42]). Moreover, [52] showed that a speaker's description signals their attitude toward an object. For example, if someone likes a team, they describe its successes, and if they do not, they note the team's failures. The descriptions a speaker chooses, even of seemingly equivalent objects, are important for listeners ([41]).As a concrete example involving the color of a GMO label, consumers tend to infer that a product has positive, nature-related attributes when it prominently displays the color green ([61]). Similarly, the color blue signals openness, peace, and tranquility ([43]), whereas yellow signals caution. Such color choices and their associated signals are highly relevant for GMO labeling. We hypothesize that policy makers' choice of a GMO label (e.g., the color green, blue, or yellow) is important as it delivers an implicit recommendation that may influence consumers' choices. H5:  The graphical format of the label determines how much impact the GMO attribute has on consumer choice, including (a) how sensitive consumers are to the GMO attribute, (b) how important price is to consumers, and (c) how likely consumers are to purchase in a given product category.Consumers' prior beliefs about GMOs could also play a role in how much a labeling policy impacts them. Previous research showed that most individuals do not know enough details to establish their own perspectives on new technologies and scientific developments ([20]) and accept the position of others they trust ([54]). As a result, we anticipate that consumers in the middle, who neither like nor dislike GM products, are affected the most by the label format policy makers select. Overview of StudiesWe include four empirical studies. The first study uses a simple between-subjects design to examine the effect of different GMO labeling policies (i.e., absence, presence, or both) on consumer choice. We subsequently conduct three choice-based conjoint studies to test H1–H5. Study 2 focuses on H1, H2, and H3 by disentangling the impact of GMO labeling on different aspects of consumer choice. Study 3 tests H4, focusing on how the findings pertaining to H1–H3 are affected by GMO information disclosure (partial vs. full). Lastly, Study 4 tests H5, focusing on how the different graphical formats of GMO labeling impact our previous findings. Study 1The goal of Study 1 was to demonstrate that different GMO labeling regimes (i.e., no GMO labeling, absence-focused labeling, presence-focused labeling, and both labeling conditions) can lead to systematic differences in demand for GM foods. Procedures and ParticipantsUsing Amazon Mechanical Turk (MTurk), we recruited respondents in exchange for monetary compensation. To begin, we asked the respondents questions to assess whether they shopped in the focal categories (marinara sauce and pickle) and whether they were paying attention to the study instructions. Of the 2,110 respondents who completed this first step of the study, 767 (36.4%) respondents did not qualify to continue because they did not shop in the two focal categories (N = 644; 30.5%) or failed to correctly answer the attention check questions (N = 123; 5.8%). A total of 1,343 respondents (Mage = 41.0 years; female = 62%) qualified to participate in the main study and completed it. We randomly assigned these respondents to one of the four study conditions in a between-subjects design (Ncontrol = 340, Nabsence = 331, Npresence = 335, Nboth = 337).We presented respondents with choice sets in two different product categories (marinara sauce and pickles) and asked them to select their preferred brand. We selected these two product categories because ( 1) they are frequently purchased and ( 2) they complement the less healthy product category (potato chips) that we use in our subsequent studies.In the marinara sauce category, the first two (Prego and Newman's Own) of the three brands included in the study were GM products while the third (Barilla) was a non-GM product. Similarly, in the pickles category, two brands (Claussen and Vlasic) were GM products and the third (Mt. Olive) was a non-GM product. In both categories, we gave respondents in the control condition no information on GMOs. In the absence labeling condition, only the non-GMO label was displayed. In the presence labeling condition, only the GMO label was displayed. Finally, in the both labeling condition, both non-GMO and GMO labels were displayed. Figure 3 shows the four GMO labeling conditions for the pickles category in this study. The stimuli for the marinara sauce category are included in the Web Appendix A (Figure W1).Graph: Figure 3. Stimuli for four labeling conditions (pickles). ResultsFigure 4 shows the share of non-GM and GM products in each category across the four labeling conditions. We found that the GMO labeling regime significantly impacts consumer choice of the brands included. In all three GMO labeling conditions (absence, presence, and both), more participants preferred the non-GM product than in the control condition, where no GMO-related information is displayed. The magnitude of these shifts in demand toward the non-GM product (Barilla or Mt. Olive) depended on the GMO labeling condition.Graph: Figure 4. Non-GMO vs GMO choice share across different labeling conditions.In the marinara sauce category (Figure 4, Panel A), the choice share of non-GM product was lower in the control condition (  π^control   = .171) than in the absence (  π^absence   = .236; z = −2.094, p = .046), presence (  π^presence   = .370; z = −5.825, p < .001), and both labeling conditions (  π^both   = .407; z = −6.780, p < .001). In addition, the non-GM product's choice share was lower in the absence labeling condition (  π^absence   = .236) than in the presence (  π^presence   = .370; z = −3.761, p < .001) and the both (  π^both   = .407; z = −4.730, p =   .001  ) labeling conditions. These results imply that consumer preference for non-GM products increases progressively from control to absence to presence labeling conditions. The difference between the presence and both labeling conditions was not statistically significant (  π^presence   = .370 vs.  π^both   = .407; z = −.984, p =   .374  ).Graph: Figure 5. Example choice tasks in study 2.The findings for the pickles category (Figure 4, Panel B) are largely consistent with those for marinara sauce. The choice share of the non-GM product was lower in the control condition (  π^control   = .229) than in the absence (  π^absence   = .293; z = −1.888, p =   .074  ), presence (  π^presence   = .421; z = −5.327, p < .001), and both (  π^both   = .469; z = −6.552, p < .001) labeling conditions. As in the marinara sauce category, the non-GM product's choice share was lower in the absence labeling condition (  π^absence   = .293) than in the presence (  π^presence   = .421; z = −3.446, p < .001) and both (  π^both   = .469; z = −4.685, p < .001) labeling conditions. Once again, the difference between the presence and both labeling conditions was not statistically significant (  π^presence   = .421 vs.  π^both   = .469; z = −1.252, p =   .241  ).Study 1 supports our central thesis—namely, that the way the GMO message is conveyed to consumers affects their choices. In all GMO labeling regimes, the share of non-GM products is greater than when no GMO-related information is revealed. The share of non-GM products increases progressively from control to absence to presence labeling conditions. Although these findings are interesting on their own, Study 1 raised important follow-up research questions. For example, should we expect GMO labeling to affect the importance of price to consumers and their willingness to pay (WTP) for a non-GM product? Does the manner in which the GMO message is delivered affect the product choice consumers make? Furthermore, Study 1 confounds brand and GM ingredients, as only the Barilla brand carried the GMO-free label and only Prego and Newman's Own displayed the ""contains GMO"" label in the marinara sauce category. To answer our next set of research questions and remove the confounding between brand and GMO ingredients, we conducted three choice-based conjoint experiments, which we report on next. Study 2The purpose of Study 2 was twofold. First, we test H1, H2, and H3, which examine the impact of GMO labeling (absence vs. presence) on consumers' sensitivity to the GMO attribute, price, and category purchase, respectively. Second, we explore the behavioral processes associated with consumer choice under the different GMO labeling regimes. Unlike Study 1, we do not have a confound between brand and GMO ingredients in this study. Procedures and ParticipantsTo introduce Study 2, we used incentive alignment instructions similar to those employed by [27], informing respondents that 25 of them would receive a total value of $5 based on their answers to the survey: either a product of their choice and a Walmart e-gift card for the remaining value, or a $5 Walmart e-gift card. Existing literature shows that incentive-alignment techniques make consumers likely to provide more realistic responses. When a study offers as an incentive a version of the product that is predicted to give consumers the highest utility, respondents put greater effort into the choice task, and their responses are more likely to reflect their actual preferences ([14]; [27]).We informed respondents that the study's purpose was to understand how they evaluated potato chips. We asked the respondents to complete 14 choice tasks. In each choice task, we showed the respondents four brands of potato chips: Lay's, Herr's, Ruffles, and a private label. The price varied by brand. As in the marketplace, we set the prices of the national brands ($2.79, $3.29, $3.79) and the private brand ($1.99, $2.39, $2.79) at different levels. The two GMO label conditions were included or not included on the package. We displayed all four brands in each choice set, varying the GMO ingredients and prices from one scenario to the next in the 14 choice tasks.We used a statistically efficient choice design based on the D-optimality criterion ([37]) for the main-effects model that included three attributes varied orthogonally across choice tasks, which prevented us from testing higher-order interactions. For example, Lay's chips could have a GMO label in one task but not in the next; by design, the brand and GMO label attribute are unconfounded, enabling us to assess consumer preference for each. For more information on conjoint analysis and how it relates to our study, please see Web Appendix B.We adopted a dual-response method, asking respondents to indicate ( 1) which of the product alternatives they prefer and ( 2) whether they would actually buy the product they had just selected ([ 4]; [27]). The dual-response method has the advantage of encouraging respondents to slow down to think through the purchase task, making the no-choice option more likely, which is similar to a real market situation. Next, respondents answered questions about their attitudes toward GMOs, which enabled us to investigate how consumer attitudes vary across the different labeling regimes. We used a nine-point ""disagree/agree"" scale to ask questions regarding attention to the GMO ingredient, risk perception of the GMO ingredient, and decision uncertainty.We randomly assigned respondents to one of the two conditions (absence- vs. presence-focused labeling) in a between-subjects design. These two between-subjects conditions differed by how the GMO label was displayed. In the absence-focused condition, only the non-GMO label was displayed, whereas in the presence-focused condition, only the GMO label was displayed. Figure 5 presents a screenshot of one of the 14 choice tasks for each condition in Study 2.We recruited students from a Midwestern university in the United States, and they participated in the study in exchange for course credit. To begin, we asked the students questions to assess whether they shopped in the focal category (potato chips) and whether they had a dietary restriction (gluten intolerance). Of the 665 students who completed this first step of the study, 55 (8.3%) respondents did not qualify to continue because they did not shop in the focal category (N = 49; 7.4%) or were gluten-intolerant (N = 6;.9%).[ 6] A total of 610 students (Mage = 19.7 years; female = 44.4%) qualified for and completed the main study. We randomly assigned them to one of the two study conditions (Nabsence = 303, Npresence = 307). Results Model-free resultsThe proportion of respondents purchasing a non-GM product was higher in the presence condition (56.38%) than in the absence condition (48.84%). The proportion of respondents who decided not to make a purchase was also higher in the presence condition (37.06%) than in the absence condition (32.77%). The average purchase price was similar in the two conditions ($3.07 in absence vs. $3.05 in presence). Statistical modelThis study focused on the impact of GMO labeling on brand and category choice. Given this, we modeled two decisions—whether to buy and which brand to choose—using a nested framework to model brand choice and purchase incidence ([ 2]; [26]), where the joint probability of a given consumer choosing brand j and buying it (B) is given by Pr(j,B)=Pr(j)×Pr(B|j). Graph( 1)We conceptualized our model at the brand level and assumed that individual h evaluating j (j = 1, ..., J) brands chooses the brand j. Each brand j has a design vector xj that contains discrete variables to indicate the different attribute levels (e.g., GMO) and a continuous variable (e.g., price). The deterministic part of individual h's utility for brand j is linear in the predictor variables (xjβh) and, with a Type I extreme value error structure, yields a multinomial logit model [40]. The probability of individual h choosing brand j is given by Pr(yhj′=1)=exp(x′j′βh)∑Jj=1exp(x′jβh), Graph( 2)where βh is an individual-level parameter vector that includes brand preference and sensitivity to attributes such as GMO and price. To model the buy/no-buy decision, we specified a threshold utility (γh) parameter in the model. The utility of the most attractive alternative (j) in the choice set must exceed γh for the individual to buy it (B = 1). A larger estimated threshold parameter implies a lower probability of buying in the category.We introduced heterogeneity across individuals hierarchically with a random effects specification ([ 4]) as  θh∼N(θ¯,Vθ)  , where  θh={βh,γh}  and  θ¯={β¯,γ¯}  . In our empirical context, the hyperparameter  β¯  includes the average brand preference parameters, GMO sensitivity (  β¯GMO  ), and price sensitivity (  β¯price  ), while  γ¯  is the average threshold for category purchase. For the remaining technical details of the model, refer to Web Appendix C. Model-based inferenceWe employed the Markov chain Monte Carlo (MCMC) method to estimate the hierarchical Bayes model. Similar to others in the literature ([ 2]; [ 3]; [ 5]; [16]), we estimate the model for each experimental condition separately (i.e.,  θhabsence∼N(θ¯absence,Vθabsence)  and  θhpresence∼N(θ¯presence,Vθpresence)  ) and use Bayesian inference to test for differences in estimates between the experimental conditions. To obtain a one-sided p-value, we calculated the fraction of the empirical posterior distribution that is inconsistent with the formulated hypothesis. For example, to test H1, we calculated the proportion of the GMO sensitivity distribution that is inconsistent with H1 (i.e.,  β¯GMOpresence−β¯GMOabsence>0  ). To claim statistical significance, we report a two-sided Bayesian p-value, which equals two times the one-sided p-value ([62]). We used similar analyses to test the remaining hypotheses.Table 1 shows the findings of Study 2 based on the model in Equations 1 and 2. This table contains the posterior means and standard deviations of the hyperparameter estimates (  β¯  ,  γ¯  ) for each experimental condition. The first three rows correspond to the  β¯Lay′s  ,  β¯Herr′s  , and  β¯Ruffles  , which are the posterior means of preference for each brand (relative to the private label). The next two rows report  β¯GMO  and  β¯price  , the posterior means of sensitivities associated with the GMO and price attribute, respectively. The last row reports the average threshold parameter  (γ¯)  estimate.GraphTable 1. Study 2 Posterior Estimates of  β¯  and  γ¯  . ParametersAbsencePresenceLay's6.114.19(.45)(.33)Herr's−1.95−1.49(.94)(.54)Ruffles6.523.55(.53)(.34)GMO.00−1.12(.11)(.12)Price−4.31−2.25(.27)(.14)Category threshold−6.49−2.15(.89)(.50) 1 Notes: Boldfaced parameters indicate that the 95% highest posterior density interval of the estimate did not include zero. The numbers in parentheses below the posterior mean are the standard deviations.A negative parameter estimate means that, on average, consumers like this product characteristic less than the baseline. For example, a negative estimate for GMO (  β¯GMO  ) implies that, on average, customers prefer a non-GM product over a GM product. In our studies, we can compare this parameter estimate across the GMO labeling conditions. For example, if the parameter estimate of the GMO attribute in the presence-focused condition (  β¯GMOpresence  ) is more negative than in the absence-focused condition (  β¯GMOabsence  ), we interpret this as showing that presence-focused GMO labeling amplifies consumer preference for non-GM over GM products.The impact of GMO ingredients on product choice was not statistically significant in the absence-focused condition, but was negative and statistically significant in the presence-focused condition (  β¯GMOabsence   = .00 vs.  β¯GMOpresence   = −1.12; p < .001). This finding supports H1. The parameter  β¯price  shows how sensitive consumers are to price changes. The more negative it is, the more consumers are sensitive to price changes. We find that the price coefficient is more negative in the absence condition than in the presence condition (  β¯priceabsence   =   −4.31  vs.  β¯pricepresence   =   −2.25  ; p < .001); this implies that participants were less sensitive to price changes under the presence-focused labeling, thereby supporting H2.To assess consumers' decision to buy in the product category or not,  γ¯  captures the average threshold value for product category purchase. The product category is purchased when the utility of at least one product exceeds this threshold; therefore, the higher the threshold parameter  γ¯  , the lower the probability of the category purchase. The category purchase threshold was higher in the presence condition than in the absence condition (  γ¯absence   = −6.49 vs.  γ¯presence   = −2.15; p < .001), indicating that consumers were less likely to purchase in the potato chips category in the presence-focused condition. This result supports H3.In summary, Study 2's findings support H1, H2, and H3 in an incentive-aligned conjoint experiment. The main takeaways thus far are that the GMO labeling regime (absence vs. presence) affects how sensitive consumers are to the GMO attribute. The labeling also impacts consumers' price sensitivity and their likelihood of making a purchase in a product category.The second aim of Study 2 was to understand why consumers' choices differ between the two GMO labeling conditions. We examined whether GMO risk perception differs by labeling format and, if so, whether it affects the extent to which consumers pay attention to the GMO information. In each condition, we asked the respondents a series of questions pertaining to GMOs (e.g., concerns, perceived risk, attention paid). We then asked questions relevant to choice deferral. To measure choice task difficulty, we asked, ""Was it difficult to decide which product to pick?"" ([32]). To measure choice task uncertainty, we asked, ""How certain were you that the product would be the best in each choice task?"" and ""How much did you regret choosing the product you picked in each choice task?"" ([34]). We used a nine-point scale (1 = ""not at all,"" and 9 = ""extremely"") for all of the questions asked.Figure 6 shows that consumers have greater concern about GMO products (Mabsence = 1.92 vs. Mpresence = 3.36; p < .001) and a greater perceived risk from them (Mabsence  = 2.08 vs. Mpresence = 3.65; p < .001) in the presence condition than in the absence condition. Consumers paid greater attention to GMO products (Mabsence  = 2.67 vs. Mpresence = 4.01; p < .001). These results confirm two things. First, presence labeling makes consumers experience more negative feelings (greater concern and risk perception) about GMOs. Second, as documented in the negativity bias literature, these negative perceptions result in consumers paying greater attention to the GMO attribute.Graph: Figure 6. GMO labeling: perception, attention, and choice difficulty.Regarding the questions pertaining to choice deferral, consumers in the presence labeling condition found the choice tasks to be more difficult (Mabsence = 2.52 vs. Mpresence = 3.20; p < .001). Consumers were less certain about their choice (Mabsence  = 6.12 vs. Mpresence = 5.70; p = .009), which is consistent with the choice deferral findings we reported previously. Consumers also experienced greater regret about the choices they made (Mabsence = 2.32 vs. Mpresence = 2.91; p < .001). Quantifying the impact of labeling changesA useful way to quantify the impact of the two labeling regimes is to examine consumers' WTP for the non-GMO attribute by condition. The basic assumption in a choice model is that respondents consider trade-offs between attributes when they select an alternative. Therefore, one can estimate each attribute's marginal utility from the parameter estimates and obtain the WTP for an attribute as a ratio of its marginal utility and the price coefficient ([47]; [57]). For the GMO attribute, WTP is measured by evaluating the quantity  −β¯GMO/β¯price  . We determined that the WTP for the non-GMO attribute was much higher (p < .001) in the presence condition ($.50, SD = .07) than in the absence condition ($.00, SD = .03).In summary, Study 2 shows that presence-focused labeling makes consumers ( 1) more sensitive toward the GMO attribute, ( 2) less sensitive toward price information, and ( 3) more reluctant to make a purchase in a category. Thus, Study 2 supports H1, H2, and H3. Presence-focused labeling enhances consumers' concerns about GMOs, encourages them to pay greater attention to GMO information, and makes their choice decision more difficult. In a presence-focused labeling regime, firms producing non-GM products could benefit from increased WTP for the non-GMO attribute, providing them with an incentive to offer more non-GM products and to charge more for them. Study 3The objective of Study 3 was to investigate the effect of aligned GMO information on consumer choice, as predicted by H4. To this end, we added a third condition (both) whereby all product alternatives display either the absence or the presence of GMO information. Procedures and ParticipantsThe structure of Study 3 was similar to that of Study 2, except for the following changes. First, we used different absence- and presence-labeling stimuli from those in the previous two studies (see Figure 7). Second, we did not use incentive alignment. Third, as previously noted, we added a labeling condition (both) in which the ""non-GMO"" label appeared on products that did not contain GMO ingredients and the ""GMO"" label appeared on products that did.Graph: Figure 7. Visual descriptions of the stimuli in study 3.Using MTurk, we recruited respondents for this study in exchange for monetary compensation. To begin, we asked the respondents questions to assess whether they shopped in the focal category for this study (potato chips) and whether they were paying attention to the study instructions. Of the 925 respondents who completed this first step of the study, 43 (4.6%) respondents did not qualify to continue because they did not shop in the focal category (N = 17; 1.8%) or failed to correctly answer the attention check questions (N = 26; 2.8%). A total of 882 respondents (Mage = 38.3 years; female = 51.5%) qualified for the main study and completed it. We randomly assigned them to one of the three study conditions (Nabsence = 290, Npresence = 299, Nboth = 293). Results Model-free resultsAs in Study 2, the proportion of participants choosing the non-GM products was higher in the presence (vs. absence) labeling condition. In addition, more participants chose the non-GM product in the both labeling condition than in the other two conditions (51.56% in absence, 54.98% in presence, 58.78% in both). The proportion of participants deciding not to purchase the product category was lowest in the absence labeling condition (10.05% in absence, 13.93% in presence, 13.24% in both). The average purchase price was slightly higher in the both (vs. absence) labeling condition ($2.93 in absence, $2.95 in presence, $2.98 in both). Model-based inferenceTable 2 summarizes the findings from Study 3 based on the model previously outlined in Equations 1 and 2. We began by investigating whether the findings in Study 2 also held for Study 3. As before, we found that the impact of GMO ingredients was stronger in the presence labeling condition (  β¯GMOabsence   = −.41 vs.  β¯GMOpresence   = −1.21; p < .001). The price coefficient was more negative in the absence labeling condition (  β¯priceabsence   = −4.47 vs.  β¯pricepresence   = −3.54; p = .032), implying that consumers were less price sensitive in the presence (vs. absence) labeling condition. The threshold parameter was higher in the presence labeling condition (  γ¯absence   = −12.39 vs.  γ¯presence   = −9.04; p = .018), meaning that consumers were less likely to purchase the product category in the presence condition. Overall, Study 3's findings also support H1, H2, and H3.GraphTable 2. Study 3 Posterior Estimates of  β¯  and  γ¯  . ParametersAbsencePresenceBothLay's3.994.093.90(.44)(.37)(.43)Herr's1.941.47.30(.60)(.43)(.54)Ruffles4.783.573.59(.48)(.38)(.45)GMO−.41−1.21−1.93(.13)(.18)(.23)Price−4.47−3.54−2.85(.33)(.26)(.21)Category threshold−12.39−9.04−7.28(1.21)(.90)(.80) 2 Notes: Boldfaced parameters indicate that the 95% highest posterior density interval of the estimate did not include zero. The numbers in parentheses below the posterior mean are the standard deviations.Returning to the primary purpose of Study 3—to investigate how GMO-related information alignment affects consumers' choice behaviors—the comparison of parameter estimates between presence and both labeling conditions revealed an interesting pattern. We found that the parameter estimates for GMO, price, and category incidence were higher in the both (vs. presence) labeling condition. Respondents became more sensitive to GMO ingredients when both non-GMO and GMO labels were displayed (  β¯GMOpresence   = −1.21 vs.  β¯GMOboth   = −1.93; p = .010); they also became less price sensitive in the both labeling condition (  β¯pricepresence   = −3.54 vs.  β¯priceboth   = −2.85; p = .036). The difference in the threshold parameters between the both and the presence labeling conditions is not statistically significant (  γ¯presence   = −9.04 vs.  γ¯both   = −7.28; p = .140). Thus, Study 3 supports H4a and H4b—namely, structurally aligned information makes the GMO attribute more salient and even more important for brand choice. The evidence pertaining to category choice in Study 3 does not support H4c.All our findings thus far have focused on the average market response. The hierarchical Bayes model also captured individual-level responses within each labeling condition. We found that some respondents had a positive GMO coefficient, meaning they preferred products with GMOs. The segment size of this ""prefer GMO"" segment was 39.31%, 29.43%, and 17.06% in the absence, presence, and both labeling conditions, respectively. The important takeaway is that the proportion of consumers who prefer GMO-based products decreases as we move from absence-focused to presence-focused to both label conditions. Therefore, the labeling policy regime showed a substantial impact on the size of the ""prefer GMO"" segment. Study 4The goal of Study 4 was to test H5—to determine whether the graphical format of the GMO label affects the impact of the GMO attribute on consumer choice. More specifically, we aimed to understand whether the graphical format affects ( 1) how sensitive consumers are to the GMO attribute, ( 2) how important price is to consumers, and ( 3) how likely consumers are to make a purchase in a product category. To accomplish this goal, we varied the appeal of the GMO label using different colors and themes, creating a ""positive"" green label that casts GM products in a favorable light and a more moderate ""neutral"" blue label (see Figure 8). The positive-looking green label bears some resemblance to the one recently adopted in the United States,[ 7] while the neutral blue label has a similar design to the one adopted in Uruguay.[ 8]Graph: Figure 8. Logos in three labeling conditions. Logo PretestThe purpose of the pretest was to show that the two GMO logos used in Study 4 (Figure 8) convey different signals to respondents. More specifically, we tested whether the green GMO label delivered a more positive signal to respondents (i.e., GMOs are beneficial and less harmful) than the neutral blue label.Using MTurk, we recruited respondents for this pretest in exchange for monetary compensation. To begin, we asked the respondents questions to assess whether they were paying attention to the pretest instructions. Of the 208 respondents who completed this first step of the study, 6 (2.9%) did not qualify to continue because they failed to correctly answer the attention check question. The remaining 202 respondents (Mage = 41.3 years; female = 56.4%) qualified for and completed the main study. We randomly assigned them to one of the two study conditions (Nboth-positive = 101, Nboth-neutral = 101).We asked respondents to indicate the extent to which they agree or disagree with the following statements on a nine-point scale (1 = ""very unlikely,"" and 9 = ""very likely""): ( 1) ""A product with the GMO label above is likely beneficial for me,"" and ( 2) ""A product with the GMO label above is likely harmful for me."" The second statement was reverse-coded, and we reported the average for the two questions for each label. Results indicate that the natural green label led to GMOs being perceived as more beneficial than the blue label (Mgreen = 5.76, SD = 2.64 vs. Mblue = 4.82, SD = 2.32; t(200) = −2.69, p =   .008  ). Procedures and ParticipantsThe structure of Study 4 was similar to that of Study 3. We created three between-subjects conditions, one absence labeling condition and two both labeling conditions. The absence condition used the same non-GMO label as Studies 1 and 2. In the both labeling conditions, we used the two different GMO label formats (see Figure 8): the ""positive"" green label and the ""neutral"" blue label. Our primary interest was to compare the two both conditions with the absence condition.Figure 9 shows the stimuli for the three labeling conditions that we used in the choice-based conjoint experiment. As in Studies 2 and 3, we focused on three attributes: brand, price, and GMO; the choice design for the conjoint part was the same as before. The rest of the procedure was similar to Study 3, except that we added questions to measure consumers' prior attitude toward GM products. As we discuss subsequently, this approach enabled us to assess the interplay among the GMO labeling regimes, consumer attitudes, and demand for GM products.Graph: Figure 9. Example of stimuli in choice tasks in Study 4.Using MTurk, we recruited respondents for this study in exchange for monetary compensation. To begin, we asked the respondents questions to assess whether they shopped in the focal category (potato chips) and whether they were paying attention to the study instructions. Of the 1,089 respondents who completed this first step of the study, 213 (19.6%) respondents did not qualify to continue because they did not shop in the focal category (N = 109; 10.0%) or failed to correctly answer the attention check questions (N = 104; 9.6%). A total of 876 respondents (Mage = 37.5 years; female = 49.5%) qualified for and completed the main study. We randomly assigned them to one of the three conditions (Nabsence = 291, Nboth-positive = 294, Nboth-neutral = 291). Results Model-free resultsThe descriptive statistics show a similar pattern to Study 3. A larger proportion of respondents chose the non-GM products in the both labeling conditions (50.15% in absence, 51.34% in both-positive, 53.91% in both-neutral). Model-based evidenceTable 3, Panel A, reports the parameter estimates for Study 4. Comparing the absence and both-neutral (blue) labeling conditions, produced results similar to those from Study 3. Participants in the both-neutral condition were more sensitive to the GMO ingredients (  β¯GMOabsence   = −.39 vs.  β¯GMOboth-neutral   = −.79; p = .034) and less price sensitive (  β¯priceabsence   = −5.60 vs.  β¯priceboth-neutral   = −4.67; p = .018). The difference in the likelihood to purchase in the product category is not significant (  γ¯absence   = −13.04 vs.  γ¯both-neutral   = −12.18; p = .616). However, a comparison between the absence and both-positive (green) GMO labeling conditions revealed a different pattern. There was no significant difference in consumers' GMO sensitivity (  β¯GMOabsence   = −.39 vs.  β¯GMOboth-positive   = −.43; p =   .832  ) or category purchase probability (  γ¯absence   = −13.04 vs.  γ¯both-positive   = −12.00; p =   .536  ). The only difference occurred in price sensitivity—consumers were less price sensitive in the both-positive labeling condition than in the absence condition, and it is marginally significant (  β¯priceabsence  = 5.60 vs.  β¯priceboth-positive   = −4.67; p = .06). The primary takeaway from the results is that the GMO labeling format determined the extent to which the GMO attribute affected consumer preference for GM foods. Stated differently, a comparison between the both-positive and both-neutral conditions revealed that the difference in the GMO sensitivity is marginally significant (  β¯GMOboth-positive   = −.43 vs.  β¯GMOboth-neutral   = −.79;  p=.059  ). This finding marginally supports H5a and has substantial policy implications, which we discuss subsequently. Neither the price sensitivity (  β¯priceboth-positive   = −4.67 vs.  β¯priceboth-neutral   = −4.39; p = .54) nor the category purchase probability (  γ¯both-positive   = −12.00 vs.  γ¯both-neutral   =−12.18; p = .912) is significantly different between the two conditions. These findings in Study 4 do not support H5b and H5c.GraphTable 3. Results from Study 4. A: Posterior Estimates of β¯ and γ¯BothParametersAbsenceBoth-PositiveBoth-NeutralLay's6.145.354.46(.52)(.47)(.42)Herr's3.282.011.32(.63)(.70)(.51)Ruffles6.315.333.99(.59)(.54)(.44)GMO−.39−.43−.79(.12)(.14)(.14)Price−5.6−4.67−4.39(.41)(.34)(.30)Category threshold−13.04−12.00−12.18(1.35)(1.33)(1.06)B: GMO Sensitivity for Each Subgroup Across ConditionsAbsenceBoth-PositiveBoth-NeutralBeneficial−.21−.2−.41(N = 238; 27.2%)(.16)(.15)(.16)No strong opinion−.20−.18−.66(N = 389; 44.4%)(.15)(.14)(.16)Harmful−.41−.92−1.21(N = 249; 28.4%)(.16)(.22)(.22) 3 Notes: Boldfaced parameters indicate that the 95% highest posterior density interval of the estimate did not include zero. The numbers in parentheses below the posterior mean are the standard deviations. Moderating role of prior attitudesIn our final analyses, we examined whether a respondent's prior attitude toward GMOs affected their reactions to the signals in the GMO logos. To test this idea, we asked the respondents whether they think GMOs are beneficial or harmful; we then divided them into groups based on their prior attitude toward GMOs (1–3: ""Beneficial,"" 4–6: ""No strong opinion,"" 7–9: ""Harmful""). Of the 876 respondents, 27.2% (N = 238) answered that GMOs are beneficial, 44.4% (N = 389) responded that they have no strong opinion about GMOs, and 28.4% (N = 249) reported that they think GMOs are harmful. Using individual-level parameters, we report the average GMO sensitivity by attitude group and label condition in Table 3, Panel B.[ 9]Drawing on the findings in the top row of Table 3, Panel B, for the group that considered GMOs beneficial (27.2% of the sample), the both-neutral GMO labeling format had a significant impact (p = .025) on GMO sensitivity. For the group with no strong opinion about GMOs (44.4% of the sample), the both-neutral GMO labeling format had a significant impact (p = .002) on GMO sensitivity (see middle row, third column of Table 3, Panel B). The other two GMO label formats (absence and both-positive) did not have any impact on GMO sensitivity for this group. This is the most important finding in Table 3, Panel B. From these results, we concluded that, for a very large segment of the sample, the GMO label format had a decisive impact on how consumers view GMOs in the product choices they make. Lastly, among participants who viewed GMOs as harmful (28.4% of the population; bottom row of Table 3, Panel B), compared with the absence condition, GMO labeling in the both-neutral condition had a larger (p = .004) impact on GMO sensitivity.Study 4 examined the impact of GMO label format on consumer choice. Our results showed that a GMO logo can systematically influence consumer choices. The signal contained in the label format can cause large shifts in consumer preference for GM foods. Importantly, we found that the GMO label format had a greater impact on consumers who had no strong opinions about GMOs, suggesting that preference for GM foods is highly pliable for a large segment of consumers. This effect occurs when using a neutral label format in this study, suggesting that a label that signals caution (e.g., Brazil's yellow transgenico logo) is likely to have an even more pronounced effect. DiscussionMost scientists claim that GM foods are safe for human consumption and that they offer substantial advantages; meanwhile, many consumers who lack scientific knowledge are skeptical about GM foods. Fueling these conflicting views, consumer advocacy groups have asserted that consumers need to know what they consume. Against this backdrop of diverging stakeholder views, we investigated the impact of different GMO labeling regimes. In particular we studied the impacts of voluntary, absence-focused labeling (""non-GMO"") versus mandatory, presence-focused labeling (""contains GMO""). A natural extension of the latter is a third regime in which both label types are displayed on products in the marketplace. We showed that each GMO policy has a substantial impact on consumer choices and creates incentives for firms that are important for policy makers to consider.Guided by the literature on negativity bias, structural alignment theory, and message presentation, and based on the findings of our four studies, we show that each of the three labeling regimes (absence, presence, and both) greatly affects consumers' demand for GM foods. Labels such as ""non-GMO"" and ""contains GMO"" serve as negative signals for GM foods and tend to shrink their market share. The market share shrinkage effect is stronger under the mandatory policy than the voluntary policy. GMO labeling reduces the importance consumers place on product price and impacts the consumers' WTP for non-GM products. The finding pertaining to increased preference for the non-GM products is amplified when both non-GMO and GMO labels are displayed on the products.Finally, we found that the signal policy makers decide to send via the GM label (e.g., a green logo may be viewed as an endorsement, a yellow logo as a cautionary signal) significantly affects consumer choice. Consumers' prior attitudes toward GMOs moderate this finding; consumers who are neutral toward GMOs are impacted most by the signal contained in the label. Implications for Policy MakersIn line with their relative impact on demand for GM foods, label regulations could be viewed at three levels: low, medium, and high impact. They correspond respectively to the three policy regimes: absence, presence, and both. Compared with a situation where no GMO labels exist, the voluntary GMO labeling policy (""non-GMO"") affects the demand for GM products the least. Mandatory labels (""contains GMO"") substantially affect the demand for GM products, and when both types of labels are present (""non-GMO"" and ""contains GMO""), the demand shifts are the highest.Figure 10 shows that GMO labeling has an economically significant impact on consumer WTP for non-GM products and reveals several important insights. First, voluntary, absence-focused labeling results in the lowest increase in WTP for non-GM products. In two of the latter three studies (Studies 3 and 4), this increase in WTP was nonetheless statistically and economically significant (9 cents and 7 cents, respectively). In comparison, in Studies 2 and 3, the increase in WTP in the mandatory, presence-focused labeling was substantially higher, at 50 cents and 34 cents, respectively. In Study 3, when both labels were present, WTP was the highest (68 cents). These higher WTP measures in Studies 2 and 3 should be viewed in light of the GMO label used: a yellow GMO logo similar to Brazil's in Study 2, and a red GMO label in Study 3. In Study 4, in which we tested positive (green) and neutral (blue) ""contains GMO"" logos, the WTP was still positive and economically significant (9 and 18 cents, respectively). The upshot here is that consumer WTP critically depends on the label policy makers adopt. Across studies, findings indicate that both the voluntary and mandatory labeling regimes create incentives for firms to add premium-priced, non-GM products to their portfolio of offerings. These incentives are substantially greater in the mandatory labeling regime than in the voluntary regime.Graph: Figure 10. Impact of GMO labeling on Willingness to Pay (WTP $).Contrary to the opinion that mandatory GMO labeling will merely satisfy consumers' right to know and give them complete information, our findings show that any form of GMO labeling has significant externalities that policy makers must consider carefully. GMO labeling reduces the demand for GM foods and creates incentives for firms to offer higher-priced non-GM foods, which raises the question of whether policy makers intend to promote this effect.In addition to deciding which labeling regime to implement, policy makers also have to wrestle with another critical decision related to the signal contained in the GMO label. For example, consumers may view a green label as an endorsement of GM foods and a yellow label as a signal to exercise caution. The United States has decided to implement the use of a mandatory GMO labeling system by 2022; the proposed label has a nature-friendly theme on a green or black-and-white background, along with the term ""bioengineered (BE)."" In contrast, the GMO label in Brazil is a yellow triangle resembling a caution sign. Our findings indicate that even a neutral GMO label may lead consumers to focus on the negative aspects of GMOs, pay less attention to price information, and become more reluctant to make a purchase in the product category. In Brazil, all these effects are likely amplified. The two labeling regimes at each end of the spectrum in Brazil and the United States, along with the insights offered by this article, may offer guidance to other countries about which labeling regime they should adopt. Implications for Marketing PractitionersOur research reveals that GM labels add an important product feature for consumers to evaluate. It draws attention away from factors such as price, allowing firms to charge a premium for non-GM products. The GM label can also drive some consumers away from a category (e.g., from crackers to another non-GM snacking category). All of these effects are amplified as we move from absence-focused (voluntary) to presence-focused (mandatory) policies. Both regimes create incentives for firms to expand their offerings to include more non-GM products for a segment of the market that prefers such products and is willing to pay more for them. As Figure 10 illustrates, consumer WTP for a non-GM product is higher in a presence-focused labeling regime. GM manufacturers inevitably lose market shares when presence-focused labeling is enforced. They face both reduced brand share and reduced category demand. Because presence-focused labeling makes consumers less price sensitive, GM food manufacturers may attempt to compensate for their sales loss by considering promotions other than price cuts. Limitations and Future DirectionsUnlike the debate on mandatory labeling, the discussion of regulatory aspects of voluntary labeling is limited. Regulators intervene when products make unsubstantiated health benefit claims ([ 8]). Along the same lines, future research could investigate the need for policies to regulate voluntary non-GMO disclosures. To complement our findings based on choice experiments, it would be instructive to rely on purchase data over time (e.g., [44]) from sources such as Nielsen and IRI to investigate how GMO labeling affects consumer behavior. Our choice experiments offer limited evidence for category shrinkage effects because of GMO labeling; a rigorous test for such effects would be in a multicategory context using household-level panel data. ConclusionGMO labels create vertical differentiation for many consumers by signaling that non-GM products are better than GM products. They draw attention away from factors such as price—making it less important—and allow firms to charge a premium for non-GM products. Even a voluntary GMO labeling policy deserves regulatory scrutiny because it causes a decrease in demand for GM products. In comparison, a mandatory GMO labeling policy shrinks the demand for GM products even more, and the signal contained in the GMO logo (e.g., color) plays a critical role in consumers' perceptions of GM products. Both voluntary and mandatory policy regimes create incentives for firms to expand their offerings to include more non-GM products for the market segment that prefers such products and is willing to pay more for them. The critical question for policy makers here is whether they wish to promote such consumer and firm behaviors. Supplemental Materialsj-pdf-1-jmx-10.1177_00222429211064901 - Supplemental material for GMO Labeling Policy and Consumer ChoiceSupplemental material, sj-pdf-1-jmx-10.1177_00222429211064901 for GMO Labeling Policy and Consumer Choice by Youngju Kim, SunAh Kim and Neeraj Arora in Journal of Marketing  "
9,"GMO Labeling Policy and Consumer Choice Most scientists claim that genetically modified organisms (GMOs) in foods are safe for human consumption and offer societal benefits such as better nutritional content. However, many consumers remain skeptical about their safety. Against this backdrop of diverging views, the authors investigate the impact of different GMO labeling policy regimes on the products consumers choose. Guided by the literature on negativity bias, structural alignment theory, and message presentation, and based on findings from four experiments, the authors show that consumer demand for GM foods depends on the labeling regime policy makers adopt. Both absence-focused (""non-GMO"") and presence-focused (""contains GMO"") labeling regimes reduce the market share of GM foods, with the reduction being greater in the latter case. GMO labels reduce the importance consumers place on price and enhance their willingness to pay for non-GM products. Results indicate that specific label design choices policy makers implement (in the form of color and style) also affect consumer responses to GM labeling. Consumer attitudes toward GMOs moderate this effect—consumers with neutral attitudes toward GMOs are influenced most significantly by the label design.Keywords: GMO; food claim; voluntary labeling; mandatory labeling; public policy and marketingAlthough the use of genetically modified (GM) foods has become widespread across the world, scientific and public opinions diverge about their safety. Most scientists agree that GM foods are invaluable because they offer increased nutritional content, a higher yield per acre, and a better shelf life ([51]). They also agree that GM foods are as safe for humans and the environment as non-GM foods ([18]). Yet some scientists disagree ([29]), citing concerns about possible long-term effects of GM foods on human health and the environment ([10]). On the demand side, many consumers question the safety of GM foods and their scientific promise ([30]). Indeed, a 2013 New York Times poll showed that 75% of Americans expressed concern about genetically modified organisms (GMOs) in their food, and most worried about their potential health effects. It is important to note here that the baseline consumer knowledge on this issue is low ([64]). The most extreme opponents of GM foods think they have the most knowledge about the issue, but research shows that their scientific literacy is low ([21]).The opposing views that firms and consumers have about GM foods create a fundamental tension in how such foods should be labeled, which is the central focus of this research. On the one hand, consumers and advocacy groups believe that GM foods are potentially risky; therefore, policy makers must mandate GMO labeling. In a mandatory labeling regime, food manufacturers are required to include labels such as ""contains GMO"" when their foods are GM. The most commonly used argument in support of such labeling is consumers' right to know. On the other hand, food manufacturers rely on scientific evidence to claim that GM foods are as safe as conventionally grown foods. As a result, they argue that mandatory labeling arbitrarily singles out GMO technology for specific attention and misleads consumers into thinking that they should be concerned about consuming GM foods ([60]). Therefore, food manufacturers support a voluntary GMO labeling policy, where firms have the freedom to use a ""non-GMO"" label when appropriate.The discordant views about the safety of GM foods between firms and consumers, as well as the demands for GMO labeling by consumer advocacy groups (e.g., the Non-GMO Project) create a substantial challenge for policy makers in their efforts to develop a GMO labeling policy. As a result, GMO labels vary a great deal around the world (see Figure 1). For example, the United States allows firms to display non-GMO labels on their products if they wish. Brazil, the world's second-largest GM producer after the United States, adopted a mandatory GM label that features a black T inside a yellow triangle. The letter T stands for the Portuguese word transgenicos (""transgenics""), and the symbol resembles a caution sign indicating an upcoming T-junction ([ 6]). Similar logos have been adopted by South American countries such as Bolivia and Uruguay.Graph: Figure 1. GMO labels.In light of the diverse GMO policy regimes that currently exist, an important prerequisite for carefully constructing a GMO labeling policy is a theory-based understanding of whether and how consumers shift their choices under the different GMO labeling regimes. The intention behind a labeling policy that requires the disclosure of a GMO ingredient as a horizontally differentiated attribute is that it simply allows consumers to make choices that reflect their taste differences. However, an externality of such a policy may be that it leads consumers to treat GMO ingredients as a vertically differentiated attribute, signaling that non-GM foods are of a higher quality than GM foods.To investigate the product quality–related implications empirically, we examine how the different GMO labeling policy regimes impact consumers' choice and their willingness to pay for non-GM products as well as the market shares of GM and non-GM products. Guided by the policy question of mandatory versus voluntary labeling for GM foods, we investigate the substantial impact of different GMO policy regimes on choices consumers make. More specifically, the purpose of our research is to answer the following research questions that have significant GMO labeling policy implications. Using the theory-driven terminology adopted by [ 1], in the remainder of the article we refer to the mandatory labeling regime as ""presence-focused"" (contains GMO) and the voluntary labeling regime as ""absence-focused"" (non-GMO). Does the labeling policy (absence-focused vs. presence-focused) affect a consumer's choice of GM products? Does the labeling policy (absence-focused vs. presence-focused) affect other aspects of the consumer's choice process, such as their price sensitivity and willingness to shop in a category? Is the consumer's choice impacted according to whether the GMO related information disclosure is complete (presence-focused and absence-focused) or partial (presence-focused or absence-focused)? Complete GMO information disclosure occurs when policy makers mandate presence-focused labeling and firms that produce non-GM products display an absence-focused label. Partial disclosure occurs when either presence or absence labels are present. Do consumers behave differently depending on the GM label's presentation format (e.g., color, theme)? Which consumers are most likely to alter choices because of the label format?To answer these key policy-related questions, we combine insights from the social psychology literature with rigorous consumer choice models to make novel predictions about the effect of GMO labeling changes on consumers' demand for GM products. We develop our theory based on the literature on negativity bias (e.g., [31]), structural alignment theory (e.g., [25]), and message presentation (e.g., [33]). We use choice experiments grounded in microeconomic theory ([40]) and a hierarchical Bayes model to test our hypotheses.Across four studies involving 3,913 respondents, we study the impact of different GMO labeling regimes on demand for GM products. Our first between-subjects experiment (Study 1) examines whether consumer choice depends on the GMO labeling regime (i.e., no labeling, absence-focused, presence-focused, or both labeling conditions). Study 2 investigates how the GMO labeling regime may impact the importance consumers place on product price and product category. Study 3 shows how the alignability of GMO information, whether partial information or full information is disclosed, affects consumer choice. Finally, Study 4 investigates how the signal used in a GMO label (e.g., color) can impact consumer demand for GM foods and reveals which market segment is most likely to be affected by the signal used.Our findings have substantive implications for two key stakeholder groups: policy makers and food manufacturers. By quantifying the effects of various GMO labeling regimes, we offer policy makers guidance on the impact of each labeling system on consumer demand. Absence-focused policies result in the smallest change in demand for GM products compared with a regime with no GM labels. Presence-focused labeling policies can substantially alter demand for GM products, and the signal contained in the GMO logo (e.g., color) also plays a critical role in consumers' perceptions of GM products. Both policy regimes create incentives for firms to expand their offerings to include more non-GM products for the market segment that prefers such products and is willing to pay more for them. The critical question for policy makers here is whether they wish to incentivize such firm behavior.For food manufacturers, our research reveals that GM labels add an important product feature for consumers to evaluate. Such labels create vertical differentiation for many consumers by signaling that non-GM products are better than GM products. They draw attention away from factors such as price—making it less important—and allow firms to charge a premium for non-GM products. The GM label can also drive some consumers away from a category (e.g., from crackers to another non-GM snack). All of the aforementioned effects are amplified when moving from an absence-focused to presence-focused regime. Background GMOs: What Science SaysThe [63], p. 1) defines GMOs as ""organisms (i.e., plants, animals, or microorganisms) in which the genetic material (DNA) has been altered in a way that does not occur naturally by mating and/or natural recombination."" Proponents of GM crops argue that they increase yield, lower food prices, reduce damage to crops after the harvest, make crops tolerant of stresses such as cold and heat, help fight malnourishment, and reduce reliance on chemical pesticides ([51]). Most scientists claim that there is no substantiated evidence that genetic crop modification makes foods less safe. For example, the National Academies of Sciences and Medicine ([46]) reported that food from GM crops is no more dangerous than food produced by conventional agriculture. More than 150 Nobel laureates in areas such as chemistry, physics, and medicine signed an open letter in 2016 to endorse the safety of GM foods, noting that ""opposition based on emotion and dogma contradicted by data must be stopped"" (Support Precision Agriculture 2016).Although the dominant view among scientific organizations is that GMOs do not harm human health, this view is not ubiquitous. In one review article, [36] noted that a group of scientists believe that each GM product should be tested over long periods for possible side effects. The author reviewed 26 animal feeding studies that identified adverse effects or animal health uncertainties, leading him to conclude that ""putative consensus about the inherent safety of transgenic crops is premature"" (p. 909). A joint statement by a group of researchers ([29]) claimed that no consensus on GM food safety exists. They indicated that a conflict of interest exists in many reported studies supporting GM food because biotechnology companies often fund this research ([15]). They further noted that no epidemiological studies have examined the effects of GM food consumption on humans. They concluded that it is necessary to test the effect of GM foods on humans and over longer periods. GMOs: What Consumers ThinkSeveral studies have documented consumers' lack of knowledge about GMOs, as noted by [64] in their review. These studies also document an overall negative attitude toward GMOs among consumers. Such negative attitudes could be driven by negative press associated with occurrences such as GM crops causing a decline in monarch butterflies, which a recent article refutes ([ 7]). The primary concerns are that growing and consuming GM crops may cause health problems and allergic reactions.Research has shown that the most extreme opponents of GM foods know the least about GMOs but think that they know the most ([21]). People's misplaced confidence stemming from the mismatch between what they think they know about science and what they actually know ([45]) polarizes attitudes even more ([22]).The controversy around GM foods also relates to the growing literature on science denial ([54]) that identifies social mechanisms as the basis for extreme confidence in beliefs that contradict scientific consensus ([33]). Specifically, many people have insufficient information to establish their own opinions on new technologies and scientific developments ([20]) and instead accept the opinions of people they trust ([54]). Well-known examples of science denial include vaccine safety, global warming and climate change, the rise in antibiotic resistance, and the safety of GM foods. GMOs: What Policy Makers DoGMO labeling policy in the United States was absence-focused when GM foods were first released in 1994. Some food companies use third-party verification, such as the Non-GMO Project (https://www.nongmoproject.org), to highlight the non-GMO aspect of their products. However, various consumer groups and nongovernmental organizations have argued for presence-focused labeling based on consumers' right to know what is in their food. They contend that the potential harm of GM foods needs to be made explicit.Over the years, political pressure to introduce presence-focused GMO labeling in the United States has grown. In July 2016, U.S. Congress passed a bill requiring the U.S. Department of Agriculture to establish a national disclosure standard for GMOs. The new policy has a two-year phase-in period that began in January 2020. The proposed label under this policy has a nature-friendly theme on a green or black-and-white background and uses the term ""bioengineered (BE)."" Dozens of nations around the world have enacted presence-focused GMO labels based on the percentage of GMOs in ingredients or how the seed was developed. The GMO percentage thresholds vary among countries that have regulations. For example, the European Union (EU) and United Kingdom set this limit at.9%, whereas Australia set it at 1% ([31]).Policy makers in many countries are uncertain whether GM foods are safe, and their labeling rules are based on such a perspective. For example, the EU has adopted the precautionary principle (European Commission [19]) for GMO labeling. This principle is often cited in cases of scientific uncertainty and the possibility of irreversible damage. It states that ""where there are threats of serious or irreversible damage, lack of full scientific certainty shall not be used as a reason for postponing cost-effective measures to prevent environmental degradation"" ([49], Principle 15).Not surprisingly, GMO labeling policies are controversial. In a compelling counterargument to EU policies, [59] acknowledged the rationality behind the precautionary principle but cautioned that rigid regulatory controls based on the idea of ""possible risk"" can paralyze progress. He explained that, for GMOs, the precautionary principle results in substitute risks because it interferes with the promise of mitigating hunger and disease in developing countries by using foods such as golden rice, which is bioengineered to be rich in vitamin A.In the United States, debate over the recently adopted logo is intense because it appears to signal the government's positive attitude toward GM foods. In Brazil, opponents of the current mandatory presence-focused GM logos accuse the country's policy makers of scaremongering. In Canada, which currently has no GMO labeling legislation, petitions have been circulated supporting mandatory presence-focused GMO labeling; in a survey of adults living in Canada, 90% of the respondents expressed support for mandatory GM labeling ([56]). PredictionsIn this section, we develop predictions on how GMO labeling affects different aspects of consumer choice, including preference for GM foods, price sensitivity, and product category purchase. We also outline how these predicted effects are moderated by the GMO label format. Figure 2 summarizes our main hypotheses.Graph: Figure 2. Overview of predictions. GMO Labeling and Consumer ChoiceIn an absence-focused labeling policy, manufacturers may use a ""non-GMO"" label when appropriate. In a world in which many consumers have negative attitudes toward GM products, non-GMO producers often use absence-focused claims (e.g., a TV ad for Triscuit[ 5]). Such claims are in line with those that highlight the absence of negatives—namely, no preservatives, no artificial colors, no chemicals, and so on. These nature-based claims that remove a negative strongly affect consumers' inferences about product's taste and healthfulness ([ 1]), even when they are irrelevant to the actual quality. In contrast, in a presence-focused GMO labeling policy, regulators mandate that GM-food products display a ""contains GMO"" label. For many consumers who have negative attitudes toward GM products, this information signals potential risk.The absence- and presence-focused labeling policies outlined thus far may impact consumers' evaluations of GM foods via two separate mechanisms—information valence and information source. With regard to information valence, it is well known that people place greater weight on negative information than positive information. This negativity bias ([50]) is at the core of how consumers may evaluate a GMO label. From an evolutionary perspective, this bias occurs because we have a greater chance of surviving and thriving if we pay greater attention to negative information; negative events are more consequential than positive ones. Some argue that negative information is more informative because it is rarer ([24]), attracting more attention and thus being more ""diagnostic or informative"" ([53]). Previous research has documented negativity bias in a variety of contexts. For example, negative attributes are more diagnostic of product quality than positive ones ([28]), and negative reviews have a stronger effect on purchase decisions ([ 9]).In addition, absence- and presence-focused labeling differs by their information source. In presence-focused GMO labeling, a regulatory body mandates the display of GMO labels, whereas in absence-focused labeling, this decision is voluntary and made by the firm. The perceived credibility of a message's source can affect the recipient's cognitive response ([58]). For trusted information sources, consumers accept a message without undertaking an extensive assessment of its content ([23]). Evidence suggests that consumers trust public sources (e.g., a government) more than private ones (e.g., a firm). For example, [17] found that advertising from a government source (the Federal Trade Commission) is more credible than that from a firm. Similar results have been noted for safety hazard information ([38]), environmental information ([11]), and forest-product certification seals ([48]). Given the differences in information valence and source between the two labeling regimes discussed thus far, we propose the following hypothesis: H1:  Presence-focused GMO labeling makes consumers more sensitive to the GMO attribute when choosing a product than absence-focused GMO labeling. GMO Labeling and Price SensitivityPrevious studies have shown that negative information is more diagnostic and, as a result, attracts more attention ([53]). In line with the argument that attention is a scarce resource, [12] demonstrated that greater attention to a previously unconsidered attribute reduces the relative importance of other attributes. Extending these theoretical findings to our research context, greater attention to GMO ingredients likely diverts consumer attention away from other product-related information, such as price. Negative presence-focused GMO labeling should reduce consumers' focus on price information more compared with absence-focused GMO labeling. Thus, H2:  Presence-focused GMO labeling makes consumers less price sensitive than absence-focused labeling. GMO Labeling and Product Category PurchaseChoice deferral is a means of mitigating the negativity generated in uncertain or difficult choice contexts. Previous research shows that this negative feeling in such contexts increases the likelihood that consumers will defer their decision ([39]). Deferral occurs when no single option dominates a choice set or when consumers face difficult trade-offs between product attributes ([13]).In our context, consider a Brand A, which contains GMOs, and a Brand B, which does not. Assume that a consumer prefers Brand A but prefers non-GMO ingredients. In the absence-focused condition, this consumer will choose between Brand A, not knowing whether it is a GM product, and Brand B, fully aware that it is a non-GM product because of the ""non-GMO"" label. Conversely, in the presence-focused condition, the same consumer will choose between Brand A, fully aware that it is a GM product because of the ""contains GMO"" label and Brand B, with no GMO-related knowledge. The trade-off between brand name and GM ingredients may be more difficult in the presence-focused condition because the consumer has to analyze the costs and benefits of a brand they prefer (Brand A) and an attribute they do not (GMO). This increased task difficulty may enhance the likelihood of choice deferral.[42] show that consumers tend to view a government's default option as an implicitly recommended course of action. In their studies, when the government uses the ""organ donor"" default, most participants inferred that ( 1) the policy makers were willing to be donors and ( 2) people ought to be donors. In the GMO labeling context, a GMO label mandated by a regulatory body may send a negative signal that consumers should avoid a product with GMO ingredients. Growing concerns and perceived risks associated with GMOs could increase customers' uncertainty about brand quality, thus leading to choice deferral. Therefore, H3:  Presence-focused GMO labeling makes consumers less likely to purchase the relevant product category than absence-focused GMO labeling. GMO Labeling Alignment and Consumer ChoiceThe first three hypotheses focus on scenarios where the product alternatives available apply either an absence-focused label or a presence-focused label. However, when a government mandates presence-focused labeling, firms that produce non-GM products may be free to use absence-focused labeling, as is the case in the United States today. Because many consumers view non-GM products favorably, firms offering non-GM products have a strong incentive to include such information on their product packaging to differentiate themselves from firms offering GM products. Therefore, when a mandatory GMO labeling policy is implemented in the marketplace, it is plausible that most—if not all—products will display either GMO or non-GMO labels. We use structural alignment theory ([35]; [55]; [65]) to discuss the impact of partial or complete GMO-related information on consumer choice and how they drive our predictions.Consider the following example involving two marinara sauce brands, A and B. Brand A is sold at $2.00, without providing any information on GMO attributes; Brand B is sold at $2.50 and includes a non-GMO label. In this example, the price is alignable information because the attribute is present in both options. In contrast, under either the absence-focused or presence-focused labeling, GMO-related information is only available for Brand B, making it nonalignable. The structural alignment literature suggests that consumers pay more attention to alignable attributes ([25]) and put greater weight on them ([55]).Consistent with this discussion, when both types of GMO labels are included (i.e., ""non-GMO"" and ""contains GMO""), consumers will give greater weight to the GMO attribute. According to the arguments used previously for H2, giving greater weight to the GMO attribute would ( 1) further reduce the weight consumers give to price information and ( 2) make them more reluctant to purchase a product in the category. Formally, H4:  Compared with a situation where only presence-focused (""contains GMO"") labels are displayed, when both absence-focused (""non-GMO"") and presence-focused (""contains GMO"") labels are displayed, consumers become (a) more sensitive to GMOs, (b) less sensitive to price, and (c) less likely to purchase in the product category. GMO Labeling Format and Consumer ChoiceA regulatory body's choice of GMO label reveals its beliefs or attitudes about GMOs and is, therefore, a critical policy decision—consumers tend to view a government's default option as an implicit recommended course of action ([42]). Moreover, [52] showed that a speaker's description signals their attitude toward an object. For example, if someone likes a team, they describe its successes, and if they do not, they note the team's failures. The descriptions a speaker chooses, even of seemingly equivalent objects, are important for listeners ([41]).As a concrete example involving the color of a GMO label, consumers tend to infer that a product has positive, nature-related attributes when it prominently displays the color green ([61]). Similarly, the color blue signals openness, peace, and tranquility ([43]), whereas yellow signals caution. Such color choices and their associated signals are highly relevant for GMO labeling. We hypothesize that policy makers' choice of a GMO label (e.g., the color green, blue, or yellow) is important as it delivers an implicit recommendation that may influence consumers' choices. H5:  The graphical format of the label determines how much impact the GMO attribute has on consumer choice, including (a) how sensitive consumers are to the GMO attribute, (b) how important price is to consumers, and (c) how likely consumers are to purchase in a given product category.Consumers' prior beliefs about GMOs could also play a role in how much a labeling policy impacts them. Previous research showed that most individuals do not know enough details to establish their own perspectives on new technologies and scientific developments ([20]) and accept the position of others they trust ([54]). As a result, we anticipate that consumers in the middle, who neither like nor dislike GM products, are affected the most by the label format policy makers select. Overview of StudiesWe include four empirical studies. The first study uses a simple between-subjects design to examine the effect of different GMO labeling policies (i.e., absence, presence, or both) on consumer choice. We subsequently conduct three choice-based conjoint studies to test H1–H5. Study 2 focuses on H1, H2, and H3 by disentangling the impact of GMO labeling on different aspects of consumer choice. Study 3 tests H4, focusing on how the findings pertaining to H1–H3 are affected by GMO information disclosure (partial vs. full). Lastly, Study 4 tests H5, focusing on how the different graphical formats of GMO labeling impact our previous findings. Study 1The goal of Study 1 was to demonstrate that different GMO labeling regimes (i.e., no GMO labeling, absence-focused labeling, presence-focused labeling, and both labeling conditions) can lead to systematic differences in demand for GM foods. Procedures and ParticipantsUsing Amazon Mechanical Turk (MTurk), we recruited respondents in exchange for monetary compensation. To begin, we asked the respondents questions to assess whether they shopped in the focal categories (marinara sauce and pickle) and whether they were paying attention to the study instructions. Of the 2,110 respondents who completed this first step of the study, 767 (36.4%) respondents did not qualify to continue because they did not shop in the two focal categories (N = 644; 30.5%) or failed to correctly answer the attention check questions (N = 123; 5.8%). A total of 1,343 respondents (Mage = 41.0 years; female = 62%) qualified to participate in the main study and completed it. We randomly assigned these respondents to one of the four study conditions in a between-subjects design (Ncontrol = 340, Nabsence = 331, Npresence = 335, Nboth = 337).We presented respondents with choice sets in two different product categories (marinara sauce and pickles) and asked them to select their preferred brand. We selected these two product categories because ( 1) they are frequently purchased and ( 2) they complement the less healthy product category (potato chips) that we use in our subsequent studies.In the marinara sauce category, the first two (Prego and Newman's Own) of the three brands included in the study were GM products while the third (Barilla) was a non-GM product. Similarly, in the pickles category, two brands (Claussen and Vlasic) were GM products and the third (Mt. Olive) was a non-GM product. In both categories, we gave respondents in the control condition no information on GMOs. In the absence labeling condition, only the non-GMO label was displayed. In the presence labeling condition, only the GMO label was displayed. Finally, in the both labeling condition, both non-GMO and GMO labels were displayed. Figure 3 shows the four GMO labeling conditions for the pickles category in this study. The stimuli for the marinara sauce category are included in the Web Appendix A (Figure W1).Graph: Figure 3. Stimuli for four labeling conditions (pickles). ResultsFigure 4 shows the share of non-GM and GM products in each category across the four labeling conditions. We found that the GMO labeling regime significantly impacts consumer choice of the brands included. In all three GMO labeling conditions (absence, presence, and both), more participants preferred the non-GM product than in the control condition, where no GMO-related information is displayed. The magnitude of these shifts in demand toward the non-GM product (Barilla or Mt. Olive) depended on the GMO labeling condition.Graph: Figure 4. Non-GMO vs GMO choice share across different labeling conditions.In the marinara sauce category (Figure 4, Panel A), the choice share of non-GM product was lower in the control condition (  π^control   = .171) than in the absence (  π^absence   = .236; z = −2.094, p = .046), presence (  π^presence   = .370; z = −5.825, p < .001), and both labeling conditions (  π^both   = .407; z = −6.780, p < .001). In addition, the non-GM product's choice share was lower in the absence labeling condition (  π^absence   = .236) than in the presence (  π^presence   = .370; z = −3.761, p < .001) and the both (  π^both   = .407; z = −4.730, p =   .001  ) labeling conditions. These results imply that consumer preference for non-GM products increases progressively from control to absence to presence labeling conditions. The difference between the presence and both labeling conditions was not statistically significant (  π^presence   = .370 vs.  π^both   = .407; z = −.984, p =   .374  ).Graph: Figure 5. Example choice tasks in study 2.The findings for the pickles category (Figure 4, Panel B) are largely consistent with those for marinara sauce. The choice share of the non-GM product was lower in the control condition (  π^control   = .229) than in the absence (  π^absence   = .293; z = −1.888, p =   .074  ), presence (  π^presence   = .421; z = −5.327, p < .001), and both (  π^both   = .469; z = −6.552, p < .001) labeling conditions. As in the marinara sauce category, the non-GM product's choice share was lower in the absence labeling condition (  π^absence   = .293) than in the presence (  π^presence   = .421; z = −3.446, p < .001) and both (  π^both   = .469; z = −4.685, p < .001) labeling conditions. Once again, the difference between the presence and both labeling conditions was not statistically significant (  π^presence   = .421 vs.  π^both   = .469; z = −1.252, p =   .241  ).Study 1 supports our central thesis—namely, that the way the GMO message is conveyed to consumers affects their choices. In all GMO labeling regimes, the share of non-GM products is greater than when no GMO-related information is revealed. The share of non-GM products increases progressively from control to absence to presence labeling conditions. Although these findings are interesting on their own, Study 1 raised important follow-up research questions. For example, should we expect GMO labeling to affect the importance of price to consumers and their willingness to pay (WTP) for a non-GM product? Does the manner in which the GMO message is delivered affect the product choice consumers make? Furthermore, Study 1 confounds brand and GM ingredients, as only the Barilla brand carried the GMO-free label and only Prego and Newman's Own displayed the ""contains GMO"" label in the marinara sauce category. To answer our next set of research questions and remove the confounding between brand and GMO ingredients, we conducted three choice-based conjoint experiments, which we report on next. Study 2The purpose of Study 2 was twofold. First, we test H1, H2, and H3, which examine the impact of GMO labeling (absence vs. presence) on consumers' sensitivity to the GMO attribute, price, and category purchase, respectively. Second, we explore the behavioral processes associated with consumer choice under the different GMO labeling regimes. Unlike Study 1, we do not have a confound between brand and GMO ingredients in this study. Procedures and ParticipantsTo introduce Study 2, we used incentive alignment instructions similar to those employed by [27], informing respondents that 25 of them would receive a total value of $5 based on their answers to the survey: either a product of their choice and a Walmart e-gift card for the remaining value, or a $5 Walmart e-gift card. Existing literature shows that incentive-alignment techniques make consumers likely to provide more realistic responses. When a study offers as an incentive a version of the product that is predicted to give consumers the highest utility, respondents put greater effort into the choice task, and their responses are more likely to reflect their actual preferences ([14]; [27]).We informed respondents that the study's purpose was to understand how they evaluated potato chips. We asked the respondents to complete 14 choice tasks. In each choice task, we showed the respondents four brands of potato chips: Lay's, Herr's, Ruffles, and a private label. The price varied by brand. As in the marketplace, we set the prices of the national brands ($2.79, $3.29, $3.79) and the private brand ($1.99, $2.39, $2.79) at different levels. The two GMO label conditions were included or not included on the package. We displayed all four brands in each choice set, varying the GMO ingredients and prices from one scenario to the next in the 14 choice tasks.We used a statistically efficient choice design based on the D-optimality criterion ([37]) for the main-effects model that included three attributes varied orthogonally across choice tasks, which prevented us from testing higher-order interactions. For example, Lay's chips could have a GMO label in one task but not in the next; by design, the brand and GMO label attribute are unconfounded, enabling us to assess consumer preference for each. For more information on conjoint analysis and how it relates to our study, please see Web Appendix B.We adopted a dual-response method, asking respondents to indicate ( 1) which of the product alternatives they prefer and ( 2) whether they would actually buy the product they had just selected ([ 4]; [27]). The dual-response method has the advantage of encouraging respondents to slow down to think through the purchase task, making the no-choice option more likely, which is similar to a real market situation. Next, respondents answered questions about their attitudes toward GMOs, which enabled us to investigate how consumer attitudes vary across the different labeling regimes. We used a nine-point ""disagree/agree"" scale to ask questions regarding attention to the GMO ingredient, risk perception of the GMO ingredient, and decision uncertainty.We randomly assigned respondents to one of the two conditions (absence- vs. presence-focused labeling) in a between-subjects design. These two between-subjects conditions differed by how the GMO label was displayed. In the absence-focused condition, only the non-GMO label was displayed, whereas in the presence-focused condition, only the GMO label was displayed. Figure 5 presents a screenshot of one of the 14 choice tasks for each condition in Study 2.We recruited students from a Midwestern university in the United States, and they participated in the study in exchange for course credit. To begin, we asked the students questions to assess whether they shopped in the focal category (potato chips) and whether they had a dietary restriction (gluten intolerance). Of the 665 students who completed this first step of the study, 55 (8.3%) respondents did not qualify to continue because they did not shop in the focal category (N = 49; 7.4%) or were gluten-intolerant (N = 6;.9%).[ 6] A total of 610 students (Mage = 19.7 years; female = 44.4%) qualified for and completed the main study. We randomly assigned them to one of the two study conditions (Nabsence = 303, Npresence = 307). Results Model-free resultsThe proportion of respondents purchasing a non-GM product was higher in the presence condition (56.38%) than in the absence condition (48.84%). The proportion of respondents who decided not to make a purchase was also higher in the presence condition (37.06%) than in the absence condition (32.77%). The average purchase price was similar in the two conditions ($3.07 in absence vs. $3.05 in presence). Statistical modelThis study focused on the impact of GMO labeling on brand and category choice. Given this, we modeled two decisions—whether to buy and which brand to choose—using a nested framework to model brand choice and purchase incidence ([ 2]; [26]), where the joint probability of a given consumer choosing brand j and buying it (B) is given by Pr(j,B)=Pr(j)×Pr(B|j). Graph( 1)We conceptualized our model at the brand level and assumed that individual h evaluating j (j = 1, ..., J) brands chooses the brand j. Each brand j has a design vector xj that contains discrete variables to indicate the different attribute levels (e.g., GMO) and a continuous variable (e.g., price). The deterministic part of individual h's utility for brand j is linear in the predictor variables (xjβh) and, with a Type I extreme value error structure, yields a multinomial logit model [40]. The probability of individual h choosing brand j is given by Pr(yhj′=1)=exp(x′j′βh)∑Jj=1exp(x′jβh), Graph( 2)where βh is an individual-level parameter vector that includes brand preference and sensitivity to attributes such as GMO and price. To model the buy/no-buy decision, we specified a threshold utility (γh) parameter in the model. The utility of the most attractive alternative (j) in the choice set must exceed γh for the individual to buy it (B = 1). A larger estimated threshold parameter implies a lower probability of buying in the category.We introduced heterogeneity across individuals hierarchically with a random effects specification ([ 4]) as  θh∼N(θ¯,Vθ)  , where  θh={βh,γh}  and  θ¯={β¯,γ¯}  . In our empirical context, the hyperparameter  β¯  includes the average brand preference parameters, GMO sensitivity (  β¯GMO  ), and price sensitivity (  β¯price  ), while  γ¯  is the average threshold for category purchase. For the remaining technical details of the model, refer to Web Appendix C. Model-based inferenceWe employed the Markov chain Monte Carlo (MCMC) method to estimate the hierarchical Bayes model. Similar to others in the literature ([ 2]; [ 3]; [ 5]; [16]), we estimate the model for each experimental condition separately (i.e.,  θhabsence∼N(θ¯absence,Vθabsence)  and  θhpresence∼N(θ¯presence,Vθpresence)  ) and use Bayesian inference to test for differences in estimates between the experimental conditions. To obtain a one-sided p-value, we calculated the fraction of the empirical posterior distribution that is inconsistent with the formulated hypothesis. For example, to test H1, we calculated the proportion of the GMO sensitivity distribution that is inconsistent with H1 (i.e.,  β¯GMOpresence−β¯GMOabsence>0  ). To claim statistical significance, we report a two-sided Bayesian p-value, which equals two times the one-sided p-value ([62]). We used similar analyses to test the remaining hypotheses.Table 1 shows the findings of Study 2 based on the model in Equations 1 and 2. This table contains the posterior means and standard deviations of the hyperparameter estimates (  β¯  ,  γ¯  ) for each experimental condition. The first three rows correspond to the  β¯Lay′s  ,  β¯Herr′s  , and  β¯Ruffles  , which are the posterior means of preference for each brand (relative to the private label). The next two rows report  β¯GMO  and  β¯price  , the posterior means of sensitivities associated with the GMO and price attribute, respectively. The last row reports the average threshold parameter  (γ¯)  estimate.GraphTable 1. Study 2 Posterior Estimates of  β¯  and  γ¯  . ParametersAbsencePresenceLay's6.114.19(.45)(.33)Herr's−1.95−1.49(.94)(.54)Ruffles6.523.55(.53)(.34)GMO.00−1.12(.11)(.12)Price−4.31−2.25(.27)(.14)Category threshold−6.49−2.15(.89)(.50) 1 Notes: Boldfaced parameters indicate that the 95% highest posterior density interval of the estimate did not include zero. The numbers in parentheses below the posterior mean are the standard deviations.A negative parameter estimate means that, on average, consumers like this product characteristic less than the baseline. For example, a negative estimate for GMO (  β¯GMO  ) implies that, on average, customers prefer a non-GM product over a GM product. In our studies, we can compare this parameter estimate across the GMO labeling conditions. For example, if the parameter estimate of the GMO attribute in the presence-focused condition (  β¯GMOpresence  ) is more negative than in the absence-focused condition (  β¯GMOabsence  ), we interpret this as showing that presence-focused GMO labeling amplifies consumer preference for non-GM over GM products.The impact of GMO ingredients on product choice was not statistically significant in the absence-focused condition, but was negative and statistically significant in the presence-focused condition (  β¯GMOabsence   = .00 vs.  β¯GMOpresence   = −1.12; p < .001). This finding supports H1. The parameter  β¯price  shows how sensitive consumers are to price changes. The more negative it is, the more consumers are sensitive to price changes. We find that the price coefficient is more negative in the absence condition than in the presence condition (  β¯priceabsence   =   −4.31  vs.  β¯pricepresence   =   −2.25  ; p < .001); this implies that participants were less sensitive to price changes under the presence-focused labeling, thereby supporting H2.To assess consumers' decision to buy in the product category or not,  γ¯  captures the average threshold value for product category purchase. The product category is purchased when the utility of at least one product exceeds this threshold; therefore, the higher the threshold parameter  γ¯  , the lower the probability of the category purchase. The category purchase threshold was higher in the presence condition than in the absence condition (  γ¯absence   = −6.49 vs.  γ¯presence   = −2.15; p < .001), indicating that consumers were less likely to purchase in the potato chips category in the presence-focused condition. This result supports H3.In summary, Study 2's findings support H1, H2, and H3 in an incentive-aligned conjoint experiment. The main takeaways thus far are that the GMO labeling regime (absence vs. presence) affects how sensitive consumers are to the GMO attribute. The labeling also impacts consumers' price sensitivity and their likelihood of making a purchase in a product category.The second aim of Study 2 was to understand why consumers' choices differ between the two GMO labeling conditions. We examined whether GMO risk perception differs by labeling format and, if so, whether it affects the extent to which consumers pay attention to the GMO information. In each condition, we asked the respondents a series of questions pertaining to GMOs (e.g., concerns, perceived risk, attention paid). We then asked questions relevant to choice deferral. To measure choice task difficulty, we asked, ""Was it difficult to decide which product to pick?"" ([32]). To measure choice task uncertainty, we asked, ""How certain were you that the product would be the best in each choice task?"" and ""How much did you regret choosing the product you picked in each choice task?"" ([34]). We used a nine-point scale (1 = ""not at all,"" and 9 = ""extremely"") for all of the questions asked.Figure 6 shows that consumers have greater concern about GMO products (Mabsence = 1.92 vs. Mpresence = 3.36; p < .001) and a greater perceived risk from them (Mabsence  = 2.08 vs. Mpresence = 3.65; p < .001) in the presence condition than in the absence condition. Consumers paid greater attention to GMO products (Mabsence  = 2.67 vs. Mpresence = 4.01; p < .001). These results confirm two things. First, presence labeling makes consumers experience more negative feelings (greater concern and risk perception) about GMOs. Second, as documented in the negativity bias literature, these negative perceptions result in consumers paying greater attention to the GMO attribute.Graph: Figure 6. GMO labeling: perception, attention, and choice difficulty.Regarding the questions pertaining to choice deferral, consumers in the presence labeling condition found the choice tasks to be more difficult (Mabsence = 2.52 vs. Mpresence = 3.20; p < .001). Consumers were less certain about their choice (Mabsence  = 6.12 vs. Mpresence = 5.70; p = .009), which is consistent with the choice deferral findings we reported previously. Consumers also experienced greater regret about the choices they made (Mabsence = 2.32 vs. Mpresence = 2.91; p < .001). Quantifying the impact of labeling changesA useful way to quantify the impact of the two labeling regimes is to examine consumers' WTP for the non-GMO attribute by condition. The basic assumption in a choice model is that respondents consider trade-offs between attributes when they select an alternative. Therefore, one can estimate each attribute's marginal utility from the parameter estimates and obtain the WTP for an attribute as a ratio of its marginal utility and the price coefficient ([47]; [57]). For the GMO attribute, WTP is measured by evaluating the quantity  −β¯GMO/β¯price  . We determined that the WTP for the non-GMO attribute was much higher (p < .001) in the presence condition ($.50, SD = .07) than in the absence condition ($.00, SD = .03).In summary, Study 2 shows that presence-focused labeling makes consumers ( 1) more sensitive toward the GMO attribute, ( 2) less sensitive toward price information, and ( 3) more reluctant to make a purchase in a category. Thus, Study 2 supports H1, H2, and H3. Presence-focused labeling enhances consumers' concerns about GMOs, encourages them to pay greater attention to GMO information, and makes their choice decision more difficult. In a presence-focused labeling regime, firms producing non-GM products could benefit from increased WTP for the non-GMO attribute, providing them with an incentive to offer more non-GM products and to charge more for them. Study 3The objective of Study 3 was to investigate the effect of aligned GMO information on consumer choice, as predicted by H4. To this end, we added a third condition (both) whereby all product alternatives display either the absence or the presence of GMO information. Procedures and ParticipantsThe structure of Study 3 was similar to that of Study 2, except for the following changes. First, we used different absence- and presence-labeling stimuli from those in the previous two studies (see Figure 7). Second, we did not use incentive alignment. Third, as previously noted, we added a labeling condition (both) in which the ""non-GMO"" label appeared on products that did not contain GMO ingredients and the ""GMO"" label appeared on products that did.Graph: Figure 7. Visual descriptions of the stimuli in study 3.Using MTurk, we recruited respondents for this study in exchange for monetary compensation. To begin, we asked the respondents questions to assess whether they shopped in the focal category for this study (potato chips) and whether they were paying attention to the study instructions. Of the 925 respondents who completed this first step of the study, 43 (4.6%) respondents did not qualify to continue because they did not shop in the focal category (N = 17; 1.8%) or failed to correctly answer the attention check questions (N = 26; 2.8%). A total of 882 respondents (Mage = 38.3 years; female = 51.5%) qualified for the main study and completed it. We randomly assigned them to one of the three study conditions (Nabsence = 290, Npresence = 299, Nboth = 293). Results Model-free resultsAs in Study 2, the proportion of participants choosing the non-GM products was higher in the presence (vs. absence) labeling condition. In addition, more participants chose the non-GM product in the both labeling condition than in the other two conditions (51.56% in absence, 54.98% in presence, 58.78% in both). The proportion of participants deciding not to purchase the product category was lowest in the absence labeling condition (10.05% in absence, 13.93% in presence, 13.24% in both). The average purchase price was slightly higher in the both (vs. absence) labeling condition ($2.93 in absence, $2.95 in presence, $2.98 in both). Model-based inferenceTable 2 summarizes the findings from Study 3 based on the model previously outlined in Equations 1 and 2. We began by investigating whether the findings in Study 2 also held for Study 3. As before, we found that the impact of GMO ingredients was stronger in the presence labeling condition (  β¯GMOabsence   = −.41 vs.  β¯GMOpresence   = −1.21; p < .001). The price coefficient was more negative in the absence labeling condition (  β¯priceabsence   = −4.47 vs.  β¯pricepresence   = −3.54; p = .032), implying that consumers were less price sensitive in the presence (vs. absence) labeling condition. The threshold parameter was higher in the presence labeling condition (  γ¯absence   = −12.39 vs.  γ¯presence   = −9.04; p = .018), meaning that consumers were less likely to purchase the product category in the presence condition. Overall, Study 3's findings also support H1, H2, and H3.GraphTable 2. Study 3 Posterior Estimates of  β¯  and  γ¯  . ParametersAbsencePresenceBothLay's3.994.093.90(.44)(.37)(.43)Herr's1.941.47.30(.60)(.43)(.54)Ruffles4.783.573.59(.48)(.38)(.45)GMO−.41−1.21−1.93(.13)(.18)(.23)Price−4.47−3.54−2.85(.33)(.26)(.21)Category threshold−12.39−9.04−7.28(1.21)(.90)(.80) 2 Notes: Boldfaced parameters indicate that the 95% highest posterior density interval of the estimate did not include zero. The numbers in parentheses below the posterior mean are the standard deviations.Returning to the primary purpose of Study 3—to investigate how GMO-related information alignment affects consumers' choice behaviors—the comparison of parameter estimates between presence and both labeling conditions revealed an interesting pattern. We found that the parameter estimates for GMO, price, and category incidence were higher in the both (vs. presence) labeling condition. Respondents became more sensitive to GMO ingredients when both non-GMO and GMO labels were displayed (  β¯GMOpresence   = −1.21 vs.  β¯GMOboth   = −1.93; p = .010); they also became less price sensitive in the both labeling condition (  β¯pricepresence   = −3.54 vs.  β¯priceboth   = −2.85; p = .036). The difference in the threshold parameters between the both and the presence labeling conditions is not statistically significant (  γ¯presence   = −9.04 vs.  γ¯both   = −7.28; p = .140). Thus, Study 3 supports H4a and H4b—namely, structurally aligned information makes the GMO attribute more salient and even more important for brand choice. The evidence pertaining to category choice in Study 3 does not support H4c.All our findings thus far have focused on the average market response. The hierarchical Bayes model also captured individual-level responses within each labeling condition. We found that some respondents had a positive GMO coefficient, meaning they preferred products with GMOs. The segment size of this ""prefer GMO"" segment was 39.31%, 29.43%, and 17.06% in the absence, presence, and both labeling conditions, respectively. The important takeaway is that the proportion of consumers who prefer GMO-based products decreases as we move from absence-focused to presence-focused to both label conditions. Therefore, the labeling policy regime showed a substantial impact on the size of the ""prefer GMO"" segment. Study 4The goal of Study 4 was to test H5—to determine whether the graphical format of the GMO label affects the impact of the GMO attribute on consumer choice. More specifically, we aimed to understand whether the graphical format affects ( 1) how sensitive consumers are to the GMO attribute, ( 2) how important price is to consumers, and ( 3) how likely consumers are to make a purchase in a product category. To accomplish this goal, we varied the appeal of the GMO label using different colors and themes, creating a ""positive"" green label that casts GM products in a favorable light and a more moderate ""neutral"" blue label (see Figure 8). The positive-looking green label bears some resemblance to the one recently adopted in the United States,[ 7] while the neutral blue label has a similar design to the one adopted in Uruguay.[ 8]Graph: Figure 8. Logos in three labeling conditions. Logo PretestThe purpose of the pretest was to show that the two GMO logos used in Study 4 (Figure 8) convey different signals to respondents. More specifically, we tested whether the green GMO label delivered a more positive signal to respondents (i.e., GMOs are beneficial and less harmful) than the neutral blue label.Using MTurk, we recruited respondents for this pretest in exchange for monetary compensation. To begin, we asked the respondents questions to assess whether they were paying attention to the pretest instructions. Of the 208 respondents who completed this first step of the study, 6 (2.9%) did not qualify to continue because they failed to correctly answer the attention check question. The remaining 202 respondents (Mage = 41.3 years; female = 56.4%) qualified for and completed the main study. We randomly assigned them to one of the two study conditions (Nboth-positive = 101, Nboth-neutral = 101).We asked respondents to indicate the extent to which they agree or disagree with the following statements on a nine-point scale (1 = ""very unlikely,"" and 9 = ""very likely""): ( 1) ""A product with the GMO label above is likely beneficial for me,"" and ( 2) ""A product with the GMO label above is likely harmful for me."" The second statement was reverse-coded, and we reported the average for the two questions for each label. Results indicate that the natural green label led to GMOs being perceived as more beneficial than the blue label (Mgreen = 5.76, SD = 2.64 vs. Mblue = 4.82, SD = 2.32; t(200) = −2.69, p =   .008  ). Procedures and ParticipantsThe structure of Study 4 was similar to that of Study 3. We created three between-subjects conditions, one absence labeling condition and two both labeling conditions. The absence condition used the same non-GMO label as Studies 1 and 2. In the both labeling conditions, we used the two different GMO label formats (see Figure 8): the ""positive"" green label and the ""neutral"" blue label. Our primary interest was to compare the two both conditions with the absence condition.Figure 9 shows the stimuli for the three labeling conditions that we used in the choice-based conjoint experiment. As in Studies 2 and 3, we focused on three attributes: brand, price, and GMO; the choice design for the conjoint part was the same as before. The rest of the procedure was similar to Study 3, except that we added questions to measure consumers' prior attitude toward GM products. As we discuss subsequently, this approach enabled us to assess the interplay among the GMO labeling regimes, consumer attitudes, and demand for GM products.Graph: Figure 9. Example of stimuli in choice tasks in Study 4.Using MTurk, we recruited respondents for this study in exchange for monetary compensation. To begin, we asked the respondents questions to assess whether they shopped in the focal category (potato chips) and whether they were paying attention to the study instructions. Of the 1,089 respondents who completed this first step of the study, 213 (19.6%) respondents did not qualify to continue because they did not shop in the focal category (N = 109; 10.0%) or failed to correctly answer the attention check questions (N = 104; 9.6%). A total of 876 respondents (Mage = 37.5 years; female = 49.5%) qualified for and completed the main study. We randomly assigned them to one of the three conditions (Nabsence = 291, Nboth-positive = 294, Nboth-neutral = 291). Results Model-free resultsThe descriptive statistics show a similar pattern to Study 3. A larger proportion of respondents chose the non-GM products in the both labeling conditions (50.15% in absence, 51.34% in both-positive, 53.91% in both-neutral). Model-based evidenceTable 3, Panel A, reports the parameter estimates for Study 4. Comparing the absence and both-neutral (blue) labeling conditions, produced results similar to those from Study 3. Participants in the both-neutral condition were more sensitive to the GMO ingredients (  β¯GMOabsence   = −.39 vs.  β¯GMOboth-neutral   = −.79; p = .034) and less price sensitive (  β¯priceabsence   = −5.60 vs.  β¯priceboth-neutral   = −4.67; p = .018). The difference in the likelihood to purchase in the product category is not significant (  γ¯absence   = −13.04 vs.  γ¯both-neutral   = −12.18; p = .616). However, a comparison between the absence and both-positive (green) GMO labeling conditions revealed a different pattern. There was no significant difference in consumers' GMO sensitivity (  β¯GMOabsence   = −.39 vs.  β¯GMOboth-positive   = −.43; p =   .832  ) or category purchase probability (  γ¯absence   = −13.04 vs.  γ¯both-positive   = −12.00; p =   .536  ). The only difference occurred in price sensitivity—consumers were less price sensitive in the both-positive labeling condition than in the absence condition, and it is marginally significant (  β¯priceabsence  = 5.60 vs.  β¯priceboth-positive   = −4.67; p = .06). The primary takeaway from the results is that the GMO labeling format determined the extent to which the GMO attribute affected consumer preference for GM foods. Stated differently, a comparison between the both-positive and both-neutral conditions revealed that the difference in the GMO sensitivity is marginally significant (  β¯GMOboth-positive   = −.43 vs.  β¯GMOboth-neutral   = −.79;  p=.059  ). This finding marginally supports H5a and has substantial policy implications, which we discuss subsequently. Neither the price sensitivity (  β¯priceboth-positive   = −4.67 vs.  β¯priceboth-neutral   = −4.39; p = .54) nor the category purchase probability (  γ¯both-positive   = −12.00 vs.  γ¯both-neutral   =−12.18; p = .912) is significantly different between the two conditions. These findings in Study 4 do not support H5b and H5c.GraphTable 3. Results from Study 4. A: Posterior Estimates of β¯ and γ¯BothParametersAbsenceBoth-PositiveBoth-NeutralLay's6.145.354.46(.52)(.47)(.42)Herr's3.282.011.32(.63)(.70)(.51)Ruffles6.315.333.99(.59)(.54)(.44)GMO−.39−.43−.79(.12)(.14)(.14)Price−5.6−4.67−4.39(.41)(.34)(.30)Category threshold−13.04−12.00−12.18(1.35)(1.33)(1.06)B: GMO Sensitivity for Each Subgroup Across ConditionsAbsenceBoth-PositiveBoth-NeutralBeneficial−.21−.2−.41(N = 238; 27.2%)(.16)(.15)(.16)No strong opinion−.20−.18−.66(N = 389; 44.4%)(.15)(.14)(.16)Harmful−.41−.92−1.21(N = 249; 28.4%)(.16)(.22)(.22) 3 Notes: Boldfaced parameters indicate that the 95% highest posterior density interval of the estimate did not include zero. The numbers in parentheses below the posterior mean are the standard deviations. Moderating role of prior attitudesIn our final analyses, we examined whether a respondent's prior attitude toward GMOs affected their reactions to the signals in the GMO logos. To test this idea, we asked the respondents whether they think GMOs are beneficial or harmful; we then divided them into groups based on their prior attitude toward GMOs (1–3: ""Beneficial,"" 4–6: ""No strong opinion,"" 7–9: ""Harmful""). Of the 876 respondents, 27.2% (N = 238) answered that GMOs are beneficial, 44.4% (N = 389) responded that they have no strong opinion about GMOs, and 28.4% (N = 249) reported that they think GMOs are harmful. Using individual-level parameters, we report the average GMO sensitivity by attitude group and label condition in Table 3, Panel B.[ 9]Drawing on the findings in the top row of Table 3, Panel B, for the group that considered GMOs beneficial (27.2% of the sample), the both-neutral GMO labeling format had a significant impact (p = .025) on GMO sensitivity. For the group with no strong opinion about GMOs (44.4% of the sample), the both-neutral GMO labeling format had a significant impact (p = .002) on GMO sensitivity (see middle row, third column of Table 3, Panel B). The other two GMO label formats (absence and both-positive) did not have any impact on GMO sensitivity for this group. This is the most important finding in Table 3, Panel B. From these results, we concluded that, for a very large segment of the sample, the GMO label format had a decisive impact on how consumers view GMOs in the product choices they make. Lastly, among participants who viewed GMOs as harmful (28.4% of the population; bottom row of Table 3, Panel B), compared with the absence condition, GMO labeling in the both-neutral condition had a larger (p = .004) impact on GMO sensitivity.Study 4 examined the impact of GMO label format on consumer choice. Our results showed that a GMO logo can systematically influence consumer choices. The signal contained in the label format can cause large shifts in consumer preference for GM foods. Importantly, we found that the GMO label format had a greater impact on consumers who had no strong opinions about GMOs, suggesting that preference for GM foods is highly pliable for a large segment of consumers. This effect occurs when using a neutral label format in this study, suggesting that a label that signals caution (e.g., Brazil's yellow transgenico logo) is likely to have an even more pronounced effect. DiscussionMost scientists claim that GM foods are safe for human consumption and that they offer substantial advantages; meanwhile, many consumers who lack scientific knowledge are skeptical about GM foods. Fueling these conflicting views, consumer advocacy groups have asserted that consumers need to know what they consume. Against this backdrop of diverging stakeholder views, we investigated the impact of different GMO labeling regimes. In particular we studied the impacts of voluntary, absence-focused labeling (""non-GMO"") versus mandatory, presence-focused labeling (""contains GMO""). A natural extension of the latter is a third regime in which both label types are displayed on products in the marketplace. We showed that each GMO policy has a substantial impact on consumer choices and creates incentives for firms that are important for policy makers to consider.Guided by the literature on negativity bias, structural alignment theory, and message presentation, and based on the findings of our four studies, we show that each of the three labeling regimes (absence, presence, and both) greatly affects consumers' demand for GM foods. Labels such as ""non-GMO"" and ""contains GMO"" serve as negative signals for GM foods and tend to shrink their market share. The market share shrinkage effect is stronger under the mandatory policy than the voluntary policy. GMO labeling reduces the importance consumers place on product price and impacts the consumers' WTP for non-GM products. The finding pertaining to increased preference for the non-GM products is amplified when both non-GMO and GMO labels are displayed on the products.Finally, we found that the signal policy makers decide to send via the GM label (e.g., a green logo may be viewed as an endorsement, a yellow logo as a cautionary signal) significantly affects consumer choice. Consumers' prior attitudes toward GMOs moderate this finding; consumers who are neutral toward GMOs are impacted most by the signal contained in the label. Implications for Policy MakersIn line with their relative impact on demand for GM foods, label regulations could be viewed at three levels: low, medium, and high impact. They correspond respectively to the three policy regimes: absence, presence, and both. Compared with a situation where no GMO labels exist, the voluntary GMO labeling policy (""non-GMO"") affects the demand for GM products the least. Mandatory labels (""contains GMO"") substantially affect the demand for GM products, and when both types of labels are present (""non-GMO"" and ""contains GMO""), the demand shifts are the highest.Figure 10 shows that GMO labeling has an economically significant impact on consumer WTP for non-GM products and reveals several important insights. First, voluntary, absence-focused labeling results in the lowest increase in WTP for non-GM products. In two of the latter three studies (Studies 3 and 4), this increase in WTP was nonetheless statistically and economically significant (9 cents and 7 cents, respectively). In comparison, in Studies 2 and 3, the increase in WTP in the mandatory, presence-focused labeling was substantially higher, at 50 cents and 34 cents, respectively. In Study 3, when both labels were present, WTP was the highest (68 cents). These higher WTP measures in Studies 2 and 3 should be viewed in light of the GMO label used: a yellow GMO logo similar to Brazil's in Study 2, and a red GMO label in Study 3. In Study 4, in which we tested positive (green) and neutral (blue) ""contains GMO"" logos, the WTP was still positive and economically significant (9 and 18 cents, respectively). The upshot here is that consumer WTP critically depends on the label policy makers adopt. Across studies, findings indicate that both the voluntary and mandatory labeling regimes create incentives for firms to add premium-priced, non-GM products to their portfolio of offerings. These incentives are substantially greater in the mandatory labeling regime than in the voluntary regime.Graph: Figure 10. Impact of GMO labeling on Willingness to Pay (WTP $).Contrary to the opinion that mandatory GMO labeling will merely satisfy consumers' right to know and give them complete information, our findings show that any form of GMO labeling has significant externalities that policy makers must consider carefully. GMO labeling reduces the demand for GM foods and creates incentives for firms to offer higher-priced non-GM foods, which raises the question of whether policy makers intend to promote this effect.In addition to deciding which labeling regime to implement, policy makers also have to wrestle with another critical decision related to the signal contained in the GMO label. For example, consumers may view a green label as an endorsement of GM foods and a yellow label as a signal to exercise caution. The United States has decided to implement the use of a mandatory GMO labeling system by 2022; the proposed label has a nature-friendly theme on a green or black-and-white background, along with the term ""bioengineered (BE)."" In contrast, the GMO label in Brazil is a yellow triangle resembling a caution sign. Our findings indicate that even a neutral GMO label may lead consumers to focus on the negative aspects of GMOs, pay less attention to price information, and become more reluctant to make a purchase in the product category. In Brazil, all these effects are likely amplified. The two labeling regimes at each end of the spectrum in Brazil and the United States, along with the insights offered by this article, may offer guidance to other countries about which labeling regime they should adopt. Implications for Marketing PractitionersOur research reveals that GM labels add an important product feature for consumers to evaluate. It draws attention away from factors such as price, allowing firms to charge a premium for non-GM products. The GM label can also drive some consumers away from a category (e.g., from crackers to another non-GM snacking category). All of these effects are amplified as we move from absence-focused (voluntary) to presence-focused (mandatory) policies. Both regimes create incentives for firms to expand their offerings to include more non-GM products for a segment of the market that prefers such products and is willing to pay more for them. As Figure 10 illustrates, consumer WTP for a non-GM product is higher in a presence-focused labeling regime. GM manufacturers inevitably lose market shares when presence-focused labeling is enforced. They face both reduced brand share and reduced category demand. Because presence-focused labeling makes consumers less price sensitive, GM food manufacturers may attempt to compensate for their sales loss by considering promotions other than price cuts. Limitations and Future DirectionsUnlike the debate on mandatory labeling, the discussion of regulatory aspects of voluntary labeling is limited. Regulators intervene when products make unsubstantiated health benefit claims ([ 8]). Along the same lines, future research could investigate the need for policies to regulate voluntary non-GMO disclosures. To complement our findings based on choice experiments, it would be instructive to rely on purchase data over time (e.g., [44]) from sources such as Nielsen and IRI to investigate how GMO labeling affects consumer behavior. Our choice experiments offer limited evidence for category shrinkage effects because of GMO labeling; a rigorous test for such effects would be in a multicategory context using household-level panel data. ConclusionGMO labels create vertical differentiation for many consumers by signaling that non-GM products are better than GM products. They draw attention away from factors such as price—making it less important—and allow firms to charge a premium for non-GM products. Even a voluntary GMO labeling policy deserves regulatory scrutiny because it causes a decrease in demand for GM products. In comparison, a mandatory GMO labeling policy shrinks the demand for GM products even more, and the signal contained in the GMO logo (e.g., color) plays a critical role in consumers' perceptions of GM products. Both voluntary and mandatory policy regimes create incentives for firms to expand their offerings to include more non-GM products for the market segment that prefers such products and is willing to pay more for them. The critical question for policy makers here is whether they wish to promote such consumer and firm behaviors. Supplemental Materialsj-pdf-1-jmx-10.1177_00222429211064901 - Supplemental material for GMO Labeling Policy and Consumer ChoiceSupplemental material, sj-pdf-1-jmx-10.1177_00222429211064901 for GMO Labeling Policy and Consumer Choice by Youngju Kim, SunAh Kim and Neeraj Arora in Journal of Marketing  "
10,"Households Under Economic Change: How Micro- and Macroeconomic Conditions Shape Grocery Shopping Behavior Economic conditions may significantly affect households' shopping behavior and, by extension, retailers' and manufacturers' firm performance. By explicitly distinguishing between two basic types of economic conditions—micro conditions, in terms of households' personal income, and macro conditions, in terms of the business cycle—this study analyzes how households adjust their grocery shopping behavior. The authors observe more than 5,000 households over eight years and analyze shopping outcomes in terms of what, where, and how much they shop and spend. Results show that micro and macro conditions substantially influence shopping outcomes, but in very different ways. Microeconomic changes lead households to adjust primarily their overall purchase volume—that is, after losing income, households buy fewer products and spend less in total. In contrast, macroeconomic changes cause pronounced structural shifts in households' shopping basket allocation and spending behavior. Specifically, during contractions, households shift purchases toward private labels while also buying and consequently spending more than during expansions. During expansions, however, households increasingly purchase national brands but keep their total spending constant. The authors discuss psychological and sociological mechanisms that can explain the differential effects of micro and macro conditions on shopping behavior and develop important diagnostic and normative implications for retailers and manufacturers.Keywords: business cycle; income shocks; consumer packaged goods; private label; national brand; discounter; supermarketHouseholds are subjected to constantly changing economic conditions. These changes may take place at a personal, microeconomic level, such as if the main breadwinner receives a pay raise or a household member loses a job (micro conditions). Alternatively, changes may manifest at a macroeconomic level, in terms of the business cycle, with its recurring expansions and contractions or in response to global events such as the Great Recession or the COVID-19 pandemic (macro conditions). These changing micro and macro conditions substantially affect household spending and, in turn, companies' profits. By one estimate, the Great Recession led to an average 8%, or $4,000, decrease in real annual spending among U.S. households, which amounts to $500 billion in forgone revenues ([20]).While households tend to simply postpone purchases of durable goods to times of economic prosperity ([16]; [18]), they engage in a variety of adjustments when shopping consumer packaged goods (CPGs): switching from national brands (NBs) to cheaper brands or private labels (PLs), from supermarkets to discounters, from regular to promotional prices, or decreasing the amounts purchased altogether (e.g., [17]; [43]; [46]).While research to date has focused intensively on how households adjust individual CPG shopping outcomes in response to changing macro conditions (e.g., [17]; [41]; [43]), this work takes a holistic view on households' CPG shopping behavior by uncovering how it is differentially affected by micro and macro conditions. This explicit distinction is important because changes in macro and micro conditions are not necessarily aligned. In fact, even the Great Recession, during which unemployment rates skyrocketed and housing prices and stock portfolios plummeted, did not equally affect the personal income and wealth of all demographic subgroups of the population ([37]) or all geographical regions ([17]). Similarly, the economic downturn caused by the COVID-19 pandemic implies particularly severe microeconomic consequences for industry sectors that depend on tourism, events, or gastronomy, with less effect on banking or the public sector ([48]). Of course, an income loss, for example, as result of sudden unemployment, may as well occur during prosperous economic times and be no lesser of an individual hardship.Furthermore, the consequences of changing micro and macro conditions differ considerably. Whereas changing micro conditions directly affect households' ability to purchase, changing macro conditions, all else being equal, affect only households' willingness to purchase ([39]). Accordingly, households' response to changing conditions depends on whether they are affected at a micro or macro level (or both) and may manifest in very different shopping outcomes. For example, households may alter what they purchase (e.g., NBs or PLs) and where they shop (e.g., in discounters or supermarkets), as well as how much they spend and purchase. Thus, to properly disentangle the distinct effects of micro and macro conditions and to provide differentiated implications for retailers and manufacturers, holistic observations of households' shopping behavior are crucial.We analyze a total of seven measurable and managerially relevant shopping outcomes. These outcomes reflect how households allocate their budget across brand types and store formats—their shopping basket allocation (in terms of PL and NB spending in discounters and nondiscounters)—as well as how much they spend and purchase—their shopping basket value (in terms of total spending, purchase volume, and an index of prices paid). Through the analysis, we uncover and characterize the differential effects of micro and macro conditions on households' shopping behavior by addressing the following research questions: To what extent do micro (i.e., income) and macro (i.e., the business cycle) conditions affect households' CPG shopping behavior? How do micro and macro conditions differ in terms of their effects on households' shopping basket allocation and shopping basket value? Do asymmetries exist between negative (i.e., income losses/economic contractions) and positive (i.e., income gains/economic expansions) conditions, and if so, do these asymmetries differ between micro and macro conditions?We use a unique, comprehensive data set tailored to the research objectives. Drawing on the GfK Germany ConsumerScan panel, we obtain detailed information about daily CPG transactions for more than 5,000 households in Germany over a period of eight years including the Great Recession. Drawing on this, we identify what and where households shop, how much they purchase, what prices they pay, and how much they spend. Annual surveys administered to the panel provide us with longitudinal data on households' demographics and psychographics, including micro conditions in terms of household income. In addition, the panel data enable us to control for important marketing-mix elements concerning prices, assortments, and promotional activities. We further enrich the data set with macroeconomic data from the German Federal Statistical Office and advertising data from the Nielsen Company on advertising spending by all manufacturers and retailers in the sample.The analyses show that micro and macro conditions both have a substantial impact on households' shopping behavior. Importantly, households adjust their shopping behavior without a concrete change in their budget constraints. In addition, micro and macro conditions differ substantially in their effects on households' shopping behavior. Whereas micro conditions primarily have an impact on households' basket value, macro conditions not only affect households' basket value but also cause shifts in households' basket allocation. During adverse micro conditions, households buy lower volumes and spend substantially less in total but do not shift spending to other brands or store formats. In contrast, as macro conditions change, households shift spending to PLs (from both discounters and nondiscounters) during contractions and to NBs during expansions. In addition, they increase their total spending and purchase volume during contractions. We argue that the shifts during macro conditions are driven by a greater society-wide acceptance of frugal consumption that does not emerge during changing micro conditions. These discrete effects of micro and macro conditions and the proposed underlying mechanisms have distinct managerial implications. The results also address some of the counterintuitive findings of prior studies, such as increasing total spending and purchase volumes ([46]) as well as higher prices paid ([ 8]) during the Great Recession. Related LiteratureOur study relates to business cycle research in marketing as summarized in Table 1.GraphTable 1. Literature Overview. AuthorsMacro ConditionsMicro ConditionsShopping Behavior(s)Data BasisGicheva, Hastings, and Villas-Boas (2007)Gasoline pricesSpending share of income, out-of-home consumption, promotion shares (individually)Weekly household-level consumption surveys, repeated cross-section, two U.S. regions from 2000 to 2004Lamey et al. (2007)Business cycle (asymmetries)PL shareAnnual, country-level longitudinal data, four countries spanning multiple decadesLamey et al. (2012)Business cyclePL shareAnnual, category-level longitudinal data, U.S. from 1985 to 2005Ma et al. (2011)Gasoline prices, GDP growth rateShopping trips, total spending, purchase volume, store format, brand type, price tier, and promotion shares (individually)Monthly, household-level longitudinal panel data, U.S. metropolitan area from 2006 to 2008Kamakura and Du (2012)GDP growthHousehold budgetSpending share of budgetAnnual, household-level consumption surveys, repeated cross-section, U.S. from 1989 to 2003Lamey (2014)Business cycle (asymmetries)Discounter shareAnnual, country-level longitudinal data, 15 countries, spanning 17 yearsCha, Chintagunta, and Dhar (2016)Regional unemployment levelTotal spending, purchase volume, prices paid, store format, brand type, price tier, and promotion shares (individually)Annual, household-level panel data, repeated cross-section, U.S. from 2006 to 2011Dubé, Hitsch, and Rossi (2018)(Post)recession phaseIncome, wealthPL shareMonthly, longitudinal household-level panel data, U.S. from 2004 to 2012This articleBusiness cycle (asymmetries)Income (asymmetries)Total spending, purchase volume, price index, brand type and store format shares (simultaneously)Quarterly, longitudinal household-level panel data, Germany from 2007 to 2013 1 Notes:[26] and [46] argue that changes in gasoline prices reflect changes in household budgets. We regard gasoline prices as macro effects because they are experienced simultaneously but not necessarily equally by all households, as some households may rely on their cars more than others. As such, they are more similar to macro rather than micro events.Pioneering studies in this stream show that during recessions, PL market shares ([43]) and discounter market shares ([41]) increase, and some of these effects carry over into subsequent expansion periods. [17] generally confirm these findings by analyzing PL demand at a household level, accounting for heterogeneous income and wealth effects caused by the Great Recession. They find significant short- and long-term effects on PL demand, albeit with notably smaller elasticities. [ 8] further extend the number of shopping behaviors observed. They find that unemployment caused by the Great Recession has led households to increasingly purchase products on price promotion, cheaper brands, and in cheaper store formats. Instead of traditional macroeconomic indicators, [26] and [46] use gasoline prices to operationalize changing economic conditions. They show that gasoline prices relate to a multitude of shopping behaviors such as spending, prices paid, and store format and brand type shares.In addition to macro conditions, some of the studies in the field observe households' micro conditions. However, they are either used as time-invariant demographic control variables ([ 8]; [26]; [46]) or conceptualized as direct consequences and part of macro conditions rather than distinct conditions with idiosyncratic effects ([17]). Our study thus contributes to this literature stream by delineating the distinct effects of changing micro as well as macro conditions on households' shopping behavior. Importantly, we also account for different magnitudes and asymmetries between adverse and beneficial micro and macro conditions.First insights into the differences between micro and macro conditions show that overall household spending on food products and alcoholic beverages increases during adverse macro conditions but decreases when micro conditions worsen ([38]). We complement these findings by analyzing a variety of shopping outcomes beyond overall spending, using actual purchase data (thus increasing external validity), and controlling for a large variety of confounding factors such as changes in the marketing mix that are associated with changes in macro conditions ([58]).Notably, studies to date either focus on individual shopping outcomes (e.g., [17]; [43]) or model several shopping outcomes independently from each other ([ 8]; [26]; [46]). However, households have a variety of means to adjust their shopping behavior that are also highly interdependent—for example, discounters carry substantially more PLs and fewer NBs and usually feature fewer promotions in favor of an everyday low-price strategy. As such, when households switch store formats, it almost automatically also affects their brand type and promotion shares ([11]). Failing to account for these interdependencies can overestimate the effect of changing conditions on individual shopping outcomes. Therefore, we analyze multiple shopping outcomes simultaneously, controlling for their interdependencies, and thus contribute to the literature by offering a holistic picture of micro and macro conditions' effects on households' shopping behavior. Conceptual FrameworkThe conceptual framework (Figure 1) depicts the two main components of our study: micro and macro conditions and their effect on households' shopping behavior. We observe these behaviors through concrete and measurable shopping outcomes that, in essence, boil down to households' shopping basket value (i.e., how much households purchase and at what price) and their shopping basket allocation (i.e., how households allocate their expenditures across brand types and store formats). To get a holistic picture of micro and macro conditions' effects on households' shopping behavior, we consider the various shopping outcomes simultaneously. We also control for household demographics and psychographics as well as manufacturer and retailer adjustments to the marketing mix.Graph: Figure 1. Conceptual framework. Economic Conditions: Micro Versus MacroWe analyze changing macro conditions in terms of the business cycle on the basis of gross domestic product (GDP) (e.g., [43]; [58]) and derive micro conditions in terms of households' income. Although changing macro conditions are experienced by an entire region, by a nation, or even globally, they do not necessarily affect all households at a micro level. For example, not all households may experience income reductions, job loss, or shrinking wealth during a recession ([17]). Thus, by differentiating between micro and macro conditions, we isolate the distinct effects on shopping outcomes of changes in households' ability to purchase (micro level) and their willingness to purchase (macro level) ([39]). A negative micro shock, for example, restricts some households' shopping budgets, while households that face only adverse macro conditions lack this budget constraint. Importantly, whereas changing micro conditions are usually a personal matter, changing macro conditions affect a society at large. Thus, shifts in macro conditions can alter what type of shopping behavior is considered the norm. During recessions, for example, frugal consumption such as buying PLs or visiting discounters may become socially acceptable and even fashionable ([22]; [38]).In addition, beneficial and adverse economic conditions exercise asymmetric effects on consumers' shopping behavior for several possible reasons, such as general pessimism following a recession, inertia in maintaining newly adopted habits, or the need to pay off debts that have accrued during a period of lower income ([11]; [43]). Thus, we investigate asymmetric effects by splitting micro and macro conditions into both adverse and beneficial changes. Households' Shopping OutcomesWe distinguish between a household's shopping basket value and shopping basket allocation. We examine shopping basket value outcomes in terms of a household's total budget spent, total volume purchased, and an index of prices paid that indicates whether a household purchases products below average market prices of these products, for example, through temporary price promotions. In this way, we can differentiate the degree to which households adjust how much they purchase and how much they spend. We discern shopping basket allocation outcomes by considering brand types and store formats jointly and differentiating between households' spending on ( 1) PLs in discounters, ( 2) PLs in nondiscounters (e.g., supermarkets, hypermarkets), ( 3) NBs in discounters, and ( 4) NBs in nondiscounters. Prior research has taken a similar approach to households' budget allocation, with studies distinguishing between PLs and NBs as different brand types (e.g., [ 3]; [63]; [56]) or discounters and nondiscounters as different store formats (e.g., [10]; [40]; [41]). This approach has the following conceptual merits. Brand typesRegarding brand types, PLs, NBs, and their competition have received ample attention from both academics and practitioners ([40]). PLs have evolved from pure economic options to covering all price tiers and even special segments such as organic foods ([27]; [35]). They have thus developed into major competitors for NBs; for example, in Germany they have gained a market share of 41%, with 95% of consumers buying PLs ([25]; [36]). The competition between NBs and PLs is distinct in that PLs are managed by retailers and, thus, they introduce an aspect of competition into their otherwise collaborative relationship with manufacturers through downward price pressure. However, at the same time, NBs and PLs benefit each other by increasing store traffic and reinforcing quality disparities ([24]; [49]). From a consumer perspective, NBs and PLs differ substantially. First, consumers perceive PLs as inexpensive and as a good value for money. Further, while NBs are generally still better known and are perceived as being of higher quality, PLs are catching up in terms of quality perception ([36]). These differences in terms of price and quality perceptions generally suggest that households will switch between these two brand types in response to changing micro or macro conditions. Thus, the explicit distinction between NBs and PLs is relevant for our research. Store formatsIn terms of store formats, previous research has contrasted discounters with ""traditional retailers"" ([40]; [41]), supermarkets ([10]), and large retail formats ([28]; [29]). In contrast to other formats, discounters are highly optimized for cost efficiency, resulting in a substantially different retail marketing mix: store design and product presentation are austere, consumer services are reduced to a minimum, and serviced fresh foods and baked goods counters are lacking. The assortment is typically limited, especially in terms of produce; shallow, with few alternatives in each product category; and dominated by PLs, featuring relatively few NBs. As such, discounters are able to offer substantially lower prices than other store formats at the cost of service quality ([41]; [64]).In contrast, the major nondiscount store formats, such as supermarkets, superstores, and hypermarkets, vary in floor size and assortments offered beyond CPGs (e.g., clothing, home decor, hardware) but are similar to each other in terms of prices, service quality, and CPG assortments ([40]; [41]; [64]). This is also evident from Table 2, in which we contrast market data from discount and nondiscount store formats in Germany. Therefore, distinguishing between discounters and nondiscounters is most obvious from both retailer and consumer perspectives. Despite their distinct characteristics, however, discounters and nondiscounters do not merely address different target groups but also compete directly with each other for the same consumers, as consistently argued and shown in previous research (e.g., [10]; [33]).GraphTable 2. Store Format Characteristics. Store Format# of StoresaSales Area (m2/store)aRevenues(€ mil.)aMarket ShareaSpace Prod. (€/m2)# of SKUsaSKU Prod.(€ mil./SKU)PL SharebService ScorecPrice Scorec1. Discounters16,05477969,80045.44%5,5842,29530.465.6%67.182.92. Small retailers8,7502974,8003.13%1,846—————3. Supermarkets10,90098244,90029.23%4,19611,8303.821.6%82.073.64. Superstores1,1273,46115,2009.90%3,89725,005.684.574.06. Hypermarkets8517,05118,90012.30%3,15048,870.419.6%79.177.9Discounters (1)16,05477969,80045.44%5,5842,29530.465.6%67.182.7Nondiscounters (2–5)21,6281,07383,80054.56%3,61223,2263.221.2%82.574.7 2 aSource:[21], based on 2016 data.3 bSource:[25], based on 2018 data.4 cSource:[16], based on 2018 data.5 Notes: Data are based on the German market. Aggregated values for nondiscounters based on sums or averages weighted by market shares. Service and price scores are indexes (0–100), scores for store formats are aggregates from the 12 major retail brands that were tested. We assigned retail brands to their primary store format based on industry convention and average store size: small retailers <400 m2, supermarkets 400–2,500 m2, superstores 2,500–5,000 m2, hypermarkets >5,000 m2 average sales area. Brand type and store format combinationsImportantly, we do not consider the defined brand types (NBs and PLs) and store formats (discounters and nondiscounters) in isolation but in combination. This combined view is important because the brand choice cannot be viewed independently of the underlying store format. For example, because discounters carry a larger PL share than nondiscounters, PLs are more visible to households at discounters and also compete with fewer NBs. At the same time, nondiscount formats usually offer more price tiers (e.g., economy, standard, and premium) and variants (e.g., organic, locally produced, or diet) for NBs as well as PLs within a product category than discounters ([27]; [35]). As such, PL and NB assortments differ structurally between discounters and nondiscounters, and we account for these differences by the combined consideration of these brand types (PLs and NBs) and store formats (discounter and nondiscounters). Thus, by crossing the two brand types and store formats, we obtain a parsimonious, mutually exclusive, collectively exhaustive, and meaningful conceptualization of households' shopping basket allocation. Altogether, the three shopping basket value outcomes and the four shopping basket allocation outcomes holistically cover the essence of households' CPG shopping behavior. Control VariablesWe control for household demographics, which play an important role in explaining differences in shopping baskets (e.g., [46]). In addition, we control for a set of household psychographics: price and quality consciousness, deal proneness, and out-of-home consumption preference. Psychographics control for household heterogeneity that is not necessarily captured by demographics because, for example, even households with high income may be deal-savvy or highly price-conscious ([ 2]). Such psychographics strongly resemble consumer traits that are largely stable in short-term environmental changes but also reflect long-term societal trends, cultural developments, and the process of consumer aging ([54]).As prior research has shown, retailers and manufacturers also react to macro conditions by adapting their marketing mix (e.g., [13]; [42]). We are less concerned with this relationship per se but control for adjustments in the marketing mix owing to their substantial influence on households' shopping behavior. Data Research ContextAs presented in Table 2, the German CPG retail market is split rather evenly between discounters and nondiscounters, with discounters accounting for 45% of revenues and 43% of stores.[ 6] Discounters in Germany are usually located in easily accessible and densely populated areas ([64]) and have an average sales area of 779 m2, which is slightly smaller than a typical supermarket (982 m2) and substantially smaller than superstores (3,461 m2) and hypermarkets ( 7,051 m_SP_2_sp_) ([21]). However, they carry far fewer stockkeeping units (SKUs) and offer a much larger PL share (65.6%) that typically outweighs NBs ([25]). Discounters' PL shares may vary by retailer (e.g., Aldi: 96%, Lidl: 61%), but even discounters with a relatively strong focus on NBs have a substantially larger PL share than nondiscounters (e.g., Penny: 42%, Netto Marken-Discount: 40% vs. nondiscounters: 21.2%). Discounters offer substantially lower prices but also limited service, as is evident from a study by the German Institute for Service Quality ([16]), which scores stores on the basis of their prices and service (higher scores mean better prices/service). The tested discounters received substantially higher (lower) price (service) scores than their nondiscounter counterparts. Discounters' focus on functionality rather than service is also reflected in their high space productivity (i.e., revenues per store space). Similarly, annual revenues per SKU are considerably higher in discounters (€30.4 million) than in nondiscounters (€3.2 million) ([21]).These data underline the similarity of the nondiscount store formats and their dissimilarity to discounters for the German market from both retailer and consumer perspectives, thus corroborating the previously introduced conceptual distinction between these two groups. Interestingly, this distinction is also reflected in the branding of different retail store formats in the German CPG market. For example, two major German retail companies—the REWE Group and the EDEKA Group—operate both regular supermarkets and superstores under their REWE and EDEKA umbrella brands. Their hypermarkets (REWE Center and E-Center) also incorporate many of the same brand cues. In contrast, their discounters—Penny and Netto Marken-Discount—carry retail brands that are completely distinct from their respective umbrella brand. Data SourcesTo reflect the particularities of the German CPG market, the data set draws on several sources and combines information across distinct aggregation levels. The primary data source is the ConsumerScan panel provided by GfK Germany, which includes transaction and survey data for panelists at the individual household level. As a major advantage, this panel covers private consumption comprehensively and representatively, spanning all German CPG retailers, including discounters that typically do not offer data for market research purposes through retail panels.[ 7] This data availability is particularly crucial, considering the substantial market share of discount stores in Germany (see Table 2). The panel also contains survey data for all panelists, based on self-reported annual demographic information (age, household size, and income) and psychographic measures (e.g., price and quality consciousness). In addition, we obtain data on weekly advertising spending that covers all major channels as well as all manufacturers and retailers from the Nielsen Company. Finally, we add publicly available GDP data from the Federal Statistical Office that indicate the aggregate economic condition. We thus build a unique, encompassing data set that combines behavioral measures with survey-based household demographics and psychographics, macroeconomic measures, and brand- and store-level advertising spending. Data PreparationThe initial raw data set from the ConsumerScan panel is composed of household characteristics and purchase decisions by 85,428 unique households—with 24,000 to 37,000 in any given year—that made more than 13 million shopping trips and 48 million purchases between 2006 and 2013. Purchase information is available at the SKU level for 39 product categories from 467 retailers, most of which maintain multiple stores. These products include alcoholic and nonalcoholic beverages (e.g., beer, fruit juice) and food (e.g., cereals, pasta, ice cream) as well as nonfood items (e.g., deodorants, detergents, toilet paper). For each purchased item, we have access to the unique product code, date and place of purchase, price paid, identifiers for store format, brand type, and temporary price reductions as well as specific product characteristics such as brand and manufacturer name and package size. In preparing these data, we took several cleaning and filtering steps at the purchase record and household levels. In particular, we eliminated inconsistent transaction records and households that did not remain in the panel for the entire period. This procedure is conservative and in line with prior literature (e.g., [17]). Data cleaning involved the following steps: ( 1) Removal of all cases with missing values, ( 2) removal of all cases with unusually high (more than four times the median) or unusually low (less than one-fourth the median) prices at the SKU level, ( 3) removal of all cases with SKUs purchased fewer than 25 times in the entire period.These data-cleaning steps preserved 97.4% of all observations and 96.1% of all expenditures. To exploit the analytical potential of panelists with long purchase histories and extensive survey information, we retain only households with at least one transaction per quarter (7,441 households) and full survey information from 2006 to 2013, leaving 5,101 unique households.To avoid structural differences between samples, we compared the filtered households with the remaining households in terms of shopping outcomes and demographics. Overall, we find only marginal deviations in purchase behaviors and demographic composition. Thus, we assume that the selected households with complete purchase histories are not structurally different from households with shorter or incomplete purchase histories. We also compare the filtered sample with information from the 2006 Microcensus ([15]). As in other studies using this type of data (e.g., [17]), our sample is only slightly older and has higher income, fewer single households and more two-person households, and fewer children. However, we find a sizable overlap in the distributions of the demographic variables, and we control for these demographics at the individual household level throughout the empirical analyses. Therefore, a lack of sample representativeness is not an issue. Detailed comparisons of the household samples are available in Web Appendix A. Variable Operationalization Shopping basket valueIn line with the conceptual framework, we consider multiple dependent variables to capture the two domains of shopping outcomes as exhaustively as possible. The first domain relates to a household's shopping basket value—that is, how much is spent by the focal household, as represented by three dependent variables. TotalSpendinght relates to the total CPG spending of household h at time t, measured in euros. PurchaseVolht refers to the total CPG purchase volume of household h at time t, again measured in euros. Note that a household's shopping basket typically contains products with different volume units (e.g., liters, grams, pieces) that cannot directly be combined into a total volume measure. Therefore, we follow [46] and use an average category price per volume unit from a one-year (here: 2006) initialization period and multiply it by the total equivalent volume units purchased in each category. This enables us to aggregate the purchase volume across categories. Accordingly, the resulting variable is expressed in euros. We note that any variations in this variable are caused by changes in volume and not changes in prices being paid that may result from switching between brand types and store formats. Therefore, we are able to clearly disentangle households' consumption (volume) from households' spending (value) of CPG purchases. Finally, PriceIndexht is constructed as an index ([ 1]) and compares, for household h at time t, the costs of the shopping basket at average market prices with the actual costs incurred by the household. These price differentials are considered for identical goods identified at the SKU level. As such, they do not reflect differences in the quality of goods purchased but whether specific SKUs in the basket were purchased at cheaper prices (e.g., through temporary price promotions). An index greater than 1 implies that a household paid more than average for the specific goods in its basket, and a value less than 1 implies that the household paid less than average. This variable, therefore, reflects households' cherry-picking behavior ([23]) and is not related to households' switching behavior between different brand types or price tiers. We provide further details on the construction of purchase volume and the price index in Web Appendix B. Shopping basket allocationThe second domain of shopping outcomes relates to a household's shopping basket allocation between combinations of brand types and store formats—that is, it captures how the household is allocating its budget. We measure this allocation with the dependent variable Spendingbht in terms of household h's total spending (in euros) at time t on the respective brand type–store format combination b: (b = 1) PLs in discounters (PLDisc), (b = 2) NBs in discounters (NBDisc), (b = 3) PLs in nondiscounters (PLNonDisc), and (b= 4) NBs in nondiscounters (NBNonDisc). Altogether, these four spending variables encompass each household's total spending.[ 8] Macro conditionsThe focal explanatory variables represent a household's individual micro conditions and the overall macro conditions. At the macro level, we first apply the Christiano–Fitzgerald random-walk filter ([ 9]) to the log-transformed quarterly GDP data to assess the general state of the economy itself. The extracted cyclical component of the GDP series constitutes the deviation from the economy's underlying long-term growth trend. Thus, periods with increases in the cyclical component indicate economic expansions, whereas periods with decreases indicate economic contractions. However, it is important to account for not only different phases of the business cycle but also the severity that comes with the depth of up- and downturns (e.g., [55]). To do so, we follow prior research ([43]; [58]) and define the magnitude of an expansion (contraction) period relative to the prior trough (peak) of the cyclical series, or the point in the cyclical component at which the quarter-on-quarter growth turns from negative to positive (from positive to negative). Therefore, we operationalize the symmetric measure of the business cycle (BCyclet) as changes in the cyclical component of GDP at time t relative to the prior peak or trough. In addition, to study potential asymmetries of macro conditions, we use the same operationalization to construct two semidummy variables that separately capture periods with an increase in the cyclical component relative to the prior trough as expansions (Expansiont) and periods with a decrease relative to the prior peak as contractions (Contractiont) of the economy. That is, Expansiont (Contractiont) takes values increasing (decreasing) with economic expansion (contraction) and 0 values during contractions (expansions).[ 9] Micro conditionsAt the individual level, micro conditions reflect a household's financial situation, captured by the household's monthly net income. The original income data included in the ConsumerScan panel are at a yearly aggregation level and are measured in 16 income brackets.[10] We construct a continuous income variable by taking midpoint values of these brackets in euros and transform the resulting series to a quarterly sequence (the aggregation level of the shopping outcome variables) by applying linear interpolation for each household.[11] We adjust income for inflation using the consumer price index. In line with the operationalization of macro conditions, we define micro conditions as a household's income change (IncomeChangeht) relative to its previous income peak or trough. This step enables us not only to capture income changes from one period to another but also to take the higher magnitude into account, which results from income changes along consecutive periods. Furthermore, we construct semidummy variables for positive (IncomeGainht) and negative (IncomeLossht) income changes that are equivalent to the operationalization of asymmetric measures at the macro level. Thus, IncomeGainht (IncomeLossht) is defined as the difference of the log-transformed net income at time t and the prior log-transformed income trough (peak), allowing us to account for the accumulated magnitude of income gains and losses over time. IncomeLossht and Contractiont are converted to positive values for ease of interpretation. Control variablesAs control variables, we include a household's value of the dependent variable from a one-year (here: 2006) initialization period t0 (TotalSpendinght0, PurchaseVolht0, PriceIndexht0, and Spendingbht0). In addition, we include demographics to control for household heterogeneity regarding household size (HhSizeht), age of the household head (Ageht), presence of children (Kidsht), and employment status (Unemployedht). We also include psychographic variables to control for heterogeneity in shopping-related traits and preferences in terms of quality (QualConsht) and price consciousness (PriceConsht), deal proneness (DealProneht), and preferences for eating out (EatOutht). While QualConsht and PriceConsht are based on fixed constructs provided by the GfK, we construct DealProneht and EatOutht from several survey questions. The associated items, factor loadings, and Cronbach's alphas appear in Web Appendix B, Table WB1. Demographic and psychographic controls are measured at an annual level, and we transform the psychographics to a quarterly series using linear interpolation.Finally, we include controls for the marketing mix. We compute this group of variables at different levels of aggregation as appropriate for each set of models and use household-specific product category weights to incorporate household heterogeneity ([46]). Except for the advertising measures, marketing-mix controls are based on transaction information from the ConsumerScan panel. Because we construct the marketing-mix controls on the basis of observed household transactions, we use only transaction information (e.g., prices, SKUs, price-promoted SKUs) of households that are not part of the analysis sample. Thus, we avoid potential biases resulting from nesting the transactions of these focal households into the marketing-mix controls. For the basket value models, we construct absolute measures for price (Priceht), assortment size (Assortht), price promotions (Promoht), PL share in assortments (PctPLht), and advertising spending of NBs (AdvNBt) and of store format j (with j = 1 for discounters and j = 2 for nondiscounters) (AdvStorejt), which includes advertising spending on retailer brands as well as their PLs. For the basket allocation models, the marketing-mix variables for each brand type–store format combination are computed relative to the average across all brand type–store format alternatives. Thereby, we parsimoniously account for potential cross-effects. In particular, we construct relative measures for price (RelPricebht), assortment size (RelAssortbht), price promotions (RelPromobht), PL share in assortments (RelPctPLjht), and advertising spending at the store level (RelAdvStorejt). Because advertising spending at the brand level refers to NBs only, we use it as an absolute measure.We adjust all spending and price variables for inflation using the consumer price index and advertising spending using the GDP deflator. Table 3 presents an overview of all variables and their operationalization, while Web Appendix B shows the detailed construction of the marketing-mix variables. Tables 4 and 5 provide the descriptives and correlations for variables in the shopping basket value models and shopping basket allocation models, respectively. Note the small correlations between micro and macro conditions, in support of the conceptualization of differential effects.GraphTable 3. Variable Operationalization. Variable GroupVariableOperationalizationShopping outcomesTotalSpendinghtTotal spending (in euros) by household h at time t.PurchaseVolhtTotal purchase volume by household h at time t measured in constant euros.PriceIndexhtIndex of prices paid by household h at time t.SpendingbhtSpending (in euros) by household h at time t for brand type–store format combination b.Micro- and macro conditionsBCycletDifference between the cyclical GDP component at time t and the prior trough/peak.ExpansiontDifference between the cyclical GDP component at time t and the prior trough.ContractiontDifference between the cyclical GDP component at time t and the prior peak.IncomeChangehtDifference between the log-transformed monthly net income (in euros) of household h at time t and the prior income trough/peak.IncomeGainhtDifference between the log-transformed monthly net income (in euros) of household h at time t and the prior income trough.IncomeLosshtDifference between the log-transformed monthly net income (in euros) of household h at time t and the prior income peak.Marketing-mix controlsPricehtNet price facing household h at time t.RelPricebhtRelative net price of brand type–store format combination b facing household h at time t.AssorthtNumber of unique SKUs facing household h at time t.RelAssortbhtRelative number of unique SKUs of brand type–store format combination b facing household h at time t.PromohtNumber of price-promoted SKUs facing household h at time t.RelPromobhtRelative number of price-promoted SKUs of brand type–store format combination b facing household h at time t.PctPLhtPercentage share of PL SKUs in the assortment facing household h at time t.RelPctPLjhtRelative share of PL SKUs in assortment of store format j facing household h at time t.AdvStoretStore-level advertising spending (in million euros) at time t.RelAdvStorejtRelative store-level advertising spending of store format j at time t.AdvNBtAdvertising spending (in million euros) of NBs at time t.Demographic controlsHhSizehtNumber of persons in household h at time t.AgehtAge of the leading person in household h at time t.KidshtDummy variable, 1 if children are present in household h at time t, 0 otherwise.UnemployedhtDummy variable, 1 if the principal earner of household h is unemployed at time t, 0 otherwise.Psychographic controlsQualConshtScale indicating quality consciousness of household h at time t; provided by GfK.PriceConshtScale indicating price consciousness of household h at time t; provided by GfK.DealPronehtFive-item scale indicating deal proneness of household h at time t.EatOuthtThree-item scale indicating preference for eating out of household h at time t.Time controlsTimetContinuous variable for time t.QuarterqtIndicator variable for quarter q of the year at time t.Other controlsCopulakhtGaussian copula for marketing-mix variable k to account for potential endogeneity.InvMillsbhtInverse Mills ratio to account for potential selection effects. 6 Notes: Items, factor loadings, and Cronbach's alphas for DealProne and EatOut are presented in Table WB1 of Web Appendix B.GraphTable 4. Descriptive Statistics and Correlation Matrix for Variables in the Shopping Basket Value Models. MSD1234567891011121314151617181920212223241. TotalSpending184.44114.4012. PurchaseVol183.06115.62.9413. PriceIndex.99.05−.02−.1514. BCycle1.054.50.00.01−.0115. Expansion2.422.69.01.02.00.8716. Contraction1.372.53.01.01.01−.85−.4917. IncomeChange51.93510.04.03.03−.01−.01−.02−.0118. IncomeGain176.70341.42.00.01−.01−.01−.05−.04.8019. IncomeLoss124.77315.41−.05−.04.00.01−.02−.03−.74−.19110. Price.99.06−.11−.16.02−.13−.10.11.02.02−.01111. Assort444.89113.02.15.13−.01.03−.03−.09.02.06.03.08112. Promo217.6555.96.04.02−.01.05−.08−.17.04.14.08.07.86113. PctPL.33.04−.17−.24.05−.03−.06.00.01.02.00.72.17.18114. AdvStore254.9827.22.03.02.00.02.17.15.00−.02−.01−.02−.03−.03−.04115. AdvNB294.8346.12.00.00.00.13.08−.15.01.04.04−.02.05.10−.01.25116. HhSize2.331.13.49.52−.14.00.01.01.07.05−.07.18.06−.01.03.01.00117. Age54.3612.07−.12−.13.07−.01−.04−.02−.10−.14.01−.18.02.10−.08−.02.01−.37118. Kids.18.38.19.21−.08.00.02.01.07.06−.04.25.03−.03.12.01.00.56−.54119. Unemployed.06.24−.06−.01−.01.01.01.00−.05.01.09−.01.00−.03−.02−.01−.01−.04−.07.02120. QualCons2.94.86.03−.07.12.00.00−.01.01−.01−.03.03−.01.03.05−.01.01−.04.11−.07−.10121. PriceCons3.14.93.00.13−.28.00.00.01−.01.02.03.02.02−.01−.03.01.00.14−.11.11.08−.39122. DealProne11.262.47.10.18−.28.00.01.01.02.02−.01−.02.00−.04−.04.01.00.18−.10.11.03−.10.38123. EatOut5.412.38−.07−.10.05.01.01−.01.05.07.00−.02−.01−.02−.01−.01.00−.07−.32.09.03.03−.05−.01124. Time−.05−.05−.01−.16−.27.00.06.20.12.05.25.61.05.08.19−.04.13−.05−.04.04−.02−.03−.011 7 Notes: Means and standard deviations are based on untransformed values, correlations are based on log-transformed variables except dummy variables. BCycle, Expansion, and Contraction are multiplied by 100 to be expressed in percentage deviations.GraphTable 5. Descriptive Statistics for Variables in the Shopping Basket Allocation Models. PLDiscNBDiscPLNonDiscNBNonDiscMSDMSDMSDMSDSpending45.4746.1023.6433.2912.9816.91102.3593.18Price.76.031.17.04.73.041.34.04Assort.74.12.66.05.46.082.15.17Promo.65.13.72.05.43.082.20.18PctPL1.46.041.46.04.54.04.54.04AdvStore1.09.071.09.07.91.07.91.07AdvNB294.8346.12294.8346.12294.8346.12294.8346.12 8 Notes: PLDisc = private labels in discounters; NBDisc = national brands in nondiscounters; PLNonDisc = private labels in nondiscounters; NBNonDisc = national brands in nondiscounters. ModelWe define regression models for the individual shopping outcomes and estimate them jointly in a system of seemingly unrelated regressions. To control for unobserved household heterogeneity, we use a random intercept specification. The three shopping basket value equations for total spending, purchase volume, and price index, as well as the four basket allocation equations for spending across four brand type–store format combinations, are specified in log-log form (excluding the dummy variables Kidsht, Unemployedht, and Quarterqt). This approach allows for an interpretation of coefficients as elasticities and accounts for the fact that households vary substantially in magnitudes of the dependent variables ([46]).[12] We first assume symmetry in each model with regard to the focal micro- and macroeconomic measures, where MacroEcont = δ1BCyclet and MicroEconht = δ2IncomeChangeht. Subsequently, we introduce asymmetric effects, where MacroEcont = γ1Expansiont + γ2Contractiont and MicroEconht = γ3IncomeGainht + γ4IncomeLossht.We provide the specifications for the shopping basket value and shopping basket allocation models subsequently. Shopping basket value modelsThe three shopping basket value models are defined as follows: ln(BasketValueaht)=αha+MacroEcont+MicroEconht+α2aln(BasketValueaht0)+α3aln(Priceht)+α4aln(Assortht)+α5aln(Promoht)+α6aln(PctPLht)+α7aln(AdvStoret)+α8aln(AdvNBt)+α9aln(HhSizeht)+α10aln(Ageht)+α11aKidsht+α12aUnemployedht+α13aln(QualConsht)+α14aln(PriceConsht)+α15aln(DealProneht)+α16aln(EatOutht)+α17aln(Timet)+∑24κq−1aQuarterqt+∑kωkaCopulakht+εaht, Graph( 1)where BasketValueaht is (a = 1) TotalSpendinght, (a = 2) PurchaseVolht, (a = 3) PriceIndexht,  αha=α0a+μha,μha∼N(0,σμ2)  , k is marketing mix variable k (k = 1, ..., K), q is quarter q in a given year (q = 1, ..., 4), and t is time period t at a quarterly level (t = 1, ..., T).We control for potential endogeneity in the marketing-mix variables resulting from unobserved shocks by including Gaussian copulas ([47]), which directly model the joint distribution of the potentially endogenous regressor and the error term through control function terms. An advantage of this method is that it does not require instrumental variables that may, as in our case given the number of marketing-mix variables across brand type–store format combinations, be difficult to find ([50]). A requirement is that the endogenous regressor is not normally distributed. Anderson–Darling tests and Kolmogorov–Smirnov tests confirm this nonnormality for all marketing-mix variables at p < .001. Given the large size of the sample, we also visually inspect quantile–quantile plots, which confirm nonnormality for all marketing-mix variables. The Gaussian copula for each marketing mix variable Xht for household h at time t is Copulaht = Φ-1[H(Xht)], where Φ-1 is the inverse distribution function of the standard normal and H(·) is the empirical cumulative distribution function of Xht. Shopping basket allocation modelsWe define the four models as follows: ln(Spendingbht)=βhb+MacroEcont+MicroEconht+β2bln(Spendingbht0)+β3bln(RelPricebht)+β4bln(RelAssortbht)+β5bln(RelPromobht)+β6bln(RelPctPLjht)+β7bln(RelAdvStorejt)+β8bln(AdvNBt)+β9bln(HhSizeht)+β10bln(Ageht)+β11bKidsht+β12bUnemployedht+β13bln(QualConsht)+β14bln(PriceConsht)+β15bln(DealProneht)+β16bln(EatOutht)+β17bln(Timet)+∑24κq−1bQuarterqt+∑kωkbCopulakbht+β18bInvMillsbht+εbht, Graph( 2)where  βhb=β0b+μhb,μhb∼N(0,σμ2)  , and the subscripts are as defined before.One issue with Equation 2 is that expenditures are zero where a household does not patronize a specific brand type–store format combination during a period. Considering only those observations with existing expenditures or adding a small constant may lead to biased estimates ([45]). This bias may be quite substantial in our case, where zero expenditures make up between 2.6% for NBs in nondiscounters and 20.8% for NBs in discounters of all the observations. To solve this issue appropriately, we follow the procedure for Type II Tobit models ([63], pp. 560–66). In a first step, we apply a probit model with a random intercept specification and pooled coefficients for brand type–store format choice. This approach allows for the fact that households may patronize multiple brand type–store format combinations. We use the same set of independent variables as in the basket allocation models and additional instrumental variables (average number of shopping trips and unique retailers visited, share of income spent on CPGs, and per capita CPG spending) for identification purposes. In a second step, we compute the inverse Mills ratio, InvMillsbht, based on the probit model results for each brand type-store format combination as InvMillsbht = φ(Xbht′η/Φ(Xbht′η), where φ is the standard normal density function, Φ is the standard normal cumulative distribution function, and η is the vector of parameters from the probit model. The inverse Mills ratio is then added for each brand type–store format combination as an additional independent variable in the basket allocation model to correct for interrelations between brand type–store format choice and spending. As before, we also add Gaussian copulas for all brand type–store format combination specific marketing-mix variables to account for potential endogeneity issues. Results Model Estimation and ValidationWe use Latent GOLD 5.1 ([59]) to estimate the seemingly unrelated regression system consisting of seven equations with a maximum likelihood approach. All the models converged before reaching the maximum number of iterations. Because we use data from 2006 for parts of the variable operationalization, we run the model on data from 2007–2013. For holdout validation, we randomly sample 500 households from the filtered data set and run the final estimations on the remaining 4,601 households. Starting with an intercept, time, and sample selection control model (Model 1), we sequentially add the dependent variable from the initialization period (Model 2); marketing-mix variables and endogeneity controls (Model 3); and demographic (Model 4), psychographic (Model 5), and symmetric micro and macro variables (Model 6). Finally, we replace the symmetric with the asymmetric micro and macro variables (Model 7). Table 6 provides an overview of the model-building process and fit statistics. Relying on the Akaike and Bayesian information criteria, Model 7 offers the best fit. We further scrutinize Model 7 for overfitting. We compare its mean squared errors and mean absolute errors between the estimation and holdout sample and find that they are very similar, showing no sign of potential overfitting.GraphTable 6. Model Building and Fit Statistics. ModelComponentsEstimation SampleParametersLLBICAICM1Intercept + Time + Sample Selection Controls−395,450791,524791,04874M2M1 + Dependent Variable from Initialization Period−287,496575,674575,15381M3M2 + Marketing Mix + Copulas−281,705564,801563,739165M4M3 + Demographics−273,889549,407548,165193M5M4 + Psychographics−270,205542,273540,852221M6M5 + Symmetric Economic Conditions−270,034542,051540,539235M7M5 + Asymmetric Economic Conditions−269,968542,036540,434249 9 Notes: LL = log-likelihood; BIC = Bayesian information criterion; AIC = Akaike information criterion. Note that only models M3–M7 can be compared to one an other as they incorporate the same set of instruments and vary only by their exogenous variables ([19]). Symmetric Effects of Micro and Macro Conditions on Shopping OutcomesAlthough the asymmetric model (Model 7) shows the best fit, we briefly present the results from the symmetric model specification (Model 6) to check for internal consistency across the two models. Table 7 provides an overview of all significant elasticities of micro and macro conditions on basket value and basket allocation measures. The complete results of the symmetric model are available in Web Appendix C, Table WC1. Overall, we find significant influences on household shopping behavior for changes in households' micro and macro conditions. However, the nature of these influences clearly varies.GraphTable 7. Overview of Significant Elasticities. Basket AllocationBasket ValueVariablePLDisc SpendingNBDisc SpendingPLNonDisc SpendingNBNonDisc SpendingTotal SpendingPurchase VolumePrice IndexSymmetric model (M6)BCycle−.70***−.63***.27***−.06*−.06*−.01*IncomeChange.08***.07***.06***Asymmetric model (M7)Expansion−.94***−.71***.52***−.01*Contraction.36**−.32*.51***.14**.11*IncomeGainIncomeLoss−.10**−.16***−.12***−.11*** 10 *p < .1.11 **p < .05.12 ***p < .01.13 Notes: The table illustrates only significant elasticities. PLDisc = private labels in discounters; NBDisc = national brands in nondiscounters; PLNonDisc = private labels in nondiscounters; NBNonDisc = national brands in nondiscounters. Complete results of the asymmetric Model 7 are provided in Table 8. Complete results of the symmetric Model 6 are provided in Table WC1 of Web Appendix C. Micro conditionsIn line with economic theory, we find significant positive elasticities of income change on shopping basket value in terms of total spending (δ = .07, p < .01) and purchase volume (δ = .06, p < .01). Given that these elasticities are very similar in size and both variables are representations of a household's shopping basket in euros featuring comparable means, we can deduce that the majority of the expenditure effect is merely driven by volume adjustments. In fact, these volume adjustments are mainly attributable to purchases of NBs in nondiscounters, as indicated by the significant positive elasticity of income change on NB spending in nondiscounters (δ = .08, p < .01). Importantly, we do not find any structural shifts in households' basket allocation in that households increase (decrease) spending for a specific brand type–store format combination and simultaneously decrease (increase) spending for another. Macro conditionsUnder changing macro conditions, the results are different. We find marginally significant negative elasticities of the business cycle on shopping basket value dimensions (i.e., total spending [δ = −.06, p < .1], purchase volume [δ = −.06, p < .1], and price index [δ = −.01, p < .1]). Though intuitively surprising, the results confirm previous studies showing countercyclical CPG spending behavior of households (in value and volume) along the business cycle (e.g., [46]). In addition, we also find several significant elasticities of the business cycle on households' shopping basket allocation. In particular, the elasticity of the business cycle on PL spending in discounters (δ = −.70, p < .01) and nondiscounters (δ = −.63, p < .01) is significantly negative, respectively, whereas it is significantly positive on NB spending in nondiscounters (δ = .27, p < .01). This finding indicates that, to some degree, households shift from PLs in discounters and nondiscounters to NBs in nondiscounters—and vice versa—when macro conditions change. Moreover, when shifting their basket allocation across brand types–store format combinations, households also tend to purchase items at lower prices, for example, through temporary price promotions, as indicated by the negative effect of macro conditions on the price index. Asymmetric Effects of Micro and Macro Conditions on Shopping OutcomesTable 8 shows the estimation results of the asymmetric model. For better comparability of the impact of micro and macro conditions, Figure 2 provides an overview of the asymmetric effects of micro and macro conditions on basket allocation and basket value at their respective mean values—specifically, 2.42 (1.37) for Expansiont (Contractiont) and €176.70 (€124.77) for IncomeGainht (IncomeLossht), which translates to 7.8% (5.5%) of mean income. The findings from the symmetric model are confirmed by the asymmetric model, although the asymmetric estimation results show that the underlying effects are not symmetric but differ strongly in terms of size as well as significance between beneficial and adverse conditions.Graph: Figure 2. Asymmetric elasticities at mean values for micro and macro conditions.GraphTable 8. Results of Asymmetric Model 7. VariableBasket AllocationBasket ValuePLDisc SpendingNBDisc SpendingPLNonDisc SpendingNBNonDisc SpendingTotalSpendingPurchaseVolumePriceIndexIntercept2.6310**(1.3047)−4.4137*(2.3163)−2.0639(1.3452)4.6453***(1.0345)−1.0209(2.6274).4872(2.6826).0460(.1136)Random intercept−.4634***(.0131)−.1435***(.0308).1604***(.0241).2319***(.0260).0279***(.0107)−.0194*(.0103)−.0006(.0006)Micro and Macro ConditionsExpansion−.9387***(.1925)−.0063(.2001)−.7139***(.1816).5186***(.1132).0198(.0546).0024(.0548)−.0083*(.0048)Contraction.3578**(.1496)−.3242*(.1968).5068***(.1602).0485(.1198).1357**(.0605).1086*(.0600).0015(.0050)IncomeGain−.0341(.0413).0479(.0471).0284(.0428).0112(.0352).0183(.0206).0153(.0205).0011(.0014)IncomeLoss−.0977**(.0447).0124(.0532)−.0508(.0463)−.1567***(.0380)−.1208***(.0224)−.1053***(.0219)−.0005(.0017)ControlsDV(t = 0).3535***(.0245).2117***(.0116).3187***(.0135).5997***(.0207).5910(.0148).5719***(.0152).7263(.0162)(Rel)Price−.6183(1.2878)1.8388(2.1456)−1.5194(1.7101)−.2376(1.5296).3850(.7840).0194(.8050)−.0384(.0863)(Rel)Assort−.4142**(.1789).4310(1.1226)−.2307(.2178)1.5306***(.3764).4395*(.2495).1980(.2603).0239(.0169)(Rel)Promo.1519**(.0725)1.2602(3.2248).4613*(.2525).8275*(.4941)−.4199(.2621)−.2884(.2691)−.0300**(.0141)(Rel)PctPL−7.0982***(2.0868)5.3710*(2.9790)−1.4206**(.6413)−.1210(.2981)−.5991***(.0864)−.5763***(.0930)−.0025(.0096)(Rel)AdvStore−.2452**(.1031).0052(.1281).5303***(.1342)−.2811***(.0778).1275***(.0173).1022***(.0194).0005(.0022)AdvNB.1265***(.0453).2301***(.0763).1619***(.0594)−.2903***(.0418)−.0396*(.0223)−.0488**(.0230)−.0008(.0022)HhSize.3706***(.0479).4305***(.0297).3406***(.0272).3722***(.0276).3077***(.0154).3250***(.0164)−.0018**(.0008)Age−.1958***(.0689)−.1135*(.0632)−.1397**(.0543).1270***(.0491).0078(.0228)−.0085(.0224).0039**(.0019)Kids−.0057(.0308)−.0594*(.0334)−.0104(.0298)−.0593**(.0234)−.0148(.0133)−.0086(.0135)−.0002(.0010)Unemployed−.0617**(.0274)−.0092(.0364).0563(.0342)−.1157***(.0278)−.0533***(.0167)−.0169(.0165)−.0010(.0012)QualCons−.0224(.0262).0286(.0278)−.1167***(.0253).0860***(.0200).0291***(.0109)−.0175(.0108).0012(.0008)PriceCons.0654***(.0216)−.0814***(.0275).0516**(.0231)−.1366***(.0177)−.0583***(.0109).0018(.0110)−.0098***(.0009)DealProne.0196(.0349).2549***(.0402)−.1277***(.0349).0485*(.0268).0499***(.0153).0830***(.0152)−.0158***(.0012)EatOut−.0581**(.0243)−.0373(.0258)−.0100(.0232)−.0282(.0186)−.0273**(.0107)−.0360***(.0105).0017**(.0008)Time−.0197(.0121).0781***(.0112).0382***(.0123)−.0425***(.0090)−.0233***(.0051)−.0270***(.0056)−.0001(.0006)Quarter 2.0327*(.0188)−.0842***(.0310)−.0355(.0220).1078***(.0174).0379***(.0098).0440***(.0102).0005(.0010)Quarter 3.0154(.0134)−.0695***(.0219)−.0559***(.0157).0260**(.0102).0002(.0066).0089(.0070).0005(.0007)Quarter 4−.0026(.0136)−.0076(.0231)−.0193(.0168).1187***(.0113).0258***(.0063).0228***(.0065)−.0001(.0006)Copula (Rel)Price.0483(.0587)−.0626(.0717).0594(.0937).0506(.0458)−.0243(.0446)−.0255(.0459).0019(.0049)Copula (Rel)Assort.0010(.0284).0143(.0742).0191(.0317)−.1207***(.0297)−.0719(.0603)−.0222(.0629)−.0041(.0041)Copula (Rel)Promo.0785***(.0136)−.0515(.2441)−.0289(.0420)−.0212(.0414).0829(.0659).0607(.0676).0051(.0036)Copula (Rel)PctPL.2990***(.0623)−.0955(.0824).1306***(.0470).0149(.0228).0431***(.0103).0201*(.0109).0015(.0012)Copula (Rel)AdvStore.0063**(.0028).0001(.0048)−.0330***(.0061).0074**(.0033)−.0017(.0012)−.0023*(.0014).0003**(.0001)Copula AdvNB−.0057*(.0031)−.0018(.0047)−.0124***(.0045).0027(.0028)−.0042***(.0016)−.0031**(.0015).0001(.0001)InvMills.1907(.1656)−.2506***(.0703)−.0832(.0734)−.0260(.2121)N131,566113,092121,787139,163142,828142,828142,828Pseudo-R2.59 14 *p < .1.15 **p < .05.16 ***p < .01.17 Notes: PLDisc = private labels in discounters; NBDisc = national brands in nondiscounters; PLNonDisc = private labels in nondiscounters; NBNonDisc = national brands in nondiscounters. Standard errors are in parentheses. Micro conditionsRegarding micro conditions, we again find that micro conditions primarily have an impact on households' shopping basket value but do not cause shifts in households' shopping basket allocation. However, the results reveal substantial asymmetries between beneficial and adverse micro conditions. Most notably, income gains have no effect on households' basket value or basket allocation; only income losses show significant effects. More precisely, a 1% loss in income decreases total spending and purchase volume by.12% (p < .01) and.11% (p < .01), respectively. Owing to the similar size of the elasticities, we can again assume that expenditure reductions are largely driven by volume reductions.[13] Given that income losses show no effect on households' price index, we can rule out the notion that expenditure reductions stem from households' shopping for lower prices.Importantly in the context of income losses, we also see no evidence that households shift their basket allocation to less expensive brand type–store format combinations. Rather, we find significant negative elasticities of income losses only on NB spending in nondiscounters (γ = −.16, p < .01) and PL spending in discounters (γ = −.10, p < .05), respectively. Thereby, we can conclude that the adjustments in purchase volume—and subsequently total spending—predominantly stem from abandoning NBs in nondiscounters and PLs in discounters when income losses occur. Instead of shifting to cheaper store formats, brand types, or both, households give up the relatively more expensive NBs in nondiscounters without substituting them with cheaper alternatives such as NBs in discounters or PLs in general. This lack of substitution is also true for PLs in discounters, but in this case options for shifting to even cheaper alternatives to reduce spending are limited, and therefore, volume adjustments are households' last resort. That is, households' primary means of coping with adverse micro conditions is to reduce expenditures on specific brand types and store formats and thereby reduce shopping basket value (i.e., spending less by purchasing lower volumes) rather than adjusting basket allocation by shifting to cheaper brand types or store formats. Macro conditionsIn contrast to adverse micro conditions (i.e., income losses), economic contractions not only have an impact on households' shopping basket value but also cause shifts in basket allocation. With regard to basket value, we find a significant increase in total spending and a marginally significant increase in purchase volume when the economy contracts: a 1% decrease in GDP, compared with its prior peak, increases total spending by.14% (p < .05) and purchase volume by.11% (p < .1). As already indicated for the symmetric model, previous studies also find countercyclical buying behavior of households during adverse macro conditions ([46]).[14] The results confirm and extend these findings by showing that increased total spending and purchase volume are not the only effects during economic downturns, as contractions also cause shifts of households' shopping basket allocation. In particular, we find significantly positive elasticities of contractions on PL spending in discounters (γ = .36, p < .05) and nondiscounters (γ = .51, p < .01), respectively; as well as a marginally significant negative elasticity of contractions on NB spending in discounters (γ = −.32, p < .1). These findings suggest that households shift from NBs to PLs during unfavorable macro conditions. Although previous studies find comparable changes (e.g., [17]; [43]), the combined results further illustrate one important phenomenon: even though households purchase PLs to a greater extent, they increase total spending and purchase volume. Moreover, the results suggest that by switching from NBs to PLs, NBs are not affected by economic downturns per se, but only in the context of discounters. That is, we only find the contraction elasticity of NB spending in discounters to be marginally significant and negative.The estimated elasticities during economic expansions further substantiate that changing macro conditions cause shifts in households' shopping basket allocation. Inversely to contractions, we find significant negative elasticities of expansions on PL spending in discounters (γ = −.94, p < .01) and nondiscounters (γ = −.71, p < .01), respectively. At the same time, we find a significant positive effect on NB spending in nondiscounters when the economy expands (γ=.52, p < .01). In addition, the results show a marginally significant and negative elasticity of an expansion on the price index (γ = −.01, p < .1). This result complements the findings on households' shifts from PLs in discounters and nondiscounters to NBs in nondiscounters during favorable economic times. In fact, to keep their purchase volume and total expenditures steady while shifting to more expensive NBs, households seem to actively seek price-promoted items to keep the prices they pay low.Overall, the results show major differences in the effects of micro and macro conditions on households' shopping behavior. While favorable micro conditions show no effect at all, adverse micro conditions lead households to reduce expenditures for specific brand types and store formats, resulting in lower total spending and purchase volumes. In contrast, favorable and unfavorable macro conditions primarily result in shifts of shopping basket allocation. These results highlight the importance of separating micro from macro conditions to identify their unique properties, effects, and implications. Effects of Control Variables on Shopping OutcomesAlthough the control variables included in the asymmetric Model 7 are not of primary interest, they are important to rule out rival explanations and thus to support the causal interpretability of the main results. Therefore, we briefly summarize them here; a more detailed discussion can be found in Web Appendix C. For the most part, when significant, the effects of the included control variables are intuitive and in line with prior research. Marketing-mix variablesAs expected, we find a marginally significant positive effect of assortment size (in terms of unique SKUs) on total spending and a significant positive effect on NB expenditures in nondiscounters. We also find several effects of promotion activity (in terms of unique SKUs sold on promotion): a negative effect on the price index, a marginally significant positive effect on NB spending in nondiscounters, a positive effect on PL spending in discounters, and a marginally significant positive effect on PL spending in nondiscounters. It is noteworthy that the effects for PLs are of smaller magnitude and confirm prior research showing that retail promotions are less positive for PLs than for NBs ([57]). We also find that the share of unique PL SKUs in the total SKU assortment has a negative effect on total spending and purchase volume, suggesting that focusing too strongly on PLs can have unfavorable consequences for retailers (e.g., [ 2]). Finally, advertising at the store level has the expected positive effect on total spending, purchase volume, and PL spending in nondiscounters, while NB advertising has an expected positive effect on NB spending in discounters.However, we also note that some of the effects are counterintuitive. This is particularly true for the negative effects of assortment size and PL share in assortments, negative own-advertising effects, and positive cross-advertising effects as well as the absence of significant price effects. Varying perceptions of PLs and NBs in assortments (e.g., [ 5]; [14]; [34]), underlying advertising spillover effects ([ 4]), or potential difficulties when measuring advertising effects ([51]; [52]) may provide reasonable explanations for these findings. Counterintuitive marketing-mix coefficients may, however, also be caused by the aggregation level of the data (quarterly, national-level aggregation across many individual brands, retailers, and product categories). Demographic variablesAs expected, we find that larger households tend to spend more across all four brand type–store format combinations, spend more in total, purchase larger volumes, and maintain a lower price index. Older households typically spend less on PLs in general as well as spend marginally significantly less on NBs in discounters, but more on NBs in nondiscounters while exhibiting a higher price index. Furthermore, the results suggest that households with children spend less on NBs in nondiscounters and marginally significantly less on NBs in discounters, respectively. Households that suffer from unemployment of the main breadwinner tend to spend less in total, corresponding to fewer expenditures on both NBs in nondiscounters and PLs in discounters. Psychographic variablesIn terms of psychographics, the analyses reveal many significant effects, generally underscoring the importance of accounting for such types of consumer characteristics ([ 2]). In particular, we find that quality-conscious households tend to spend more in total, more on NBs in nondiscounters, and less on PLs in nondiscounters. In comparison, price-conscious households typically spend more on PLs and less on NBs in general, spend less overall, and exhibit a lower price index. Deal-prone households, furthermore, spend more in total, purchase larger volumes, exhibit a lower price index, and spend less on PLs in nondiscounters, but significantly more on NBs in discounters and marginally significantly more in nondiscounters. Finally, households with preferences for eating out tend to spend less overall, purchase lower volumes, but exhibit a higher price index and typically show lower spending for PLs in discounters. Robustness ChecksWe perform several robustness checks to confirm the validity of the findings by applying alternative measures and indicators for micro and macro conditions. First, we use the growth rate of real GDP (e.g., [38]; [46]) and an index of consumer confidence (e.g., [ 3]) to assess the general state of the economy. To a large extent, the results are consistent in significance, direction, and magnitude with the main symmetric model (Model 6). Second, we use first-difference specifications of micro conditions rather than differences relative to prior income peaks and troughs as in the main asymmetric model (Model 7). All effects are consistent in significance and direction, even though the elasticities are of a higher order of magnitude. Third, we introduce an individual-level measure of a household's perceived financial situation into both main models. This measure captures changing perceptions of micro conditions that are not reflected in household income (e.g., wealth). Controlling for individual financial perceptions does not alter the findings regarding income, and we can confirm all effects to be consistent in terms of significance, direction, and the order of magnitude. All significant effects of the financial perception measure itself are in line with economic theory. We present and discuss these results in greater detail in Web Appendix C. DiscussionMicro and macro conditions have significant effects on households' shopping behavior and outcomes that, by extension, may affect firm performance of retailers and manufacturers. By observing shopping basket allocation across brand types and store formats as well as shopping basket value in terms of total spending, purchase volume, and an index of prices paid, this research provides an extensive analysis of how (through shopping basket allocation) and how much (through shopping basket value) households adjust the various facets of their CPG shopping behavior. Thus, we distinguish the effects caused by micro conditions in terms of income and macro conditions in terms of the business cycle. In addition, we account for possible asymmetries between adverse and beneficial conditions. These findings, based on a rigorous modeling approach and longitudinal field data, have important diagnostic and normative value for managers and contribute to previous research on business cycle effects. We provide an overview of the results and associated implications in Table 9.GraphTable 9. Overview of Results and Implications. OutcomesMain FindingsInterpretation and ImplicationsShopping basket allocationPL discounter spendingMoves countercyclically with macro conditions, decreasing in expansions and increasing in contractions. Decreases with adverse micro conditions.As social acceptance of and demand for PLs increase during contractions, discounters can narrow their price gap to NBs. This allows for more profitable price reductions that discounters should deploy cyclically to counteract shifts to nondiscounters and NBs during expansions and adverse micro conditions. Soft discounters should extend their PL portfolio during contractions.NB discounter spendingMoves cyclically with macro conditions, decreasing substantially in contractions.Buffer discounters' and manufacturers' revenue losses during adverse micro conditions. Brand managers should extend their portfolio to discounters in these conditions to counteract losses from NBs sold in nondiscounters. Especially hard discounters may profit from a larger NB portfolio.PL nondiscounter spendingMoves countercyclically with macro conditions, decreasing in expansions and increasing in contractions.Allow nondiscounters to grow revenues even during contractions. Nondiscounters can use this opportunity to extend their PL portfolios to new product categories and price-tiers and strengthen their branding to counteract shifts back to NBs during expansions. As they are unaffected by increasing budget constraints, nondiscounters may adjust prices countercyclically to reap additional revenues during contractions and defend against NBs by deploying price reductions during expansions.NB nondiscounter spendingMoves cyclically, increasing during expansions. Decreases with adverse micro conditions.Are affected the strongest by adverse micro conditions. Manufacturers and nondiscounters can react to this through status appeals in their communication. As households do not switch due to budget constraints, marketers should not waste budgets on price promotions but provide ""cheap"" mechanisms that provide consumers with a sense of control and frugality such as loyalty and reward programs or (digital) store fliers.Shopping basket valueTotal spendingGrows with adverse macro conditions. Shrinks with adverse micro conditions.As long as households are not affected at a micro level, they increase their purchased volumes and total spending during contractions. Managers can leverage households' increased consumption and cognitive load from shifts in spending through larger package sizes and in-store promotions. Measures that provide a sense of control and frugality such as loyalty programs or quality and status appeals may further increase compensatory consumption. During expansions, retailers and manufacturers should utilize the increased deal proneness and price savviness through price promotions and couponing.Purchase volumeGrows with adverse macro conditions. Shrinks with adverse micro conditions.Price indexGrows with adverse macro conditions. The results uncover and juxtapose the specific effects of micro and macro conditions on shopping behavior. We find that both micro and macro conditions have pronounced effects on households' shopping behavior that are distinct from one another and asymmetric for positive versus negative conditions. Some findings are especially intriguing: micro conditions affect only households' overall consumption levels, whereas macro conditions also lead to structural shifts in households' budget allocation across brand types and store formats. In addition, during changing macro conditions, household adjust their shopping behavior even if they are not affected financially (as we control for income). In this section, we first summarize the results and subsequently discuss potential underlying psychological and sociological mechanisms before addressing interaction effects and asymmetries. Micro ConditionsAlthough no significant adjustments in shopping basket allocation or value emerge for income gains, income losses lead to a general decline in CPG expenditures. This drop is largely driven by households purchasing less and thus spending less. The overall decrease in consumption specifically affects PLs purchased in discounters and NBs purchased in nondiscounters. These findings show that, rather intuitively, budgetary constraints lead to decreased consumption, adding to extant research that has mostly taken a spending perspective (e.g., [38]). However, the absence of structural shifts in households' budget allocation is noteworthy. Theoretically, households could also reduce spending by switching to a cheaper store format or brand type, but instead they generate savings primarily through volume reductions. Macro ConditionsIn contrast, changing macro conditions evoke structural shifts in households' basket allocation. During contractions, we see expenditures for NBs purchased in discounters being reallocated to PLs purchased in discounters and nondiscounters. While this seems intuitive, it is interesting to note that this shift is accompanied by a general increase in total spending driven by households buying more. In other words, even though households switch to PLs during contractions, they end up spending more in total.During expansions, households reallocate their purchases from PLs (purchased in nondiscounters as well as discounters) to NBs purchased in nondiscounters. Interestingly, we also find that total spending and volumes purchased remain unaffected at the same time, because households focus more on getting deals, as indicated by a decline of the index for prices paid. As such, households switch to a more expensive brand type during expansions although their budget remains constant (as we control for income), which seems to be feasible as they increasingly purchase products on price promotion. Plausible Mechanisms Underlying Micro and Macro EffectsSeveral theoretical mechanisms can explain our findings. First, the findings suggest that adverse macro conditions may have a societal impact that trickles down to individual households even if they are not affected at a financial level. In trying times, frugal consumption, such as buying PLs or shopping at discounters, seems to become more socially acceptable and even fashionable ([22]; [38]), which is in line with the shifts of budgets toward PLs in (non)discounters that we observe during contractions. Just as much as frugal consumption may become increasingly commonplace during contractions, purchasing NBs may become a societal norm and is required if households want to maintain their social standing during expansions ([38]). In accordance with that norm, households seem to drop PLs in favor of NBs in nondiscounters even though they have no increase in budgets, as we see in the results. They seem to accommodate this shopping behavior by being price-savvy, shopping products on price promotion. Price promotions may also offer a welcome justification for households to abandon the PLs they have adopted during prior contractions in favor of NBs.This reasoning is also consistent with the lack of shifts in the face of adverse micro conditions, as described previously. An income loss, independent of macro conditions, is first a personal hardship rather than one shared by society. Therefore, there is not a general move to and acceptance of PLs and discounters, as in the case of adverse macro conditions ([22]; [38])—households do not switch to these cheaper brand types or store formats but instead reduce their overall consumption. In addition, income losses may weaken self-confidence and, thus, awaken a desire to bolster one's social status ([31]; [53]), which may lead households to continue buying NBs while economizing on volume to accommodate their lower income.Another explanation for these findings may lie in households' perception of the nature of micro and macro conditions. While a nationwide or global contraction is beyond households' direct control, personal income can be influenced through concrete actions. This discrepancy in the ""mutability"" of the conditions leads to different reactions in households: whereas high-mutability conditions (here: micro conditions) result in high self-regulation, planning, and prioritizing, low-mutability conditions (here: macro conditions) elicit a desire for restoration of control ([ 7]; [31]). Adverse micro conditions lead households to self-regulate by reducing their overall consumption, whereas adverse macro conditions result in a desire to restore control through actions that are perceived as more frugal (i.e., purchasing PLs). Control-restoration behaviors are also associated with compensatory consumption, such as in the form of overspending and higher food intake ([ 7]; [44]), which may explain the overall increase in household spending and which is potentially aggravated by the lack of a budgetary constraint that would limit this behavior ([62]).Other explanations of the increased consumption may lie in households' shift to PLs, which usually are associated with larger package sizes and lower product prices and which have been shown to increase consumption ([ 6]; [61]). Similarly, these factors contribute to households' purchase of increased quantities when shopping in warehouse club stores ([ 4]). In addition, adding discounter visits to a shopping trip may increase households' spending owing to self-licensing and self-control depletion ([31]). Asymmetries and InteractionsLike previous studies in the field, we find asymmetries between adverse and beneficial conditions for both micro and macro conditions. In the case of micro conditions, we find that income gains generally have no significant effects on shopping outcomes, whereas income losses do. This finding suggests that households are quick to decrease spending when income decreases but are slow to respond when income increases, potentially because they need to compensate for postponed purchases of durables or paying off debts ([11]). While contractions affect households' shopping basket value more extensively than expansions, the expansion elasticities for shopping basket allocation are mostly larger than during contractions. This response seems reasonable, as failing to keep up with one's surroundings during an expansion would translate into a loss of status, whereas not adopting a more frugal shopping behavior during a contraction implies an increase in status ([38]). In addition, we find more pronounced asymmetries between adverse and beneficial conditions at the macro level than at the micro level. Thus, adjustments in shopping behaviors may reverse more quickly when they are caused by changing micro conditions compared with macro conditions. Given that adverse macro conditions shift the societal acceptance of certain brand types and store formats, households' attitudes may change ([32]). This reasoning implies that macro conditions' effects on shopping outcomes linger longer than micro conditions, during which households engage in status-maintaining shopping behaviors. Therefore, these status-maintaining shopping behaviors may be a means to an end rather than an attitudinal shift and households would quickly discard them once conditions improve.Finally, we investigate whether micro and macro conditions and the underlying mechanisms that affect households' shopping behavior moderate each other. Thus, we perform a post hoc analysis to test for possible interaction effects for which we present complete results in Web Appendix C, Table WC3.[15] Interestingly, the main effects remain unchanged while all interaction effects are insignificant, which suggests that micro and macro conditions do not moderate each other. Thus, the results indicate that the effects and mechanisms that micro and macro conditions elicit occur independently from each other. That is, if both conditions change simultaneously, their individual effects on households' shopping outcomes work in parallel. Managerial Implications Micro ConditionsChanging micro conditions affect shopping outcomes only when households suffer income losses rather than gains, leading to a decrease in PLs purchased in discounters and NBs purchased in nondiscounters. To buffer the negative effects of when and where they expect wages to decrease, manufacturers as well as discounters can profit from listing NBs in discounters. In particular, hard discounters such as Aldi and Lidl, whose overwhelming majority of revenues stem from their own PLs, may profit from this strategy. Thus, we provide an additional perspective to the literature investigating the role of NBs in discounters (e.g., [12]). If households indeed suffer from weakened self-confidence and want to bolster their social status as a result of adverse micro conditions ([31]; [53]), NB manufacturers and nondiscounters may leverage this reaction by using status appeals in their advertising. Because adverse micro conditions lead to a general decline in consumption, retailers and manufacturers could target product categories that are affected the most with marketing-mix actions. Changing micro conditions may be especially hard for manufacturers and retailers to identify, but with increasing availability of data through loyalty cards and online shopping, managers could detect the specific shopping outcomes associated with these changes and address those households through personalized coupons and deals. Macro ConditionsChanging macro conditions substantially affect households' shopping basket allocation as well as value. Given the increased acceptance of PLs during contractions, retailers can use the opportunity to extend their PL portfolio into higher price tiers and product categories with high involvement and complexity ([56]). In addition, they may narrow their price gap to NBs and strengthen their branding to preemptively counteract households' shifts back to NBs during subsequent expansions. During expansions, they could then offer more attractive and profitable price promotions. In particular, nondiscounter PLs may get away with raising prices because they are unaffected by increasing budgetary constraints. Given the countercyclical susceptibility of PLs, retailers should adjust their assortment accordingly, reducing their PL share in expansions and increasing it in contractions. While hard discounters are especially susceptible to adverse micro conditions, soft discounters (i.e., discounters with a relatively low PL assortment share) should be aware of contractions owing to the substantial negative effect of NBs purchased in discounters and their comparatively low share of PLs that may compensate the losses.Because we control for micro conditions, the reallocation of budgets to PLs that we observe during adverse macro conditions is apparently not driven by monetary factors but instead may result from changing attitudes toward frugal consumption across society ([38]) and a desire to restore control ([ 7]). If this reasoning holds, it has important implications for managers. NBs and retailers can avoid costly price reductions that are ineffective given the lack of a more constrained budget and instead use measures that provide a perception of frugality.[16] These measures may allow households to engage in behaviors that they associate with economizing but, at the same time, are economical for the retailer or manufacturer. For example, loyalty programs can offer low price discounts and small rewards, giving households the perception that they engage in frugal consumption ([45]). Distribution of (digital) store fliers may create a sense of greater control over the planned shopping trip. In addition, communication may highlight the quality and reliability of products to reduce uncertainty and increase compensatory consumption. NB managers might also consider increasing package size, as larger package size is often associated with a lower per unit price ([ 6]). Finally, NB managers and retailers can leverage the higher cognitive load and depletion of self-control resulting from switching stores and/or brands ([60]), rendering shoppers more susceptible to in-store promotions ([31]). Limitations and Directions for Future ResearchWhen individual income is controlled for, changes in observed shopping behaviors resulting from macro conditions are clearly linked to households' willingness, rather than ability, to purchase. Potential underlying changes in attitudes and societal acceptance of certain shopping behaviors provide a conclusive basis for our argumentation. However, we do not observe these changes of attitudes in the data directly. Therefore, we encourage field experiments and laboratory studies to dive deeper into the underlying psychological and sociological mechanisms that might drive these findings. These insights can be crucial in predicting how households will change their CPG shopping in reaction to other types of macro conditions, such as a worldwide pandemic.Including demographics and psychographics, we control for household characteristics but do not account for heterogeneity in households' reaction to changing conditions, which should be addressed by future research. Heterogeneity may originate, for example, from households' differing preferences for high-quality products, with those preferring high quality potentially opting for adjustments in the volume purchased and the price paid for a good over switches to low-tier NBs and PLs. Alternatively, heterogeneity might stem from households' usual ""baseline"" shopping behavior because it influences whether and how they are able to economize during adverse conditions.Future analyses could also differentiate among different product categories, especially relating to the reduction in consumption levels caused by adverse micro conditions. Some product categories may be more essential than others and, thus, consumption may not simply be reduced ([38]). Some product categories may even experience increasing consumption—for example, as households shift from soft drinks and juices to plain water.Finally, previous research has shown that macro conditions affect marketing-mix decisions ([58]). Thus, future research could take a corporate rather than household perspective, investigating how managers detect and react to changes in micro conditions. Supplemental Materialsj-pdf-1-jmx-10.1177_00222429211036882 - Supplemental material for Households Under Economic Change: How Micro- and Macroeconomic Conditions Shape Grocery Shopping BehaviorSupplemental material, sj-pdf-1-jmx-10.1177_00222429211036882 for Households Under Economic Change: How Micro- and Macroeconomic Conditions Shape Grocery Shopping Behavior by Thomas P. Scholdra, Julian R.K. Wichmann, Maik Eisenbeiss and Werner J. Reinartz in Journal of Marketing  "
10,"Households Under Economic Change: How Micro- and Macroeconomic Conditions Shape Grocery Shopping Behavior Economic conditions may significantly affect households' shopping behavior and, by extension, retailers' and manufacturers' firm performance. By explicitly distinguishing between two basic types of economic conditions—micro conditions, in terms of households' personal income, and macro conditions, in terms of the business cycle—this study analyzes how households adjust their grocery shopping behavior. The authors observe more than 5,000 households over eight years and analyze shopping outcomes in terms of what, where, and how much they shop and spend. Results show that micro and macro conditions substantially influence shopping outcomes, but in very different ways. Microeconomic changes lead households to adjust primarily their overall purchase volume—that is, after losing income, households buy fewer products and spend less in total. In contrast, macroeconomic changes cause pronounced structural shifts in households' shopping basket allocation and spending behavior. Specifically, during contractions, households shift purchases toward private labels while also buying and consequently spending more than during expansions. During expansions, however, households increasingly purchase national brands but keep their total spending constant. The authors discuss psychological and sociological mechanisms that can explain the differential effects of micro and macro conditions on shopping behavior and develop important diagnostic and normative implications for retailers and manufacturers.Keywords: business cycle; income shocks; consumer packaged goods; private label; national brand; discounter; supermarketHouseholds are subjected to constantly changing economic conditions. These changes may take place at a personal, microeconomic level, such as if the main breadwinner receives a pay raise or a household member loses a job (micro conditions). Alternatively, changes may manifest at a macroeconomic level, in terms of the business cycle, with its recurring expansions and contractions or in response to global events such as the Great Recession or the COVID-19 pandemic (macro conditions). These changing micro and macro conditions substantially affect household spending and, in turn, companies' profits. By one estimate, the Great Recession led to an average 8%, or $4,000, decrease in real annual spending among U.S. households, which amounts to $500 billion in forgone revenues ([20]).While households tend to simply postpone purchases of durable goods to times of economic prosperity ([16]; [18]), they engage in a variety of adjustments when shopping consumer packaged goods (CPGs): switching from national brands (NBs) to cheaper brands or private labels (PLs), from supermarkets to discounters, from regular to promotional prices, or decreasing the amounts purchased altogether (e.g., [17]; [43]; [46]).While research to date has focused intensively on how households adjust individual CPG shopping outcomes in response to changing macro conditions (e.g., [17]; [41]; [43]), this work takes a holistic view on households' CPG shopping behavior by uncovering how it is differentially affected by micro and macro conditions. This explicit distinction is important because changes in macro and micro conditions are not necessarily aligned. In fact, even the Great Recession, during which unemployment rates skyrocketed and housing prices and stock portfolios plummeted, did not equally affect the personal income and wealth of all demographic subgroups of the population ([37]) or all geographical regions ([17]). Similarly, the economic downturn caused by the COVID-19 pandemic implies particularly severe microeconomic consequences for industry sectors that depend on tourism, events, or gastronomy, with less effect on banking or the public sector ([48]). Of course, an income loss, for example, as result of sudden unemployment, may as well occur during prosperous economic times and be no lesser of an individual hardship.Furthermore, the consequences of changing micro and macro conditions differ considerably. Whereas changing micro conditions directly affect households' ability to purchase, changing macro conditions, all else being equal, affect only households' willingness to purchase ([39]). Accordingly, households' response to changing conditions depends on whether they are affected at a micro or macro level (or both) and may manifest in very different shopping outcomes. For example, households may alter what they purchase (e.g., NBs or PLs) and where they shop (e.g., in discounters or supermarkets), as well as how much they spend and purchase. Thus, to properly disentangle the distinct effects of micro and macro conditions and to provide differentiated implications for retailers and manufacturers, holistic observations of households' shopping behavior are crucial.We analyze a total of seven measurable and managerially relevant shopping outcomes. These outcomes reflect how households allocate their budget across brand types and store formats—their shopping basket allocation (in terms of PL and NB spending in discounters and nondiscounters)—as well as how much they spend and purchase—their shopping basket value (in terms of total spending, purchase volume, and an index of prices paid). Through the analysis, we uncover and characterize the differential effects of micro and macro conditions on households' shopping behavior by addressing the following research questions: To what extent do micro (i.e., income) and macro (i.e., the business cycle) conditions affect households' CPG shopping behavior? How do micro and macro conditions differ in terms of their effects on households' shopping basket allocation and shopping basket value? Do asymmetries exist between negative (i.e., income losses/economic contractions) and positive (i.e., income gains/economic expansions) conditions, and if so, do these asymmetries differ between micro and macro conditions?We use a unique, comprehensive data set tailored to the research objectives. Drawing on the GfK Germany ConsumerScan panel, we obtain detailed information about daily CPG transactions for more than 5,000 households in Germany over a period of eight years including the Great Recession. Drawing on this, we identify what and where households shop, how much they purchase, what prices they pay, and how much they spend. Annual surveys administered to the panel provide us with longitudinal data on households' demographics and psychographics, including micro conditions in terms of household income. In addition, the panel data enable us to control for important marketing-mix elements concerning prices, assortments, and promotional activities. We further enrich the data set with macroeconomic data from the German Federal Statistical Office and advertising data from the Nielsen Company on advertising spending by all manufacturers and retailers in the sample.The analyses show that micro and macro conditions both have a substantial impact on households' shopping behavior. Importantly, households adjust their shopping behavior without a concrete change in their budget constraints. In addition, micro and macro conditions differ substantially in their effects on households' shopping behavior. Whereas micro conditions primarily have an impact on households' basket value, macro conditions not only affect households' basket value but also cause shifts in households' basket allocation. During adverse micro conditions, households buy lower volumes and spend substantially less in total but do not shift spending to other brands or store formats. In contrast, as macro conditions change, households shift spending to PLs (from both discounters and nondiscounters) during contractions and to NBs during expansions. In addition, they increase their total spending and purchase volume during contractions. We argue that the shifts during macro conditions are driven by a greater society-wide acceptance of frugal consumption that does not emerge during changing micro conditions. These discrete effects of micro and macro conditions and the proposed underlying mechanisms have distinct managerial implications. The results also address some of the counterintuitive findings of prior studies, such as increasing total spending and purchase volumes ([46]) as well as higher prices paid ([ 8]) during the Great Recession. Related LiteratureOur study relates to business cycle research in marketing as summarized in Table 1.GraphTable 1. Literature Overview. AuthorsMacro ConditionsMicro ConditionsShopping Behavior(s)Data BasisGicheva, Hastings, and Villas-Boas (2007)Gasoline pricesSpending share of income, out-of-home consumption, promotion shares (individually)Weekly household-level consumption surveys, repeated cross-section, two U.S. regions from 2000 to 2004Lamey et al. (2007)Business cycle (asymmetries)PL shareAnnual, country-level longitudinal data, four countries spanning multiple decadesLamey et al. (2012)Business cyclePL shareAnnual, category-level longitudinal data, U.S. from 1985 to 2005Ma et al. (2011)Gasoline prices, GDP growth rateShopping trips, total spending, purchase volume, store format, brand type, price tier, and promotion shares (individually)Monthly, household-level longitudinal panel data, U.S. metropolitan area from 2006 to 2008Kamakura and Du (2012)GDP growthHousehold budgetSpending share of budgetAnnual, household-level consumption surveys, repeated cross-section, U.S. from 1989 to 2003Lamey (2014)Business cycle (asymmetries)Discounter shareAnnual, country-level longitudinal data, 15 countries, spanning 17 yearsCha, Chintagunta, and Dhar (2016)Regional unemployment levelTotal spending, purchase volume, prices paid, store format, brand type, price tier, and promotion shares (individually)Annual, household-level panel data, repeated cross-section, U.S. from 2006 to 2011Dubé, Hitsch, and Rossi (2018)(Post)recession phaseIncome, wealthPL shareMonthly, longitudinal household-level panel data, U.S. from 2004 to 2012This articleBusiness cycle (asymmetries)Income (asymmetries)Total spending, purchase volume, price index, brand type and store format shares (simultaneously)Quarterly, longitudinal household-level panel data, Germany from 2007 to 2013 1 Notes:[26] and [46] argue that changes in gasoline prices reflect changes in household budgets. We regard gasoline prices as macro effects because they are experienced simultaneously but not necessarily equally by all households, as some households may rely on their cars more than others. As such, they are more similar to macro rather than micro events.Pioneering studies in this stream show that during recessions, PL market shares ([43]) and discounter market shares ([41]) increase, and some of these effects carry over into subsequent expansion periods. [17] generally confirm these findings by analyzing PL demand at a household level, accounting for heterogeneous income and wealth effects caused by the Great Recession. They find significant short- and long-term effects on PL demand, albeit with notably smaller elasticities. [ 8] further extend the number of shopping behaviors observed. They find that unemployment caused by the Great Recession has led households to increasingly purchase products on price promotion, cheaper brands, and in cheaper store formats. Instead of traditional macroeconomic indicators, [26] and [46] use gasoline prices to operationalize changing economic conditions. They show that gasoline prices relate to a multitude of shopping behaviors such as spending, prices paid, and store format and brand type shares.In addition to macro conditions, some of the studies in the field observe households' micro conditions. However, they are either used as time-invariant demographic control variables ([ 8]; [26]; [46]) or conceptualized as direct consequences and part of macro conditions rather than distinct conditions with idiosyncratic effects ([17]). Our study thus contributes to this literature stream by delineating the distinct effects of changing micro as well as macro conditions on households' shopping behavior. Importantly, we also account for different magnitudes and asymmetries between adverse and beneficial micro and macro conditions.First insights into the differences between micro and macro conditions show that overall household spending on food products and alcoholic beverages increases during adverse macro conditions but decreases when micro conditions worsen ([38]). We complement these findings by analyzing a variety of shopping outcomes beyond overall spending, using actual purchase data (thus increasing external validity), and controlling for a large variety of confounding factors such as changes in the marketing mix that are associated with changes in macro conditions ([58]).Notably, studies to date either focus on individual shopping outcomes (e.g., [17]; [43]) or model several shopping outcomes independently from each other ([ 8]; [26]; [46]). However, households have a variety of means to adjust their shopping behavior that are also highly interdependent—for example, discounters carry substantially more PLs and fewer NBs and usually feature fewer promotions in favor of an everyday low-price strategy. As such, when households switch store formats, it almost automatically also affects their brand type and promotion shares ([11]). Failing to account for these interdependencies can overestimate the effect of changing conditions on individual shopping outcomes. Therefore, we analyze multiple shopping outcomes simultaneously, controlling for their interdependencies, and thus contribute to the literature by offering a holistic picture of micro and macro conditions' effects on households' shopping behavior. Conceptual FrameworkThe conceptual framework (Figure 1) depicts the two main components of our study: micro and macro conditions and their effect on households' shopping behavior. We observe these behaviors through concrete and measurable shopping outcomes that, in essence, boil down to households' shopping basket value (i.e., how much households purchase and at what price) and their shopping basket allocation (i.e., how households allocate their expenditures across brand types and store formats). To get a holistic picture of micro and macro conditions' effects on households' shopping behavior, we consider the various shopping outcomes simultaneously. We also control for household demographics and psychographics as well as manufacturer and retailer adjustments to the marketing mix.Graph: Figure 1. Conceptual framework. Economic Conditions: Micro Versus MacroWe analyze changing macro conditions in terms of the business cycle on the basis of gross domestic product (GDP) (e.g., [43]; [58]) and derive micro conditions in terms of households' income. Although changing macro conditions are experienced by an entire region, by a nation, or even globally, they do not necessarily affect all households at a micro level. For example, not all households may experience income reductions, job loss, or shrinking wealth during a recession ([17]). Thus, by differentiating between micro and macro conditions, we isolate the distinct effects on shopping outcomes of changes in households' ability to purchase (micro level) and their willingness to purchase (macro level) ([39]). A negative micro shock, for example, restricts some households' shopping budgets, while households that face only adverse macro conditions lack this budget constraint. Importantly, whereas changing micro conditions are usually a personal matter, changing macro conditions affect a society at large. Thus, shifts in macro conditions can alter what type of shopping behavior is considered the norm. During recessions, for example, frugal consumption such as buying PLs or visiting discounters may become socially acceptable and even fashionable ([22]; [38]).In addition, beneficial and adverse economic conditions exercise asymmetric effects on consumers' shopping behavior for several possible reasons, such as general pessimism following a recession, inertia in maintaining newly adopted habits, or the need to pay off debts that have accrued during a period of lower income ([11]; [43]). Thus, we investigate asymmetric effects by splitting micro and macro conditions into both adverse and beneficial changes. Households' Shopping OutcomesWe distinguish between a household's shopping basket value and shopping basket allocation. We examine shopping basket value outcomes in terms of a household's total budget spent, total volume purchased, and an index of prices paid that indicates whether a household purchases products below average market prices of these products, for example, through temporary price promotions. In this way, we can differentiate the degree to which households adjust how much they purchase and how much they spend. We discern shopping basket allocation outcomes by considering brand types and store formats jointly and differentiating between households' spending on ( 1) PLs in discounters, ( 2) PLs in nondiscounters (e.g., supermarkets, hypermarkets), ( 3) NBs in discounters, and ( 4) NBs in nondiscounters. Prior research has taken a similar approach to households' budget allocation, with studies distinguishing between PLs and NBs as different brand types (e.g., [ 3]; [63]; [56]) or discounters and nondiscounters as different store formats (e.g., [10]; [40]; [41]). This approach has the following conceptual merits. Brand typesRegarding brand types, PLs, NBs, and their competition have received ample attention from both academics and practitioners ([40]). PLs have evolved from pure economic options to covering all price tiers and even special segments such as organic foods ([27]; [35]). They have thus developed into major competitors for NBs; for example, in Germany they have gained a market share of 41%, with 95% of consumers buying PLs ([25]; [36]). The competition between NBs and PLs is distinct in that PLs are managed by retailers and, thus, they introduce an aspect of competition into their otherwise collaborative relationship with manufacturers through downward price pressure. However, at the same time, NBs and PLs benefit each other by increasing store traffic and reinforcing quality disparities ([24]; [49]). From a consumer perspective, NBs and PLs differ substantially. First, consumers perceive PLs as inexpensive and as a good value for money. Further, while NBs are generally still better known and are perceived as being of higher quality, PLs are catching up in terms of quality perception ([36]). These differences in terms of price and quality perceptions generally suggest that households will switch between these two brand types in response to changing micro or macro conditions. Thus, the explicit distinction between NBs and PLs is relevant for our research. Store formatsIn terms of store formats, previous research has contrasted discounters with ""traditional retailers"" ([40]; [41]), supermarkets ([10]), and large retail formats ([28]; [29]). In contrast to other formats, discounters are highly optimized for cost efficiency, resulting in a substantially different retail marketing mix: store design and product presentation are austere, consumer services are reduced to a minimum, and serviced fresh foods and baked goods counters are lacking. The assortment is typically limited, especially in terms of produce; shallow, with few alternatives in each product category; and dominated by PLs, featuring relatively few NBs. As such, discounters are able to offer substantially lower prices than other store formats at the cost of service quality ([41]; [64]).In contrast, the major nondiscount store formats, such as supermarkets, superstores, and hypermarkets, vary in floor size and assortments offered beyond CPGs (e.g., clothing, home decor, hardware) but are similar to each other in terms of prices, service quality, and CPG assortments ([40]; [41]; [64]). This is also evident from Table 2, in which we contrast market data from discount and nondiscount store formats in Germany. Therefore, distinguishing between discounters and nondiscounters is most obvious from both retailer and consumer perspectives. Despite their distinct characteristics, however, discounters and nondiscounters do not merely address different target groups but also compete directly with each other for the same consumers, as consistently argued and shown in previous research (e.g., [10]; [33]).GraphTable 2. Store Format Characteristics. Store Format# of StoresaSales Area (m2/store)aRevenues(€ mil.)aMarket ShareaSpace Prod. (€/m2)# of SKUsaSKU Prod.(€ mil./SKU)PL SharebService ScorecPrice Scorec1. Discounters16,05477969,80045.44%5,5842,29530.465.6%67.182.92. Small retailers8,7502974,8003.13%1,846—————3. Supermarkets10,90098244,90029.23%4,19611,8303.821.6%82.073.64. Superstores1,1273,46115,2009.90%3,89725,005.684.574.06. Hypermarkets8517,05118,90012.30%3,15048,870.419.6%79.177.9Discounters (1)16,05477969,80045.44%5,5842,29530.465.6%67.182.7Nondiscounters (2–5)21,6281,07383,80054.56%3,61223,2263.221.2%82.574.7 2 aSource:[21], based on 2016 data.3 bSource:[25], based on 2018 data.4 cSource:[16], based on 2018 data.5 Notes: Data are based on the German market. Aggregated values for nondiscounters based on sums or averages weighted by market shares. Service and price scores are indexes (0–100), scores for store formats are aggregates from the 12 major retail brands that were tested. We assigned retail brands to their primary store format based on industry convention and average store size: small retailers <400 m2, supermarkets 400–2,500 m2, superstores 2,500–5,000 m2, hypermarkets >5,000 m2 average sales area. Brand type and store format combinationsImportantly, we do not consider the defined brand types (NBs and PLs) and store formats (discounters and nondiscounters) in isolation but in combination. This combined view is important because the brand choice cannot be viewed independently of the underlying store format. For example, because discounters carry a larger PL share than nondiscounters, PLs are more visible to households at discounters and also compete with fewer NBs. At the same time, nondiscount formats usually offer more price tiers (e.g., economy, standard, and premium) and variants (e.g., organic, locally produced, or diet) for NBs as well as PLs within a product category than discounters ([27]; [35]). As such, PL and NB assortments differ structurally between discounters and nondiscounters, and we account for these differences by the combined consideration of these brand types (PLs and NBs) and store formats (discounter and nondiscounters). Thus, by crossing the two brand types and store formats, we obtain a parsimonious, mutually exclusive, collectively exhaustive, and meaningful conceptualization of households' shopping basket allocation. Altogether, the three shopping basket value outcomes and the four shopping basket allocation outcomes holistically cover the essence of households' CPG shopping behavior. Control VariablesWe control for household demographics, which play an important role in explaining differences in shopping baskets (e.g., [46]). In addition, we control for a set of household psychographics: price and quality consciousness, deal proneness, and out-of-home consumption preference. Psychographics control for household heterogeneity that is not necessarily captured by demographics because, for example, even households with high income may be deal-savvy or highly price-conscious ([ 2]). Such psychographics strongly resemble consumer traits that are largely stable in short-term environmental changes but also reflect long-term societal trends, cultural developments, and the process of consumer aging ([54]).As prior research has shown, retailers and manufacturers also react to macro conditions by adapting their marketing mix (e.g., [13]; [42]). We are less concerned with this relationship per se but control for adjustments in the marketing mix owing to their substantial influence on households' shopping behavior. Data Research ContextAs presented in Table 2, the German CPG retail market is split rather evenly between discounters and nondiscounters, with discounters accounting for 45% of revenues and 43% of stores.[ 6] Discounters in Germany are usually located in easily accessible and densely populated areas ([64]) and have an average sales area of 779 m2, which is slightly smaller than a typical supermarket (982 m2) and substantially smaller than superstores (3,461 m2) and hypermarkets ( 7,051 m_SP_2_sp_) ([21]). However, they carry far fewer stockkeeping units (SKUs) and offer a much larger PL share (65.6%) that typically outweighs NBs ([25]). Discounters' PL shares may vary by retailer (e.g., Aldi: 96%, Lidl: 61%), but even discounters with a relatively strong focus on NBs have a substantially larger PL share than nondiscounters (e.g., Penny: 42%, Netto Marken-Discount: 40% vs. nondiscounters: 21.2%). Discounters offer substantially lower prices but also limited service, as is evident from a study by the German Institute for Service Quality ([16]), which scores stores on the basis of their prices and service (higher scores mean better prices/service). The tested discounters received substantially higher (lower) price (service) scores than their nondiscounter counterparts. Discounters' focus on functionality rather than service is also reflected in their high space productivity (i.e., revenues per store space). Similarly, annual revenues per SKU are considerably higher in discounters (€30.4 million) than in nondiscounters (€3.2 million) ([21]).These data underline the similarity of the nondiscount store formats and their dissimilarity to discounters for the German market from both retailer and consumer perspectives, thus corroborating the previously introduced conceptual distinction between these two groups. Interestingly, this distinction is also reflected in the branding of different retail store formats in the German CPG market. For example, two major German retail companies—the REWE Group and the EDEKA Group—operate both regular supermarkets and superstores under their REWE and EDEKA umbrella brands. Their hypermarkets (REWE Center and E-Center) also incorporate many of the same brand cues. In contrast, their discounters—Penny and Netto Marken-Discount—carry retail brands that are completely distinct from their respective umbrella brand. Data SourcesTo reflect the particularities of the German CPG market, the data set draws on several sources and combines information across distinct aggregation levels. The primary data source is the ConsumerScan panel provided by GfK Germany, which includes transaction and survey data for panelists at the individual household level. As a major advantage, this panel covers private consumption comprehensively and representatively, spanning all German CPG retailers, including discounters that typically do not offer data for market research purposes through retail panels.[ 7] This data availability is particularly crucial, considering the substantial market share of discount stores in Germany (see Table 2). The panel also contains survey data for all panelists, based on self-reported annual demographic information (age, household size, and income) and psychographic measures (e.g., price and quality consciousness). In addition, we obtain data on weekly advertising spending that covers all major channels as well as all manufacturers and retailers from the Nielsen Company. Finally, we add publicly available GDP data from the Federal Statistical Office that indicate the aggregate economic condition. We thus build a unique, encompassing data set that combines behavioral measures with survey-based household demographics and psychographics, macroeconomic measures, and brand- and store-level advertising spending. Data PreparationThe initial raw data set from the ConsumerScan panel is composed of household characteristics and purchase decisions by 85,428 unique households—with 24,000 to 37,000 in any given year—that made more than 13 million shopping trips and 48 million purchases between 2006 and 2013. Purchase information is available at the SKU level for 39 product categories from 467 retailers, most of which maintain multiple stores. These products include alcoholic and nonalcoholic beverages (e.g., beer, fruit juice) and food (e.g., cereals, pasta, ice cream) as well as nonfood items (e.g., deodorants, detergents, toilet paper). For each purchased item, we have access to the unique product code, date and place of purchase, price paid, identifiers for store format, brand type, and temporary price reductions as well as specific product characteristics such as brand and manufacturer name and package size. In preparing these data, we took several cleaning and filtering steps at the purchase record and household levels. In particular, we eliminated inconsistent transaction records and households that did not remain in the panel for the entire period. This procedure is conservative and in line with prior literature (e.g., [17]). Data cleaning involved the following steps: ( 1) Removal of all cases with missing values, ( 2) removal of all cases with unusually high (more than four times the median) or unusually low (less than one-fourth the median) prices at the SKU level, ( 3) removal of all cases with SKUs purchased fewer than 25 times in the entire period.These data-cleaning steps preserved 97.4% of all observations and 96.1% of all expenditures. To exploit the analytical potential of panelists with long purchase histories and extensive survey information, we retain only households with at least one transaction per quarter (7,441 households) and full survey information from 2006 to 2013, leaving 5,101 unique households.To avoid structural differences between samples, we compared the filtered households with the remaining households in terms of shopping outcomes and demographics. Overall, we find only marginal deviations in purchase behaviors and demographic composition. Thus, we assume that the selected households with complete purchase histories are not structurally different from households with shorter or incomplete purchase histories. We also compare the filtered sample with information from the 2006 Microcensus ([15]). As in other studies using this type of data (e.g., [17]), our sample is only slightly older and has higher income, fewer single households and more two-person households, and fewer children. However, we find a sizable overlap in the distributions of the demographic variables, and we control for these demographics at the individual household level throughout the empirical analyses. Therefore, a lack of sample representativeness is not an issue. Detailed comparisons of the household samples are available in Web Appendix A. Variable Operationalization Shopping basket valueIn line with the conceptual framework, we consider multiple dependent variables to capture the two domains of shopping outcomes as exhaustively as possible. The first domain relates to a household's shopping basket value—that is, how much is spent by the focal household, as represented by three dependent variables. TotalSpendinght relates to the total CPG spending of household h at time t, measured in euros. PurchaseVolht refers to the total CPG purchase volume of household h at time t, again measured in euros. Note that a household's shopping basket typically contains products with different volume units (e.g., liters, grams, pieces) that cannot directly be combined into a total volume measure. Therefore, we follow [46] and use an average category price per volume unit from a one-year (here: 2006) initialization period and multiply it by the total equivalent volume units purchased in each category. This enables us to aggregate the purchase volume across categories. Accordingly, the resulting variable is expressed in euros. We note that any variations in this variable are caused by changes in volume and not changes in prices being paid that may result from switching between brand types and store formats. Therefore, we are able to clearly disentangle households' consumption (volume) from households' spending (value) of CPG purchases. Finally, PriceIndexht is constructed as an index ([ 1]) and compares, for household h at time t, the costs of the shopping basket at average market prices with the actual costs incurred by the household. These price differentials are considered for identical goods identified at the SKU level. As such, they do not reflect differences in the quality of goods purchased but whether specific SKUs in the basket were purchased at cheaper prices (e.g., through temporary price promotions). An index greater than 1 implies that a household paid more than average for the specific goods in its basket, and a value less than 1 implies that the household paid less than average. This variable, therefore, reflects households' cherry-picking behavior ([23]) and is not related to households' switching behavior between different brand types or price tiers. We provide further details on the construction of purchase volume and the price index in Web Appendix B. Shopping basket allocationThe second domain of shopping outcomes relates to a household's shopping basket allocation between combinations of brand types and store formats—that is, it captures how the household is allocating its budget. We measure this allocation with the dependent variable Spendingbht in terms of household h's total spending (in euros) at time t on the respective brand type–store format combination b: (b = 1) PLs in discounters (PLDisc), (b = 2) NBs in discounters (NBDisc), (b = 3) PLs in nondiscounters (PLNonDisc), and (b= 4) NBs in nondiscounters (NBNonDisc). Altogether, these four spending variables encompass each household's total spending.[ 8] Macro conditionsThe focal explanatory variables represent a household's individual micro conditions and the overall macro conditions. At the macro level, we first apply the Christiano–Fitzgerald random-walk filter ([ 9]) to the log-transformed quarterly GDP data to assess the general state of the economy itself. The extracted cyclical component of the GDP series constitutes the deviation from the economy's underlying long-term growth trend. Thus, periods with increases in the cyclical component indicate economic expansions, whereas periods with decreases indicate economic contractions. However, it is important to account for not only different phases of the business cycle but also the severity that comes with the depth of up- and downturns (e.g., [55]). To do so, we follow prior research ([43]; [58]) and define the magnitude of an expansion (contraction) period relative to the prior trough (peak) of the cyclical series, or the point in the cyclical component at which the quarter-on-quarter growth turns from negative to positive (from positive to negative). Therefore, we operationalize the symmetric measure of the business cycle (BCyclet) as changes in the cyclical component of GDP at time t relative to the prior peak or trough. In addition, to study potential asymmetries of macro conditions, we use the same operationalization to construct two semidummy variables that separately capture periods with an increase in the cyclical component relative to the prior trough as expansions (Expansiont) and periods with a decrease relative to the prior peak as contractions (Contractiont) of the economy. That is, Expansiont (Contractiont) takes values increasing (decreasing) with economic expansion (contraction) and 0 values during contractions (expansions).[ 9] Micro conditionsAt the individual level, micro conditions reflect a household's financial situation, captured by the household's monthly net income. The original income data included in the ConsumerScan panel are at a yearly aggregation level and are measured in 16 income brackets.[10] We construct a continuous income variable by taking midpoint values of these brackets in euros and transform the resulting series to a quarterly sequence (the aggregation level of the shopping outcome variables) by applying linear interpolation for each household.[11] We adjust income for inflation using the consumer price index. In line with the operationalization of macro conditions, we define micro conditions as a household's income change (IncomeChangeht) relative to its previous income peak or trough. This step enables us not only to capture income changes from one period to another but also to take the higher magnitude into account, which results from income changes along consecutive periods. Furthermore, we construct semidummy variables for positive (IncomeGainht) and negative (IncomeLossht) income changes that are equivalent to the operationalization of asymmetric measures at the macro level. Thus, IncomeGainht (IncomeLossht) is defined as the difference of the log-transformed net income at time t and the prior log-transformed income trough (peak), allowing us to account for the accumulated magnitude of income gains and losses over time. IncomeLossht and Contractiont are converted to positive values for ease of interpretation. Control variablesAs control variables, we include a household's value of the dependent variable from a one-year (here: 2006) initialization period t0 (TotalSpendinght0, PurchaseVolht0, PriceIndexht0, and Spendingbht0). In addition, we include demographics to control for household heterogeneity regarding household size (HhSizeht), age of the household head (Ageht), presence of children (Kidsht), and employment status (Unemployedht). We also include psychographic variables to control for heterogeneity in shopping-related traits and preferences in terms of quality (QualConsht) and price consciousness (PriceConsht), deal proneness (DealProneht), and preferences for eating out (EatOutht). While QualConsht and PriceConsht are based on fixed constructs provided by the GfK, we construct DealProneht and EatOutht from several survey questions. The associated items, factor loadings, and Cronbach's alphas appear in Web Appendix B, Table WB1. Demographic and psychographic controls are measured at an annual level, and we transform the psychographics to a quarterly series using linear interpolation.Finally, we include controls for the marketing mix. We compute this group of variables at different levels of aggregation as appropriate for each set of models and use household-specific product category weights to incorporate household heterogeneity ([46]). Except for the advertising measures, marketing-mix controls are based on transaction information from the ConsumerScan panel. Because we construct the marketing-mix controls on the basis of observed household transactions, we use only transaction information (e.g., prices, SKUs, price-promoted SKUs) of households that are not part of the analysis sample. Thus, we avoid potential biases resulting from nesting the transactions of these focal households into the marketing-mix controls. For the basket value models, we construct absolute measures for price (Priceht), assortment size (Assortht), price promotions (Promoht), PL share in assortments (PctPLht), and advertising spending of NBs (AdvNBt) and of store format j (with j = 1 for discounters and j = 2 for nondiscounters) (AdvStorejt), which includes advertising spending on retailer brands as well as their PLs. For the basket allocation models, the marketing-mix variables for each brand type–store format combination are computed relative to the average across all brand type–store format alternatives. Thereby, we parsimoniously account for potential cross-effects. In particular, we construct relative measures for price (RelPricebht), assortment size (RelAssortbht), price promotions (RelPromobht), PL share in assortments (RelPctPLjht), and advertising spending at the store level (RelAdvStorejt). Because advertising spending at the brand level refers to NBs only, we use it as an absolute measure.We adjust all spending and price variables for inflation using the consumer price index and advertising spending using the GDP deflator. Table 3 presents an overview of all variables and their operationalization, while Web Appendix B shows the detailed construction of the marketing-mix variables. Tables 4 and 5 provide the descriptives and correlations for variables in the shopping basket value models and shopping basket allocation models, respectively. Note the small correlations between micro and macro conditions, in support of the conceptualization of differential effects.GraphTable 3. Variable Operationalization. Variable GroupVariableOperationalizationShopping outcomesTotalSpendinghtTotal spending (in euros) by household h at time t.PurchaseVolhtTotal purchase volume by household h at time t measured in constant euros.PriceIndexhtIndex of prices paid by household h at time t.SpendingbhtSpending (in euros) by household h at time t for brand type–store format combination b.Micro- and macro conditionsBCycletDifference between the cyclical GDP component at time t and the prior trough/peak.ExpansiontDifference between the cyclical GDP component at time t and the prior trough.ContractiontDifference between the cyclical GDP component at time t and the prior peak.IncomeChangehtDifference between the log-transformed monthly net income (in euros) of household h at time t and the prior income trough/peak.IncomeGainhtDifference between the log-transformed monthly net income (in euros) of household h at time t and the prior income trough.IncomeLosshtDifference between the log-transformed monthly net income (in euros) of household h at time t and the prior income peak.Marketing-mix controlsPricehtNet price facing household h at time t.RelPricebhtRelative net price of brand type–store format combination b facing household h at time t.AssorthtNumber of unique SKUs facing household h at time t.RelAssortbhtRelative number of unique SKUs of brand type–store format combination b facing household h at time t.PromohtNumber of price-promoted SKUs facing household h at time t.RelPromobhtRelative number of price-promoted SKUs of brand type–store format combination b facing household h at time t.PctPLhtPercentage share of PL SKUs in the assortment facing household h at time t.RelPctPLjhtRelative share of PL SKUs in assortment of store format j facing household h at time t.AdvStoretStore-level advertising spending (in million euros) at time t.RelAdvStorejtRelative store-level advertising spending of store format j at time t.AdvNBtAdvertising spending (in million euros) of NBs at time t.Demographic controlsHhSizehtNumber of persons in household h at time t.AgehtAge of the leading person in household h at time t.KidshtDummy variable, 1 if children are present in household h at time t, 0 otherwise.UnemployedhtDummy variable, 1 if the principal earner of household h is unemployed at time t, 0 otherwise.Psychographic controlsQualConshtScale indicating quality consciousness of household h at time t; provided by GfK.PriceConshtScale indicating price consciousness of household h at time t; provided by GfK.DealPronehtFive-item scale indicating deal proneness of household h at time t.EatOuthtThree-item scale indicating preference for eating out of household h at time t.Time controlsTimetContinuous variable for time t.QuarterqtIndicator variable for quarter q of the year at time t.Other controlsCopulakhtGaussian copula for marketing-mix variable k to account for potential endogeneity.InvMillsbhtInverse Mills ratio to account for potential selection effects. 6 Notes: Items, factor loadings, and Cronbach's alphas for DealProne and EatOut are presented in Table WB1 of Web Appendix B.GraphTable 4. Descriptive Statistics and Correlation Matrix for Variables in the Shopping Basket Value Models. MSD1234567891011121314151617181920212223241. TotalSpending184.44114.4012. PurchaseVol183.06115.62.9413. PriceIndex.99.05−.02−.1514. BCycle1.054.50.00.01−.0115. Expansion2.422.69.01.02.00.8716. Contraction1.372.53.01.01.01−.85−.4917. IncomeChange51.93510.04.03.03−.01−.01−.02−.0118. IncomeGain176.70341.42.00.01−.01−.01−.05−.04.8019. IncomeLoss124.77315.41−.05−.04.00.01−.02−.03−.74−.19110. Price.99.06−.11−.16.02−.13−.10.11.02.02−.01111. Assort444.89113.02.15.13−.01.03−.03−.09.02.06.03.08112. Promo217.6555.96.04.02−.01.05−.08−.17.04.14.08.07.86113. PctPL.33.04−.17−.24.05−.03−.06.00.01.02.00.72.17.18114. AdvStore254.9827.22.03.02.00.02.17.15.00−.02−.01−.02−.03−.03−.04115. AdvNB294.8346.12.00.00.00.13.08−.15.01.04.04−.02.05.10−.01.25116. HhSize2.331.13.49.52−.14.00.01.01.07.05−.07.18.06−.01.03.01.00117. Age54.3612.07−.12−.13.07−.01−.04−.02−.10−.14.01−.18.02.10−.08−.02.01−.37118. Kids.18.38.19.21−.08.00.02.01.07.06−.04.25.03−.03.12.01.00.56−.54119. Unemployed.06.24−.06−.01−.01.01.01.00−.05.01.09−.01.00−.03−.02−.01−.01−.04−.07.02120. QualCons2.94.86.03−.07.12.00.00−.01.01−.01−.03.03−.01.03.05−.01.01−.04.11−.07−.10121. PriceCons3.14.93.00.13−.28.00.00.01−.01.02.03.02.02−.01−.03.01.00.14−.11.11.08−.39122. DealProne11.262.47.10.18−.28.00.01.01.02.02−.01−.02.00−.04−.04.01.00.18−.10.11.03−.10.38123. EatOut5.412.38−.07−.10.05.01.01−.01.05.07.00−.02−.01−.02−.01−.01.00−.07−.32.09.03.03−.05−.01124. Time−.05−.05−.01−.16−.27.00.06.20.12.05.25.61.05.08.19−.04.13−.05−.04.04−.02−.03−.011 7 Notes: Means and standard deviations are based on untransformed values, correlations are based on log-transformed variables except dummy variables. BCycle, Expansion, and Contraction are multiplied by 100 to be expressed in percentage deviations.GraphTable 5. Descriptive Statistics for Variables in the Shopping Basket Allocation Models. PLDiscNBDiscPLNonDiscNBNonDiscMSDMSDMSDMSDSpending45.4746.1023.6433.2912.9816.91102.3593.18Price.76.031.17.04.73.041.34.04Assort.74.12.66.05.46.082.15.17Promo.65.13.72.05.43.082.20.18PctPL1.46.041.46.04.54.04.54.04AdvStore1.09.071.09.07.91.07.91.07AdvNB294.8346.12294.8346.12294.8346.12294.8346.12 8 Notes: PLDisc = private labels in discounters; NBDisc = national brands in nondiscounters; PLNonDisc = private labels in nondiscounters; NBNonDisc = national brands in nondiscounters. ModelWe define regression models for the individual shopping outcomes and estimate them jointly in a system of seemingly unrelated regressions. To control for unobserved household heterogeneity, we use a random intercept specification. The three shopping basket value equations for total spending, purchase volume, and price index, as well as the four basket allocation equations for spending across four brand type–store format combinations, are specified in log-log form (excluding the dummy variables Kidsht, Unemployedht, and Quarterqt). This approach allows for an interpretation of coefficients as elasticities and accounts for the fact that households vary substantially in magnitudes of the dependent variables ([46]).[12] We first assume symmetry in each model with regard to the focal micro- and macroeconomic measures, where MacroEcont = δ1BCyclet and MicroEconht = δ2IncomeChangeht. Subsequently, we introduce asymmetric effects, where MacroEcont = γ1Expansiont + γ2Contractiont and MicroEconht = γ3IncomeGainht + γ4IncomeLossht.We provide the specifications for the shopping basket value and shopping basket allocation models subsequently. Shopping basket value modelsThe three shopping basket value models are defined as follows: ln(BasketValueaht)=αha+MacroEcont+MicroEconht+α2aln(BasketValueaht0)+α3aln(Priceht)+α4aln(Assortht)+α5aln(Promoht)+α6aln(PctPLht)+α7aln(AdvStoret)+α8aln(AdvNBt)+α9aln(HhSizeht)+α10aln(Ageht)+α11aKidsht+α12aUnemployedht+α13aln(QualConsht)+α14aln(PriceConsht)+α15aln(DealProneht)+α16aln(EatOutht)+α17aln(Timet)+∑24κq−1aQuarterqt+∑kωkaCopulakht+εaht, Graph( 1)where BasketValueaht is (a = 1) TotalSpendinght, (a = 2) PurchaseVolht, (a = 3) PriceIndexht,  αha=α0a+μha,μha∼N(0,σμ2)  , k is marketing mix variable k (k = 1, ..., K), q is quarter q in a given year (q = 1, ..., 4), and t is time period t at a quarterly level (t = 1, ..., T).We control for potential endogeneity in the marketing-mix variables resulting from unobserved shocks by including Gaussian copulas ([47]), which directly model the joint distribution of the potentially endogenous regressor and the error term through control function terms. An advantage of this method is that it does not require instrumental variables that may, as in our case given the number of marketing-mix variables across brand type–store format combinations, be difficult to find ([50]). A requirement is that the endogenous regressor is not normally distributed. Anderson–Darling tests and Kolmogorov–Smirnov tests confirm this nonnormality for all marketing-mix variables at p < .001. Given the large size of the sample, we also visually inspect quantile–quantile plots, which confirm nonnormality for all marketing-mix variables. The Gaussian copula for each marketing mix variable Xht for household h at time t is Copulaht = Φ-1[H(Xht)], where Φ-1 is the inverse distribution function of the standard normal and H(·) is the empirical cumulative distribution function of Xht. Shopping basket allocation modelsWe define the four models as follows: ln(Spendingbht)=βhb+MacroEcont+MicroEconht+β2bln(Spendingbht0)+β3bln(RelPricebht)+β4bln(RelAssortbht)+β5bln(RelPromobht)+β6bln(RelPctPLjht)+β7bln(RelAdvStorejt)+β8bln(AdvNBt)+β9bln(HhSizeht)+β10bln(Ageht)+β11bKidsht+β12bUnemployedht+β13bln(QualConsht)+β14bln(PriceConsht)+β15bln(DealProneht)+β16bln(EatOutht)+β17bln(Timet)+∑24κq−1bQuarterqt+∑kωkbCopulakbht+β18bInvMillsbht+εbht, Graph( 2)where  βhb=β0b+μhb,μhb∼N(0,σμ2)  , and the subscripts are as defined before.One issue with Equation 2 is that expenditures are zero where a household does not patronize a specific brand type–store format combination during a period. Considering only those observations with existing expenditures or adding a small constant may lead to biased estimates ([45]). This bias may be quite substantial in our case, where zero expenditures make up between 2.6% for NBs in nondiscounters and 20.8% for NBs in discounters of all the observations. To solve this issue appropriately, we follow the procedure for Type II Tobit models ([63], pp. 560–66). In a first step, we apply a probit model with a random intercept specification and pooled coefficients for brand type–store format choice. This approach allows for the fact that households may patronize multiple brand type–store format combinations. We use the same set of independent variables as in the basket allocation models and additional instrumental variables (average number of shopping trips and unique retailers visited, share of income spent on CPGs, and per capita CPG spending) for identification purposes. In a second step, we compute the inverse Mills ratio, InvMillsbht, based on the probit model results for each brand type-store format combination as InvMillsbht = φ(Xbht′η/Φ(Xbht′η), where φ is the standard normal density function, Φ is the standard normal cumulative distribution function, and η is the vector of parameters from the probit model. The inverse Mills ratio is then added for each brand type–store format combination as an additional independent variable in the basket allocation model to correct for interrelations between brand type–store format choice and spending. As before, we also add Gaussian copulas for all brand type–store format combination specific marketing-mix variables to account for potential endogeneity issues. Results Model Estimation and ValidationWe use Latent GOLD 5.1 ([59]) to estimate the seemingly unrelated regression system consisting of seven equations with a maximum likelihood approach. All the models converged before reaching the maximum number of iterations. Because we use data from 2006 for parts of the variable operationalization, we run the model on data from 2007–2013. For holdout validation, we randomly sample 500 households from the filtered data set and run the final estimations on the remaining 4,601 households. Starting with an intercept, time, and sample selection control model (Model 1), we sequentially add the dependent variable from the initialization period (Model 2); marketing-mix variables and endogeneity controls (Model 3); and demographic (Model 4), psychographic (Model 5), and symmetric micro and macro variables (Model 6). Finally, we replace the symmetric with the asymmetric micro and macro variables (Model 7). Table 6 provides an overview of the model-building process and fit statistics. Relying on the Akaike and Bayesian information criteria, Model 7 offers the best fit. We further scrutinize Model 7 for overfitting. We compare its mean squared errors and mean absolute errors between the estimation and holdout sample and find that they are very similar, showing no sign of potential overfitting.GraphTable 6. Model Building and Fit Statistics. ModelComponentsEstimation SampleParametersLLBICAICM1Intercept + Time + Sample Selection Controls−395,450791,524791,04874M2M1 + Dependent Variable from Initialization Period−287,496575,674575,15381M3M2 + Marketing Mix + Copulas−281,705564,801563,739165M4M3 + Demographics−273,889549,407548,165193M5M4 + Psychographics−270,205542,273540,852221M6M5 + Symmetric Economic Conditions−270,034542,051540,539235M7M5 + Asymmetric Economic Conditions−269,968542,036540,434249 9 Notes: LL = log-likelihood; BIC = Bayesian information criterion; AIC = Akaike information criterion. Note that only models M3–M7 can be compared to one an other as they incorporate the same set of instruments and vary only by their exogenous variables ([19]). Symmetric Effects of Micro and Macro Conditions on Shopping OutcomesAlthough the asymmetric model (Model 7) shows the best fit, we briefly present the results from the symmetric model specification (Model 6) to check for internal consistency across the two models. Table 7 provides an overview of all significant elasticities of micro and macro conditions on basket value and basket allocation measures. The complete results of the symmetric model are available in Web Appendix C, Table WC1. Overall, we find significant influences on household shopping behavior for changes in households' micro and macro conditions. However, the nature of these influences clearly varies.GraphTable 7. Overview of Significant Elasticities. Basket AllocationBasket ValueVariablePLDisc SpendingNBDisc SpendingPLNonDisc SpendingNBNonDisc SpendingTotal SpendingPurchase VolumePrice IndexSymmetric model (M6)BCycle−.70***−.63***.27***−.06*−.06*−.01*IncomeChange.08***.07***.06***Asymmetric model (M7)Expansion−.94***−.71***.52***−.01*Contraction.36**−.32*.51***.14**.11*IncomeGainIncomeLoss−.10**−.16***−.12***−.11*** 10 *p < .1.11 **p < .05.12 ***p < .01.13 Notes: The table illustrates only significant elasticities. PLDisc = private labels in discounters; NBDisc = national brands in nondiscounters; PLNonDisc = private labels in nondiscounters; NBNonDisc = national brands in nondiscounters. Complete results of the asymmetric Model 7 are provided in Table 8. Complete results of the symmetric Model 6 are provided in Table WC1 of Web Appendix C. Micro conditionsIn line with economic theory, we find significant positive elasticities of income change on shopping basket value in terms of total spending (δ = .07, p < .01) and purchase volume (δ = .06, p < .01). Given that these elasticities are very similar in size and both variables are representations of a household's shopping basket in euros featuring comparable means, we can deduce that the majority of the expenditure effect is merely driven by volume adjustments. In fact, these volume adjustments are mainly attributable to purchases of NBs in nondiscounters, as indicated by the significant positive elasticity of income change on NB spending in nondiscounters (δ = .08, p < .01). Importantly, we do not find any structural shifts in households' basket allocation in that households increase (decrease) spending for a specific brand type–store format combination and simultaneously decrease (increase) spending for another. Macro conditionsUnder changing macro conditions, the results are different. We find marginally significant negative elasticities of the business cycle on shopping basket value dimensions (i.e., total spending [δ = −.06, p < .1], purchase volume [δ = −.06, p < .1], and price index [δ = −.01, p < .1]). Though intuitively surprising, the results confirm previous studies showing countercyclical CPG spending behavior of households (in value and volume) along the business cycle (e.g., [46]). In addition, we also find several significant elasticities of the business cycle on households' shopping basket allocation. In particular, the elasticity of the business cycle on PL spending in discounters (δ = −.70, p < .01) and nondiscounters (δ = −.63, p < .01) is significantly negative, respectively, whereas it is significantly positive on NB spending in nondiscounters (δ = .27, p < .01). This finding indicates that, to some degree, households shift from PLs in discounters and nondiscounters to NBs in nondiscounters—and vice versa—when macro conditions change. Moreover, when shifting their basket allocation across brand types–store format combinations, households also tend to purchase items at lower prices, for example, through temporary price promotions, as indicated by the negative effect of macro conditions on the price index. Asymmetric Effects of Micro and Macro Conditions on Shopping OutcomesTable 8 shows the estimation results of the asymmetric model. For better comparability of the impact of micro and macro conditions, Figure 2 provides an overview of the asymmetric effects of micro and macro conditions on basket allocation and basket value at their respective mean values—specifically, 2.42 (1.37) for Expansiont (Contractiont) and €176.70 (€124.77) for IncomeGainht (IncomeLossht), which translates to 7.8% (5.5%) of mean income. The findings from the symmetric model are confirmed by the asymmetric model, although the asymmetric estimation results show that the underlying effects are not symmetric but differ strongly in terms of size as well as significance between beneficial and adverse conditions.Graph: Figure 2. Asymmetric elasticities at mean values for micro and macro conditions.GraphTable 8. Results of Asymmetric Model 7. VariableBasket AllocationBasket ValuePLDisc SpendingNBDisc SpendingPLNonDisc SpendingNBNonDisc SpendingTotalSpendingPurchaseVolumePriceIndexIntercept2.6310**(1.3047)−4.4137*(2.3163)−2.0639(1.3452)4.6453***(1.0345)−1.0209(2.6274).4872(2.6826).0460(.1136)Random intercept−.4634***(.0131)−.1435***(.0308).1604***(.0241).2319***(.0260).0279***(.0107)−.0194*(.0103)−.0006(.0006)Micro and Macro ConditionsExpansion−.9387***(.1925)−.0063(.2001)−.7139***(.1816).5186***(.1132).0198(.0546).0024(.0548)−.0083*(.0048)Contraction.3578**(.1496)−.3242*(.1968).5068***(.1602).0485(.1198).1357**(.0605).1086*(.0600).0015(.0050)IncomeGain−.0341(.0413).0479(.0471).0284(.0428).0112(.0352).0183(.0206).0153(.0205).0011(.0014)IncomeLoss−.0977**(.0447).0124(.0532)−.0508(.0463)−.1567***(.0380)−.1208***(.0224)−.1053***(.0219)−.0005(.0017)ControlsDV(t = 0).3535***(.0245).2117***(.0116).3187***(.0135).5997***(.0207).5910(.0148).5719***(.0152).7263(.0162)(Rel)Price−.6183(1.2878)1.8388(2.1456)−1.5194(1.7101)−.2376(1.5296).3850(.7840).0194(.8050)−.0384(.0863)(Rel)Assort−.4142**(.1789).4310(1.1226)−.2307(.2178)1.5306***(.3764).4395*(.2495).1980(.2603).0239(.0169)(Rel)Promo.1519**(.0725)1.2602(3.2248).4613*(.2525).8275*(.4941)−.4199(.2621)−.2884(.2691)−.0300**(.0141)(Rel)PctPL−7.0982***(2.0868)5.3710*(2.9790)−1.4206**(.6413)−.1210(.2981)−.5991***(.0864)−.5763***(.0930)−.0025(.0096)(Rel)AdvStore−.2452**(.1031).0052(.1281).5303***(.1342)−.2811***(.0778).1275***(.0173).1022***(.0194).0005(.0022)AdvNB.1265***(.0453).2301***(.0763).1619***(.0594)−.2903***(.0418)−.0396*(.0223)−.0488**(.0230)−.0008(.0022)HhSize.3706***(.0479).4305***(.0297).3406***(.0272).3722***(.0276).3077***(.0154).3250***(.0164)−.0018**(.0008)Age−.1958***(.0689)−.1135*(.0632)−.1397**(.0543).1270***(.0491).0078(.0228)−.0085(.0224).0039**(.0019)Kids−.0057(.0308)−.0594*(.0334)−.0104(.0298)−.0593**(.0234)−.0148(.0133)−.0086(.0135)−.0002(.0010)Unemployed−.0617**(.0274)−.0092(.0364).0563(.0342)−.1157***(.0278)−.0533***(.0167)−.0169(.0165)−.0010(.0012)QualCons−.0224(.0262).0286(.0278)−.1167***(.0253).0860***(.0200).0291***(.0109)−.0175(.0108).0012(.0008)PriceCons.0654***(.0216)−.0814***(.0275).0516**(.0231)−.1366***(.0177)−.0583***(.0109).0018(.0110)−.0098***(.0009)DealProne.0196(.0349).2549***(.0402)−.1277***(.0349).0485*(.0268).0499***(.0153).0830***(.0152)−.0158***(.0012)EatOut−.0581**(.0243)−.0373(.0258)−.0100(.0232)−.0282(.0186)−.0273**(.0107)−.0360***(.0105).0017**(.0008)Time−.0197(.0121).0781***(.0112).0382***(.0123)−.0425***(.0090)−.0233***(.0051)−.0270***(.0056)−.0001(.0006)Quarter 2.0327*(.0188)−.0842***(.0310)−.0355(.0220).1078***(.0174).0379***(.0098).0440***(.0102).0005(.0010)Quarter 3.0154(.0134)−.0695***(.0219)−.0559***(.0157).0260**(.0102).0002(.0066).0089(.0070).0005(.0007)Quarter 4−.0026(.0136)−.0076(.0231)−.0193(.0168).1187***(.0113).0258***(.0063).0228***(.0065)−.0001(.0006)Copula (Rel)Price.0483(.0587)−.0626(.0717).0594(.0937).0506(.0458)−.0243(.0446)−.0255(.0459).0019(.0049)Copula (Rel)Assort.0010(.0284).0143(.0742).0191(.0317)−.1207***(.0297)−.0719(.0603)−.0222(.0629)−.0041(.0041)Copula (Rel)Promo.0785***(.0136)−.0515(.2441)−.0289(.0420)−.0212(.0414).0829(.0659).0607(.0676).0051(.0036)Copula (Rel)PctPL.2990***(.0623)−.0955(.0824).1306***(.0470).0149(.0228).0431***(.0103).0201*(.0109).0015(.0012)Copula (Rel)AdvStore.0063**(.0028).0001(.0048)−.0330***(.0061).0074**(.0033)−.0017(.0012)−.0023*(.0014).0003**(.0001)Copula AdvNB−.0057*(.0031)−.0018(.0047)−.0124***(.0045).0027(.0028)−.0042***(.0016)−.0031**(.0015).0001(.0001)InvMills.1907(.1656)−.2506***(.0703)−.0832(.0734)−.0260(.2121)N131,566113,092121,787139,163142,828142,828142,828Pseudo-R2.59 14 *p < .1.15 **p < .05.16 ***p < .01.17 Notes: PLDisc = private labels in discounters; NBDisc = national brands in nondiscounters; PLNonDisc = private labels in nondiscounters; NBNonDisc = national brands in nondiscounters. Standard errors are in parentheses. Micro conditionsRegarding micro conditions, we again find that micro conditions primarily have an impact on households' shopping basket value but do not cause shifts in households' shopping basket allocation. However, the results reveal substantial asymmetries between beneficial and adverse micro conditions. Most notably, income gains have no effect on households' basket value or basket allocation; only income losses show significant effects. More precisely, a 1% loss in income decreases total spending and purchase volume by.12% (p < .01) and.11% (p < .01), respectively. Owing to the similar size of the elasticities, we can again assume that expenditure reductions are largely driven by volume reductions.[13] Given that income losses show no effect on households' price index, we can rule out the notion that expenditure reductions stem from households' shopping for lower prices.Importantly in the context of income losses, we also see no evidence that households shift their basket allocation to less expensive brand type–store format combinations. Rather, we find significant negative elasticities of income losses only on NB spending in nondiscounters (γ = −.16, p < .01) and PL spending in discounters (γ = −.10, p < .05), respectively. Thereby, we can conclude that the adjustments in purchase volume—and subsequently total spending—predominantly stem from abandoning NBs in nondiscounters and PLs in discounters when income losses occur. Instead of shifting to cheaper store formats, brand types, or both, households give up the relatively more expensive NBs in nondiscounters without substituting them with cheaper alternatives such as NBs in discounters or PLs in general. This lack of substitution is also true for PLs in discounters, but in this case options for shifting to even cheaper alternatives to reduce spending are limited, and therefore, volume adjustments are households' last resort. That is, households' primary means of coping with adverse micro conditions is to reduce expenditures on specific brand types and store formats and thereby reduce shopping basket value (i.e., spending less by purchasing lower volumes) rather than adjusting basket allocation by shifting to cheaper brand types or store formats. Macro conditionsIn contrast to adverse micro conditions (i.e., income losses), economic contractions not only have an impact on households' shopping basket value but also cause shifts in basket allocation. With regard to basket value, we find a significant increase in total spending and a marginally significant increase in purchase volume when the economy contracts: a 1% decrease in GDP, compared with its prior peak, increases total spending by.14% (p < .05) and purchase volume by.11% (p < .1). As already indicated for the symmetric model, previous studies also find countercyclical buying behavior of households during adverse macro conditions ([46]).[14] The results confirm and extend these findings by showing that increased total spending and purchase volume are not the only effects during economic downturns, as contractions also cause shifts of households' shopping basket allocation. In particular, we find significantly positive elasticities of contractions on PL spending in discounters (γ = .36, p < .05) and nondiscounters (γ = .51, p < .01), respectively; as well as a marginally significant negative elasticity of contractions on NB spending in discounters (γ = −.32, p < .1). These findings suggest that households shift from NBs to PLs during unfavorable macro conditions. Although previous studies find comparable changes (e.g., [17]; [43]), the combined results further illustrate one important phenomenon: even though households purchase PLs to a greater extent, they increase total spending and purchase volume. Moreover, the results suggest that by switching from NBs to PLs, NBs are not affected by economic downturns per se, but only in the context of discounters. That is, we only find the contraction elasticity of NB spending in discounters to be marginally significant and negative.The estimated elasticities during economic expansions further substantiate that changing macro conditions cause shifts in households' shopping basket allocation. Inversely to contractions, we find significant negative elasticities of expansions on PL spending in discounters (γ = −.94, p < .01) and nondiscounters (γ = −.71, p < .01), respectively. At the same time, we find a significant positive effect on NB spending in nondiscounters when the economy expands (γ=.52, p < .01). In addition, the results show a marginally significant and negative elasticity of an expansion on the price index (γ = −.01, p < .1). This result complements the findings on households' shifts from PLs in discounters and nondiscounters to NBs in nondiscounters during favorable economic times. In fact, to keep their purchase volume and total expenditures steady while shifting to more expensive NBs, households seem to actively seek price-promoted items to keep the prices they pay low.Overall, the results show major differences in the effects of micro and macro conditions on households' shopping behavior. While favorable micro conditions show no effect at all, adverse micro conditions lead households to reduce expenditures for specific brand types and store formats, resulting in lower total spending and purchase volumes. In contrast, favorable and unfavorable macro conditions primarily result in shifts of shopping basket allocation. These results highlight the importance of separating micro from macro conditions to identify their unique properties, effects, and implications. Effects of Control Variables on Shopping OutcomesAlthough the control variables included in the asymmetric Model 7 are not of primary interest, they are important to rule out rival explanations and thus to support the causal interpretability of the main results. Therefore, we briefly summarize them here; a more detailed discussion can be found in Web Appendix C. For the most part, when significant, the effects of the included control variables are intuitive and in line with prior research. Marketing-mix variablesAs expected, we find a marginally significant positive effect of assortment size (in terms of unique SKUs) on total spending and a significant positive effect on NB expenditures in nondiscounters. We also find several effects of promotion activity (in terms of unique SKUs sold on promotion): a negative effect on the price index, a marginally significant positive effect on NB spending in nondiscounters, a positive effect on PL spending in discounters, and a marginally significant positive effect on PL spending in nondiscounters. It is noteworthy that the effects for PLs are of smaller magnitude and confirm prior research showing that retail promotions are less positive for PLs than for NBs ([57]). We also find that the share of unique PL SKUs in the total SKU assortment has a negative effect on total spending and purchase volume, suggesting that focusing too strongly on PLs can have unfavorable consequences for retailers (e.g., [ 2]). Finally, advertising at the store level has the expected positive effect on total spending, purchase volume, and PL spending in nondiscounters, while NB advertising has an expected positive effect on NB spending in discounters.However, we also note that some of the effects are counterintuitive. This is particularly true for the negative effects of assortment size and PL share in assortments, negative own-advertising effects, and positive cross-advertising effects as well as the absence of significant price effects. Varying perceptions of PLs and NBs in assortments (e.g., [ 5]; [14]; [34]), underlying advertising spillover effects ([ 4]), or potential difficulties when measuring advertising effects ([51]; [52]) may provide reasonable explanations for these findings. Counterintuitive marketing-mix coefficients may, however, also be caused by the aggregation level of the data (quarterly, national-level aggregation across many individual brands, retailers, and product categories). Demographic variablesAs expected, we find that larger households tend to spend more across all four brand type–store format combinations, spend more in total, purchase larger volumes, and maintain a lower price index. Older households typically spend less on PLs in general as well as spend marginally significantly less on NBs in discounters, but more on NBs in nondiscounters while exhibiting a higher price index. Furthermore, the results suggest that households with children spend less on NBs in nondiscounters and marginally significantly less on NBs in discounters, respectively. Households that suffer from unemployment of the main breadwinner tend to spend less in total, corresponding to fewer expenditures on both NBs in nondiscounters and PLs in discounters. Psychographic variablesIn terms of psychographics, the analyses reveal many significant effects, generally underscoring the importance of accounting for such types of consumer characteristics ([ 2]). In particular, we find that quality-conscious households tend to spend more in total, more on NBs in nondiscounters, and less on PLs in nondiscounters. In comparison, price-conscious households typically spend more on PLs and less on NBs in general, spend less overall, and exhibit a lower price index. Deal-prone households, furthermore, spend more in total, purchase larger volumes, exhibit a lower price index, and spend less on PLs in nondiscounters, but significantly more on NBs in discounters and marginally significantly more in nondiscounters. Finally, households with preferences for eating out tend to spend less overall, purchase lower volumes, but exhibit a higher price index and typically show lower spending for PLs in discounters. Robustness ChecksWe perform several robustness checks to confirm the validity of the findings by applying alternative measures and indicators for micro and macro conditions. First, we use the growth rate of real GDP (e.g., [38]; [46]) and an index of consumer confidence (e.g., [ 3]) to assess the general state of the economy. To a large extent, the results are consistent in significance, direction, and magnitude with the main symmetric model (Model 6). Second, we use first-difference specifications of micro conditions rather than differences relative to prior income peaks and troughs as in the main asymmetric model (Model 7). All effects are consistent in significance and direction, even though the elasticities are of a higher order of magnitude. Third, we introduce an individual-level measure of a household's perceived financial situation into both main models. This measure captures changing perceptions of micro conditions that are not reflected in household income (e.g., wealth). Controlling for individual financial perceptions does not alter the findings regarding income, and we can confirm all effects to be consistent in terms of significance, direction, and the order of magnitude. All significant effects of the financial perception measure itself are in line with economic theory. We present and discuss these results in greater detail in Web Appendix C. DiscussionMicro and macro conditions have significant effects on households' shopping behavior and outcomes that, by extension, may affect firm performance of retailers and manufacturers. By observing shopping basket allocation across brand types and store formats as well as shopping basket value in terms of total spending, purchase volume, and an index of prices paid, this research provides an extensive analysis of how (through shopping basket allocation) and how much (through shopping basket value) households adjust the various facets of their CPG shopping behavior. Thus, we distinguish the effects caused by micro conditions in terms of income and macro conditions in terms of the business cycle. In addition, we account for possible asymmetries between adverse and beneficial conditions. These findings, based on a rigorous modeling approach and longitudinal field data, have important diagnostic and normative value for managers and contribute to previous research on business cycle effects. We provide an overview of the results and associated implications in Table 9.GraphTable 9. Overview of Results and Implications. OutcomesMain FindingsInterpretation and ImplicationsShopping basket allocationPL discounter spendingMoves countercyclically with macro conditions, decreasing in expansions and increasing in contractions. Decreases with adverse micro conditions.As social acceptance of and demand for PLs increase during contractions, discounters can narrow their price gap to NBs. This allows for more profitable price reductions that discounters should deploy cyclically to counteract shifts to nondiscounters and NBs during expansions and adverse micro conditions. Soft discounters should extend their PL portfolio during contractions.NB discounter spendingMoves cyclically with macro conditions, decreasing substantially in contractions.Buffer discounters' and manufacturers' revenue losses during adverse micro conditions. Brand managers should extend their portfolio to discounters in these conditions to counteract losses from NBs sold in nondiscounters. Especially hard discounters may profit from a larger NB portfolio.PL nondiscounter spendingMoves countercyclically with macro conditions, decreasing in expansions and increasing in contractions.Allow nondiscounters to grow revenues even during contractions. Nondiscounters can use this opportunity to extend their PL portfolios to new product categories and price-tiers and strengthen their branding to counteract shifts back to NBs during expansions. As they are unaffected by increasing budget constraints, nondiscounters may adjust prices countercyclically to reap additional revenues during contractions and defend against NBs by deploying price reductions during expansions.NB nondiscounter spendingMoves cyclically, increasing during expansions. Decreases with adverse micro conditions.Are affected the strongest by adverse micro conditions. Manufacturers and nondiscounters can react to this through status appeals in their communication. As households do not switch due to budget constraints, marketers should not waste budgets on price promotions but provide ""cheap"" mechanisms that provide consumers with a sense of control and frugality such as loyalty and reward programs or (digital) store fliers.Shopping basket valueTotal spendingGrows with adverse macro conditions. Shrinks with adverse micro conditions.As long as households are not affected at a micro level, they increase their purchased volumes and total spending during contractions. Managers can leverage households' increased consumption and cognitive load from shifts in spending through larger package sizes and in-store promotions. Measures that provide a sense of control and frugality such as loyalty programs or quality and status appeals may further increase compensatory consumption. During expansions, retailers and manufacturers should utilize the increased deal proneness and price savviness through price promotions and couponing.Purchase volumeGrows with adverse macro conditions. Shrinks with adverse micro conditions.Price indexGrows with adverse macro conditions. The results uncover and juxtapose the specific effects of micro and macro conditions on shopping behavior. We find that both micro and macro conditions have pronounced effects on households' shopping behavior that are distinct from one another and asymmetric for positive versus negative conditions. Some findings are especially intriguing: micro conditions affect only households' overall consumption levels, whereas macro conditions also lead to structural shifts in households' budget allocation across brand types and store formats. In addition, during changing macro conditions, household adjust their shopping behavior even if they are not affected financially (as we control for income). In this section, we first summarize the results and subsequently discuss potential underlying psychological and sociological mechanisms before addressing interaction effects and asymmetries. Micro ConditionsAlthough no significant adjustments in shopping basket allocation or value emerge for income gains, income losses lead to a general decline in CPG expenditures. This drop is largely driven by households purchasing less and thus spending less. The overall decrease in consumption specifically affects PLs purchased in discounters and NBs purchased in nondiscounters. These findings show that, rather intuitively, budgetary constraints lead to decreased consumption, adding to extant research that has mostly taken a spending perspective (e.g., [38]). However, the absence of structural shifts in households' budget allocation is noteworthy. Theoretically, households could also reduce spending by switching to a cheaper store format or brand type, but instead they generate savings primarily through volume reductions. Macro ConditionsIn contrast, changing macro conditions evoke structural shifts in households' basket allocation. During contractions, we see expenditures for NBs purchased in discounters being reallocated to PLs purchased in discounters and nondiscounters. While this seems intuitive, it is interesting to note that this shift is accompanied by a general increase in total spending driven by households buying more. In other words, even though households switch to PLs during contractions, they end up spending more in total.During expansions, households reallocate their purchases from PLs (purchased in nondiscounters as well as discounters) to NBs purchased in nondiscounters. Interestingly, we also find that total spending and volumes purchased remain unaffected at the same time, because households focus more on getting deals, as indicated by a decline of the index for prices paid. As such, households switch to a more expensive brand type during expansions although their budget remains constant (as we control for income), which seems to be feasible as they increasingly purchase products on price promotion. Plausible Mechanisms Underlying Micro and Macro EffectsSeveral theoretical mechanisms can explain our findings. First, the findings suggest that adverse macro conditions may have a societal impact that trickles down to individual households even if they are not affected at a financial level. In trying times, frugal consumption, such as buying PLs or shopping at discounters, seems to become more socially acceptable and even fashionable ([22]; [38]), which is in line with the shifts of budgets toward PLs in (non)discounters that we observe during contractions. Just as much as frugal consumption may become increasingly commonplace during contractions, purchasing NBs may become a societal norm and is required if households want to maintain their social standing during expansions ([38]). In accordance with that norm, households seem to drop PLs in favor of NBs in nondiscounters even though they have no increase in budgets, as we see in the results. They seem to accommodate this shopping behavior by being price-savvy, shopping products on price promotion. Price promotions may also offer a welcome justification for households to abandon the PLs they have adopted during prior contractions in favor of NBs.This reasoning is also consistent with the lack of shifts in the face of adverse micro conditions, as described previously. An income loss, independent of macro conditions, is first a personal hardship rather than one shared by society. Therefore, there is not a general move to and acceptance of PLs and discounters, as in the case of adverse macro conditions ([22]; [38])—households do not switch to these cheaper brand types or store formats but instead reduce their overall consumption. In addition, income losses may weaken self-confidence and, thus, awaken a desire to bolster one's social status ([31]; [53]), which may lead households to continue buying NBs while economizing on volume to accommodate their lower income.Another explanation for these findings may lie in households' perception of the nature of micro and macro conditions. While a nationwide or global contraction is beyond households' direct control, personal income can be influenced through concrete actions. This discrepancy in the ""mutability"" of the conditions leads to different reactions in households: whereas high-mutability conditions (here: micro conditions) result in high self-regulation, planning, and prioritizing, low-mutability conditions (here: macro conditions) elicit a desire for restoration of control ([ 7]; [31]). Adverse micro conditions lead households to self-regulate by reducing their overall consumption, whereas adverse macro conditions result in a desire to restore control through actions that are perceived as more frugal (i.e., purchasing PLs). Control-restoration behaviors are also associated with compensatory consumption, such as in the form of overspending and higher food intake ([ 7]; [44]), which may explain the overall increase in household spending and which is potentially aggravated by the lack of a budgetary constraint that would limit this behavior ([62]).Other explanations of the increased consumption may lie in households' shift to PLs, which usually are associated with larger package sizes and lower product prices and which have been shown to increase consumption ([ 6]; [61]). Similarly, these factors contribute to households' purchase of increased quantities when shopping in warehouse club stores ([ 4]). In addition, adding discounter visits to a shopping trip may increase households' spending owing to self-licensing and self-control depletion ([31]). Asymmetries and InteractionsLike previous studies in the field, we find asymmetries between adverse and beneficial conditions for both micro and macro conditions. In the case of micro conditions, we find that income gains generally have no significant effects on shopping outcomes, whereas income losses do. This finding suggests that households are quick to decrease spending when income decreases but are slow to respond when income increases, potentially because they need to compensate for postponed purchases of durables or paying off debts ([11]). While contractions affect households' shopping basket value more extensively than expansions, the expansion elasticities for shopping basket allocation are mostly larger than during contractions. This response seems reasonable, as failing to keep up with one's surroundings during an expansion would translate into a loss of status, whereas not adopting a more frugal shopping behavior during a contraction implies an increase in status ([38]). In addition, we find more pronounced asymmetries between adverse and beneficial conditions at the macro level than at the micro level. Thus, adjustments in shopping behaviors may reverse more quickly when they are caused by changing micro conditions compared with macro conditions. Given that adverse macro conditions shift the societal acceptance of certain brand types and store formats, households' attitudes may change ([32]). This reasoning implies that macro conditions' effects on shopping outcomes linger longer than micro conditions, during which households engage in status-maintaining shopping behaviors. Therefore, these status-maintaining shopping behaviors may be a means to an end rather than an attitudinal shift and households would quickly discard them once conditions improve.Finally, we investigate whether micro and macro conditions and the underlying mechanisms that affect households' shopping behavior moderate each other. Thus, we perform a post hoc analysis to test for possible interaction effects for which we present complete results in Web Appendix C, Table WC3.[15] Interestingly, the main effects remain unchanged while all interaction effects are insignificant, which suggests that micro and macro conditions do not moderate each other. Thus, the results indicate that the effects and mechanisms that micro and macro conditions elicit occur independently from each other. That is, if both conditions change simultaneously, their individual effects on households' shopping outcomes work in parallel. Managerial Implications Micro ConditionsChanging micro conditions affect shopping outcomes only when households suffer income losses rather than gains, leading to a decrease in PLs purchased in discounters and NBs purchased in nondiscounters. To buffer the negative effects of when and where they expect wages to decrease, manufacturers as well as discounters can profit from listing NBs in discounters. In particular, hard discounters such as Aldi and Lidl, whose overwhelming majority of revenues stem from their own PLs, may profit from this strategy. Thus, we provide an additional perspective to the literature investigating the role of NBs in discounters (e.g., [12]). If households indeed suffer from weakened self-confidence and want to bolster their social status as a result of adverse micro conditions ([31]; [53]), NB manufacturers and nondiscounters may leverage this reaction by using status appeals in their advertising. Because adverse micro conditions lead to a general decline in consumption, retailers and manufacturers could target product categories that are affected the most with marketing-mix actions. Changing micro conditions may be especially hard for manufacturers and retailers to identify, but with increasing availability of data through loyalty cards and online shopping, managers could detect the specific shopping outcomes associated with these changes and address those households through personalized coupons and deals. Macro ConditionsChanging macro conditions substantially affect households' shopping basket allocation as well as value. Given the increased acceptance of PLs during contractions, retailers can use the opportunity to extend their PL portfolio into higher price tiers and product categories with high involvement and complexity ([56]). In addition, they may narrow their price gap to NBs and strengthen their branding to preemptively counteract households' shifts back to NBs during subsequent expansions. During expansions, they could then offer more attractive and profitable price promotions. In particular, nondiscounter PLs may get away with raising prices because they are unaffected by increasing budgetary constraints. Given the countercyclical susceptibility of PLs, retailers should adjust their assortment accordingly, reducing their PL share in expansions and increasing it in contractions. While hard discounters are especially susceptible to adverse micro conditions, soft discounters (i.e., discounters with a relatively low PL assortment share) should be aware of contractions owing to the substantial negative effect of NBs purchased in discounters and their comparatively low share of PLs that may compensate the losses.Because we control for micro conditions, the reallocation of budgets to PLs that we observe during adverse macro conditions is apparently not driven by monetary factors but instead may result from changing attitudes toward frugal consumption across society ([38]) and a desire to restore control ([ 7]). If this reasoning holds, it has important implications for managers. NBs and retailers can avoid costly price reductions that are ineffective given the lack of a more constrained budget and instead use measures that provide a perception of frugality.[16] These measures may allow households to engage in behaviors that they associate with economizing but, at the same time, are economical for the retailer or manufacturer. For example, loyalty programs can offer low price discounts and small rewards, giving households the perception that they engage in frugal consumption ([45]). Distribution of (digital) store fliers may create a sense of greater control over the planned shopping trip. In addition, communication may highlight the quality and reliability of products to reduce uncertainty and increase compensatory consumption. NB managers might also consider increasing package size, as larger package size is often associated with a lower per unit price ([ 6]). Finally, NB managers and retailers can leverage the higher cognitive load and depletion of self-control resulting from switching stores and/or brands ([60]), rendering shoppers more susceptible to in-store promotions ([31]). Limitations and Directions for Future ResearchWhen individual income is controlled for, changes in observed shopping behaviors resulting from macro conditions are clearly linked to households' willingness, rather than ability, to purchase. Potential underlying changes in attitudes and societal acceptance of certain shopping behaviors provide a conclusive basis for our argumentation. However, we do not observe these changes of attitudes in the data directly. Therefore, we encourage field experiments and laboratory studies to dive deeper into the underlying psychological and sociological mechanisms that might drive these findings. These insights can be crucial in predicting how households will change their CPG shopping in reaction to other types of macro conditions, such as a worldwide pandemic.Including demographics and psychographics, we control for household characteristics but do not account for heterogeneity in households' reaction to changing conditions, which should be addressed by future research. Heterogeneity may originate, for example, from households' differing preferences for high-quality products, with those preferring high quality potentially opting for adjustments in the volume purchased and the price paid for a good over switches to low-tier NBs and PLs. Alternatively, heterogeneity might stem from households' usual ""baseline"" shopping behavior because it influences whether and how they are able to economize during adverse conditions.Future analyses could also differentiate among different product categories, especially relating to the reduction in consumption levels caused by adverse micro conditions. Some product categories may be more essential than others and, thus, consumption may not simply be reduced ([38]). Some product categories may even experience increasing consumption—for example, as households shift from soft drinks and juices to plain water.Finally, previous research has shown that macro conditions affect marketing-mix decisions ([58]). Thus, future research could take a corporate rather than household perspective, investigating how managers detect and react to changes in micro conditions. Supplemental Materialsj-pdf-1-jmx-10.1177_00222429211036882 - Supplemental material for Households Under Economic Change: How Micro- and Macroeconomic Conditions Shape Grocery Shopping BehaviorSupplemental material, sj-pdf-1-jmx-10.1177_00222429211036882 for Households Under Economic Change: How Micro- and Macroeconomic Conditions Shape Grocery Shopping Behavior by Thomas P. Scholdra, Julian R.K. Wichmann, Maik Eisenbeiss and Werner J. Reinartz in Journal of Marketing  "
11,"Identifying Market Structure: A Deep Network Representation Learning of Social Engagement With rapid technological developments, product-market boundaries have become more dynamic. Consequently, competition for products and services is emerging outside the product-market boundaries traditionally defined based on Standard Industrial Classification and North American Industry Classification System codes. Identifying these fluid product-market boundaries is critical for firms not only to compete effectively within a market but also to identify lurking threats and latent opportunities outside market boundaries. Newly available big data on social media engagement presents such an opportunity. The authors propose a deep network representation learning framework to capture latent relationships among thousands of brands and across many categories, using millions of social media users' brand engagement data. They build a brand–user network and then compress the network into a lower-dimensional space using a deep autoencoder technique. The authors evaluate this approach quantitatively and qualitatively and visually display the market structure using the learned representations of brands. They validate the learned brand relationships using multiple external data sources. They also illustrate how this method can capture the dynamic changes of product-market boundaries using two well-known events—the acquisition of Whole Foods by Amazon and the introduction of the Model 3 by Tesla—and how managers can use the insights that emerge from this analysis.Keywords: artificial intelligence; deep representation learning; social media; competitive market structure; big dataFirms compete in a market to satisfy the specific needs of consumers in the market. The market and the competing products make up a ""product-market,"" with the boundary defining the brands competing within that market. Market structure is defined on the basis of these product-markets and their (possibly overlapping) boundaries. Identifying the product-market boundary and examining the strength of competition between brands within the product-market have long been important issues with strategic implications for next-generation product design, product positioning, new customer acquisition, and pricing and promotion decisions. Rapid changes to the competitive environment, however, have made identifying the product-market boundaries increasingly challenging. With technological advances, the product-market boundaries themselves are changing and competitive threats and opportunities are emerging outside of narrowly defined product-market boundaries.Numerous recent competitive events support the idea that product-market boundaries are highly fluid. For example, the digital camera product-market was upended by technological developments in smartphone categories. Similarly, Tesla, which initially entered the product-market of high-end automobiles with an innovative fuel technology, has since rolled out products for the lower-end market, thereby changing competition in the lower-end product-market as well. Amazon, previously an online platform, essentially crossed product-market boundaries when it acquired Whole Foods and entered the offline product-market. In many such situations, product-market boundaries based on traditional Standard Industrial Classification and North American Industry Classification System codes are inadequate indicators of emerging threats and opportunities. Given the potential for new and unforeseen relationships between brands, managers need deeper insights into the fluid product-market boundaries to be able to spot potential competitors and complements, identify cross-promotion strategies, and develop firm-level strategies.These observations naturally lead to several important questions: How can managers accurately identify potential threats and opportunities? If a competitive threat emerges from a different market, how can managers proactively anticipate such threats? How can we answer these questions and derive marketing insights using easy-to-obtain and publicly available data? Our article aims to answer these questions using large-scale (>100 million) social media user engagement data (likes and comments) spanning several thousands of brands in different product/service categories.Over the years, academics and practitioners have contributed significantly to developing various methods to define and identify market structure (see the review by [46]]). These include survey-based methods such as brand concept maps ([19]) and the Zaltman metaphor elicitation technique ([53]), methodologies based on observational purchase data (e.g., brand switching) ([21]; [37]), consideration sets ([42]), and scanner-based purchase data ([10]; [36]; [45]). Within the online context, researchers have used unstructured user click streams ([32]), online search logs ([24]; [42]), and customer reviews ([26]). Many of these methods use data from the bottom of the purchase funnel, such as evaluation- and purchase-stage data, and thus assume that the product-market boundaries are prespecified. Even those methods that use data from the top of the funnel at the awareness or preevaluation stage, such as forum discussions ([35]) and hashtags ([33]), define a product-market boundary first and then examine the competition within the prespecified product-market to make these methods implementable. Thus, many of the methods are unable to capture changes to the product-market boundaries and/or the impact that a brand from outside the boundary may have on brands within a product-market.Our methodology creates a more inclusive representation of brands by examining brand–user relationships at the top of the purchase funnel. Unlike the extant methods for identifying market structure that use data from consumers' lower funnel activities such as purchase data, brand switching, price comparison data, or consideration data that prespecify boundaries (e.g., [10]; [21]; [37]; [42]; [50]), we use upper-funnel user–brand engagement data (such as liking and commenting on brand posts) from social media that spans product-markets. At the lower end of the purchase funnel, consumers winnow down the brands they consider to a few substitutes. Thus, interactions at this stage are not as informative of the broader (and possibly complementary) linkages between the brands across product-markets, which are captured more easily at the upper funnel. For example, a consumer considering travel may consider hotel or Airbnb options, airline options or travel intermediaries. At this early stage (the upper end of the funnel), understanding such user–brand linkages could be more informative of the broader relationships between the brands on a continuum from substitutes to complements. Our methodology uses such upper-funnel user–brand engagement data to identify these latent relationships among a large number of brands.Many extant studies in market structure, including those mentioned previously and those using big data technologies (e.g., [ 7]; [12]; [26]; [35]; [42]), view the competing/complementary brands as brand–brand networks. That is, they specify the relationship between any two brands using similarity metrics derived from brand switching, co-occurrences, and word embeddings, without directly modeling the entities (customers, individual consideration sets, or individual reviews) that give rise to such similarities. Our methodology based on brand–user networks considers both brands and users as primitives and uses as input the relationship in terms of each user's liking and commenting on brands. The essential difference between these approaches and our methodology is that extant research considers aggregate data of relationships between brands (brand–brand) as input, whereas our methodology considers the disaggregate individual-level relationships between users and brands (brand–user) as input.The distinction becomes more salient when a product-market boundary is not prespecified. Consider, for example, User 1, who likes United Airlines and Hyatt, while User 2 likes Southwest Airlines and Hyatt. When the product-market is prespecified as ""airline brands,"" information about the users liking the Hyatt brand is discarded. As a result, information that could provide insights into the relationship between United Airlines and Southwest Airlines through their relationships with Hyatt is not considered. However, when we do not prespecify the product-market boundaries, we are able to leverage all such information and create a more accurate representation of the brands.From this premise, we first construct a large-scale brand–user network based on user engagement on brands' social media public fan pages. Then, we propose a deep network representation learning method to discover relationships within the data. Specifically, we use a deep learning method suitable for ( 1) handling large data efficiently and ( 2) learning complex patterns from data effectively (see [ 2]; [49]). The process leads to a low-dimensional representation (i.e., a vector) for each brand and each user by training a deep autoencoder on the network data. The deep autoencoder is similar to traditional dimensionality reduction methods such as principal component analysis (PCA) in capturing latent factors in data with few dimensions. It is, however, very different from those methods in that it uses a nonlinear transformation function to learn the latent patterns in data while reducing the noise in the data. In our context, the deep autoencoder can preserve the first-order (user–brand direct connection) and the second-order (two users connecting to the same brand, or one user connecting to two different brands) network topologies. As a result, brands with network structural equivalence are located closer together in the representation space, while brands with dissimilar network structures are located further away from each other. This method also projects users and brands onto the same dimensional space, which can be used for many different follow-up analyses. We use an illustrative example (in Figure 1) to demonstrate how network representation learning works.Graph: Figure 1. An illustration of deep network representation learning.Suppose we have three brand nodes (B1, B2, and B3) and five user nodes (U1, U2, U3, U4, and U5) in a network. Our network representation learning approach aims to find a function that maps each node into a low-dimensional vector (e.g., three dimensions, for the sake of illustration) while the network structural information is preserved maximally. That is, when nodes exhibit similar structures (first order and/or second order), they are projected onto similar vectors and located closer in the reduced three-dimensional embedding space. Because U1 engaged with B1, we expect the vector representation of B1 and U1 to be close. Similarly, B2 is closer to B1 than to B3 because B2 shares more common users with B1 than with B3. Because B2 has connections to U4 and U5, this makes B2 lean toward them.We establish the face validity of our approach through the identification of product-market boundaries. Our analysis of the brand–user engagement data of over 5,000 brands and nearly 26 million users reveals product-market boundaries with high face validity—grouping of specific categories, high-end brands, and overlaps. We then conduct external validation checks using additional sources including survey and Google search trend data. The market structure derived using our approach is highly correlated with those derived using external data sources. Our approach also overcomes common limitations in extant methods such as data sparsity. Our event studies on Amazon's acquisition of Whole Foods and Tesla's introduction of the Model 3 illustrate how our methodology captures the changes in product-markets associated with these events. We also discuss how the market structure maps can reveal opportunities and threats facing a brand. For instance, our market structure identifies Disney Cruise Line and Hyatt—two brands outside the airline market—as proximal brands to Southwest Airlines. Such findings provide opportunities for Southwest, as it can target those who like Disney Cruise and Hyatt in social media, cross-promote its brand by teaming up with Disney Cruise and/or Hyatt on each other's websites, or launch coalition loyalty programs.Our article contributes to product-market research by leveraging the information embedded in big data of user–brand engagement networks to identify product-markets without having to prespecify boundaries. User–brand engagement network data at a high level in the purchase funnel (interest phase), together with deep learning techniques, provide us with insights at a greater scale and level of detail than extant methods. Our ability to map a large number of brands and precisely visualize brand relationships using learned vector representations enables managers to identify opportunities and threats that lie beyond product-market boundaries. Moreover, our method satisfies the three elements widely regarded as essential to successful real-world applications of artificial intelligence: data, algorithm, and computing power ([ 2]). In this article, we leverage deep learning and a network representation learning (algorithm) to understand market structure using large-scale social media data (data). This model implementation is efficient under NVIDIA P100 graphics processing unit, with Tensorflow as the backend framework (computing power). In summary, our study is an apt illustration of how artificial intelligence can be used to tackle a traditional marketing problem and provide richer insights for mangers in a rapidly changing competitive environment. Background and PositioningExtant work in identifying competitive market structures dates to the 1970s (e.g., [ 8]; [20]), when diary panel–based brand-switching purchase data and survey-based consumer judgments of substitution in use or similarities were used to construct market structure maps. These studies depended on customer data generated either at a late stage of the customer journey or at the very beginning of the journey. The increased availability of scanner-panel data of purchases, market structure models with marketing mix (e.g., [ 5]; Kannan and Wright 1991), and dynamic market structure models (e.g., [10]) provided more detailed insights into interbrand relationships and competition. Approaches such as brand concept maps ([19]) and the Zaltman metaphor elicitation technique ([53]) relied on data collected using surveys and, therefore, were effort intensive. Given the scaling issues with the maximum likelihood–based models and the limitations of survey data, the product-market boundaries were prespecified generally at the industry level so that a smaller number of brands within an industry could be analyzed. The advent of online sources, such as review platforms, social media platforms, and clickstream data, dramatically increased the volume and variety of data for market structure studies, especially at the awareness, search, and consideration stages of the customer journey ([24]; [26]; [35]; [42]; see Tables 1 and 2). Even with a large volume of data, these studies predefine the product-market boundaries at the industry level to make the analyses viable.GraphTable 1. Comparison of Different Types of Work on Market Structure Discovery. Primary/Survey DataText MiningSocial Tag–BasedSearch DataShopping DataSocial EngagementData volumeSmallLargeLargeLargeVery largeVery largeData veracityAuthenticNoisyModerately noisyModerately noisyAuthenticModerately noisyPrivacy preservingYesYesYesNo (need to insert a tracking pixel)YesYesData availabilityLow (need to do survey)High (publicly available)High (publicly available)Low (need to insert a tracking pixel)Low (need to partner with retailers)High (publicly available)Data preprocessing costLow (use consideration set directly)High (text mining is error-prone)High (text mining is error-prone)Low (use consideration set directly)Low (use product co-occurrence)Low (use network raw data) GraphTable 2. Summary of Difference Among Extant Literature on Market Structure Discovery. Kim, Albuquerque, and Bronnenberg (2011)Lee and Bradlow (2011)Netzer et al. (2012)Ringel and Skiera (2016)Culotta and Cutler (2016)Nam, Joshi, and Kannan (2017)Gabel, Guhl, and Klapper (2019)Our StudyObjectiveTo visualize user search behavior and understand market structureTo visualize competitive market structure using text mining on customer reviewTo visualize competitive market structure using text mining on forum discussionTo understand asymmetric competition in the product categoriesTo infer attribute-specific brand ratingsTo analyze user-generated tags for marketing researchTo leverage NLP and ML for analyzing market structureTo propose a novel deep network representation learning framework for market structurePrespecifying market categoryYesYesYesYesNoNoNoNoNetwork typesBrand–brandBrand–brandBrand–brandProduct–productBrand–brandBrand–brandProduct–productBrand–userBrands/products62 products, 4 brands9 brands169 products, 30 brands1,124 products200 brands7 brands133 categories, 30,763 products5,478 brandsConsumers/usersN.A.N.A.76,587100,000+14.6 millionN.A.N.A.25,992,832Data sourcesAmazonCustomer review at EpinionsOnline discussion forumProduct comparison websiteTwitterSocial tagging platform DeliciousRetailerFacebook public fan pageData typeConsumer searchTextTextConsumer searchNetworkSocial tagsShopping basketsNetworkBrand association methodologyConsideration setText-miningText-miningConsideration setNetwork learningNetwork learningNetwork learningNetwork learningBrand relationship asymmetryYesNoNoYesNoNoYesYesDimension reductionYesYesNoNoNoYesYesYesExternal validationN.A.N.A.Purchase data,surveySurveySurveyBrand concept map (survey)N.A.Event study,survey, Google search trends 1 Notes: NLP = natural language processing; ML = machine learning; N.A. = not applicable.There are other studies where the product-market boundaries are not predefined: [11] using online reviews, [33] using social tags, and [ 7] using Twitter hashtags. More recently, using word embeddings, [12] analyze customers' market baskets of items purchased on shopping trips. Still, from a methodological perspective all these studies use brand–brand networks—a distinct disadvantage, as we discussed previously. Our methodology uses brand–user networks, and the scale at which we analyze the data is much larger than any of the extant methods (cf. [12]). Social Media EngagementWe analyze social media engagement data in the form of user–brand links. Social media platforms such as Facebook, Twitter, and Instagram host public fan pages created by firms to facilitate communication with customers and promote products. The user–brand engagement could be in the form of a user liking a post by the brand, sharing a brand post, or commenting on a brand post. Because each of these likes, shares, and comments/posts is a user–brand link in our study, it is important to understand what they represent. Surveys of fans of brands have revealed many reasons as to why users ""like"" a brand or post/share comments. Positive motivations for interacting with a brand include to support a brand they like, to get a coupon or discount, to receive regular updates from the brand, to participate in contests, to share personal experiences, to share their interests/lifestyles with others, to research brands, to imitate a friend who likes the brand, or to act on a recommendation from another fan ([25]; [34]; [39]; [41]). Conversely, users may also leave negative comments to hurt a brand in favor of its rival brand ([17]).In our approach, we make a minimal assumption by creating a user–brand link regardless of the type of engagement (like, share, or comment/post). This assumption is based on the rationale that users interacting with a brand online exhibit their interest toward the brand to some extent. Thus, the two brands are related to one another on a spectrum ranging from substitutes to independents to complements. Prior research has examined such contexts and studied the impact of user engagement on brand image and customer purchase intentions with mixed results ([16]; [34]). [31] use a field experiment to find that users who liked a gym brand online were likely to become members of that gym offline. In another field experiment setting, [18] find that ""liking"" is simply a symptom of a positive brand attitude and does not imply the fan is any more loyal to the brand or any more likely to purchase the brand. In addition, it is only when users who liked the brand are targeted using promotional communication by the firm that purchase probabilities increase. Thus, for our research purposes we treat a like or a comment/post as exhibiting an interest toward the brand at the beginning of the customer journey. Such a tendency for users to connect to brands is generally interpreted as interest and may indicate broader (e.g., offline) interactions ([ 7]; [25]; [34]; [35]), which is consistent with our treatment. Our proposed approach is also consistent with research in social network analysis suggesting that social network structure equivalence reflects value/interest homophily and can be used to measure social proximity ([29]). MethodologySocial network platforms, such as Facebook, Instagram, and Twitter, can be abstracted as a network containing business (firm) accounts and individual user accounts. Firms use the public fan pages of business accounts to communicate with their customers and fans. Users interact with brands and with each other in different ways, such as commenting, liking, sharing, and following. To discover latent relationships among brands, we propose a deep network representation learning framework with the following steps. Step 1: Data collectionWe specify a set of brands that is of interest in the social network platform. We then download all available user engagement data from the brands' public fan pages covering an appropriate time window based on managerial interest. A user engagement is defined as either liking or commenting on a firm's post on its public fan page. Note that for the sake of privacy, we do not attempt to collect any personal information of users. Rather, the only user information we obtain is the unique user identifier, assigned by platform, and the user's public engagement activities, consistent with recent studies on social media marketing ([17]; [23]).[ 5] Moreover, different platforms may have their own specific data policy. For example, Facebook does not permit collecting personal information from individuals who liked a given page. Such data restrictions and potential ethical concerns do come at a research cost, as we are unable to verify how representative they are of the population at large. Step 2: Network constructionWe start with a cleansing operation to remove spurious users. We then construct a brand–user network including all selected brands and all users engaging with them. A brand node and a user node are connected if the user engages with the brand. The strength of an edge between a brand node and a user node is the engagement frequency. Step 3: Deep network representation learningThe deep network representation learning algorithm represents each node (brand or user) as a low-dimensional vector, also known as a node embedding. Embedding techniques are not new in marketing. For example, [49] adopt pretrained word embeddings, where each word is represented as a low-dimensional vector, to extract insights from textual reviews. However, our node embeddings are trained via an unsupervised deep autoencoder. This representation learning is essential to data-driven analysis, and the learned low-dimensional embeddings are useful for the downstream task of identifying and visualizing the product-markets.The objective in using an autoencoder is to learn the representation of the data so that each node can be represented in a lower-dimensional space while the network structure between users and brands is preserved. It trains the network to ignore the ""noise"" in the data and focus on the primary latent structure. The autoencoder reduces the dimensionality of the input data to a ""bottleneck"" (the reduced encoding) and, using the reduced encoding as input, reconstructs a representation of the original data. Learning occurs through backpropagation of the loss (see detailed definition in Web Appendix WA1) to achieve a reconstructed representation as close as possible to the original representation. We are interested in the bottleneck-reduced encoding for developing market structure. In essence, we can compare the dimensionality reduction functionality of the autoencoder with that of PCA. Whereas in PCA the reduced dimensions are linear combinations of the input variables, the reduced dimensions in autoencoder are nonlinear and nonorthogonal, which is achieved through nonlinear activations of the neurons, allowing the model to learn more powerful generalizations than PCA can.In our application, the autoencoder works on the large brand–user network in an attempt to preserve the network structure such that ( 1) nodes that are directly connected have similar vectors (are closer to each other) in the reduced embedding space, and ( 2) nodes that are not directly connected but share structural equivalence (such as many common neighbors) are also similar in the embedding space. These two types of similarity are referred to as the first-order (direct connection) similarity and the second-order (network structural equivalence) similarity. Formally, we denote the aforementioned network as  G=(Vb,Vu,E)  , where  Vb=(v1b,v2b,...,vnb)  represents a set of n brand nodes,  Vu=(v1u,v2u,...,vmu)  represents  m  user nodes, and  E={ei,j},i≤m,j≤n  represents all links between users and brands.  ei,j  indicates an engagement between user  i  and brand  j  . Given such a network  G  , the network representation aims to learn a mapping function  f:vib,vju↦wib,wju∈Rd  , where  d≪min(m,n)  .  wib,wju  are called brand embedding and user embedding, respectively. A commonly used embedding dimensionality  d  is 300 ([30]; [40]). The objective of the mapping function is to develop appropriate embeddings so that the brand proximities, brand–user proximities, and user proximities exhibited in the original network are preserved as much as possible in the reduced embedding space. (Technical details of the autoencoder methodology and parameter tuning are discussed in Web Appendix WA1.) Representing brands as dense low-dimensional vectors allows us to capture brand relations from multiple facets, as opposed to using unique vectors for each user and each brand as in a network adjacent matrix representation. An example is illustrated in Figure 1. Step 4: Market structure discoveryDrawing on vector representation for brands and users, we use learned embeddings to efficiently compute similarity among brands and to visualize natural clusters of related brands. Finding similar brands to a focal brand can be achieved by a nearest-neighbor search based on the widely used cosine similarity, which measures the cosine of the angle between two vectors and has a range [−1, 1]. Visualizing natural clusters of related brands can be achieved by a dimension reduction method, such as t-distributed stochastic neighbor embedding (t-SNE; [30]), which projects high-dimensional data into a low-dimensional space (e.g., two or three dimensions).[ 6] It has been used for visualization in a wide range of applications and is especially well-suited for visualizing high-dimensional representations learned from deep neural networks. t-SNE preserves the distance of data points well, such that data points nearby in a high-dimensional space (d = 300 in our case) would be close in a lower-dimensional (e.g., two-dimensional) space, while distant data points would be further apart in a lower-dimensional space. Thus, we observe that related brands are surrounding each other in the reduced two-dimensional space after t-SNE. DataWe use Facebook as our empirical benchmark, as it is one of the largest and most representative online social network platforms. (Our model can be generalized to other similar social network platforms.) To collect Facebook data, we first obtain a list of U.S. brands with the most followers from the social media marketing website Socialbakers.[ 7] Facebook public fan pages are categorized into several groups on Socialbakers, such as Brands, Celebrities, Community, Entertainment, Media, Place, Society, and Sport. We focus on the ""Brands"" category because it covers a wide range of industries and is more interesting to marketers. On Facebook, every brand is associated with a category chosen from the predefined Facebook option when creating the public page. This category label is solely determined by the brand and is aligned with its core business (e.g., Walmart is in the category of ""retail,"" Amazon is in the ""ecommerce"" category). In total, we obtain 5,478 different brands, covering 25 different categories. The largest brand, in terms of number of followers, is Walmart, with 30 million followers. The smallest brand is Bladz Jewelry in the ""fashion"" category, with 100,000 followers. Figure 2 shows the histogram of number of followers of brand Facebook page. We observe that the data set contains brands with varying popularity, making it representative of brands on Facebook.Graph: Figure 2. Histogram of number of followers of 5,478 Facebook brands.On Facebook, firms post on their public fan pages and allow users to comment, like, and share posts. The posts become an important marketing channel for businesses to interact with their customers. We use Facebook Graph API[ 8] to download all activities visible on a brand page such as posts by the brand administrator, as well as posts by users, including comments and likes on brand posts. It is worth emphasizing that to ensure privacy protection, we do not download any user profile information or examine the content of user comments. All engagement activities are represented by unique user identifiers, regardless of whether the user has a public or private Facebook profile, and brand identifiers. The data set collected for this study covers the period from January 1, 2017, through January 1, 2018. In total, we obtain 106,580,172 user–brand engagement activities from 25,992,832 unique users. Because prior research has shown that online interaction is a reflection of broader and even offline interaction ([38]), given the scale of user online engagement in this study, we believe it is a good proxy of how the overall consumer population perceives these brands.To ensure data quality and robust results (i.e., that the comments on Facebook brand pages reflect genuine user experiences, opinions, and interactions with brands), we design a set of rules, following [54], to remove fake users and their corresponding activities. For example, we find one user who liked posts across 475 different brands. As most users are likely to be interested in far fewer brands, we remove users who like posts on more than 200 brands, which accounts for.01% of the total users and 1.6% of the total user–brand engagement. We also remove users who posted duplicate comments containing URL links. Table 3 describes the data details. The brands' degree distribution (number of connections) exhibits a scale-free distribution (shown in Figure 3), a well-documented phenomenon in most social networks.Graph: Figure 3. Degree distribution of brands in the user–brand network.GraphTable 3. Data Description and Statistics. Number of brands5,478Number of users25,992,832Number of unique user–brand interactions36,927,613Number of like interactions87,876,623Number of unique user–brand like interactions29,611,805Number of comment interactions18,703,549Number of unique user–brand comment interactions7,612,358Total number of user–brand interactions106,580,172  Evaluation and ResultsIn this section, we extensively evaluate the market structure derived from our approach from both quantitative and qualitative perspectives. We also validate the derived market structure using two external data sources: consumer survey and Google search trend. Visualization of Market StructureWith the learned brand representation vectors, we can visualize how the brands are grouped and focus on local fine-grained brand proximity. We use t-SNE to obtain market structure visualization by reducing the learned 300-dimensional brand representations to obtain the associated 2-dimensional visualization map. Figure 4 presents the global structure of the brands in our Facebook data. Each data point in the figure denotes a brand belonging to one of the 25 categories, and each category is indicated by a different color. We interpret the visualization as follows: the closer any two brands are in the figure, the more similar their brand representations are in the 300-dimensional space (see Figure 4). The color codes in the map indicate brands in the same Facebook category, with the category label selected by the brands themselves on Facebook.Graph: Figure 4. The global structure among brands.The global Facebook brand market structure map yields several interesting observations. First, there are clear grouping patterns into clusters, particularly between brands in the same industry (points with the same color tend to be in a group). For example, Cluster 1 in Figure 4 (expanded in Figure 5) includes nonluxury domestic and imported automobile brands such as Toyota, Nissan, and Mazda, as well as some automobile accessories brands such as Michelin, DENSO, and Auto Parts. Note that in our data we have several luxury automobile brands such as BMW, Mercedes-Benz, Audi, Tesla, and Maserati, which are not close to the brands in Cluster 1. In fact, they are clustered in a different region of the map with other luxury brands such as Chanel, Gucci, and Cartier. Such a separation between luxury car brands and nonluxury car brands further confirms that brand representation learned from our approach captures latent semantics in multiple dimensions not only on the industry dimension but also on the price and luxury dimensions. The strength of our methodology lies in its ease of capturing these relationships on a single map, which it does by locating thousands of brands in the market structure map and highlighting the complex and possibly overlapping product-market boundaries characterizing these brands. We present a robustness check for different visualization methods in Web Appendix WA5.Graph: Figure 5. Enhanced view of Clusters 1 (top left), 2 (top right), 3 (bottom left), and 4 (bottom right).We provide an enhanced view of the four clusters in Figure 5 to examine the fine-grained local market structures. Panel A displays automobile brands along with automobile accessories and motorcycle brands at the top. Panel B displays premium vacation resort brands, such as The Signature at MGM Grand and the Coconut Bay Beach Resort & Spa. Panels C and D contain airline brands and cosmetic brands, respectively. Taken together, these maps provide face validity to our methodology in terms of core brands making up an industry and the overlaps among product-markets. Identifying Proximal BrandsWhile visual mapping is sufficient to provide a gestalt picture of all 5,000 plus brands in the aggregate, it does not provide the actual distance between the brand vectors in the reduced dimension space. Because identifying proximal brands for substitute/complement analysis is a critical task in marketing decisions ([ 8]), we focus on identifying proximal brands from the perspective of a focal brand. In doing so, we offer a new perspective that reflects the nature of the varied relationships ranging from substitutes to complements in the social network space.In this illustration, we choose United Airlines and Southwest Airlines from the airlines category and Audi USA and Nissan from the automobile category, as these brands are generally regarded as having different consumer bases and belonging to different submarkets. Each of the four brands is referred to as a focal brand, and we find their top ten proximal brands according to cosine similarity. Table 4 provides several interesting insights. First, our method is able to capture specific brand latent characteristics. For example, Southwest Airlines is generally considered a low-budget airline compared with United. The brands most proximal to Southwest Airlines and United reflect this difference. The proximal brands for Southwest Airlines are JetBlue, Frontier Airlines, and Allegiant, while the most proximal brands for United are major domestic and international airlines, such as American Airlines, Delta, Lufthansa, All Nippon Airways, Air China, LATAM Airlines, and Air New Zealand. Similar results also are identified in the automobile industry. Second, we observe asymmetric competition (see [42]). For example, Southwest Airlines is the fourth-most-proximal brand to United Airlines, while United Airlines ranks sixth in the set of top proximal brands to Southwest Airlines.GraphTable 4. Top 10 Proximal Brands to Each Focal Brand. RankFocal BrandUnitedSouthwestAirlinesAudi USANissan1AmericanJetBlueMercedes-Benz USAMazda2DeltaFrontierBMW USAToyota3LufthansaAllegiantLand RoverVolkswagen4SouthwestDeltaLexusKia Motors America5AlaskaAlaskaChevrolet CamaroSubaru of America6All NipponUnitedMaserati USAChrysler7Air ChinaAirfarewatchdogKawasaki USAFiat8LATAMAmericanFirestone TiresJaguar9Air New ZealandVirgin AmericaTeslaAlfa Romeo10AirfarewatchdogHyattRam TrucksKlim Third, unlike prior market structure analysis, where proximal brands are usually from the same industry as the focal brand, the top most proximal brands derived from our analysis are from different industries. For example, a brand called ""Airfarewatchdog"" is proximal to both United and Southwest Airlines. Airfarewatchdog is a deal-finder for flight tickets and has a large follower base (over 1 million) on Facebook. Traditional market analysis would simply ignore this brand, as it is not an airline. Further, it is also interesting to see that Southwest Airlines is closer to Airfarewatchdog than to United, which may indicate that the fans of Southwest Airlines are more likely to use a deal finder before purchasing flight tickets; thus, Airfarewatchdog could be a complement to Southwest when customers look for cheap flights on that site and end up at Southwest, or it could potentially compete with Southwest. In either case, Southwest could focus more on this site and examine the nature of the relationship. Identifying Opportunities/ThreatsOur market structure map can help managers identify brands outside of the product-market that are close to a specific brand and, thus, identify opportunities and threats posed by different brands. Take the airline product-market (Figure 5, Panel C) as an example. In our analysis, Disney Cruise Line and Hyatt are two brands outside of the airline product-market but are identified as proximal brands to Southwest but not for United. These proximal locations simply are due to a greater number of users in our data set liking both Southwest and Hyatt ( 2,709) versus the number of users liking both United and Hyatt (954). Similarly, a greater number of users like both Southwest and Disney Cruise Line ( 3,050) than like both United and Disney Cruise (729).Such findings can provide opportunities for Southwest, as it could target users who like Disney Cruise and Hyatt on social media. Southwest could cross-promote these brands by teaming up with Disney Cruise and/or Hyatt on each other's websites and launch coalition loyalty programs. From the viewpoint of other hotel chains that are competitors to Hyatt, these could be potential threats, so gleaning such insights early on may help them take proactive actions. Such opportunities/threats are difficult to identify when product-markets are prespecified, and they cannot be obtained easily through other means. Large Brand Versus Small BrandOur user engagement data set contains top 5,478 primarily large brands, ranked by their popularity (number of followers as of data collection period) on Facebook. A key question is whether our proposed approach is still able to identify meaningful market structure for smaller brands. If they can find the right position in the product-market structure, smaller brands have the potential to increase consumer awareness and interest in their brands ([13]), which could lead to a permanent benefit in terms of competitive advantage ([47]). Therefore, to test whether our methodology is able to capture relationships among large brands as well as small and local business brands, we add a set of smaller brands to the original data set. Specifically, we focus on the ""Travel"" category, as it includes many small, local travel agencies, and their followers on Facebook range from a few hundred to a few thousand on average. In total, we have 241 travel brands. Figure 6, Panels A and B, plot the distribution of the number of followers of these travel brands and shows that it is quite diverse.Graph: Figure 6. Size and market structure of 241 travel brands.Upon applying our methodology to the enlarged data set, we observe (Figure 6, Panel B) that these 241 travel brands are predominantly located in two areas. This pattern indicates that the latent brand relationship is well captured, even when brands have few engagement activities due to their smaller user bases. In a brand–brand network, such a small number of shared user bases could result in a failure to capture proximal locations, in essence treating them as noise.The market structure uncovered for these small businesses by identifying their proximal brands has good face validity. For example, ""The Luxury Travel Expert"" is an information portal for luxury travel and premium tours, with about 11,000 followers on Facebook as of our data collection period. Most posts receive fewer than ten comments and likes. The top proximal brands based on the cosine similarity are Smithsonian Journeys, The Peninsula Beverly Hills, Peter Sommer Travels, Quasar Expeditions, and DuVine Cycling. It is noteworthy that these are small travel brands that focus on expert-led, small-group, luxury, and premium tours. The results further confirm that our deep network representation learning method is generalizable to both small and large brands. This analysis also allows brand marketing managers to identify business opportunities. For example, in our analysis, the two brands The Luxury Travel Expert and The Peninsula Beverly Hills are quite close. The former is an information portal for luxury travel and premium tours, and the latter is a five-star luxury hotel. Therefore, the marketing manager of the Peninsula Beverly Hills could promote the brand on the information portal website to attract users from The Luxury Travel Expert to expand its customer base. Within-Industry Market Structure AnalysisExtant methods typically predefine the product-market boundary to derive market structure and brand relationships. In contrast, we allow product-market boundaries to emerge from the data. Therefore, a natural question is whether it is necessary to have a broader range of brands from other industries to derive a highly precise market structure for a specific industry. Although managers would typically focus on engagement data for their brands and for brands within the same industry, how does engagement data from brands in different categories help? To answer this question, we choose the ""auto"" category and only use the engagement data from the auto brands to derive the market structure. In the data set, we have 163 auto brands, including cars and car accessories brands (e.g., tires, oil), with 2.7 million user engagements in total. The analysis shows (Figure 7, Panels A and B) that structures with reasonable face validity still emerge using only the auto brands data. For example, the top left corner in Figure 7, Panel B, presents a cluster of imported auto brands such as Kia Motor America, Toyota, and Nissan. However, compared with the derived auto brand market structure learned from using all brand data, as shown in Figure 5, Panel A, the market structure is less clustered and more ambiguous.Graph: Figure 7. Visualization of market structure of using engagement data only from ""auto"" brands. Notes: The right panel is the zoomed in visualization with BMW as centroid.Next, we compare the market structure using the engagement data from the auto brands alone with that from all brands across categories in a qualitative manner. Specifically, we choose the brand FMF Racing, which is a company that develops dirt bike exhausts for off-road or racing motocross riding. Using the engagement data from the auto brands alone, the top proximal brands are Lucas Oil, KTM USA, Yamaha Motor, Arctic Cat, Two Brothers Racing, Phoenix Pro Scooters, Auto Alliance, Valvoline USA, Lance Camper, and Castrol. Some are related to off-road motocross riding, while others are not. For example, Lucas Oil, Valvoline USA, and Castrol are global automotive oil brands.In contrast, the top ten proximal brands to FMF Racing emerging from using all categories of data are KTM USA, Polaris Snowmobiles, Fox Racing, Mickey Thompson Performance Tires & Wheels, Two Brothers Racing, King Shocks, Arctic Cat, Addictive Desert Designs, NISMO, Skunk2 Racing, and MBRP performance exhaust. Upon further investigation, we find that they are all related to off-road motocross riding. These results indicate that our approach with engagement data from brands across industries can learn better brand representation and thus reveal a highly precise market structure. External Validity and Comparison with Other Approaches Market structure identified based on consumer surveyTo assess the external validity of our approach, we conduct a survey on Amazon Mechanical Turk (MTurk), which is a reliable source for data collection and marketing analytics ([43]). Prior market structure literature has also administered brand perception survey on MTurk ([ 7]). Following this prior study, we surveyed 28 automobile brands (after ignoring the other 150 brands that are related to motorcycle or automobile accessory such as tires, parts, and oil). Specifically, we recruited 500 MTurk participants, each of whom was required to be in the United States and have a good MTurk record (successful completion of at least 100 assignments with a minimum 95% rate of approval). Each participant was asked to rate the similarity between a focal automobile brand and the other 27 automobile brands on a scale of one to five. To avoid fatigue due to information overload, each participant was randomly assigned to work on one task. Participants were also asked to indicate their age, gender, and whether they owned an automobile. Details of the participants' demographics information and the survey design are presented in Web Appendix WA6.In the survey, participants could choose ""N/A"" if they were not aware of the automobile brands. Brand recognition rate was 88.2%, implying that 11.8% of ratings were not applicable due to lack of brand awareness. We aggregated the survey data and built a 28 × 28 matrix, where each cell represented the pairwise brand similarity, and denoted it as the ""survey matrix."" We also used the brand representations learned from our approach to construct another 28 × 28 matrix of brand similarity, which we denoted as the ""deep learning–based matrix."" The correlation between two matrices is significantly positive (r = .385, p = .000). This result provides additional evidence on the validity of our deep learning–based approach for market structure identification. We also did an additional check where we calculated the correlation between the survey response and that constructed by our approach but using only automobile (within-industry) data. The correlation is.152 (p < .05), which is not as substantial and significant as the correlation between the survey response and our approach using all industry data. We present the market structure learned from the survey data in Web Appendix WA7 as an external validity. Market structure identified based on Google TrendsTo provide further external validity of our approach, we use Google Trends data to identify market structure and examine how it aligns with our approach of using online social media users' brand engagement. Google Trends provides an interest score for every search query across regions and languages, as measured by an aggregated search volume over time. A higher interest score means that queries are more popular in a specific region and time. Google Trends data have been widely used by industry ([44]) and academia ([ 6]; [ 9]; [22]; [48]) to address marketing and economic problems (e.g., competitive analysis). Researchers have also shown that this score is consistent with consumers' purchase interest in general ([ 6]; [ 9]).To determine relative popularity for every pair of brands, we make a search query consisting of two brand names—for example, ""Toyota BMW"" for the brands Toyota and BMW. For every brand pair, we can obtain an interest score returned by Google. For example, in the United States in 2017, the interest score is 13 and 85 for the query ""Toyota BMW"" and ""Toyota Honda,"" respectively. This indicates that consumers in general are more interested in searching Toyota and Honda together, compared with searching Toyota and BMW together. Validation on airline industryIn the first validation exercise, we focus on the airline industry and the derived market structure. We have 19 airline brands in our data set, including U.S. domestic airlines and international airlines (Figure 5, Panel C). For every brand pair, we first obtain a Google search interest score in the U.S. region in 2017 (the same as our engagement data period). Then, following previous work ([35]), we calculate the similarity between two brands A and B as  sim(A,B)=interest(A,B)∑b∈Sinterest(b,B)  , where  S  is the set of all brands (e.g., 19 here). [35] use the co-occurrence of two brands in an online discussion forum instead of a Google search interest score. We also calculate similarity for every pair of 19 airline brands using 300-dimensional vectors derived from our deep network representation learning on the engagement data using cosine similarity.To check whether the two aforementioned similarity scores are similar to each other, we calculate their Pearson's two-tailed correlation between two sets of 361 (= 19 × 19) similarity scores. It is significantly and highly correlated (  r=.630,p=.0000  ). This indicates that our social engagement-based market structure is similar to that derived from Google Trends. Because prior studies have shown that the Google search data have a high correlation with a consumer's actual purchase interest ([ 6]; [ 9]), we can conclude that users' social engagement with brands also contains valuable information for deriving brand relationships. Validation on travel industryIn the second validation exercise, we focus on the travel industry, including not only major travel brands but also many small and local travel brands (see the ""Large Brand Versus Small Brand"" subsection). There are 241 travel brands in the data set. Similar to the first validation exercise, for every brand pair, we obtain a Google search interest score in the United States in 2017 (the same as our engagement data period). Among the 241 travel brands, Google Trends does not return scores for 90 brands (i.e., showing ""your search doesn't have enough data to show here""), which results in data for 151 remaining brands. Although individual brands show a considerable amount of search, only four brand pairs return nonempty interest scores.[ 9] This data sparsity may be attributed to the uniqueness of the travel category. Many of the travel brands are local/small businesses, such as the travel agencies ""Spirit of Boston"" and ""Historic Philadelphia."" Naturally, they do not receive as many queries as large brands. Moreover, consumers may search travel agency brands in different queries, but they very rarely search two travel brands in the same query. Therefore, there is not enough data for Google to aggregate and return the cosearch score. This analysis highlights the limitation of the cosearch-based approach, which is likely to suffer from the data sparsity issue. In contrast, our approach built on large-scale brand–user social engagement data can provide valuable marketing insights not only for large international brands but also for small local brands. Practical Actionability How to compare market structure maps?In a practical setting, marketing managers may need to quantitatively determine the quality of derived market structure maps, based on which they can infer actionable insights. We evaluate the conceptual maps using a standard metric—silhouette score ([ 1])—which has been adopted in prior market structure literature ([12]). The silhouette coefficient is calculated using the mean intracluster distance (a) and the mean nearest-cluster distance (b) for each sample, as  b−amax(a,b)  . The values of silhouette score range between −1 and 1 (1 being the best and −1 the worst). Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster. Recall that our approach can naturally group brands that have similar representations in the high-dimensional space. An ideal market structure would favor brands that are concentrated and exhibit clean cluster structures. We conduct K-means clustering on the brand representations and compute the mean silhouette coefficient of all samples.In the ""Within-Industry Market Structure Analysis"" subsection, we qualitatively show that our approach—without prespecifying product-market—reveals more interesting and coherent brand insight than using brand engagement data within only one industry. Next, we vary the number of clusters in K-means and calculate the silhouette coefficient of different methods. The result in Figure 8 shows that our approach using all brand engagement data consistently achieves better clustering than using only the automobile brand engagement data. For example, when we cluster 168 automobile brands into two clusters (i.e., K = 2 in K-means), our approach achieves a silhouette coefficient of.334, while the approach using only the automobile engagement data has a low silhouette coefficient of.043. The silhouette coefficient of our approach gradually converges to.10 as the number of clusters increases. In contrast, the approach using only automobile industry data stays near.01, indicating a poor separation among automobile brands. This analysis not only confirms the superiority of our approach without prespecifying product-market boundary but also enables marketing managers to determine the quality of derived market structure maps. How much brand–user engagement data are needed to derive a good market structure?We have shown that our approach can derive good market structure with large-scale social engagement data. In practice, it is easy to obtain a relatively comprehensive set of brands across different categories and associated user engagement from social media marketing platforms, such as Socialbakers. However, a marketing manager may not have enough resources to collect as large a data set as we have, raising the question as to whether our approach is sensitive to the size of data for obtaining a good market structure. To answer this question, we calculate the correlation between the similarity of pairwise brands generated using the full data set and that generated by a fraction of data selected at random. Figure 9 presents this result, showing that the correlation reaches over.90 when 40% data is used (and.7 when 12.5% data is used), and it starts to converge to the market structure generated by using the full data. This analysis suggests that our approach is relatively robust to the amount of engagement data used. Marketing managers can use this analysis as guidance to determine the amount of data resources needed. In addition, we examine how the number of prespecified industries affects the robustness of market structure maps and present the analysis in Web Appendix WA8.Graph: Figure 8. The clustering silhouette coefficient of 168 automobile brands.Graph: Figure 9. Pearson correlation between the similarity of pairwise brands generated using a percentage of full data and the full data. Evaluation using link predictionIn studying market structure, there is a lack of ground truth about the identified structure, that is, an understanding of what the ""true"" structure is, which makes demonstrating the performance of various proposed methods challenging. We introduce an alternative approach, adopted from network analysis literature ([27]), to evaluate the identified market structure. An identified market structure is a function of the brand representation, and so an accurate representation is more likely to identify valid market structures. This approach is supported by prior research showing a strong relationship between brand image and the characteristics of a brand's supporters and followers ([ 7]; [25]; [34]). If a network learning method were capable of accurately representing network nodes accounting for these relationships between brands and users, then it would be able to predict the future links between brands and users accurately. Therefore, we use a cross-validation procedure under a link prediction research design, where we predict the most likely newly formed links of user–brand engagement in an out-of-sample network given the brand vectors and user vectors learned from a training network. This research design is widely used in the network analysis community to evaluate network clustering algorithm performance ([27]; [51]). In our context, we use the user–brand interactions from the first half of the time span in our data to build a training network (G0,1) and use the second half to build a testing network (G1,2). The likelihood of a link formation is measured by the proximity of a learned brand vector and a learned user vector. Note that link prediction performance is significantly correlated with the quality of learned vectors, given the assumption that a better network representation learning can predict new interactions between users and brands with a high degree of accuracy. We provide details of the link prediction experiments in Web Appendices WA2–WA4. Overall, our analysis shows that ( 1) link prediction using representation learned from our brand–user network performs better than a reduced brand–brand network (a widely used method in extant approaches), ( 2) deep learning–based methods learn better representation than shallow machine learning methods, and ( 3) our deep learning–based model is robust and able to handle sparse networks as compared with baselines. Case Studies on Market Structure DynamicsMarket structure evolves over time and can change dramatically, especially under an unexpected industry shock. Whether our proposed method can be adaptively learned is also of interest as it could provide useful insights to marketing practitioners. In this section, we analyze how market structure changes under exogenous shocks by analyzing two case studies: Amazon acquiring Whole Foods and Tesla introducing the Model 3. We take a before-and-after strategy where we use data for the three months pre- and postevent announcement day and calculate the change in distance from the focal brand (i.e., Amazon and Tesla) to other representative brands selected from the same category. The purpose of the event study is to examine how a focal brand relationship with other brands changes as a major event occurs. Specifically, for Amazon–Whole Foods, we select several brands from the retail and e-commerce category, and for Tesla, we select several brands from the automobile category. We calculate the change between focal brand  i  's representation  wib  and target brand  j  's representation  wjb  before and after the specific event using cosine similarity:  cossim(wiafterb,wjafterb)−cossim(wibeforeb,wjbeforeb)  . Therefore, positive numbers indicate a similarity increase, whereas negative numbers indicate a decrease in similarity. Amazon Acquires Whole FoodsAmazon acquired Whole Foods in June 2017. This acquisition has had a significant impact on the grocery and retail industries. At the time, it was widely believed that Amazon planned to use its acquisition of Whole Foods to enter the online grocery delivery business. Amazon and Whole Foods ran separate Facebook pages. After the merger of the two firms, we see from Figure 10 that Amazon is more proximal to retail brands as measured by cosine similarity, while the proximity to other relevant brands decreases slightly. For example, the cosine similarity between Amazon and Lowe's Home Improvement decreases by.184. In contrast, the cosine similarity between Amazon and other super-market retailer brands increases. Among them, proximity of Amazon to Whole Foods increases by.202, and between Amazon and Kroger by.165. As inferred from our data-driven model, Amazon even becomes more proximal to Walmart, indicating that Amazon's competitive market structure landscape has shifted. By further examining our data, we find that, after the Whole Foods acquisition, the number of common users who interact with both Amazon and Whole Foods on their public Facebook pages increases. Some Amazon users posted comments on Whole Foods' fan page mentioning Amazon. For example, in the Whole Foods post, ""Here are 6 New Healthy Products Coming to Whole Foods in March,"" a user who had liked an Amazon post earlier commented, ""You mean AMAZON... as they bought Whole Foods...right?"" This direct link between Amazon and Whole Foods leads the deep autoencoder to increase the proximity between the two brands. Moreover, in another Whole Foods post, a user who had liked a Kroger post earlier posted, ""The quality has gone downhill and prices have soared.... You've made Kroger look appealing...."" Although we do not find that this user has ever interacted with Amazon's Facebook page before, her interaction with Whole Foods leaves an implicit connection between Amazon and Kroger, which can be captured by the deep autoencoder. In short, after Amazon acquired Whole Foods, online social media users who are Amazon's fans pay more attention to Whole Foods, and users who are fans of other supermarket brands engage more with Whole Foods due to the acquisition event. As a result, the deep autoencoder captures the dynamics and updates the brand representation accordingly.Graph: Figure 10. Similarity change of Amazon to other brands in retail and e-commerce industry.The acquisition by Amazon has an impact on the market structure of Whole Foods as well. In Figure 11, we consider Whole Foods as the focal brand and calculate the change in proximities to other brands before and after the acquisition. Drawing on the results, we observe that Whole Foods' proximity to other retail brands such as Target, Walmart, and Best Buy increases. Among them, the proximity to Amazon increases the most due to the increase in the number of common users between them. In contrast, Whole Foods' proximity to supermarket brands such as Goya Foods, Enjoy Life Foods, and HelloFresh decreases slightly. Second, the magnitude of change in proximity values is smaller than those of Amazon to other brands. This seems to indicate that the acquisition has had less impact on Whole Foods, as it is still positioned around other supermarket brands, while Amazon is expanding closer to the grocery retail category.Graph: Figure 11. Similarity change of Whole Foods to other brands in retail and e-commerce industry.Although this analysis is retrospective, it highlights that our approach offers managers a series of multiple snapshots of the structure over time to measure a brand's relative position change, thus identifying potential market structure change. Suppose a supermarket chain brand A observes that Amazon is moving closer to A's position on the map. This may indicate that Amazon is getting more engagements (likes or comments) from A's customers. Given that one motivation of liking a brand's Facebook post is to receive some benefit from the brand (e.g., coupon, discount), it could further indicate that Amazon is conducting effective promotional marketing campaigns on social media. No matter the underlying reasons, the increasing proximity of Amazon on the brand map can at least provide an early warning to A's marketing managers to the potential threat. Late response to the competition may harm the brand and eventually the whole business.Next, we validate the case study of Amazon's Whole Foods acquisition using Google Trends data. Similar to the first external validity exercise, we choose 29 ""retail"" brands (including Walmart, Target, Macy's, Best Buy, Walgreens, Lowe's, Whole Foods, IKEA, Sears, 7-Eleven, Dollar General, Sam's Club, Dollar Tree, CVS Pharmacy, Aldi, Barnes & Noble, Costco, Kroger, Meijer, Safeway, Office Depot, Rite Aid, Albertsons, ShopRite, and The Fresh Market) plus Amazon and obtain their interest scores for every brand pair in the United States in 2017. Note that we exclude some small retail brands such as Goya Foods because their Google cosearch interest scores with other brands are mostly 0, indicating insufficient search data for the brand.The Pearson's two-tailed correlation between two sets of 900 (= 30 × 30) similarity scores is significantly high before (  r=.675,p=.0000  ) and after (  r=.758,p=.0000  ) acquisition. This result confirms the external validity of our social engagement–based method. We observe that for Amazon, before the Whole Foods acquisition, the most similar brands were Barnes & Noble, Macy's, and Best Buy. After the acquisition, the most similar brands are Whole Foods, Barnes & Noble, and Macy's. For Whole Foods, before the acquisition the most similar brands were The Fresh Market, Albertsons, and ShopRite. After the acquisition, the most similar brands are The Fresh Market, Amazon, and Safeway.We obtain further search interest data for one year after the acquisition (June 2017 to June 2018) to examine whether the market structure change is sustained for a long period after the acquisition announcement. For Amazon, the most similar brands are still Whole Foods, Barnes & Noble, and Macy's. Other grocery ""retail"" brands such as Kroger and The Fresh Market become more similar to Amazon than before the acquisition. For Whole Foods, the most similar brands are The Fresh Market, Safeway, ShopRite, and Amazon. Because Whole Foods is still Amazon's most similar brand among these retailer brands, this indicates that for Amazon, the acquisition impact holds for the extended period of analysis. It seems that the acquisition has less of an impact on Whole Foods, as Whole Foods is still positioned around other supermarket brands. All findings are consistent with our case study using social engagement data, which provides external validity to our results. Tesla Announces the Model 3Tesla sells two types of sedans: the Model S and the Model 3. The Model S is a luxury premium sedan with a larger range of acceleration and customization options, while the Model 3 is designed as a more affordable mass-market electric vehicle. The Model S can cost over $100,000 depending on the configuration, while the Model 3 costs approximately $35,000. After the announcement of the new Model 3, we see that Tesla becomes more distant from luxury car brands and moves closer to nonluxury car brands. We can see in Figure 12 that the cosine similarity between Tesla and the luxury car brand Maserati decreases by.209. Similar trends exist between Tesla and other high-end or luxury car brands such as BMW, Mercedes-Benz, Audi, and so on. Meanwhile, Tesla becomes more proximal to Kia, Mazda, and other more affordable car brands.[10]Graph: Figure 12. Similarity change of Tesla to other selected brands in the auto industry. Testing for SignificanceIn the previous analysis, we compute the distance change between the focal brand (i.e., Amazon or Whole Foods) and other brands before and after the acquisition. We can see that there is a significant increase in similarity between Amazon and Whole Foods after the acquisition. However, whether this distance change is caused by the acquisition or other unobserved factors, such as the difference of data split and/or noise, still remains unclear. Therefore, we conduct further analysis by randomly splitting all data before the acquisition into two parts (i.e., d1 and d2, with d1 before d2). We then measure the distance between Amazon and Whole Foods using d1 and d2 separately. We repeat this process 30 times using different data cuts in the preacquisition data. The average distances between the two brands across using all d1s and d2s are.228 and.232, respectively. The two-tailed t-test on the distance is.055, which indicates that there is no statistically significant difference between the distances between Amazon and Whole Foods before the acquisition in different cuts of the preacquisition data. Accordingly, the substantial increase in similarity between Amazon and Whole Foods is not attributed to sample differences.We perform a similar process on Tesla's introduction of the Model 3. In particular, we choose one nonluxury brand, Mazda, and compute its distances to Tesla before the event using various data splits. The average distances between Mazda and Tesla across using all d1s and d2s are.185 and.191, respectively, with a p-value of.076. This seems to indicate there is no statistically significant difference between Mazda and Tesla when the cutting point of data varies before the event. Therefore, we conclude that after Model 3's announcement, Tesla becomes more similar to nonluxury automobile brands on the social media platform. Note that we also conduct analyses on Tesla and other automobile brands, and the results are consistent. Implications and ConclusionOur proposed approach examines millions of user engagements with thousands of brands and focuses on the early stage of the customer journey. This allows for visualization of potentially overlapping product-market boundaries across many categories and helps managers identify latent threats and potential opportunities, which cannot be done with extant methods that focus on later stages of the customer journey (lower levels of the purchase funnel) within categories. As an example, for Southwest, is Airfarewatchdog a potential competitor that might draw visitors away, or is it a complementor that would increase visits to Southwest? Having identified the overlapping market with Airfarewatchdog, Southwest could invest more attention to evaluate the exact nature of this relationship. If Airfarewatchdog is a competitor, then Southwest might focus on developing strategies to differentiate itself and channel visitors to its website exclusively. If it is a complementor, then Southwest might run display ad campaigns on Airfarewatchdog's website. In addition, both Disney Cruise and Hyatt are closely associated with Southwest, with common users who like these brands on social media; therefore, Southwest could run mutually beneficial joint and cross-promotions with these other brands. In fact, all these brands could join in a dynamic coalition loyalty ecosystem built around a fluid partnership of products, services, and experiences, thereby providing a unifying customer value proposition that could be difficult to compete against ([ 4]). Identifying such unusual or unforeseen insights is the greatest advantage of our approach.Another important strategic use of our market structure maps is to identify competitors and complementors across industries and track how these relationships change over time. While [15] apply text analysis to 10-K statements to identify such grouping based on product descriptions that the firms provide, we provide a more dynamic structure based on actual customer/user social media activities. Moreover, our market structure map is more forward looking and predictive of emerging competition and complementors and more proactive than those based on 10-K statements, which can be viewed as reactive. Because [14] show that merging firms with more similar product descriptions in their 10-Ks results in more successful outcomes, using our market structure maps to identify merger-and-acquisitions targets (firms sharing common users) may have similar benefits. We leave this for future research.The power of our method lies in its ability to capture the dynamic changes in market structure. Because the maps are based on the analysis of big data that can be collected in a relatively short window of time, our methodology can track changes in their relative position when firms introduce new products, new promotions, and new marketing initiatives. The case studies that we highlighted provide good illustrations of this. In addition, although we have not analyzed this in the article, firms can deploy our method to enhance their social network-based marketing efforts by better targeting specific potential customers, because user nodes in the network are also learned and represented as vectors in the same multidimensional space as brands. Our link prediction design demonstrates a possible use for targeting. Finally, our proposed method is generalizable to other similar platforms if we can construct a brand–user network from public fan pages' engagement data. We implemented our proposed method using NVIDIA P100 graphics processing unit, with Tensorflow as the back-end deep learning framework. For future research to replicate or practitioners to adopt, we have provided details regarding data collection, data cleaning, and deep model architecture, and model the fine-tuning process in Web Appendix WA1.While marketing analytics techniques have extensively used consumer personal data to derive valuable insights, they raise many privacy and ethical concerns. How to balance these two important aspects has become a key consideration for many marketing scholars ([ 3]; [52]). Our approach provides a useful example. The only input to our network representation learning method is the brand–user network, which can be publicly obtained from brands' social media page.Our research has some limitations. Given the nature of our data, our method cannot examine stockkeeping unit–level competition as is done by some of the extant methodologies using lower funnel data. From this perspective, we recommend our methodology as a complement to extant methods and for higher-level brand strategies and tactics. Future work could examine how perceptual maps vary by customer segment using lower-funnel data such as purchase frequency and purchase amount. Second, our analysis is conducted on one social network, Facebook. Even though Facebook is one of the largest online social networks, with billions of users and thousands of brands, it is likely that users on different platforms exhibit different engagement behavior, and some of the research findings may not be generalized to other platforms. For example, it is reported that Instagram users and Facebook users fall into different age groups (Pew Research Center 2021). We could apply the same technique to other social media platforms and compare findings. Finally, each link in the user–brand network is created when the user engages with the brand on the public page. Facebook has introduced various reaction emotions to the platform to allow users interact with brands in different ways, such as ""Like,"" ""Love,"" ""Care,"" ""Haha,"" ""Wow,"" ""Sad,"" and ""Angry."" Future work could build a multirelational network to deeply capture brand–user engagement heterogeneity. Supplemental Materialsj-pdf-1-jmx-10.1177_00222429211033585 - Supplemental material for Identifying Market Structure: A Deep Network Representation Learning of Social EngagementSupplemental material, sj-pdf-1-jmx-10.1177_00222429211033585 for Identifying Market Structure: A Deep Network Representation Learning of Social Engagement by Yi Yang, Kunpeng Zhang and P.K. Kannan in Journal of Marketing  "
11,"Identifying Market Structure: A Deep Network Representation Learning of Social Engagement With rapid technological developments, product-market boundaries have become more dynamic. Consequently, competition for products and services is emerging outside the product-market boundaries traditionally defined based on Standard Industrial Classification and North American Industry Classification System codes. Identifying these fluid product-market boundaries is critical for firms not only to compete effectively within a market but also to identify lurking threats and latent opportunities outside market boundaries. Newly available big data on social media engagement presents such an opportunity. The authors propose a deep network representation learning framework to capture latent relationships among thousands of brands and across many categories, using millions of social media users' brand engagement data. They build a brand–user network and then compress the network into a lower-dimensional space using a deep autoencoder technique. The authors evaluate this approach quantitatively and qualitatively and visually display the market structure using the learned representations of brands. They validate the learned brand relationships using multiple external data sources. They also illustrate how this method can capture the dynamic changes of product-market boundaries using two well-known events—the acquisition of Whole Foods by Amazon and the introduction of the Model 3 by Tesla—and how managers can use the insights that emerge from this analysis.Keywords: artificial intelligence; deep representation learning; social media; competitive market structure; big dataFirms compete in a market to satisfy the specific needs of consumers in the market. The market and the competing products make up a ""product-market,"" with the boundary defining the brands competing within that market. Market structure is defined on the basis of these product-markets and their (possibly overlapping) boundaries. Identifying the product-market boundary and examining the strength of competition between brands within the product-market have long been important issues with strategic implications for next-generation product design, product positioning, new customer acquisition, and pricing and promotion decisions. Rapid changes to the competitive environment, however, have made identifying the product-market boundaries increasingly challenging. With technological advances, the product-market boundaries themselves are changing and competitive threats and opportunities are emerging outside of narrowly defined product-market boundaries.Numerous recent competitive events support the idea that product-market boundaries are highly fluid. For example, the digital camera product-market was upended by technological developments in smartphone categories. Similarly, Tesla, which initially entered the product-market of high-end automobiles with an innovative fuel technology, has since rolled out products for the lower-end market, thereby changing competition in the lower-end product-market as well. Amazon, previously an online platform, essentially crossed product-market boundaries when it acquired Whole Foods and entered the offline product-market. In many such situations, product-market boundaries based on traditional Standard Industrial Classification and North American Industry Classification System codes are inadequate indicators of emerging threats and opportunities. Given the potential for new and unforeseen relationships between brands, managers need deeper insights into the fluid product-market boundaries to be able to spot potential competitors and complements, identify cross-promotion strategies, and develop firm-level strategies.These observations naturally lead to several important questions: How can managers accurately identify potential threats and opportunities? If a competitive threat emerges from a different market, how can managers proactively anticipate such threats? How can we answer these questions and derive marketing insights using easy-to-obtain and publicly available data? Our article aims to answer these questions using large-scale (>100 million) social media user engagement data (likes and comments) spanning several thousands of brands in different product/service categories.Over the years, academics and practitioners have contributed significantly to developing various methods to define and identify market structure (see the review by [46]]). These include survey-based methods such as brand concept maps ([19]) and the Zaltman metaphor elicitation technique ([53]), methodologies based on observational purchase data (e.g., brand switching) ([21]; [37]), consideration sets ([42]), and scanner-based purchase data ([10]; [36]; [45]). Within the online context, researchers have used unstructured user click streams ([32]), online search logs ([24]; [42]), and customer reviews ([26]). Many of these methods use data from the bottom of the purchase funnel, such as evaluation- and purchase-stage data, and thus assume that the product-market boundaries are prespecified. Even those methods that use data from the top of the funnel at the awareness or preevaluation stage, such as forum discussions ([35]) and hashtags ([33]), define a product-market boundary first and then examine the competition within the prespecified product-market to make these methods implementable. Thus, many of the methods are unable to capture changes to the product-market boundaries and/or the impact that a brand from outside the boundary may have on brands within a product-market.Our methodology creates a more inclusive representation of brands by examining brand–user relationships at the top of the purchase funnel. Unlike the extant methods for identifying market structure that use data from consumers' lower funnel activities such as purchase data, brand switching, price comparison data, or consideration data that prespecify boundaries (e.g., [10]; [21]; [37]; [42]; [50]), we use upper-funnel user–brand engagement data (such as liking and commenting on brand posts) from social media that spans product-markets. At the lower end of the purchase funnel, consumers winnow down the brands they consider to a few substitutes. Thus, interactions at this stage are not as informative of the broader (and possibly complementary) linkages between the brands across product-markets, which are captured more easily at the upper funnel. For example, a consumer considering travel may consider hotel or Airbnb options, airline options or travel intermediaries. At this early stage (the upper end of the funnel), understanding such user–brand linkages could be more informative of the broader relationships between the brands on a continuum from substitutes to complements. Our methodology uses such upper-funnel user–brand engagement data to identify these latent relationships among a large number of brands.Many extant studies in market structure, including those mentioned previously and those using big data technologies (e.g., [ 7]; [12]; [26]; [35]; [42]), view the competing/complementary brands as brand–brand networks. That is, they specify the relationship between any two brands using similarity metrics derived from brand switching, co-occurrences, and word embeddings, without directly modeling the entities (customers, individual consideration sets, or individual reviews) that give rise to such similarities. Our methodology based on brand–user networks considers both brands and users as primitives and uses as input the relationship in terms of each user's liking and commenting on brands. The essential difference between these approaches and our methodology is that extant research considers aggregate data of relationships between brands (brand–brand) as input, whereas our methodology considers the disaggregate individual-level relationships between users and brands (brand–user) as input.The distinction becomes more salient when a product-market boundary is not prespecified. Consider, for example, User 1, who likes United Airlines and Hyatt, while User 2 likes Southwest Airlines and Hyatt. When the product-market is prespecified as ""airline brands,"" information about the users liking the Hyatt brand is discarded. As a result, information that could provide insights into the relationship between United Airlines and Southwest Airlines through their relationships with Hyatt is not considered. However, when we do not prespecify the product-market boundaries, we are able to leverage all such information and create a more accurate representation of the brands.From this premise, we first construct a large-scale brand–user network based on user engagement on brands' social media public fan pages. Then, we propose a deep network representation learning method to discover relationships within the data. Specifically, we use a deep learning method suitable for ( 1) handling large data efficiently and ( 2) learning complex patterns from data effectively (see [ 2]; [49]). The process leads to a low-dimensional representation (i.e., a vector) for each brand and each user by training a deep autoencoder on the network data. The deep autoencoder is similar to traditional dimensionality reduction methods such as principal component analysis (PCA) in capturing latent factors in data with few dimensions. It is, however, very different from those methods in that it uses a nonlinear transformation function to learn the latent patterns in data while reducing the noise in the data. In our context, the deep autoencoder can preserve the first-order (user–brand direct connection) and the second-order (two users connecting to the same brand, or one user connecting to two different brands) network topologies. As a result, brands with network structural equivalence are located closer together in the representation space, while brands with dissimilar network structures are located further away from each other. This method also projects users and brands onto the same dimensional space, which can be used for many different follow-up analyses. We use an illustrative example (in Figure 1) to demonstrate how network representation learning works.Graph: Figure 1. An illustration of deep network representation learning.Suppose we have three brand nodes (B1, B2, and B3) and five user nodes (U1, U2, U3, U4, and U5) in a network. Our network representation learning approach aims to find a function that maps each node into a low-dimensional vector (e.g., three dimensions, for the sake of illustration) while the network structural information is preserved maximally. That is, when nodes exhibit similar structures (first order and/or second order), they are projected onto similar vectors and located closer in the reduced three-dimensional embedding space. Because U1 engaged with B1, we expect the vector representation of B1 and U1 to be close. Similarly, B2 is closer to B1 than to B3 because B2 shares more common users with B1 than with B3. Because B2 has connections to U4 and U5, this makes B2 lean toward them.We establish the face validity of our approach through the identification of product-market boundaries. Our analysis of the brand–user engagement data of over 5,000 brands and nearly 26 million users reveals product-market boundaries with high face validity—grouping of specific categories, high-end brands, and overlaps. We then conduct external validation checks using additional sources including survey and Google search trend data. The market structure derived using our approach is highly correlated with those derived using external data sources. Our approach also overcomes common limitations in extant methods such as data sparsity. Our event studies on Amazon's acquisition of Whole Foods and Tesla's introduction of the Model 3 illustrate how our methodology captures the changes in product-markets associated with these events. We also discuss how the market structure maps can reveal opportunities and threats facing a brand. For instance, our market structure identifies Disney Cruise Line and Hyatt—two brands outside the airline market—as proximal brands to Southwest Airlines. Such findings provide opportunities for Southwest, as it can target those who like Disney Cruise and Hyatt in social media, cross-promote its brand by teaming up with Disney Cruise and/or Hyatt on each other's websites, or launch coalition loyalty programs.Our article contributes to product-market research by leveraging the information embedded in big data of user–brand engagement networks to identify product-markets without having to prespecify boundaries. User–brand engagement network data at a high level in the purchase funnel (interest phase), together with deep learning techniques, provide us with insights at a greater scale and level of detail than extant methods. Our ability to map a large number of brands and precisely visualize brand relationships using learned vector representations enables managers to identify opportunities and threats that lie beyond product-market boundaries. Moreover, our method satisfies the three elements widely regarded as essential to successful real-world applications of artificial intelligence: data, algorithm, and computing power ([ 2]). In this article, we leverage deep learning and a network representation learning (algorithm) to understand market structure using large-scale social media data (data). This model implementation is efficient under NVIDIA P100 graphics processing unit, with Tensorflow as the backend framework (computing power). In summary, our study is an apt illustration of how artificial intelligence can be used to tackle a traditional marketing problem and provide richer insights for mangers in a rapidly changing competitive environment. Background and PositioningExtant work in identifying competitive market structures dates to the 1970s (e.g., [ 8]; [20]), when diary panel–based brand-switching purchase data and survey-based consumer judgments of substitution in use or similarities were used to construct market structure maps. These studies depended on customer data generated either at a late stage of the customer journey or at the very beginning of the journey. The increased availability of scanner-panel data of purchases, market structure models with marketing mix (e.g., [ 5]; Kannan and Wright 1991), and dynamic market structure models (e.g., [10]) provided more detailed insights into interbrand relationships and competition. Approaches such as brand concept maps ([19]) and the Zaltman metaphor elicitation technique ([53]) relied on data collected using surveys and, therefore, were effort intensive. Given the scaling issues with the maximum likelihood–based models and the limitations of survey data, the product-market boundaries were prespecified generally at the industry level so that a smaller number of brands within an industry could be analyzed. The advent of online sources, such as review platforms, social media platforms, and clickstream data, dramatically increased the volume and variety of data for market structure studies, especially at the awareness, search, and consideration stages of the customer journey ([24]; [26]; [35]; [42]; see Tables 1 and 2). Even with a large volume of data, these studies predefine the product-market boundaries at the industry level to make the analyses viable.GraphTable 1. Comparison of Different Types of Work on Market Structure Discovery. Primary/Survey DataText MiningSocial Tag–BasedSearch DataShopping DataSocial EngagementData volumeSmallLargeLargeLargeVery largeVery largeData veracityAuthenticNoisyModerately noisyModerately noisyAuthenticModerately noisyPrivacy preservingYesYesYesNo (need to insert a tracking pixel)YesYesData availabilityLow (need to do survey)High (publicly available)High (publicly available)Low (need to insert a tracking pixel)Low (need to partner with retailers)High (publicly available)Data preprocessing costLow (use consideration set directly)High (text mining is error-prone)High (text mining is error-prone)Low (use consideration set directly)Low (use product co-occurrence)Low (use network raw data) GraphTable 2. Summary of Difference Among Extant Literature on Market Structure Discovery. Kim, Albuquerque, and Bronnenberg (2011)Lee and Bradlow (2011)Netzer et al. (2012)Ringel and Skiera (2016)Culotta and Cutler (2016)Nam, Joshi, and Kannan (2017)Gabel, Guhl, and Klapper (2019)Our StudyObjectiveTo visualize user search behavior and understand market structureTo visualize competitive market structure using text mining on customer reviewTo visualize competitive market structure using text mining on forum discussionTo understand asymmetric competition in the product categoriesTo infer attribute-specific brand ratingsTo analyze user-generated tags for marketing researchTo leverage NLP and ML for analyzing market structureTo propose a novel deep network representation learning framework for market structurePrespecifying market categoryYesYesYesYesNoNoNoNoNetwork typesBrand–brandBrand–brandBrand–brandProduct–productBrand–brandBrand–brandProduct–productBrand–userBrands/products62 products, 4 brands9 brands169 products, 30 brands1,124 products200 brands7 brands133 categories, 30,763 products5,478 brandsConsumers/usersN.A.N.A.76,587100,000+14.6 millionN.A.N.A.25,992,832Data sourcesAmazonCustomer review at EpinionsOnline discussion forumProduct comparison websiteTwitterSocial tagging platform DeliciousRetailerFacebook public fan pageData typeConsumer searchTextTextConsumer searchNetworkSocial tagsShopping basketsNetworkBrand association methodologyConsideration setText-miningText-miningConsideration setNetwork learningNetwork learningNetwork learningNetwork learningBrand relationship asymmetryYesNoNoYesNoNoYesYesDimension reductionYesYesNoNoNoYesYesYesExternal validationN.A.N.A.Purchase data,surveySurveySurveyBrand concept map (survey)N.A.Event study,survey, Google search trends 1 Notes: NLP = natural language processing; ML = machine learning; N.A. = not applicable.There are other studies where the product-market boundaries are not predefined: [11] using online reviews, [33] using social tags, and [ 7] using Twitter hashtags. More recently, using word embeddings, [12] analyze customers' market baskets of items purchased on shopping trips. Still, from a methodological perspective all these studies use brand–brand networks—a distinct disadvantage, as we discussed previously. Our methodology uses brand–user networks, and the scale at which we analyze the data is much larger than any of the extant methods (cf. [12]). Social Media EngagementWe analyze social media engagement data in the form of user–brand links. Social media platforms such as Facebook, Twitter, and Instagram host public fan pages created by firms to facilitate communication with customers and promote products. The user–brand engagement could be in the form of a user liking a post by the brand, sharing a brand post, or commenting on a brand post. Because each of these likes, shares, and comments/posts is a user–brand link in our study, it is important to understand what they represent. Surveys of fans of brands have revealed many reasons as to why users ""like"" a brand or post/share comments. Positive motivations for interacting with a brand include to support a brand they like, to get a coupon or discount, to receive regular updates from the brand, to participate in contests, to share personal experiences, to share their interests/lifestyles with others, to research brands, to imitate a friend who likes the brand, or to act on a recommendation from another fan ([25]; [34]; [39]; [41]). Conversely, users may also leave negative comments to hurt a brand in favor of its rival brand ([17]).In our approach, we make a minimal assumption by creating a user–brand link regardless of the type of engagement (like, share, or comment/post). This assumption is based on the rationale that users interacting with a brand online exhibit their interest toward the brand to some extent. Thus, the two brands are related to one another on a spectrum ranging from substitutes to independents to complements. Prior research has examined such contexts and studied the impact of user engagement on brand image and customer purchase intentions with mixed results ([16]; [34]). [31] use a field experiment to find that users who liked a gym brand online were likely to become members of that gym offline. In another field experiment setting, [18] find that ""liking"" is simply a symptom of a positive brand attitude and does not imply the fan is any more loyal to the brand or any more likely to purchase the brand. In addition, it is only when users who liked the brand are targeted using promotional communication by the firm that purchase probabilities increase. Thus, for our research purposes we treat a like or a comment/post as exhibiting an interest toward the brand at the beginning of the customer journey. Such a tendency for users to connect to brands is generally interpreted as interest and may indicate broader (e.g., offline) interactions ([ 7]; [25]; [34]; [35]), which is consistent with our treatment. Our proposed approach is also consistent with research in social network analysis suggesting that social network structure equivalence reflects value/interest homophily and can be used to measure social proximity ([29]). MethodologySocial network platforms, such as Facebook, Instagram, and Twitter, can be abstracted as a network containing business (firm) accounts and individual user accounts. Firms use the public fan pages of business accounts to communicate with their customers and fans. Users interact with brands and with each other in different ways, such as commenting, liking, sharing, and following. To discover latent relationships among brands, we propose a deep network representation learning framework with the following steps. Step 1: Data collectionWe specify a set of brands that is of interest in the social network platform. We then download all available user engagement data from the brands' public fan pages covering an appropriate time window based on managerial interest. A user engagement is defined as either liking or commenting on a firm's post on its public fan page. Note that for the sake of privacy, we do not attempt to collect any personal information of users. Rather, the only user information we obtain is the unique user identifier, assigned by platform, and the user's public engagement activities, consistent with recent studies on social media marketing ([17]; [23]).[ 5] Moreover, different platforms may have their own specific data policy. For example, Facebook does not permit collecting personal information from individuals who liked a given page. Such data restrictions and potential ethical concerns do come at a research cost, as we are unable to verify how representative they are of the population at large. Step 2: Network constructionWe start with a cleansing operation to remove spurious users. We then construct a brand–user network including all selected brands and all users engaging with them. A brand node and a user node are connected if the user engages with the brand. The strength of an edge between a brand node and a user node is the engagement frequency. Step 3: Deep network representation learningThe deep network representation learning algorithm represents each node (brand or user) as a low-dimensional vector, also known as a node embedding. Embedding techniques are not new in marketing. For example, [49] adopt pretrained word embeddings, where each word is represented as a low-dimensional vector, to extract insights from textual reviews. However, our node embeddings are trained via an unsupervised deep autoencoder. This representation learning is essential to data-driven analysis, and the learned low-dimensional embeddings are useful for the downstream task of identifying and visualizing the product-markets.The objective in using an autoencoder is to learn the representation of the data so that each node can be represented in a lower-dimensional space while the network structure between users and brands is preserved. It trains the network to ignore the ""noise"" in the data and focus on the primary latent structure. The autoencoder reduces the dimensionality of the input data to a ""bottleneck"" (the reduced encoding) and, using the reduced encoding as input, reconstructs a representation of the original data. Learning occurs through backpropagation of the loss (see detailed definition in Web Appendix WA1) to achieve a reconstructed representation as close as possible to the original representation. We are interested in the bottleneck-reduced encoding for developing market structure. In essence, we can compare the dimensionality reduction functionality of the autoencoder with that of PCA. Whereas in PCA the reduced dimensions are linear combinations of the input variables, the reduced dimensions in autoencoder are nonlinear and nonorthogonal, which is achieved through nonlinear activations of the neurons, allowing the model to learn more powerful generalizations than PCA can.In our application, the autoencoder works on the large brand–user network in an attempt to preserve the network structure such that ( 1) nodes that are directly connected have similar vectors (are closer to each other) in the reduced embedding space, and ( 2) nodes that are not directly connected but share structural equivalence (such as many common neighbors) are also similar in the embedding space. These two types of similarity are referred to as the first-order (direct connection) similarity and the second-order (network structural equivalence) similarity. Formally, we denote the aforementioned network as  G=(Vb,Vu,E)  , where  Vb=(v1b,v2b,...,vnb)  represents a set of n brand nodes,  Vu=(v1u,v2u,...,vmu)  represents  m  user nodes, and  E={ei,j},i≤m,j≤n  represents all links between users and brands.  ei,j  indicates an engagement between user  i  and brand  j  . Given such a network  G  , the network representation aims to learn a mapping function  f:vib,vju↦wib,wju∈Rd  , where  d≪min(m,n)  .  wib,wju  are called brand embedding and user embedding, respectively. A commonly used embedding dimensionality  d  is 300 ([30]; [40]). The objective of the mapping function is to develop appropriate embeddings so that the brand proximities, brand–user proximities, and user proximities exhibited in the original network are preserved as much as possible in the reduced embedding space. (Technical details of the autoencoder methodology and parameter tuning are discussed in Web Appendix WA1.) Representing brands as dense low-dimensional vectors allows us to capture brand relations from multiple facets, as opposed to using unique vectors for each user and each brand as in a network adjacent matrix representation. An example is illustrated in Figure 1. Step 4: Market structure discoveryDrawing on vector representation for brands and users, we use learned embeddings to efficiently compute similarity among brands and to visualize natural clusters of related brands. Finding similar brands to a focal brand can be achieved by a nearest-neighbor search based on the widely used cosine similarity, which measures the cosine of the angle between two vectors and has a range [−1, 1]. Visualizing natural clusters of related brands can be achieved by a dimension reduction method, such as t-distributed stochastic neighbor embedding (t-SNE; [30]), which projects high-dimensional data into a low-dimensional space (e.g., two or three dimensions).[ 6] It has been used for visualization in a wide range of applications and is especially well-suited for visualizing high-dimensional representations learned from deep neural networks. t-SNE preserves the distance of data points well, such that data points nearby in a high-dimensional space (d = 300 in our case) would be close in a lower-dimensional (e.g., two-dimensional) space, while distant data points would be further apart in a lower-dimensional space. Thus, we observe that related brands are surrounding each other in the reduced two-dimensional space after t-SNE. DataWe use Facebook as our empirical benchmark, as it is one of the largest and most representative online social network platforms. (Our model can be generalized to other similar social network platforms.) To collect Facebook data, we first obtain a list of U.S. brands with the most followers from the social media marketing website Socialbakers.[ 7] Facebook public fan pages are categorized into several groups on Socialbakers, such as Brands, Celebrities, Community, Entertainment, Media, Place, Society, and Sport. We focus on the ""Brands"" category because it covers a wide range of industries and is more interesting to marketers. On Facebook, every brand is associated with a category chosen from the predefined Facebook option when creating the public page. This category label is solely determined by the brand and is aligned with its core business (e.g., Walmart is in the category of ""retail,"" Amazon is in the ""ecommerce"" category). In total, we obtain 5,478 different brands, covering 25 different categories. The largest brand, in terms of number of followers, is Walmart, with 30 million followers. The smallest brand is Bladz Jewelry in the ""fashion"" category, with 100,000 followers. Figure 2 shows the histogram of number of followers of brand Facebook page. We observe that the data set contains brands with varying popularity, making it representative of brands on Facebook.Graph: Figure 2. Histogram of number of followers of 5,478 Facebook brands.On Facebook, firms post on their public fan pages and allow users to comment, like, and share posts. The posts become an important marketing channel for businesses to interact with their customers. We use Facebook Graph API[ 8] to download all activities visible on a brand page such as posts by the brand administrator, as well as posts by users, including comments and likes on brand posts. It is worth emphasizing that to ensure privacy protection, we do not download any user profile information or examine the content of user comments. All engagement activities are represented by unique user identifiers, regardless of whether the user has a public or private Facebook profile, and brand identifiers. The data set collected for this study covers the period from January 1, 2017, through January 1, 2018. In total, we obtain 106,580,172 user–brand engagement activities from 25,992,832 unique users. Because prior research has shown that online interaction is a reflection of broader and even offline interaction ([38]), given the scale of user online engagement in this study, we believe it is a good proxy of how the overall consumer population perceives these brands.To ensure data quality and robust results (i.e., that the comments on Facebook brand pages reflect genuine user experiences, opinions, and interactions with brands), we design a set of rules, following [54], to remove fake users and their corresponding activities. For example, we find one user who liked posts across 475 different brands. As most users are likely to be interested in far fewer brands, we remove users who like posts on more than 200 brands, which accounts for.01% of the total users and 1.6% of the total user–brand engagement. We also remove users who posted duplicate comments containing URL links. Table 3 describes the data details. The brands' degree distribution (number of connections) exhibits a scale-free distribution (shown in Figure 3), a well-documented phenomenon in most social networks.Graph: Figure 3. Degree distribution of brands in the user–brand network.GraphTable 3. Data Description and Statistics. Number of brands5,478Number of users25,992,832Number of unique user–brand interactions36,927,613Number of like interactions87,876,623Number of unique user–brand like interactions29,611,805Number of comment interactions18,703,549Number of unique user–brand comment interactions7,612,358Total number of user–brand interactions106,580,172  Evaluation and ResultsIn this section, we extensively evaluate the market structure derived from our approach from both quantitative and qualitative perspectives. We also validate the derived market structure using two external data sources: consumer survey and Google search trend. Visualization of Market StructureWith the learned brand representation vectors, we can visualize how the brands are grouped and focus on local fine-grained brand proximity. We use t-SNE to obtain market structure visualization by reducing the learned 300-dimensional brand representations to obtain the associated 2-dimensional visualization map. Figure 4 presents the global structure of the brands in our Facebook data. Each data point in the figure denotes a brand belonging to one of the 25 categories, and each category is indicated by a different color. We interpret the visualization as follows: the closer any two brands are in the figure, the more similar their brand representations are in the 300-dimensional space (see Figure 4). The color codes in the map indicate brands in the same Facebook category, with the category label selected by the brands themselves on Facebook.Graph: Figure 4. The global structure among brands.The global Facebook brand market structure map yields several interesting observations. First, there are clear grouping patterns into clusters, particularly between brands in the same industry (points with the same color tend to be in a group). For example, Cluster 1 in Figure 4 (expanded in Figure 5) includes nonluxury domestic and imported automobile brands such as Toyota, Nissan, and Mazda, as well as some automobile accessories brands such as Michelin, DENSO, and Auto Parts. Note that in our data we have several luxury automobile brands such as BMW, Mercedes-Benz, Audi, Tesla, and Maserati, which are not close to the brands in Cluster 1. In fact, they are clustered in a different region of the map with other luxury brands such as Chanel, Gucci, and Cartier. Such a separation between luxury car brands and nonluxury car brands further confirms that brand representation learned from our approach captures latent semantics in multiple dimensions not only on the industry dimension but also on the price and luxury dimensions. The strength of our methodology lies in its ease of capturing these relationships on a single map, which it does by locating thousands of brands in the market structure map and highlighting the complex and possibly overlapping product-market boundaries characterizing these brands. We present a robustness check for different visualization methods in Web Appendix WA5.Graph: Figure 5. Enhanced view of Clusters 1 (top left), 2 (top right), 3 (bottom left), and 4 (bottom right).We provide an enhanced view of the four clusters in Figure 5 to examine the fine-grained local market structures. Panel A displays automobile brands along with automobile accessories and motorcycle brands at the top. Panel B displays premium vacation resort brands, such as The Signature at MGM Grand and the Coconut Bay Beach Resort & Spa. Panels C and D contain airline brands and cosmetic brands, respectively. Taken together, these maps provide face validity to our methodology in terms of core brands making up an industry and the overlaps among product-markets. Identifying Proximal BrandsWhile visual mapping is sufficient to provide a gestalt picture of all 5,000 plus brands in the aggregate, it does not provide the actual distance between the brand vectors in the reduced dimension space. Because identifying proximal brands for substitute/complement analysis is a critical task in marketing decisions ([ 8]), we focus on identifying proximal brands from the perspective of a focal brand. In doing so, we offer a new perspective that reflects the nature of the varied relationships ranging from substitutes to complements in the social network space.In this illustration, we choose United Airlines and Southwest Airlines from the airlines category and Audi USA and Nissan from the automobile category, as these brands are generally regarded as having different consumer bases and belonging to different submarkets. Each of the four brands is referred to as a focal brand, and we find their top ten proximal brands according to cosine similarity. Table 4 provides several interesting insights. First, our method is able to capture specific brand latent characteristics. For example, Southwest Airlines is generally considered a low-budget airline compared with United. The brands most proximal to Southwest Airlines and United reflect this difference. The proximal brands for Southwest Airlines are JetBlue, Frontier Airlines, and Allegiant, while the most proximal brands for United are major domestic and international airlines, such as American Airlines, Delta, Lufthansa, All Nippon Airways, Air China, LATAM Airlines, and Air New Zealand. Similar results also are identified in the automobile industry. Second, we observe asymmetric competition (see [42]). For example, Southwest Airlines is the fourth-most-proximal brand to United Airlines, while United Airlines ranks sixth in the set of top proximal brands to Southwest Airlines.GraphTable 4. Top 10 Proximal Brands to Each Focal Brand. RankFocal BrandUnitedSouthwestAirlinesAudi USANissan1AmericanJetBlueMercedes-Benz USAMazda2DeltaFrontierBMW USAToyota3LufthansaAllegiantLand RoverVolkswagen4SouthwestDeltaLexusKia Motors America5AlaskaAlaskaChevrolet CamaroSubaru of America6All NipponUnitedMaserati USAChrysler7Air ChinaAirfarewatchdogKawasaki USAFiat8LATAMAmericanFirestone TiresJaguar9Air New ZealandVirgin AmericaTeslaAlfa Romeo10AirfarewatchdogHyattRam TrucksKlim Third, unlike prior market structure analysis, where proximal brands are usually from the same industry as the focal brand, the top most proximal brands derived from our analysis are from different industries. For example, a brand called ""Airfarewatchdog"" is proximal to both United and Southwest Airlines. Airfarewatchdog is a deal-finder for flight tickets and has a large follower base (over 1 million) on Facebook. Traditional market analysis would simply ignore this brand, as it is not an airline. Further, it is also interesting to see that Southwest Airlines is closer to Airfarewatchdog than to United, which may indicate that the fans of Southwest Airlines are more likely to use a deal finder before purchasing flight tickets; thus, Airfarewatchdog could be a complement to Southwest when customers look for cheap flights on that site and end up at Southwest, or it could potentially compete with Southwest. In either case, Southwest could focus more on this site and examine the nature of the relationship. Identifying Opportunities/ThreatsOur market structure map can help managers identify brands outside of the product-market that are close to a specific brand and, thus, identify opportunities and threats posed by different brands. Take the airline product-market (Figure 5, Panel C) as an example. In our analysis, Disney Cruise Line and Hyatt are two brands outside of the airline product-market but are identified as proximal brands to Southwest but not for United. These proximal locations simply are due to a greater number of users in our data set liking both Southwest and Hyatt ( 2,709) versus the number of users liking both United and Hyatt (954). Similarly, a greater number of users like both Southwest and Disney Cruise Line ( 3,050) than like both United and Disney Cruise (729).Such findings can provide opportunities for Southwest, as it could target users who like Disney Cruise and Hyatt on social media. Southwest could cross-promote these brands by teaming up with Disney Cruise and/or Hyatt on each other's websites and launch coalition loyalty programs. From the viewpoint of other hotel chains that are competitors to Hyatt, these could be potential threats, so gleaning such insights early on may help them take proactive actions. Such opportunities/threats are difficult to identify when product-markets are prespecified, and they cannot be obtained easily through other means. Large Brand Versus Small BrandOur user engagement data set contains top 5,478 primarily large brands, ranked by their popularity (number of followers as of data collection period) on Facebook. A key question is whether our proposed approach is still able to identify meaningful market structure for smaller brands. If they can find the right position in the product-market structure, smaller brands have the potential to increase consumer awareness and interest in their brands ([13]), which could lead to a permanent benefit in terms of competitive advantage ([47]). Therefore, to test whether our methodology is able to capture relationships among large brands as well as small and local business brands, we add a set of smaller brands to the original data set. Specifically, we focus on the ""Travel"" category, as it includes many small, local travel agencies, and their followers on Facebook range from a few hundred to a few thousand on average. In total, we have 241 travel brands. Figure 6, Panels A and B, plot the distribution of the number of followers of these travel brands and shows that it is quite diverse.Graph: Figure 6. Size and market structure of 241 travel brands.Upon applying our methodology to the enlarged data set, we observe (Figure 6, Panel B) that these 241 travel brands are predominantly located in two areas. This pattern indicates that the latent brand relationship is well captured, even when brands have few engagement activities due to their smaller user bases. In a brand–brand network, such a small number of shared user bases could result in a failure to capture proximal locations, in essence treating them as noise.The market structure uncovered for these small businesses by identifying their proximal brands has good face validity. For example, ""The Luxury Travel Expert"" is an information portal for luxury travel and premium tours, with about 11,000 followers on Facebook as of our data collection period. Most posts receive fewer than ten comments and likes. The top proximal brands based on the cosine similarity are Smithsonian Journeys, The Peninsula Beverly Hills, Peter Sommer Travels, Quasar Expeditions, and DuVine Cycling. It is noteworthy that these are small travel brands that focus on expert-led, small-group, luxury, and premium tours. The results further confirm that our deep network representation learning method is generalizable to both small and large brands. This analysis also allows brand marketing managers to identify business opportunities. For example, in our analysis, the two brands The Luxury Travel Expert and The Peninsula Beverly Hills are quite close. The former is an information portal for luxury travel and premium tours, and the latter is a five-star luxury hotel. Therefore, the marketing manager of the Peninsula Beverly Hills could promote the brand on the information portal website to attract users from The Luxury Travel Expert to expand its customer base. Within-Industry Market Structure AnalysisExtant methods typically predefine the product-market boundary to derive market structure and brand relationships. In contrast, we allow product-market boundaries to emerge from the data. Therefore, a natural question is whether it is necessary to have a broader range of brands from other industries to derive a highly precise market structure for a specific industry. Although managers would typically focus on engagement data for their brands and for brands within the same industry, how does engagement data from brands in different categories help? To answer this question, we choose the ""auto"" category and only use the engagement data from the auto brands to derive the market structure. In the data set, we have 163 auto brands, including cars and car accessories brands (e.g., tires, oil), with 2.7 million user engagements in total. The analysis shows (Figure 7, Panels A and B) that structures with reasonable face validity still emerge using only the auto brands data. For example, the top left corner in Figure 7, Panel B, presents a cluster of imported auto brands such as Kia Motor America, Toyota, and Nissan. However, compared with the derived auto brand market structure learned from using all brand data, as shown in Figure 5, Panel A, the market structure is less clustered and more ambiguous.Graph: Figure 7. Visualization of market structure of using engagement data only from ""auto"" brands. Notes: The right panel is the zoomed in visualization with BMW as centroid.Next, we compare the market structure using the engagement data from the auto brands alone with that from all brands across categories in a qualitative manner. Specifically, we choose the brand FMF Racing, which is a company that develops dirt bike exhausts for off-road or racing motocross riding. Using the engagement data from the auto brands alone, the top proximal brands are Lucas Oil, KTM USA, Yamaha Motor, Arctic Cat, Two Brothers Racing, Phoenix Pro Scooters, Auto Alliance, Valvoline USA, Lance Camper, and Castrol. Some are related to off-road motocross riding, while others are not. For example, Lucas Oil, Valvoline USA, and Castrol are global automotive oil brands.In contrast, the top ten proximal brands to FMF Racing emerging from using all categories of data are KTM USA, Polaris Snowmobiles, Fox Racing, Mickey Thompson Performance Tires & Wheels, Two Brothers Racing, King Shocks, Arctic Cat, Addictive Desert Designs, NISMO, Skunk2 Racing, and MBRP performance exhaust. Upon further investigation, we find that they are all related to off-road motocross riding. These results indicate that our approach with engagement data from brands across industries can learn better brand representation and thus reveal a highly precise market structure. External Validity and Comparison with Other Approaches Market structure identified based on consumer surveyTo assess the external validity of our approach, we conduct a survey on Amazon Mechanical Turk (MTurk), which is a reliable source for data collection and marketing analytics ([43]). Prior market structure literature has also administered brand perception survey on MTurk ([ 7]). Following this prior study, we surveyed 28 automobile brands (after ignoring the other 150 brands that are related to motorcycle or automobile accessory such as tires, parts, and oil). Specifically, we recruited 500 MTurk participants, each of whom was required to be in the United States and have a good MTurk record (successful completion of at least 100 assignments with a minimum 95% rate of approval). Each participant was asked to rate the similarity between a focal automobile brand and the other 27 automobile brands on a scale of one to five. To avoid fatigue due to information overload, each participant was randomly assigned to work on one task. Participants were also asked to indicate their age, gender, and whether they owned an automobile. Details of the participants' demographics information and the survey design are presented in Web Appendix WA6.In the survey, participants could choose ""N/A"" if they were not aware of the automobile brands. Brand recognition rate was 88.2%, implying that 11.8% of ratings were not applicable due to lack of brand awareness. We aggregated the survey data and built a 28 × 28 matrix, where each cell represented the pairwise brand similarity, and denoted it as the ""survey matrix."" We also used the brand representations learned from our approach to construct another 28 × 28 matrix of brand similarity, which we denoted as the ""deep learning–based matrix."" The correlation between two matrices is significantly positive (r = .385, p = .000). This result provides additional evidence on the validity of our deep learning–based approach for market structure identification. We also did an additional check where we calculated the correlation between the survey response and that constructed by our approach but using only automobile (within-industry) data. The correlation is.152 (p < .05), which is not as substantial and significant as the correlation between the survey response and our approach using all industry data. We present the market structure learned from the survey data in Web Appendix WA7 as an external validity. Market structure identified based on Google TrendsTo provide further external validity of our approach, we use Google Trends data to identify market structure and examine how it aligns with our approach of using online social media users' brand engagement. Google Trends provides an interest score for every search query across regions and languages, as measured by an aggregated search volume over time. A higher interest score means that queries are more popular in a specific region and time. Google Trends data have been widely used by industry ([44]) and academia ([ 6]; [ 9]; [22]; [48]) to address marketing and economic problems (e.g., competitive analysis). Researchers have also shown that this score is consistent with consumers' purchase interest in general ([ 6]; [ 9]).To determine relative popularity for every pair of brands, we make a search query consisting of two brand names—for example, ""Toyota BMW"" for the brands Toyota and BMW. For every brand pair, we can obtain an interest score returned by Google. For example, in the United States in 2017, the interest score is 13 and 85 for the query ""Toyota BMW"" and ""Toyota Honda,"" respectively. This indicates that consumers in general are more interested in searching Toyota and Honda together, compared with searching Toyota and BMW together. Validation on airline industryIn the first validation exercise, we focus on the airline industry and the derived market structure. We have 19 airline brands in our data set, including U.S. domestic airlines and international airlines (Figure 5, Panel C). For every brand pair, we first obtain a Google search interest score in the U.S. region in 2017 (the same as our engagement data period). Then, following previous work ([35]), we calculate the similarity between two brands A and B as  sim(A,B)=interest(A,B)∑b∈Sinterest(b,B)  , where  S  is the set of all brands (e.g., 19 here). [35] use the co-occurrence of two brands in an online discussion forum instead of a Google search interest score. We also calculate similarity for every pair of 19 airline brands using 300-dimensional vectors derived from our deep network representation learning on the engagement data using cosine similarity.To check whether the two aforementioned similarity scores are similar to each other, we calculate their Pearson's two-tailed correlation between two sets of 361 (= 19 × 19) similarity scores. It is significantly and highly correlated (  r=.630,p=.0000  ). This indicates that our social engagement-based market structure is similar to that derived from Google Trends. Because prior studies have shown that the Google search data have a high correlation with a consumer's actual purchase interest ([ 6]; [ 9]), we can conclude that users' social engagement with brands also contains valuable information for deriving brand relationships. Validation on travel industryIn the second validation exercise, we focus on the travel industry, including not only major travel brands but also many small and local travel brands (see the ""Large Brand Versus Small Brand"" subsection). There are 241 travel brands in the data set. Similar to the first validation exercise, for every brand pair, we obtain a Google search interest score in the United States in 2017 (the same as our engagement data period). Among the 241 travel brands, Google Trends does not return scores for 90 brands (i.e., showing ""your search doesn't have enough data to show here""), which results in data for 151 remaining brands. Although individual brands show a considerable amount of search, only four brand pairs return nonempty interest scores.[ 9] This data sparsity may be attributed to the uniqueness of the travel category. Many of the travel brands are local/small businesses, such as the travel agencies ""Spirit of Boston"" and ""Historic Philadelphia."" Naturally, they do not receive as many queries as large brands. Moreover, consumers may search travel agency brands in different queries, but they very rarely search two travel brands in the same query. Therefore, there is not enough data for Google to aggregate and return the cosearch score. This analysis highlights the limitation of the cosearch-based approach, which is likely to suffer from the data sparsity issue. In contrast, our approach built on large-scale brand–user social engagement data can provide valuable marketing insights not only for large international brands but also for small local brands. Practical Actionability How to compare market structure maps?In a practical setting, marketing managers may need to quantitatively determine the quality of derived market structure maps, based on which they can infer actionable insights. We evaluate the conceptual maps using a standard metric—silhouette score ([ 1])—which has been adopted in prior market structure literature ([12]). The silhouette coefficient is calculated using the mean intracluster distance (a) and the mean nearest-cluster distance (b) for each sample, as  b−amax(a,b)  . The values of silhouette score range between −1 and 1 (1 being the best and −1 the worst). Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster. Recall that our approach can naturally group brands that have similar representations in the high-dimensional space. An ideal market structure would favor brands that are concentrated and exhibit clean cluster structures. We conduct K-means clustering on the brand representations and compute the mean silhouette coefficient of all samples.In the ""Within-Industry Market Structure Analysis"" subsection, we qualitatively show that our approach—without prespecifying product-market—reveals more interesting and coherent brand insight than using brand engagement data within only one industry. Next, we vary the number of clusters in K-means and calculate the silhouette coefficient of different methods. The result in Figure 8 shows that our approach using all brand engagement data consistently achieves better clustering than using only the automobile brand engagement data. For example, when we cluster 168 automobile brands into two clusters (i.e., K = 2 in K-means), our approach achieves a silhouette coefficient of.334, while the approach using only the automobile engagement data has a low silhouette coefficient of.043. The silhouette coefficient of our approach gradually converges to.10 as the number of clusters increases. In contrast, the approach using only automobile industry data stays near.01, indicating a poor separation among automobile brands. This analysis not only confirms the superiority of our approach without prespecifying product-market boundary but also enables marketing managers to determine the quality of derived market structure maps. How much brand–user engagement data are needed to derive a good market structure?We have shown that our approach can derive good market structure with large-scale social engagement data. In practice, it is easy to obtain a relatively comprehensive set of brands across different categories and associated user engagement from social media marketing platforms, such as Socialbakers. However, a marketing manager may not have enough resources to collect as large a data set as we have, raising the question as to whether our approach is sensitive to the size of data for obtaining a good market structure. To answer this question, we calculate the correlation between the similarity of pairwise brands generated using the full data set and that generated by a fraction of data selected at random. Figure 9 presents this result, showing that the correlation reaches over.90 when 40% data is used (and.7 when 12.5% data is used), and it starts to converge to the market structure generated by using the full data. This analysis suggests that our approach is relatively robust to the amount of engagement data used. Marketing managers can use this analysis as guidance to determine the amount of data resources needed. In addition, we examine how the number of prespecified industries affects the robustness of market structure maps and present the analysis in Web Appendix WA8.Graph: Figure 8. The clustering silhouette coefficient of 168 automobile brands.Graph: Figure 9. Pearson correlation between the similarity of pairwise brands generated using a percentage of full data and the full data. Evaluation using link predictionIn studying market structure, there is a lack of ground truth about the identified structure, that is, an understanding of what the ""true"" structure is, which makes demonstrating the performance of various proposed methods challenging. We introduce an alternative approach, adopted from network analysis literature ([27]), to evaluate the identified market structure. An identified market structure is a function of the brand representation, and so an accurate representation is more likely to identify valid market structures. This approach is supported by prior research showing a strong relationship between brand image and the characteristics of a brand's supporters and followers ([ 7]; [25]; [34]). If a network learning method were capable of accurately representing network nodes accounting for these relationships between brands and users, then it would be able to predict the future links between brands and users accurately. Therefore, we use a cross-validation procedure under a link prediction research design, where we predict the most likely newly formed links of user–brand engagement in an out-of-sample network given the brand vectors and user vectors learned from a training network. This research design is widely used in the network analysis community to evaluate network clustering algorithm performance ([27]; [51]). In our context, we use the user–brand interactions from the first half of the time span in our data to build a training network (G0,1) and use the second half to build a testing network (G1,2). The likelihood of a link formation is measured by the proximity of a learned brand vector and a learned user vector. Note that link prediction performance is significantly correlated with the quality of learned vectors, given the assumption that a better network representation learning can predict new interactions between users and brands with a high degree of accuracy. We provide details of the link prediction experiments in Web Appendices WA2–WA4. Overall, our analysis shows that ( 1) link prediction using representation learned from our brand–user network performs better than a reduced brand–brand network (a widely used method in extant approaches), ( 2) deep learning–based methods learn better representation than shallow machine learning methods, and ( 3) our deep learning–based model is robust and able to handle sparse networks as compared with baselines. Case Studies on Market Structure DynamicsMarket structure evolves over time and can change dramatically, especially under an unexpected industry shock. Whether our proposed method can be adaptively learned is also of interest as it could provide useful insights to marketing practitioners. In this section, we analyze how market structure changes under exogenous shocks by analyzing two case studies: Amazon acquiring Whole Foods and Tesla introducing the Model 3. We take a before-and-after strategy where we use data for the three months pre- and postevent announcement day and calculate the change in distance from the focal brand (i.e., Amazon and Tesla) to other representative brands selected from the same category. The purpose of the event study is to examine how a focal brand relationship with other brands changes as a major event occurs. Specifically, for Amazon–Whole Foods, we select several brands from the retail and e-commerce category, and for Tesla, we select several brands from the automobile category. We calculate the change between focal brand  i  's representation  wib  and target brand  j  's representation  wjb  before and after the specific event using cosine similarity:  cossim(wiafterb,wjafterb)−cossim(wibeforeb,wjbeforeb)  . Therefore, positive numbers indicate a similarity increase, whereas negative numbers indicate a decrease in similarity. Amazon Acquires Whole FoodsAmazon acquired Whole Foods in June 2017. This acquisition has had a significant impact on the grocery and retail industries. At the time, it was widely believed that Amazon planned to use its acquisition of Whole Foods to enter the online grocery delivery business. Amazon and Whole Foods ran separate Facebook pages. After the merger of the two firms, we see from Figure 10 that Amazon is more proximal to retail brands as measured by cosine similarity, while the proximity to other relevant brands decreases slightly. For example, the cosine similarity between Amazon and Lowe's Home Improvement decreases by.184. In contrast, the cosine similarity between Amazon and other super-market retailer brands increases. Among them, proximity of Amazon to Whole Foods increases by.202, and between Amazon and Kroger by.165. As inferred from our data-driven model, Amazon even becomes more proximal to Walmart, indicating that Amazon's competitive market structure landscape has shifted. By further examining our data, we find that, after the Whole Foods acquisition, the number of common users who interact with both Amazon and Whole Foods on their public Facebook pages increases. Some Amazon users posted comments on Whole Foods' fan page mentioning Amazon. For example, in the Whole Foods post, ""Here are 6 New Healthy Products Coming to Whole Foods in March,"" a user who had liked an Amazon post earlier commented, ""You mean AMAZON... as they bought Whole Foods...right?"" This direct link between Amazon and Whole Foods leads the deep autoencoder to increase the proximity between the two brands. Moreover, in another Whole Foods post, a user who had liked a Kroger post earlier posted, ""The quality has gone downhill and prices have soared.... You've made Kroger look appealing...."" Although we do not find that this user has ever interacted with Amazon's Facebook page before, her interaction with Whole Foods leaves an implicit connection between Amazon and Kroger, which can be captured by the deep autoencoder. In short, after Amazon acquired Whole Foods, online social media users who are Amazon's fans pay more attention to Whole Foods, and users who are fans of other supermarket brands engage more with Whole Foods due to the acquisition event. As a result, the deep autoencoder captures the dynamics and updates the brand representation accordingly.Graph: Figure 10. Similarity change of Amazon to other brands in retail and e-commerce industry.The acquisition by Amazon has an impact on the market structure of Whole Foods as well. In Figure 11, we consider Whole Foods as the focal brand and calculate the change in proximities to other brands before and after the acquisition. Drawing on the results, we observe that Whole Foods' proximity to other retail brands such as Target, Walmart, and Best Buy increases. Among them, the proximity to Amazon increases the most due to the increase in the number of common users between them. In contrast, Whole Foods' proximity to supermarket brands such as Goya Foods, Enjoy Life Foods, and HelloFresh decreases slightly. Second, the magnitude of change in proximity values is smaller than those of Amazon to other brands. This seems to indicate that the acquisition has had less impact on Whole Foods, as it is still positioned around other supermarket brands, while Amazon is expanding closer to the grocery retail category.Graph: Figure 11. Similarity change of Whole Foods to other brands in retail and e-commerce industry.Although this analysis is retrospective, it highlights that our approach offers managers a series of multiple snapshots of the structure over time to measure a brand's relative position change, thus identifying potential market structure change. Suppose a supermarket chain brand A observes that Amazon is moving closer to A's position on the map. This may indicate that Amazon is getting more engagements (likes or comments) from A's customers. Given that one motivation of liking a brand's Facebook post is to receive some benefit from the brand (e.g., coupon, discount), it could further indicate that Amazon is conducting effective promotional marketing campaigns on social media. No matter the underlying reasons, the increasing proximity of Amazon on the brand map can at least provide an early warning to A's marketing managers to the potential threat. Late response to the competition may harm the brand and eventually the whole business.Next, we validate the case study of Amazon's Whole Foods acquisition using Google Trends data. Similar to the first external validity exercise, we choose 29 ""retail"" brands (including Walmart, Target, Macy's, Best Buy, Walgreens, Lowe's, Whole Foods, IKEA, Sears, 7-Eleven, Dollar General, Sam's Club, Dollar Tree, CVS Pharmacy, Aldi, Barnes & Noble, Costco, Kroger, Meijer, Safeway, Office Depot, Rite Aid, Albertsons, ShopRite, and The Fresh Market) plus Amazon and obtain their interest scores for every brand pair in the United States in 2017. Note that we exclude some small retail brands such as Goya Foods because their Google cosearch interest scores with other brands are mostly 0, indicating insufficient search data for the brand.The Pearson's two-tailed correlation between two sets of 900 (= 30 × 30) similarity scores is significantly high before (  r=.675,p=.0000  ) and after (  r=.758,p=.0000  ) acquisition. This result confirms the external validity of our social engagement–based method. We observe that for Amazon, before the Whole Foods acquisition, the most similar brands were Barnes & Noble, Macy's, and Best Buy. After the acquisition, the most similar brands are Whole Foods, Barnes & Noble, and Macy's. For Whole Foods, before the acquisition the most similar brands were The Fresh Market, Albertsons, and ShopRite. After the acquisition, the most similar brands are The Fresh Market, Amazon, and Safeway.We obtain further search interest data for one year after the acquisition (June 2017 to June 2018) to examine whether the market structure change is sustained for a long period after the acquisition announcement. For Amazon, the most similar brands are still Whole Foods, Barnes & Noble, and Macy's. Other grocery ""retail"" brands such as Kroger and The Fresh Market become more similar to Amazon than before the acquisition. For Whole Foods, the most similar brands are The Fresh Market, Safeway, ShopRite, and Amazon. Because Whole Foods is still Amazon's most similar brand among these retailer brands, this indicates that for Amazon, the acquisition impact holds for the extended period of analysis. It seems that the acquisition has less of an impact on Whole Foods, as Whole Foods is still positioned around other supermarket brands. All findings are consistent with our case study using social engagement data, which provides external validity to our results. Tesla Announces the Model 3Tesla sells two types of sedans: the Model S and the Model 3. The Model S is a luxury premium sedan with a larger range of acceleration and customization options, while the Model 3 is designed as a more affordable mass-market electric vehicle. The Model S can cost over $100,000 depending on the configuration, while the Model 3 costs approximately $35,000. After the announcement of the new Model 3, we see that Tesla becomes more distant from luxury car brands and moves closer to nonluxury car brands. We can see in Figure 12 that the cosine similarity between Tesla and the luxury car brand Maserati decreases by.209. Similar trends exist between Tesla and other high-end or luxury car brands such as BMW, Mercedes-Benz, Audi, and so on. Meanwhile, Tesla becomes more proximal to Kia, Mazda, and other more affordable car brands.[10]Graph: Figure 12. Similarity change of Tesla to other selected brands in the auto industry. Testing for SignificanceIn the previous analysis, we compute the distance change between the focal brand (i.e., Amazon or Whole Foods) and other brands before and after the acquisition. We can see that there is a significant increase in similarity between Amazon and Whole Foods after the acquisition. However, whether this distance change is caused by the acquisition or other unobserved factors, such as the difference of data split and/or noise, still remains unclear. Therefore, we conduct further analysis by randomly splitting all data before the acquisition into two parts (i.e., d1 and d2, with d1 before d2). We then measure the distance between Amazon and Whole Foods using d1 and d2 separately. We repeat this process 30 times using different data cuts in the preacquisition data. The average distances between the two brands across using all d1s and d2s are.228 and.232, respectively. The two-tailed t-test on the distance is.055, which indicates that there is no statistically significant difference between the distances between Amazon and Whole Foods before the acquisition in different cuts of the preacquisition data. Accordingly, the substantial increase in similarity between Amazon and Whole Foods is not attributed to sample differences.We perform a similar process on Tesla's introduction of the Model 3. In particular, we choose one nonluxury brand, Mazda, and compute its distances to Tesla before the event using various data splits. The average distances between Mazda and Tesla across using all d1s and d2s are.185 and.191, respectively, with a p-value of.076. This seems to indicate there is no statistically significant difference between Mazda and Tesla when the cutting point of data varies before the event. Therefore, we conclude that after Model 3's announcement, Tesla becomes more similar to nonluxury automobile brands on the social media platform. Note that we also conduct analyses on Tesla and other automobile brands, and the results are consistent. Implications and ConclusionOur proposed approach examines millions of user engagements with thousands of brands and focuses on the early stage of the customer journey. This allows for visualization of potentially overlapping product-market boundaries across many categories and helps managers identify latent threats and potential opportunities, which cannot be done with extant methods that focus on later stages of the customer journey (lower levels of the purchase funnel) within categories. As an example, for Southwest, is Airfarewatchdog a potential competitor that might draw visitors away, or is it a complementor that would increase visits to Southwest? Having identified the overlapping market with Airfarewatchdog, Southwest could invest more attention to evaluate the exact nature of this relationship. If Airfarewatchdog is a competitor, then Southwest might focus on developing strategies to differentiate itself and channel visitors to its website exclusively. If it is a complementor, then Southwest might run display ad campaigns on Airfarewatchdog's website. In addition, both Disney Cruise and Hyatt are closely associated with Southwest, with common users who like these brands on social media; therefore, Southwest could run mutually beneficial joint and cross-promotions with these other brands. In fact, all these brands could join in a dynamic coalition loyalty ecosystem built around a fluid partnership of products, services, and experiences, thereby providing a unifying customer value proposition that could be difficult to compete against ([ 4]). Identifying such unusual or unforeseen insights is the greatest advantage of our approach.Another important strategic use of our market structure maps is to identify competitors and complementors across industries and track how these relationships change over time. While [15] apply text analysis to 10-K statements to identify such grouping based on product descriptions that the firms provide, we provide a more dynamic structure based on actual customer/user social media activities. Moreover, our market structure map is more forward looking and predictive of emerging competition and complementors and more proactive than those based on 10-K statements, which can be viewed as reactive. Because [14] show that merging firms with more similar product descriptions in their 10-Ks results in more successful outcomes, using our market structure maps to identify merger-and-acquisitions targets (firms sharing common users) may have similar benefits. We leave this for future research.The power of our method lies in its ability to capture the dynamic changes in market structure. Because the maps are based on the analysis of big data that can be collected in a relatively short window of time, our methodology can track changes in their relative position when firms introduce new products, new promotions, and new marketing initiatives. The case studies that we highlighted provide good illustrations of this. In addition, although we have not analyzed this in the article, firms can deploy our method to enhance their social network-based marketing efforts by better targeting specific potential customers, because user nodes in the network are also learned and represented as vectors in the same multidimensional space as brands. Our link prediction design demonstrates a possible use for targeting. Finally, our proposed method is generalizable to other similar platforms if we can construct a brand–user network from public fan pages' engagement data. We implemented our proposed method using NVIDIA P100 graphics processing unit, with Tensorflow as the back-end deep learning framework. For future research to replicate or practitioners to adopt, we have provided details regarding data collection, data cleaning, and deep model architecture, and model the fine-tuning process in Web Appendix WA1.While marketing analytics techniques have extensively used consumer personal data to derive valuable insights, they raise many privacy and ethical concerns. How to balance these two important aspects has become a key consideration for many marketing scholars ([ 3]; [52]). Our approach provides a useful example. The only input to our network representation learning method is the brand–user network, which can be publicly obtained from brands' social media page.Our research has some limitations. Given the nature of our data, our method cannot examine stockkeeping unit–level competition as is done by some of the extant methodologies using lower funnel data. From this perspective, we recommend our methodology as a complement to extant methods and for higher-level brand strategies and tactics. Future work could examine how perceptual maps vary by customer segment using lower-funnel data such as purchase frequency and purchase amount. Second, our analysis is conducted on one social network, Facebook. Even though Facebook is one of the largest online social networks, with billions of users and thousands of brands, it is likely that users on different platforms exhibit different engagement behavior, and some of the research findings may not be generalized to other platforms. For example, it is reported that Instagram users and Facebook users fall into different age groups (Pew Research Center 2021). We could apply the same technique to other social media platforms and compare findings. Finally, each link in the user–brand network is created when the user engages with the brand on the public page. Facebook has introduced various reaction emotions to the platform to allow users interact with brands in different ways, such as ""Like,"" ""Love,"" ""Care,"" ""Haha,"" ""Wow,"" ""Sad,"" and ""Angry."" Future work could build a multirelational network to deeply capture brand–user engagement heterogeneity. Supplemental Materialsj-pdf-1-jmx-10.1177_00222429211033585 - Supplemental material for Identifying Market Structure: A Deep Network Representation Learning of Social EngagementSupplemental material, sj-pdf-1-jmx-10.1177_00222429211033585 for Identifying Market Structure: A Deep Network Representation Learning of Social Engagement by Yi Yang, Kunpeng Zhang and P.K. Kannan in Journal of Marketing  "
12,"Is Distance Really Dead in the Online World? The Moderating Role of Geographical Distance on the Effectiveness of Electronic Word of Mouth The authors investigate how the geographical distance between online users is associated with electronic word-of-mouth (eWOM) effectiveness. Their research leverages variation in the visibility of eWOM messages on the social media platform of Twitter to address the issue of correlated user behaviors and preferences. The study shows that the likelihood that followers who are exposed to users' WOM subsequently make purchases increases with followers' geographic proximity to the users. The authors propose social identification as a potential mechanism for why geographical distance still matters online in eWOM: because consumers may form a sense of social identity based on their physical location, information regarding the spatial proximity of users could trigger online social identification with others. The findings are robust to alternative methods and specifications, such as further controlling for latent user homophily by incorporating user characteristics and embeddings based on advanced machine-learning and deep-learning models and a corpus of 140 million messages. The authors also rule out several alternative explanations. The findings have important implications for platform design, content curation, and seeding and targeting strategies.Keywords: electronic word of mouth; social media; geographical distance; deep learningElectronic word of mouth (eWOM) plays a key role in shaping consumer attitudes and behaviors. More consumers digitally share information, research what others say about products and services, and rely on eWOM to gain knowledge or make decisions than ever before ([25]; [81]). While eWOM is becoming one of the most influential sources of information among consumers, the effectiveness of traditional communication channels employed by marketers is declining ([93]; [102]; [104]). These contrasting trajectories have motivated marketers to harness the power of eWOM. To do so, they are paying more attention to their user base and the corresponding eWOM episodes ([ 3], [ 4]; [49]). At the same time, technological advancements and data opportunities in the online space enable marketers to access data such as user-generated content and user location ([ 6]; [112]). As the availability of location data in particular grows, marketers wonder whether location plays an important role in eWOM effectiveness. Some marketers believe that consumers may consider eWOM from farther distances as more widely accepted and universal and, thus, more valuable and informative ([42]). Others believe that eWOM from closer distances may be perceived as more relatable and relevant and, thus, more trustworthy ([19]; [95]). Yet other marketers suppose that distance does not matter online and, thus, does not influence how consumers value eWOM ([90]).The literature on the role of geographical distance in eWOM effectiveness is not decisive either. On the one hand, a common belief for the effectiveness of eWOM is that technology bridges the geographical distance between consumers. Information technology reduces communication costs by decoupling the interaction process from geographic constraints. For instance, mobile devices enable shopping and communication between consumers independent from location, while the internet allows instant access to message exchanges and marketplaces. These reduced communication and access costs have led some scholars to pronounce the ""death of distance"" ([29]) and ""end of geography"" ([52]).On the other hand, geographical distance may still play a role in online consumer behavior and WOM for several reasons ([51]). These include location-specific goods as well as economic costs related to shipping, contracting, monitoring, enforcement, travel, and inconvenience ([ 2]; [59]). Local user preferences and spatially correlated social ties as well as limited regional availability of alternative product options can also lead to commonalities in product adoption among nearby consumers (e.g., [21]; [72]; [79]). These reasons suggest that geography may still constrain the effectiveness of eWOM.Our research investigates whether geographical distance is significantly associated with the effect of eWOM, beyond the aforementioned explanations. Specifically, we ask whether the geographical distance between pairs of familiar disseminators and receivers of online WOM messages in social media plays a role in driving recipients' purchase behaviors or whether distance does not matter for eWOM beyond utilitarian reasons—such as transaction costs— and proxying for correlated user behaviors and preferences. We therefore examine this research question in an online setting where economic costs, such as those relating to contracting and travel, are not of concern.To investigate whether geographical distance is associated with the effectiveness of eWOM, we use rich data from the social media platform of Twitter, where users disseminated messages about their real-world product purchases ([103]). Our main identification strategy leverages variation in the visibility of these eWOM messages—enabled by a unique feature of the platform—causing some purchase messages to be visible and others to be invisible to followers ([ 1]). We compare the purchase behaviors of followers for whom users' purchase messages were included in their newsfeeds (i.e., the stream of tweets presented on the home screen of a user from accounts the user follows on Twitter) with the behaviors of followers for whom users' purchase messages were not included in their newsfeeds due to this design feature, while employing an extensive set of controls (discussed in the ""Econometric Model Identification"" subsection). We also use the salience of user locations on Twitter to provide evidence for the potential mechanism. In our empirical setting, users are familiar with each other and thus aware of the location of their peers. Their location is often salient, as users may highlight this information in their profiles; user profile information is observable in peers' timelines when hovering with the cursor over the profile picture, the username, or the person's name.We find that the relationship between eWOM and the likelihood that message recipients make a purchase strengthens as the geographical distance between disseminators and receivers decreases. This relationship is economically significant, managerially relevant, and robust to alternative methods and identification strategies. Social identity may explain why geographic proximity could increase the effectiveness of eWOM, beyond utilitarian reasons and proxying for correlated user behaviors, as consumers may form their social identities based in part on their geographic location ([44]; [83]; [107]; [108]). To provide evidence for this potential mechanism, we investigate, for instance, how the salience of geographic cues as well as conditions that strengthen the role of geographical location in the social identification process ([101]; [109]) further enhance the effectiveness of eWOM.Our findings of geographical distance variation in eWOM effectiveness have important implications for both theory and practice. We contribute to the online WOM influence studies by demonstrating that despite technology's promise, spatial distance continues to play an important role in a digital world ([ 9]; [71]). Specifically, we show how geographic proximity is significantly related to an increase in eWOM effectiveness. Geographic constraints thus tether the impact of electronic communications. This finding also contributes to the literature that shows how features of disseminators and receivers can drive eWOM outcomes ([20]; [49]) by identifying geographic proximity as a relational characteristic that can affect dyadic influence. Our work thus provides actionable strategies for boosting the effectiveness of online interpersonal communications. Our findings also imply that customizing seeding and targeting with more proximate connections or highlighting information and cues associated with social identity formation can increase the effectiveness of online advertising, product recommendations, social advertising, referral programs, and other marketing strategies and tools by leveraging local appeals and social identification. Related LiteratureSeveral studies demonstrate the importance of eWOM, documenting how it can be a major driver of consumer behaviors. For example, user opinions have been found to influence the consumption of movies ([34]; [70]), television shows ([47]; [71]), video games ([114]), and books ([33]; [69]). Whereas these studies examine the direct impact of eWOM on purchase decisions, the present research extends this line of inquiry by focusing on conditions under which the effect of online WOM may be attenuated or accentuated. Contextual Factors Affecting eWOMResearch has begun to investigate the contextual factors that impact the effectiveness of eWOM. These factors include characteristics of the product ([22]), brand ([73]), or WOM message itself ([85]). Scholars have also examined how certain features of disseminators (senders) and recipients shape the effect of eWOM. For instance, a sender's brand loyalty ([49]), expertise ([11]), and identity disclosure ([44]) can each affect the influence of WOM messages. Recipient characteristics such as the number of ties to adopters, demographics ([65]), and product experience ([87]) can also affect the influence of online communications. In addition, characteristics of the sender–recipient dyad, such as the tie strength ([13]; [20]), similarity across personality traits ([ 1]), and sociodemographic similarity ([45]), can also affect online WOM performance. We add to this stream of research by shedding light on an important factor that characterizes the pairwise relationship between senders and receivers of eWOM: the geographical distance between them. The Impact of Geographical Distance in Various Commercial ContextsGeographical distance has been shown to affect certain outcomes in multiple online and offline commerce settings. For example, geographical distance can affect online trade flows and volume due to costs related to shipping, contracting, monitoring, enforcement, travel, and inconvenience, as well as in cases of location-specific goods ([59]). Such transaction costs can engender a ""home bias"" that leads consumers to prefer transacting with nearby others ([ 7]; [59]; [68]). Similarly, geographical distance has been shown to correlate with product diffusion in the offline world due to imitation or direct observation, as physically close neighbors are more likely to adopt the same product ([21]; [36]; [43]).[ 5] In this study, however, we investigate how the geographic proximity between eWOM message disseminators and receivers accentuates or attenuates the effectiveness of online WOM, whereas the extant explanations for the impact of geography—albeit in different online settings—are not applicable to the context of eWOM; similarly, other explanations that do not apply to our eWOM setting relate to the location specificity of products, distribution networks, or local network externalities. Geographical Distance as a Potential Factor Affecting Online PersuasionRecent studies hint at the potential role geographical distance may play in facilitating persuasion. For instance, lab studies find that when consumers have no identifying information about online reviewers, they assume that the reviewers are similar to them and so are as persuaded by them as they are by reviewers who appear similar to them, and more persuaded than by reviewers who appear dissimilar to them ([82]). When ambiguous reviewers appear less similar, consumers are also less persuaded by their boastful reviews ([86]). One of the many ways these studies manipulate cues to indicate reviewer similarity is to show that the reviewer appears to live in the same or a nearby city as lab participants. In instances of familiar peers, rather than ambiguous reviewers, field studies find that consumers are more persuaded the more recent or intense their relationships are with their peers ([13]; [32]). One of the ways [13] measure relationship recency is to use as a proxy whether peers currently live in the same town. Whereas the aforementioned studies focus on how ambiguity about, trust in, or relationship with peers affects persuasion in settings without financial transactions or product purchases, our research focuses on whether the geographical distance between senders and receivers of online WOM messages is significantly related to eWOM effectiveness. Prior Research on How Geographical Distance Impacts eWOM EffectivenessMost relevant to our work are two studies examining whether geographical distance affects information diffusion and adoption. Specifically, [45] examine how geographic proximity, measured as contiguous relationships between states, and sociodemographic proximity, measured as similarity in demographics between states, affect overall message propagation at the state level. They find that geographic proximity has no impact when sociodemographic similarity is accounted for. Because of data limitations, their operationalization of geographic proximity may mask the actual effect of distance. Importantly, they conduct an aggregate state-level analysis, noting that the ""state-level nature of the data is a limitation"" (p. 249) and ""access to more disaggregated data would allow for a more granular analysis"" (p. 264). Their analysis also prevents them from observing whether consumers are familiar with the message propagator or the social distance between peers ([75]); familiarity and similarity between users as well as relationship strength have been shown to impact peer influence ([13]; [82]). Besides, diffusion may rely on a different mechanism than eWOM ([98]).[79] is also very relevant to our work. These authors first conduct a descriptive study in the offline world and find that living nearby adopters of a cellular service provider is associated with faster switching to that provider. Because they do not directly observe WOM episodes, they conduct their analysis at an aggregate level, where it is not possible to account for homophily (i.e., the tendency of individuals to choose friends with similar tastes and preferences) or attribute WOM influence among the ties of each user ([75], [76]). In addition, they measure geographical distance as the average distance among all possible ties of each consumer.[ 6] These limitations motivate them to conduct scenario-based experiments, in which they investigate the mediating role of perceived homophily and show that geographic proximity to an ambiguous online reviewer increases the likelihood of following the reviewer's opinions, as geographic proximity is used as a cue for perceived similarity. However, the effects of geography are likely to be different in such lab settings due to the lack of familiarity and social ties with online reviewers, which play an important role in persuasion, as the extant literature shows ([13]; [82]; [86]). In addition, while consumers may rely on any available cues to address these concerns of ambiguity and lack of familiarity, lab settings often provide few such cues beyond geography to inform their decisions ([82]). Thus, it remains unclear whether geographic proximity can still play a role in facilitating eWOM influence in real-world settings, where users are familiar with and socially connected to each other and have access to an abundance of available cues.In summary, we empirically investigate whether the geographical distance between individual pairs of familiar disseminators and receivers of organic eWOM in social media plays an important role in driving recipients' subsequent purchase behaviors of physical goods. Table 1 outlines the related studies and additional important differences of our research.GraphTable 1. Overview of Related Studies on the Role of Geography in eWOM. Study (Chronological Order)Research FocusContextSettingWOM TypeSocial TiesFamiliarity with SenderGeographical Distance MeasureAnalysis LevelNotesMain FindingsNaylor, Lamberton, and Norton (2011)How ambiguous reviewers affect persuasionScenario-basedOnline reviewsIndirect; active; artificialNoneNoBinary (same vs. different city)Choice instanceSame vs. different city domicile is one of many reviewer variables manipulated to match that of a lab participant.Consumers assume that reviewers with no identifying information have similar tastes as them, so they are similarly persuaded as they are by reviewers who appear similar to them and more persuaded than by reviewers who appear dissimilar.Aral and Walker (2014)How tie strength and embeddedness affect app adoptionFree Facebook app adoptionSocial media platform (Facebook)Indirect; passive (automated notifications without variation)ObservedFamiliarity with the subject (not sender)Binary (same vs. different town)Notification episodeThe authors note that they ""estimate how relationship-level covariates [same town] are correlated with the extent or impact of influence"" (p. 1363), and the ""results may have limited generalizability to ... cases where there is a significant financial cost to adopting a product"" (p. 1366).Consumers will adopt a free Facebook (movie review) app more quickly the more recent their relationship is and the more embedded they are with an existing user.Packard, Gershoff, and Wooten (2016)How boastful WOM affects persuasionScenario-basedOnline reviewsIndirect; active; artificialNoneNoBinary (nearby vs. distant location)Choice instanceNearby vs. distant location is one of several variables used to manipulate how similar the reviewer appears to a lab participant.Consumers are less persuaded by boasters' WOM when trust cues are low.Chen, Van der Lans, and Phan (2017)How relationship characteristics in a social network affect diffusionMicrofinance program adoptionPotential offline WOM episodeUnobservedSurveysYesNoneAggregateWOM occurs in geographically bounded networks (e.g., village, university), so geographical distance is not material.Social (economical) relationships are the most (least) important drivers of adoption for a microfinance program.Message propagationUniversity network platformDirect; activeObservedYesNoneWOM episodeRelationship intensity (e.g., message volume) is the most important driver of information diffusion in a network.Fossen, Andrews, and Schweidel (2017)How social vs. geographic proximity affect diffusionMessage propagationSocial media platform (Twitter)Direct; activeUnobservedUnobservedBinary (contiguous vs. noncontiguous state)AggregateThe authors note that ""the state-level nature of the data is a limitation"" (p. 249). The study does not control for advertising or other marketing activities and focuses on diffusion.Sociodemographic similarity propagates online message spread. Geographic proximity has no effect when accounting for sociodemographic similarity.Meyners et al. (2017)How geographic proximity affects adoptionCellular service provider adoptionPotential offline WOM episodeUnobservedObserved (aggregate)Yes (aggregate)Average distance from all peer adoptersAggregateThe authors note their field study ""did not have information on either the valence of the signals from the social network or the location of nonadopters"" (p. 63), averages distance across adopters, does not observe WOM, does not control for network externalities, and studies switch behavior. The lab studies have no homophily cues beyond age and gender, and nofamiliarity with or social ties to reviewers.Geographic proximity to adopters of a cellular provider is associated with faster switching to the provider.Scenario-basedOnline reviewsIndirect; active; artificialNoneNoContinuous (miles), categorical (state)Choice instanceGeographic proximity to an ambiguous reviewer increases the likelihood of following the reviewer's recommendation due to perceived homophily.Present studyHow geographical distance affects eWOM effectivenessProduct purchases of significant financial costSocial media platform (Twitter)Direct; activeObservedYesContinuous (miles)WOM episodeOur study has information on nonadopters and social ties; controls for homophily, advertising, and message content; and studies actual product purchases of significant financial cost in a social network where receivers are familiar with the WOM sender.Geographical distance is negatively associated with the likelihood that eWOM influences purchases, above and beyond other effects.  Empirical Setting and Data DescriptionOur empirical setting concerns a large-scale venture of American Express (the service provider) on the microblogging social media platform Twitter. This collaboration introduced a new purchasing service that was seamlessly integrated for two months into the social media platform (Twitter) to leverage users' connections to stimulate eWOM. Specifically, the service enabled consumers to make purchases on the social media platform while simultaneously spreading the word about their purchases to their social media peers (receivers).We further discuss this novel service by describing the data-generating process. In particular, the service provider first posts a short message (tweet) on the platform broadcasting the list of participating merchants and the corresponding products available for purchase. This announcement includes information about the product offerings (e.g., product, respective sale price) and the designated hashtags (i.e., a phrase or word preceded by a hash sign [#]) consumers must use to make a purchase. Consumers must have a microblogging account and sync their service provider account with their microblogging account. Once the service provider broadcasts the products available for purchase, users can purchase these products by posting a tweet that includes the designated hashtag. In addition to the necessary hashtag, consumers can choose to personalize the purchasing tweets that are (automatically) shared with their social media peers. Typically, such messages are posted on the users' social media profiles, and their followers (i.e., those subscribed to their timeline) automatically receive these messages on their own (home) newsfeeds (for an explanation of how we use the variation in message visibility in our main identification strategy, see the ""Econometric Model Identification"" subsection and Figure A1 in the Web Appendix). The social commerce provider tracks the tweets containing the designated hashtag and pairs them with the corresponding product. After the purchase confirmation, the service provider bills the users and ships the product. Empirical DataOur data set includes all the confirmed transactions generated through this purchasing process. Specifically, each transaction in our data set contains the message ID of the purchasing message, the message content, the designated product hashtag, the date and time the message was posted, the corresponding user ID, and whether the message was rendered visible or invisible to each of the user's followers (i.e., included or not included in the follower's newsfeed).For each user, our data set also contains the screen name of the user on the social media platform, when they joined the platform, the set of the user's followers and followees, all the messages users have posted on the platform since they joined, the geolocation of the user, and the self-reported description of the user's profile. Our data set also contains the same information for users who chose not to make a purchase.In addition, our data set contains information about the product offerings. The service provider collaborated with known retailers and offered various products for purchase (e.g., see Figure A2 in the Web Appendix). In particular, the products involve video game consoles and related accessories, electronics and sports equipment (e.g., high-definition tablets, sports and action cameras), general-purpose gift cards, and fashion accessories (e.g., designer bracelets, handbags). These offerings from the social commerce service provider were available for purchase at a reduced price only through the platform (about a 25% discount, yielding an average retail price of $125).Overall, our data set tracks the corresponding purchasing decisions of 132,995 social media users on Twitter with 1.4% of the users purchasing available products. The users are located across the continental United States, as illustrated in Figure 1, and on average follow 996 Twitter users and have 342 followers. Table 2 presents the summary statistics and description of the main variables and Figure 2 shows the corresponding correlations.Graph: Figure 1. Geolocations of users.Graph: Figure 2. Correlation levels among the main variables of interest.GraphTable 2. Descriptive Statistics VariableDescriptionMean/MedianSDMinMaxPurchaseWhether the recipient of the message made a purchase.014.1201Visible messageWhether the message was visible to the recipient.77.4201Geographical distanceGeographical distance between sender and recipient971.08a894.9005,585Number of followersNumber of followers of user342a101,0000376,000Number of followeesNumber of followees of user996a12,6290115,000Reciprocal relationshipWhether the relationship between the users is reciprocal.08.2701Number of interactionsNumber of interactions between users.266.1001,612Sentiment of messageIntensity of message advocacy.21.35−11Personalized messageWhether the message was personalized by the sender.82.3801 1 aWe report the median instead of the mean value.2 Notes: The values of the variables Number of followers, Number of followees, Reciprocal relationship, and Number of interactions correspond to the time of posting the WOM message.We enhanced our data set with a proprietary data set from the ad intelligence company Kantar Media, which includes the local (and national) advertising expenditures of each brand and for each product. We also supplemented our data set with additional information from the American Community Survey five-year estimates of the U.S. Census Bureau regarding local demographics. Empirical MethodologyWe model users' purchase decisions as a function of eWOM message, sender, and recipient as well as relationship characteristics, including observed and latent homophily controls; homophily creates a natural correlation in behaviors that could be incorrectly interpreted as a causal effect (e.g., [10]; [76]). To further control for any unobserved confounds, we use in our research design the variation in the visibility of eWOM messages. Econometric Model SpecificationConsistent with prior literature (e.g., [79]), we use a continuous-time single-failure survival model. In particular, we model how quickly users purchase a product, if any, using a Cox ([39]) proportional hazard model and correcting for censoring of transactions that might have been intended to occur after the observation window ([64]). Specifically, our main estimation equation for the decision of peer i (eWOM recipient) is as follows: λi(t)=λ0(t)exp(VisiblemessageijβM+GeographicaldistanceijβD+Visiblemessageij×GeographicaldistanceijβDM+XijβX+MjβM+DjβDS+RiβR), Graph( 1)where  λi(t)  is the hazard of peer (follower)  i  of consumer  j  to purchase the same product as consumer  j after[ 7] an eWOM message from  j,λ0(t)  represents the baseline hazard,  Visiblemessageij  captures whether the eWOM message of consumer j (sender) was rendered visible to (i.e., included in the home newsfeed of) peer  i  (recipient), and  Geographicaldistanceij  measures the physical distance of i from consumer j in (log) miles. The coefficient of interest is  βDM  and captures the relationship between geographical distance and the effectiveness of eWOM, while the coefficient  βD  accounts for spatially and nonspatially correlated user behaviors and preferences (e.g., homophily) that would have otherwise manifested as the association between geographical distance and the effectiveness of WOM; put simply, our research design leverages the variation in the visibility of eWOM messages, as influence can occur if and only if the eWOM message is rendered visible, whereas correlated user behaviors and preferences are extant even when the eWOM message is nonvisible. We also control for user-relationship (sender–recipient dyad) characteristics, X; message characteristics, M; disseminator characteristics, D; and recipient characteristics, R; as well as product fixed effects and geography (  statei  ) and time (day of message) fixed effects. Model features and machine-learning methodsTo construct the aforementioned user-relationship, message, and disseminator and recipient controls, we employed machine-learning techniques to leverage the vast amount of unstructured and structured data.More specifically, the user-relationship (sender–recipient dyad) characteristics,  X  , include controls for observed and latent pairwise user similarity and tie strength. This set of variables—in addition to the research design—allows us to capture the correlation in latent tastes and preferences between WOM message disseminators and recipients to better distinguish the relationship of interest. In particular, the user similarity between the disseminator and the recipient of a WOM message is measured based on ( 1) the similarity of topics discussed in social media posts using the results of a machine-learning model for natural language processing (NLP), as well as the overlap of the local communities as captured by the ( 2) Jaccard similarity coefficient of followers ([40]) and ( 3) Jaccard similarity coefficient of followees ([40]).The NLP model we employ for this task is the latent Dirichlet allocation (LDA) model ([21]); LDA is a probabilistic generative NLP model that we use to model the user-generated content of each user in our data set (a document in our corpus) as a distribution over topics and every topic as a distribution over words in the English dictionary. We build this model on the corpus of all the messages of the users in our data set as the topics users discuss online reflect their latent interests ([113]); using the complete corpus instead of only the user messages during the observation window improves the inference of the NLP model. In particular, for the implementation of the LDA model, we used 139,850,033 messages in total. We also use a part-of-speech tagger/tokenizer developed specifically for Twitter ([84]) for more accurate tokenization, the removal of stopwords, symbols, typos and uninterpretable words ([97]), and the creation of ngrams, while retaining online-specific textual features (i.e., hashtags, at-mentions, and emoticons) ([92]; [110]). We also use [58] online variational Bayes algorithm to efficiently estimate the LDA model on our corpus. In addition, instead of arbitrarily determining the value of the LDA parameter corresponding to the number of latent topics, we find the natural number of topics present in our corpus using the procedure and measure proposed by [15], evaluated in terms of the Kullback–Leibler ([66]) divergence measure; nonetheless, the findings are not sensitive to the number of topics. Finally, regarding the hyperparameters of our model, we learn an asymmetric prior directly from our data. Beyond capturing the disseminator–recipient similarity with these metrics, we alternatively measure the similarity as a single standardized factor, based on the principal factors method, to avoid any potential multicollinearity; we present both sets of results.In addition, our model specifications include constructs capturing whether the user relationship is reciprocal and (the log of) the number of interactions between the two users ([32]). Finally, we also control for the sociodemographic distance of the users—based on the difference in average age and percentages of male, Black or African American, Hispanic, and Asian-origin residents in the locations of the disseminator and recipient of the message using Census data at the zip code level ([45])—as well as the time zone difference. Overall, the various metrics of user pair similarity capture both observed similarity (e.g., the number of user interactions) and latent similarity (e.g., common latent interests) to further control for potentially unobserved confounds and homophily.The message characteristics,  M  , capture the sentiment of the message (i.e., intensity of WOM advocacy) as well as whether the message was personalized (i.e., explicit rather than implicit advocacy). The sentiment of the message (measured on a continuous scale between −1 and +1) provides a richer metric of the advocacy intensity of the sender compared with other simple metrics, such as lexicon-based scores. The main method we employed uses a publicly available commercial sentiment analysis mechanism based on deep learning ([ 9]). Nonetheless, the results are robust to employing alternative machine learning methods for sentiment analysis.[ 8] Moreover, the message controls also include whether a user account handle is mentioned in the message and whether the sender started the message with a period as the first character; starting a message with a period as the first character affects the visibility of the message and is a common norm among Twitter users when they want to explicitly make a message visible to all users and not only the account mentioned in the message.[ 9] Finally, we also control for advertising expenditures of each brand during our observation period in the local region of the eWOM message recipient expressed in (logarithm of thousands of) U.S. dollars.The disseminator and recipient characteristics,  D  and  R  , include the user (opinion) leadership and expertise measures following the extant literature. We capture these disseminator and recipient characteristics to further control for factors that could bias the true relationship between geographical distance and WOM effectiveness due to omitted individual characteristics and preferences. The user expertise levels are measured on the basis of the standardized similarity of the timeline of a user with the corresponding timelines of the participating vendor and product employing the probabilistic NLP machine-learning model we previously described ([23]; [80]). The motivation for this measure is, for instance, that users who frequently tweet about technological trends and topics similar to those in the social media accounts of the specific product and the corresponding vendor are more likely to be perceived by their social media peers as experts in the area of technological products ([61]). In addition, we further control for correlated user preferences and interests ([26]; [72]) by including in the econometric specifications the latent interests based on the aforementioned LDA model ([ 1]). The user (opinion) leadership is measured based on the additive smoothed ratio ([74]) of followers to followees of the user. The additive smoothed ratio is frequently used in empirical studies to prevent this metric from being oversensitive to small-scale changes in the numbers of followees or followers ([ 1]; [41]). Furthermore, we also control for the number of followers of each user, whether the user has a default profile on the platform, and how many months have elapsed since the user joined the social media platform. Finally, we also control for the age, gender, and income of the users based on the Census data ([45]). Econometric Model IdentificationWe next discuss the research design we use to further distinguish the effect of geographical distance on eWOM effectiveness from unobserved confounds and correlated user behaviors and preferences, such as homophily. Our research design is enabled by a unique feature of the platform that causes some WOM messages to be visible and other messages to be invisible (not included) on the newsfeeds of other peers, as described next.Typically, a message posted by a user on the Twitter platform appears in the newsfeeds of all her followers, as the newsfeeds were not algorithmically curated during this venture. Thus, in our context, whenever a user makes a purchase, her social media peers are exposed to her advocacy toward the product if the purchase is visible in their (home) newsfeeds and their purchasing decisions might, in turn, be affected through such WOM episodes; the specific offers were available for purchase only through Twitter. The research design framework we employ leverages information on whether a message broadcasted on the platform was indeed rendered visible in the newsfeeds of other users or not (see Web Appendix Figure A1).More specifically, on Twitter, a publicly broadcasted message may not be included in the newsfeeds of some followers in instances when a Twitter account handle (if any) appears in a specific position of the message (i.e., if any Twitter account user name following the ""@"" symbol appeared at the very beginning of the message). For instance, if a consumer purchases a product by posting the message ""#BuyActionCamPack @AmericanExpress"" (see Web Appendix Figure A2, Panel A), the message is visible on the newsfeeds of all her followers. However, if the same consumer purchases the same product by posting the message ""@AmericanExpress #BuyActionCamPack"" (see Web Appendix Figure A2, Panel B), then the message is visible only to the social media users who follow both this consumer and the mentioned account.Interestingly, due to the design of Twitter during our observation window, if a consumer initialized the purchase process by clicking on the announcements of the product offerings or a post of another consumer, the handle of the corresponding account (e.g., ""@AmericanExpress"") is automatically prepopulated in the purchasing message. Then, the consumer must click anywhere on the prepopulated field and manually write the purchasing message that includes the required designated hashtag. Notably, if the click happens to be recorded on the left of the prepopulated account handle, the cursor will be placed at the beginning of the field and the consumer can write the purchasing message starting at this position (see Web Appendix Figure A3, Panel A). Thus, if the consumer wants to complete the transaction, she is likely to write a message similar to the one in Web Appendix Figure A2, Panel A, starting immediately with her message, and the tweet will be rendered visible to all of her peers (recipients). Alternatively, if the click happens to be recorded on the right of the prepopulated account handle, the cursor will be placed at the end of the field and the consumer can write the purchasing message starting after the mentioned account handle (see Web Appendix Figure A3, Panel B). Then, if the consumer wants to complete the transaction, she is likely to write a message similar to the one in Web Appendix Figure A2, Panel B, starting immediately with an account handle, and the tweet will not be included in the newsfeeds of her peers (recipients) who do not follow the mentioned account.Thus, the visibility of each WOM message to the peers of a consumer depends on ( 1) whether an account is mentioned (e.g., @AmericanExpress or another account), ( 2) the exact position of the account handle (whether it is at the very beginning of the message or not), ( 3) what social media accounts the consumer's peers follow, ( 4) whether a user click will be recorded more to the left or more to the right of the screen, and ( 5) whether an account handle will be prepopulated by the platform. Thus, the visibility of the messages is affected by platform design factors (e.g., prepopulating accounts) rather than certain user characteristics exploited by a curation algorithm. Importantly, with regard to the visibility of eWOM messages, the recipient does not control the visibility of these messages from the users she follows; this is important because the purchase decisions we study are the recipients' decisions.Because our main identification strategy assumes that the visibility of eWOM messages and the geographical distance of consumers are not endogenous in this study, we also empirically investigate whether the visible and nonvisible message groups differ across the pretreatment variables in our model based on the normalized differences tests ([60]) that provide scale-invariant measures of the size of the differences. The normalized differences range from −.1768 to.1692, indicating that there are no significant differences between the two groups in observable characteristics, further enhancing the validity of the identification strategy; differences of.25 or less indicate a good balance between the two groups ([60]). Similarly, kernel distributions, quantile-quantile plots, and the orthogonality test ([60]) further confirm the validity. We also examine the geographical distance in the same way, reaching the same conclusion.Overall, these unique features of Twitter induce an important variation in the visibility of messages, enabling us to examine the behaviors of peers in treatment (i.e., visible message) and comparison (i.e., nonvisible message) groups in a potential outcomes framework. Thus, differences in purchases between treatment and control groups can be attributed to the corresponding WOM messages and their characteristics, addressing the issue of correlated user behaviors and preferences. In this respect, our research is also relevant to the stream of work that has leveraged the variation in the visibility of advertising messages to estimate the effect of online ads ([ 1]; [48]; [46]).We further enhance our identification strategy using only observations corresponding to dyadic relationships and social media peers who did not receive messages (either visible or invisible) from multiple disseminators ([ 1]; [12]). In addition to taking advantage of this nonintrusive research design, we also avoid any observer biases; the subjects are unaware of being part of the study and, thus, do not alter their behavior in anticipation of the study. Nevertheless, we also control for differences in the pairwise relationships between users by employing an extensive set of variables and fixed effects in our data-rich setting. Table A1 in the Web Appendix presents additional identification strategies. Empirical Results Main ResultsTable 3 presents the results of the main econometric specifications of the eWOM effectiveness model. In particular, Model 1 constitutes our baseline specification as it models eWOM effectiveness based on the constructs of dyadic similarity and relationship strength between the disseminator and recipient of the eWOM message (i.e., pairwise user similarity, reciprocity of users' relationship, and number of user interactions), eWOM message advocacy (i.e., personalized message and sentiment of message), and user characteristics (i.e., expertise and leadership). Then, Model 2 introduces the notion of geographical distance, and Model 3 adds the information of eWOM message visibility and leverages the corresponding variation to further distinguish the relationship between geographical distance and eWOM effectiveness  (βDM)  from correlated behaviors and homophily among users. That is, Model 3 disentangles the relationship between geographical distance and eWOM effectiveness from the correlational effect of geographical distance reported by Model 2; the eWOM influence is transmitted through visible messages only, whereas correlation in user behaviors and preferences (i.e., homophily) is present even with invisible messages. Then, Model 4 further controls for whether a user account was mentioned in the eWOM message and if the message was explicitly made visible while introducing additional disseminator and recipient controls (i.e., number of followers, leadership, default profile, time on the platform, interests, age, gender, and income) and other controls (i.e., the difference in average age and percentage of male, Black or African American, Hispanic, and Asian-origin residents in the locations of the disseminator and recipient of the message, the log of brand advertising expenditures in USD (in 1,000s) in the location of the recipient of the message, and the time zone difference between the locations of the disseminator and recipient of the message). Finally, Model 5 controls for the specific product mentioned in the eWOM message as well as state and day fixed effects.GraphTable 3. Estimation Results of eWOM Effectiveness Model. Model 1Model 2Model 3Model 4Model 5User similarity1.244***1.240***1.239***1.257***1.264***(.009)(.009)(.009)(.010)(.011)Reciprocal relationship5.868***5.378***5.403***4.829***4.527***(.317)(.290)(.291)(.269)(.254)Number of interactions1.015.979.977.987.980(.034)(.033)(.033)(.034)(.035)Sentiment of message1.540***1.614***1.557***1.728***1.556***(.107)(.112)(.111)(.133)(.123)Personalized message1.041***1.046***1.047***1.053***1.041***(.003)(.003)(.004)(.005)(.007)User expertise1.204***1.183***1.184***1.595***1.191**(.030)(.029)(.029)(.130)(.100)User leadership1.006***1.006***1.005***1.011***.998(.001)(.001)(.001)(.002)(.002)Geographical distance.891***.929***.946***.954**(.007)(.017)(.020)(.020)Visible message1.451***1.598***1.608***(.171)(.193)(.188)Visible message × Geographical distance.951**.954**.945***(.019)(.020)(.020)Additional user controlsNoNoNoYesYesAdditional message controlsNoNoNoYesYesAdditional controlsNoNoNoYesYesProduct fixed effectsNoNoNoNoYesGeography fixed effectsNoNoNoNoYesTime fixed effectsNoNoNoNoYesLog-likelihood−22,422.8−22,312.8−22,307.4−22,035.2−21,763.6χ22,763.5−2,983.42,994.23,538.64,081.8N132,955132,955132,955132,955132,955 3 *p < .1.4 **p < .05.5 ***p < .01.6 Notes: eWOM effectiveness analysis. The hazard ratios (HRs) represent the percent increase (HR > 1) or decrease (HR <1) in postpurchase hazard associated with each attribute.According to the results presented in Table 3, we find that the coefficient of the variable capturing the relationship between geographical distance and WOM effectiveness is negative and statistically significant (Visible message × Geographical distance:.945, p < .01, Model 5); all reported coefficients correspond to hazard ratios (HRs) representing the increase (HR > 1) or decrease (HR < 1) in purchase likelihood associated with each attribute. (Table A2 in the Web Appendix also shows the coefficients of the control variables.) Beyond the effect of interest, we also find a negative and statistically significant spatial homophily–based effect of geographical distance (Geographical distance:.954) as well as a positive and statistically significant effect of similarity (homophily). Interestingly, these findings show that despite the ""death of distance"" postulated in the literature ([28]), geographical distance is negatively associated with the effectiveness of eWOM. Thus, our research is the first to establish that geographical distance has a negative relationship with eWOM outcomes even among familiar social media peers, in addition to the previously known homophily-based effect of geographical distance.Moreover, the coefficients of all the other variables are in accordance with what one would expect as well as the extant literature on WOM (e.g., [13]; [20]; [45]). Specifically, we find that the increased user similarity and strength of relationship between users (User similarity: 1.264; Reciprocal relationship: 4.527) as well as more intense WOM advocacy (Sentiment of message: 1.556; Personalized message: 1.041) are associated with higher levels of purchase likelihood after exposure to WOM (Visible message: 1.608). Similarly, users with higher product expertise (User expertise: 1.191) seem more persuasive; thus, their followers are associated with a higher purchase likelihood after being exposed to their advocacy. Economic Significance and Managerial RelevanceThe relationship of interest is also of economic significance, as a decrease of 10 miles in the distance between users accentuates the effectiveness of eWOM by 12.78% based on the aforementioned coefficients; similarly, an increase of 100 miles in the distance corresponds to a decrease of 25.56%, and an increase of 1,000 miles corresponds to a decrease of 38.34%. For instance, for a recipient living in New York City, the relationship is reduced by 24.39% when the eWOM message originates from a sender in Philadelphia, and 38.82% from Miami, compared with the same message from a sender in New York City.We further assess the economic significance of the findings by measuring the out-of-sample performance of the models. Specifically, we use a holdout evaluation scheme with an 80/20 random split of data and evaluate the models in terms of Harrell's C concordance coefficient, which measures the likelihood of correctly ordering survival times for pairs of senders and recipients of eWOM messages; the concordance measure is similar to the Mann–Whitney–Wilcoxon test statistic as well as the area under the receiver operating characteristic curve. The results show that our model achieves a predictive performance of.840. Thus, it outperforms the baseline by a large margin, as the baseline performance corresponds to a value of.5. This statistically significant difference further illustrates the managerial relevance of the findings, as they can enhance seeding and targeting strategies ([57]; [99]).We further quantify the (dollar) value of this increase in out-of-sample performance ([89]). To conduct this calculation ([89]), we use estimates of the cost of targeting (e.g., promoting eWOM messages) and the average product price (Goldfarb and Tucker 2011); the cost of this type of targeting on Twitter is estimated to be $1.35 based on [111], while the average product price in our data set is $125. Combining these data reveals that our model suggests a profit of $.85 per targeted user, which corresponds to a 9% increase over the baseline of not using the information of geographical distance (i.e., $.78), while for random targeting the corresponding profit is only $.008. Potential MechanismOur findings are surprising, as in such an empirical setting the products are not location-specific, there are no transportation fees for consumers, and there is no contracting or potential conflict or ambiguity between senders and recipients of WOM messages ([59]; [79]). Thus, to fully understand our findings, we delve into a likely underlying mechanism of the identified effect and conduct additional analyses that allow us to assess the likelihood of this potential mechanism.We hypothesize that the negative relationship between geographical distance and eWOM effectiveness could be due to the identification processes of social media users. Specifically, a user who resides near the sender of the message is likely to share a common social identity with the sender based on their geographic proximity ([44]; [62]; [83]; [107]) and thus might be more susceptible to WOM influence originating from this (local) sender.[10] Conversely, a recipient who resides farther away from the disseminator of the WOM message is not likely to share the same location-based social identity and thus is less likely to be persuaded ([37]; [38]; [96]) by mere exposure to eWOM advocacy. Location-based social identity activationTo empirically assess this potential underlying mechanism, we first examine the moderating effect of the salience of the geographical distance from the source of WOM. Salience is activating common social identity identification ([44]; [55]; [56]; [68]; [77]; [106]), and thus, we empirically test the likely underlying mechanism by examining the moderating effect of the salience of the sender's location on the impact of geographical distance on the effectiveness of WOM. If the relationship is accentuated when the geographic proximity of the source of the WOM message is more salient, this would provide empirical support for the hypothesized mechanism of common social identity, as the salience of the location—and thus the salience of the geographic proximity—enhances the social identification processes of the recipient ([44]; [55]; [56]; [68]; [77]; [106]). This would also provide additional empirical evidence in favor of the main identification strategy. Alternatively, if more salient location cues attenuate the relationship, this would provide evidence against the hypothesized mechanism.According to the results presented in Table 4, we find a negative and significant moderating effect of the salience of the sender's location on the impact of geographical distance on the effectiveness of WOM; the salience of location variable corresponds to whether the location of the WOM sender is explicitly mentioned in her profile. That is, the relationship between geographic distance and eWOM effectiveness is even more negative when the distance is more salient. This finding indicates that common social identity is a likely mechanism for the identified relationship. The results are robust to alternative econometric specifications.[11]GraphTable 4. Estimation Results of eWOM Effectiveness Model with the Moderating Effect of the Salience of Geographical Distance. Model 1Model 2Model 3Model 4Model 5User similarity1.244***1.240***1.238***1.256***1.260***(.009)(.009)(.009)(.010)(.010)Reciprocal relationship5.868***5.378***5.525***4.881***4.606***(.317)(.290)(.298)(.272)(.258)Number of interactions1.015.979.976.989.969(.034)(.033)(.033)(.034)(.035)Sentiment of message1.540***1.614***1.642***1.781***1.568***(.107)(.112)(.118)(.138)(.124)Personalized message1.041***1.046***1.043***1.050***1.035***(.003)(.003)(.004)(.005)(.008)User expertise1.204***1.183***1.156***1.581***1.172*(.030)(.029)(.028)(.129)(.099)User leadership1.006***1.006***1.004***1.010***.998(.001)(.001)(.001)(.002)(.002)Geographical distance.891***.947***.961*.965*(.007)(.019)(.021)(.021)Visible message1.430***1.588***1.783***(.168)(.192)(.214)Visible message × Geographical distance.949**.950**.918***(.021)(.021)(.020)Visible message × Geographical distance × Salience of location.920***.948***.974*(.011)(.012)(.013)Additional user controlsNoNoNoYesYesAdditional message controlsNoNoNoYesYesAdditional controlsNoNoNoYesYesProduct fixed effectsNoNoNoNoYesGeography fixed effectsNoNoNoNoYesTime fixed effectsNoNoNoNoYesLog-likelihood−22,422.8−22,312.8−22,280.2−22,024.7−21,751.1χ22,763.52,983.43,048.53,559.64,106.8N132,955132,955132,955132,955132,955 7 *p < .1.8 **p < .05.9 ***p < .01.10 Notes: eWOM effectiveness analysis with the moderating effect of salience of geographical distance. The salience of location variable corresponds to whether the location of the disseminator is explicitly mentioned in the profile of the disseminator. Additional table notes as in Table 3. Location-based social identity prominenceWe also examine the likelihood of the hypothesized mechanism in additional ways. For instance, we examine the effectiveness of eWOM under conditions that strengthen the role of geographical location in the social identification process. Specifically, increased political homogeneity in the local area of the recipient of the eWOM message is likely to enhance the importance of the location-based social identity of the recipient as it increases the salience and significance of individuals' social identity due to political entities operating at geographic levels (e.g., precinct, county, state) and the characteristics of the local information environment (e.g., increased number of times an individual is reminded of the local identity, positive perceptions of the local community) ([62]; [91]; [101]). As a result, a pronounced location-based social identity of the recipient is likely to engender biases based on geographical distance, accentuating the relationship. Thus, if the relationship is accentuated when the political homogeneity in the local area of the recipient of the WOM increases, this would provide empirical support for the potential mechanism of social identification. Conversely, the opposite would provide empirical support against this potential mechanism.Based on the results in Table A3 in the Web Appendix, we find a negative and significant moderating role of the political homogeneity in the local area of the recipient; we have collected data from the MIT Election Lab (https://electionlab.mit.edu) on political voting patterns at the precinct level for the 2016 elections and measure political homogeneity on the basis of the percentage of voters that would need to switch from the majority party to the minority party for the two parties to have equal votes. Put simply, the negative relationship between geographical distance and eWOM effectiveness is amplified when location-based social identity might be more pronounced due to increased political homogeneity. This finding lends support to the hypothesized mechanism of social identity. The results are also robust to alternative specifications.Similarly, we also examine the moderating role of exogenous hardships in the local area of the recipient of the WOM message. If there have been significant local community hardships or natural disasters, then geography-based common social identity is likely to be more prominent for the residents of the affected area ([109]). Thus, if the relationship is accentuated when the geographic proximity of the source of the WOM message is combined with local community hardships for the recipient, this would provide additional support for the potential mechanism of common identity; we measure local community hardships using (exogenous) deaths related to extreme weather events in the location of the recipient of the message during the last five years prior to our observation window based on data from the National Oceanic and Atmospheric Administration. According to the results in Web Appendix Table A4, we find that the relationship between geographical distance and the effectiveness of eWOM is even more negative for users for whom location-based social identity is likely more pronounced due to local community hardships, lending empirical support to the hypothesized potential mechanism.[12] Location-based social identity versus other peer effectsWe also examine whether the estimated moderating role of geographical distance is above and beyond other potential peer effects. Table A5 in the Web Appendix presents the corresponding results controlling for both the interaction between user similarity and visible message and the interaction between sociodemographic distance and visible message. Our findings remain highly robust, further illustrating that the estimated moderating effect of geographical distance is above and beyond other peer effects, including actual and perceived homophily. The results are also robust to alternative specifications, such as including additional interactions. Ruling Out Additional Alternative ExplanationsIn addition to the aforementioned evidence and identification strategies, we assess various alternative explanations. Table 5 presents an overview of these, with the main ones discussed next and additional ones discussed in Web Appendix B.GraphTable 5. Alternative Explanations Alternative ExplanationRationaleIdentification StrategyTable(s)Location-RelatedLocation characteristicsLocation-based characteristics may affect purchase likelihoodsMain identification strategy including geography fixed effectsAllSpatially correlated user preferencesUser interests and brand preferences may be spatially correlatedMain identification strategy including latent user interestsAllAdditional brand preference controlsA7Local time-difference effectsTime zone differences are correlated with geographical distance and may relate to differences in users' activities or moodsMain identification strategy including time zone differencesAllLocal weather conditionsLocal weather conditions may affect consumers' activities and moodsAdditional weather controlsA9Small-city effectsGeographic distances are shorter in smaller and more remote locations, where the demand for products sold online might be higher due to potentially limited availability of other productsMain identification strategy including geography fixed effectsAllExclusion of small and remote locationsA8Local marketing effectsLocal marketing effects may affect purchase likelihoodsMain identification strategy including ad controlsAllAdditional ad response controlsA6User-RelatedHomophilyCorrelated behaviors among similar (across observed characteristics) peersMain identification strategy including user similarity controlsAllAdditional brand preferences similarityA7Additional user similarity controlsA11Propensity-score matchingA13Latent or unobserved homophilyCorrelated behaviors among similar (across latent or unobserved characteristics) peersMain identification strategy including user latent similarity controlsAllAdditional latent homophily controlsA10Additional user similarity controlsA11Latent variable modelA14Propensity-score matchingA13User characteristicsUser characteristics may affect purchase likelihoodsMain identification strategy including user controlsAllLocal demographic effectsSociodemographic distance between users may affect purchase likelihoodsMain identification strategy including sociodemographic controlsAllIncome-level effectsIncome levels may affect where users select to live; as such, geographical distance may be correlated with income levelsMain identification strategy including income controlsAllMessage-RelatedMessage contentMessage content characteristics may affect purchase likelihoodsMain identification strategy including message controlsAllPropensity-score matchingA13Nonrandom message visibilityMessage visibility may not be random, despite provided evidenceStatistical tests—Main identification strategy including message visibility controlsAllPropensity-score matchingA13Covariate adjustmentA12Product-RelatedProduct characteristicsProduct characteristics may affect purchase likelihoodsMain identification strategy including product fixed effectsAllMarketing promotionsAdvertising or other marketing activities may affect purchase likelihoodsMain identification strategy including ad controlsAllAdditional ad response controlsA6Other-RelatedUnobserved effectsUnobserved effects correlated with geographical distance of visible eWOM messagesPropensity-score matchingA13Other unobserved time-varying effectsAny other unobserved effects that vary with time and are correlated with geographical distance of visible eWOM messagesLimited time-horizon—Main identification strategy including time fixed effectsAllGeographic distribution of tiesUser might have more geographically proximate than geographically distant tiesMain identification strategyAllModel idiosyncrasiesModel and model specification choices could potentially affect the resultsLogistic regressionA15Alternative specificationsAllSpurious effectsSpurious effects or other statistical artifacts""Placebo"" studiesA16  Local marketing promotion effectsOne may be concerned that the results might be driven by unpaid or organic marketing effects in the local region of the eWOM message recipient. To evaluate this, we supplement our data set with local web search trends for each product from Google Trends ([14]). Table A6 in the Web Appendix presents the corresponding results controlling for both local marketing expenditures and ad response (via search behaviors) of the local audience. The results remain robust, alleviating concerns that local marketing promotion activities drive the results. The results are also robust to alternative specifications, such as using national Google Trends and advertising expenditures or estimating separate models for each potential confound. Local user-preferences effectsWe also examine the robustness of the findings to alternative specifications, such as controlling for homophily based on the overlap in brands that each social media user follows on the platform. Table A7 in the Web Appendix presents the corresponding results. The results remain robust, further corroborating our findings. Small-city effectsAnother potential alternative explanation is that the results are driven by disseminator–recipient pairs located in small and remote locations as in such locations distances are in general shorter and demand for products sold online is higher due to the limited availability of other product alternatives ([21]; [79]). We assess this alternative mechanism by repeating the analysis excluding any observations that correspond to small and remote locations, as determined by the Census (i.e., locale assignments). Table A8 in the Web Appendix presents the results; the results remain robust. Local weather conditions effectsWe also assess the alternative explanation that the results are driven by the local weather conditions affecting the moods and activities of users ([46]; [67]). We assess this potential explanation by controlling for the temperature, precipitation, and sunshine levels in the location of the recipient using data from the National Oceanic and Atmospheric Administration. Web Appendix Table A9 presents the corresponding results; the results remain robust. Robustness Checks and Alternative Identification StrategiesWe also undertake an extensive set of tests to assess the robustness of the results and further strengthen the findings, as discussed next; see Table 5 and Web Appendix B for additional details. Extended econometric specificationsFirst, to enhance the employed identification strategy and examine the robustness of the findings, we further control for latent user characteristics by tapping into the social network structure and recent deep-learning advances. Specifically, we use the method of DeepWalk, a deep-learning method for graphs ([88]), to learn the latent representations of the users and their similarity and further account for both network structure roles and latent user homophily. Table A10 in the Web Appendix presents the corresponding results. The results corroborate our findings. The results also remain robust to employing alternative deep-learning methods, such as the node2vec method ([53]).We also repeat the analysis including multiple user similarity measures. In particular, the similarity measures correspond to the similarity levels between disseminators and recipients based on ( 1) the Jaccard coefficient of their followers, ( 2) the Jaccard coefficient of their followees, ( 3) the topics discussed in social media posts using the results of the LDA model, ( 4) the intrinsic brand and product preferences of the users based on the overlap in brands that each social media user follows on the platform, ( 5) the demographic information at the corresponding geographic locations (i.e., average age and percentages of male, Black or African American, Hispanic, and Asian-origin residents based on Census data), and ( 6) the latent characteristics of the users based on the deep-learning methods for representation learning; in addition to ( 7) the reciprocity of the relationship and ( 8) the number of interactions between the users. Table A11 in the Web Appendix presents the corresponding results; the results remain robust. Alternative identification strategiesWe also examine additional alternative identification strategies to control for any potentially remaining differences between the visible and nonvisible messages; Table A1 in the Web Appendix presents a summary of the different identification strategies. First, we enhance our identification strategy following the covariate adjustment method of [60]. Table A12 in the Web Appendix presents the corresponding results. The results remain robust; the results are also robust to including additional covariate interactions.Moreover, as an alternative identification strategy, we combine propensity-score matching with the main research design. In particular, we model the propensity of each message to be rendered visible using all the variables that describe the users' relationship and the message characteristics as well as the geographical distance between the users.[13] We conduct the matching based on the propensity scores before estimating again the same econometric models (for additional details, refer to the corresponding table notes). For this robustness check, we use one-to-one matching with replacement and a caliper of.05, yielding a standardized mean (median) absolute difference of.009 (.007) across all the variables, which ensures that covariate balance has been successfully achieved ([18]); the density distributions of the propensity scores also indicate significant overlap and common support. As shown in Table A13 in the Web Appendix, the results remain robust. The results are also robust to nearest-neighbor matching with the generalized Mahalanobis distance.Furthermore, as an additional alternative identification strategy, we build latent variable models where the sender–recipient similarity is latent and measured based on the various similarity features. Web Appendix Table A14 shows the corresponding results. Model 1 corresponds to the aforementioned latent variable model, while Model 2 combines the latent variable model with propensity-score matching estimating the model over the matched sample. The results of all the aforementioned alternative models are highly consistent and further corroborate our findings.Finally, as an alternative strategy, to estimate the relationship between geographical distance and eWOM effectiveness, we also use a logit model ([105]) examining whether—rather than how quickly—a user purchases a product. As Web Appendix Table A15 shows, the results remain robust. Falsification testsWe supplement these robustness checks with falsification tests to further assess whether the previous models are picking up spurious effects. As shown in Web Appendix Table A16, the results indicate our findings are not a statistical artifact of the specifications.Overall, the findings remain highly robust to various alternative identification strategies, econometric specifications, robustness checks, and falsification tests. Figure 3 illustrates the corresponding estimated effects across the specifications.Graph: Figure 3. Hazard ratios (HRs) with 95% confidence intervals (whiskers) representing the percentage increase (HR > 1) or decrease (HR < 1) in postpurchase hazard across estimated models. Discussion and ImplicationsIn this study, we investigate the relationship between geographical distance and the effectiveness of eWOM. Specifically, we examine whether the geographical distance between familiar disseminators and receivers of eWOM messages plays an important role—beyond utilitarian reasons and proxying for consumer tastes—in driving recipients' subsequent purchase behaviors. Our results show that the relationship between eWOM and the likelihood that message recipients subsequently also make product purchases significantly strengthens as the spatial proximity between disseminators and receivers grows. Implications for TheoryOur findings help advance understanding of conditions that affect online WOM performance. Many of the characteristics previously shown to impact eWOM outcomes relate to the product, brand, or message ([73]; [85]). We contribute by illustrating the role of the important but often overlooked construct of geographical distance in eWOM effectiveness. In showing how geographical distance is still associated with the effectiveness of online WOM in the absence of geography-specific transaction costs between unambiguous users, we demonstrate how the social force field of geography can tether the potential of eWOM. That is, despite the promise of technology to reduce communication barriers and the proclaimed ""death of distance"" ([29]; [52]), we find that geographic constraints persist online in unexpected ways. Therefore, our results also help address the debate on whether and how geographical distance still matters online ([51]) by showing that it can shape the influence of eWOM.We also contribute to the theory of eWOM examining why geographical distance is associated with eWOM effectiveness. Specifically, we find evidence that social identification may explain why the influence of online WOM is negatively related to the distance between WOM message disseminators and receivers. That is, our results suggest consumers are susceptible to online information and cues related to social identification as they can, in turn, enhance message persuasiveness. Thus, information and cues relating to social identity can be agents of eWOM influence. Whereas much of the literature on the role of geographic distance in e-commerce and other online settings offers economically driven explanations for the impact of geography, our study proposes behavioral bias relating to social identification may be an underlying mechanism that drives the relationship between geographic distance and eWOM. This finding highlights the need for future research to study additional non–economically driven explanations that can induce such biases. Implications for PracticeOur findings have important implications for managers as well. For instance, a controversial argument in the industry is that solely characteristics of the disseminators catalyze the adoption of behaviors and products and thus much of marketing efforts to engineer WOM in social media focus on identifying such characteristics. However, our findings indicate that marketers should expand their focus to take into account the disseminator–recipient pairings and understand that factors pertaining to these pairs can be significantly related to the effectiveness of eWOM. In particular, our results suggest geographical distance matters in online WOM and,, thus marketers can readily take advantage of how geographical distance is associated with eWOM persuasion. Marketers may thus adopt data-driven strategies to selectively promote eWOM episodes according to the proximity of such episodes to each consumer, or to strategically engineer such episodes based on geolocation information. Interestingly, although research has begun to identify pairwise characteristics between senders and receivers that shape the influence of eWOM, such as tie strength and similarity across personality traits ([ 1]; [20]), many of these factors are not readily observable to managers who wish to capitalize on them. The distance between social media users, though, is more easily observable to managers.Beyond promoting or engineering geographically proximate eWOM episodes, marketers may also benefit from promoting and/or engineering episodes containing other social identity cues. The likely connection between eWOM outcomes and social identity suggests that firms may also consider other cues relevant to social identity formation to further boost the success of interpersonal communications and WOM messages, as enhancing social identification may significantly increase message persuasion and user engagement in the online world.Our findings also have important implications for the effective design of viral marketing campaigns and ad content. Specifically, brands may boost the persuasiveness of their marketing campaigns by infusing into their content local cues or other identification triggers to induce consumers' social identification processes. Relatedly, marketers are beginning to leverage users' connections on social networks to develop and deliver marketing communications as part of their social advertising efforts. Our research suggests that they could further improve the effectiveness of these strategies by selecting geographically proximate connections to their targets.Furthermore, going beyond advertising strategies, the implications of our work also provide actionable guidelines for optimizing the delivery of digital content. In particular, our findings can help platforms increase the effectiveness of their content curation and ranking algorithms by incorporating information on content location or source origin and by factoring geolocation into their determination of which user-generated content to disseminate. For instance, content generated by spatially proximate consumers may draw more attention due to identification processes and thereby increase the effectiveness of content provision. In a similar vein, social media platforms may also consider incorporating location information in other functions. For instance, social media platforms may incorporate such information into various other machine-learning algorithms, such as their whom-to-follow recommendations. In addition, our findings could also be used by marketers and platforms to better predict the diffusion of information, products, and user behaviors in social media ([ 5]).Lastly, deepening our understanding of the factors that can attenuate or accentuate the effectiveness of eWOM has important implications that extend to public policies. For instance, revealing how geographic proximity is positively associated with eWOM effectiveness is critical for the development of effective public policies to induce positive behavioral changes, such as voter turnout, civic engagement, and public health actions. Limitations and Future ResearchWhile our work makes important strides in understanding how geographic proximity is related to eWOM effectiveness, we acknowledge certain limitations, which mostly stem from data availability issues. For instance, we examine the relationship between geographical distance and eWOM in a single social media platform because the service provider launched this venture on only one platform. Future research could examine whether the observed relationship manifests differently on other platforms. Moreover, we did not manipulate the visibility of the messages on Twitter because the venture did not alter the functionality of the platform in any way; future research could consider directly manipulating the visibility of the messages. Similarly, we did not manipulate the geographical distance of users from their followers. In addition, while we capture actual purchases in our data, we do not capture other consumer behaviors that could indicate interest in the products, such as online searches, as this type of information was not available to us. It would be interesting for future research to further examine such potential effects. Future research could also further examine and validate the underlying mechanisms. While we provide evidence that social identification may account for the relationship, future work may conduct experiments to verify this. Lastly, we do not observe in our data private communications between individuals due to privacy reasons and ethical concerns. Nevertheless, we hope these limitations provide avenues for future research that can deepen understanding of the critical role geographic proximity plays in eWOM and other online settings. Supplemental Materialsj-pdf-1-jmx-10.1177_00222429211034414 - Supplemental material for Is Distance Really Dead in the Online World? The Moderating Role of Geographical Distance on the Effectiveness of Electronic Word of MouthSupplemental material, sj-pdf-1-jmx-10.1177_00222429211034414 for Is Distance Really Dead in the Online World? The Moderating Role of Geographical Distance on the Effectiveness of Electronic Word of Mouth by Vilma Todri, Panagiotis (Panos) Adamopoulos and Michelle Andrews in Journal of Marketing  "
12,"Is Distance Really Dead in the Online World? The Moderating Role of Geographical Distance on the Effectiveness of Electronic Word of Mouth The authors investigate how the geographical distance between online users is associated with electronic word-of-mouth (eWOM) effectiveness. Their research leverages variation in the visibility of eWOM messages on the social media platform of Twitter to address the issue of correlated user behaviors and preferences. The study shows that the likelihood that followers who are exposed to users' WOM subsequently make purchases increases with followers' geographic proximity to the users. The authors propose social identification as a potential mechanism for why geographical distance still matters online in eWOM: because consumers may form a sense of social identity based on their physical location, information regarding the spatial proximity of users could trigger online social identification with others. The findings are robust to alternative methods and specifications, such as further controlling for latent user homophily by incorporating user characteristics and embeddings based on advanced machine-learning and deep-learning models and a corpus of 140 million messages. The authors also rule out several alternative explanations. The findings have important implications for platform design, content curation, and seeding and targeting strategies.Keywords: electronic word of mouth; social media; geographical distance; deep learningElectronic word of mouth (eWOM) plays a key role in shaping consumer attitudes and behaviors. More consumers digitally share information, research what others say about products and services, and rely on eWOM to gain knowledge or make decisions than ever before ([25]; [81]). While eWOM is becoming one of the most influential sources of information among consumers, the effectiveness of traditional communication channels employed by marketers is declining ([93]; [102]; [104]). These contrasting trajectories have motivated marketers to harness the power of eWOM. To do so, they are paying more attention to their user base and the corresponding eWOM episodes ([ 3], [ 4]; [49]). At the same time, technological advancements and data opportunities in the online space enable marketers to access data such as user-generated content and user location ([ 6]; [112]). As the availability of location data in particular grows, marketers wonder whether location plays an important role in eWOM effectiveness. Some marketers believe that consumers may consider eWOM from farther distances as more widely accepted and universal and, thus, more valuable and informative ([42]). Others believe that eWOM from closer distances may be perceived as more relatable and relevant and, thus, more trustworthy ([19]; [95]). Yet other marketers suppose that distance does not matter online and, thus, does not influence how consumers value eWOM ([90]).The literature on the role of geographical distance in eWOM effectiveness is not decisive either. On the one hand, a common belief for the effectiveness of eWOM is that technology bridges the geographical distance between consumers. Information technology reduces communication costs by decoupling the interaction process from geographic constraints. For instance, mobile devices enable shopping and communication between consumers independent from location, while the internet allows instant access to message exchanges and marketplaces. These reduced communication and access costs have led some scholars to pronounce the ""death of distance"" ([29]) and ""end of geography"" ([52]).On the other hand, geographical distance may still play a role in online consumer behavior and WOM for several reasons ([51]). These include location-specific goods as well as economic costs related to shipping, contracting, monitoring, enforcement, travel, and inconvenience ([ 2]; [59]). Local user preferences and spatially correlated social ties as well as limited regional availability of alternative product options can also lead to commonalities in product adoption among nearby consumers (e.g., [21]; [72]; [79]). These reasons suggest that geography may still constrain the effectiveness of eWOM.Our research investigates whether geographical distance is significantly associated with the effect of eWOM, beyond the aforementioned explanations. Specifically, we ask whether the geographical distance between pairs of familiar disseminators and receivers of online WOM messages in social media plays a role in driving recipients' purchase behaviors or whether distance does not matter for eWOM beyond utilitarian reasons—such as transaction costs— and proxying for correlated user behaviors and preferences. We therefore examine this research question in an online setting where economic costs, such as those relating to contracting and travel, are not of concern.To investigate whether geographical distance is associated with the effectiveness of eWOM, we use rich data from the social media platform of Twitter, where users disseminated messages about their real-world product purchases ([103]). Our main identification strategy leverages variation in the visibility of these eWOM messages—enabled by a unique feature of the platform—causing some purchase messages to be visible and others to be invisible to followers ([ 1]). We compare the purchase behaviors of followers for whom users' purchase messages were included in their newsfeeds (i.e., the stream of tweets presented on the home screen of a user from accounts the user follows on Twitter) with the behaviors of followers for whom users' purchase messages were not included in their newsfeeds due to this design feature, while employing an extensive set of controls (discussed in the ""Econometric Model Identification"" subsection). We also use the salience of user locations on Twitter to provide evidence for the potential mechanism. In our empirical setting, users are familiar with each other and thus aware of the location of their peers. Their location is often salient, as users may highlight this information in their profiles; user profile information is observable in peers' timelines when hovering with the cursor over the profile picture, the username, or the person's name.We find that the relationship between eWOM and the likelihood that message recipients make a purchase strengthens as the geographical distance between disseminators and receivers decreases. This relationship is economically significant, managerially relevant, and robust to alternative methods and identification strategies. Social identity may explain why geographic proximity could increase the effectiveness of eWOM, beyond utilitarian reasons and proxying for correlated user behaviors, as consumers may form their social identities based in part on their geographic location ([44]; [83]; [107]; [108]). To provide evidence for this potential mechanism, we investigate, for instance, how the salience of geographic cues as well as conditions that strengthen the role of geographical location in the social identification process ([101]; [109]) further enhance the effectiveness of eWOM.Our findings of geographical distance variation in eWOM effectiveness have important implications for both theory and practice. We contribute to the online WOM influence studies by demonstrating that despite technology's promise, spatial distance continues to play an important role in a digital world ([ 9]; [71]). Specifically, we show how geographic proximity is significantly related to an increase in eWOM effectiveness. Geographic constraints thus tether the impact of electronic communications. This finding also contributes to the literature that shows how features of disseminators and receivers can drive eWOM outcomes ([20]; [49]) by identifying geographic proximity as a relational characteristic that can affect dyadic influence. Our work thus provides actionable strategies for boosting the effectiveness of online interpersonal communications. Our findings also imply that customizing seeding and targeting with more proximate connections or highlighting information and cues associated with social identity formation can increase the effectiveness of online advertising, product recommendations, social advertising, referral programs, and other marketing strategies and tools by leveraging local appeals and social identification. Related LiteratureSeveral studies demonstrate the importance of eWOM, documenting how it can be a major driver of consumer behaviors. For example, user opinions have been found to influence the consumption of movies ([34]; [70]), television shows ([47]; [71]), video games ([114]), and books ([33]; [69]). Whereas these studies examine the direct impact of eWOM on purchase decisions, the present research extends this line of inquiry by focusing on conditions under which the effect of online WOM may be attenuated or accentuated. Contextual Factors Affecting eWOMResearch has begun to investigate the contextual factors that impact the effectiveness of eWOM. These factors include characteristics of the product ([22]), brand ([73]), or WOM message itself ([85]). Scholars have also examined how certain features of disseminators (senders) and recipients shape the effect of eWOM. For instance, a sender's brand loyalty ([49]), expertise ([11]), and identity disclosure ([44]) can each affect the influence of WOM messages. Recipient characteristics such as the number of ties to adopters, demographics ([65]), and product experience ([87]) can also affect the influence of online communications. In addition, characteristics of the sender–recipient dyad, such as the tie strength ([13]; [20]), similarity across personality traits ([ 1]), and sociodemographic similarity ([45]), can also affect online WOM performance. We add to this stream of research by shedding light on an important factor that characterizes the pairwise relationship between senders and receivers of eWOM: the geographical distance between them. The Impact of Geographical Distance in Various Commercial ContextsGeographical distance has been shown to affect certain outcomes in multiple online and offline commerce settings. For example, geographical distance can affect online trade flows and volume due to costs related to shipping, contracting, monitoring, enforcement, travel, and inconvenience, as well as in cases of location-specific goods ([59]). Such transaction costs can engender a ""home bias"" that leads consumers to prefer transacting with nearby others ([ 7]; [59]; [68]). Similarly, geographical distance has been shown to correlate with product diffusion in the offline world due to imitation or direct observation, as physically close neighbors are more likely to adopt the same product ([21]; [36]; [43]).[ 5] In this study, however, we investigate how the geographic proximity between eWOM message disseminators and receivers accentuates or attenuates the effectiveness of online WOM, whereas the extant explanations for the impact of geography—albeit in different online settings—are not applicable to the context of eWOM; similarly, other explanations that do not apply to our eWOM setting relate to the location specificity of products, distribution networks, or local network externalities. Geographical Distance as a Potential Factor Affecting Online PersuasionRecent studies hint at the potential role geographical distance may play in facilitating persuasion. For instance, lab studies find that when consumers have no identifying information about online reviewers, they assume that the reviewers are similar to them and so are as persuaded by them as they are by reviewers who appear similar to them, and more persuaded than by reviewers who appear dissimilar to them ([82]). When ambiguous reviewers appear less similar, consumers are also less persuaded by their boastful reviews ([86]). One of the many ways these studies manipulate cues to indicate reviewer similarity is to show that the reviewer appears to live in the same or a nearby city as lab participants. In instances of familiar peers, rather than ambiguous reviewers, field studies find that consumers are more persuaded the more recent or intense their relationships are with their peers ([13]; [32]). One of the ways [13] measure relationship recency is to use as a proxy whether peers currently live in the same town. Whereas the aforementioned studies focus on how ambiguity about, trust in, or relationship with peers affects persuasion in settings without financial transactions or product purchases, our research focuses on whether the geographical distance between senders and receivers of online WOM messages is significantly related to eWOM effectiveness. Prior Research on How Geographical Distance Impacts eWOM EffectivenessMost relevant to our work are two studies examining whether geographical distance affects information diffusion and adoption. Specifically, [45] examine how geographic proximity, measured as contiguous relationships between states, and sociodemographic proximity, measured as similarity in demographics between states, affect overall message propagation at the state level. They find that geographic proximity has no impact when sociodemographic similarity is accounted for. Because of data limitations, their operationalization of geographic proximity may mask the actual effect of distance. Importantly, they conduct an aggregate state-level analysis, noting that the ""state-level nature of the data is a limitation"" (p. 249) and ""access to more disaggregated data would allow for a more granular analysis"" (p. 264). Their analysis also prevents them from observing whether consumers are familiar with the message propagator or the social distance between peers ([75]); familiarity and similarity between users as well as relationship strength have been shown to impact peer influence ([13]; [82]). Besides, diffusion may rely on a different mechanism than eWOM ([98]).[79] is also very relevant to our work. These authors first conduct a descriptive study in the offline world and find that living nearby adopters of a cellular service provider is associated with faster switching to that provider. Because they do not directly observe WOM episodes, they conduct their analysis at an aggregate level, where it is not possible to account for homophily (i.e., the tendency of individuals to choose friends with similar tastes and preferences) or attribute WOM influence among the ties of each user ([75], [76]). In addition, they measure geographical distance as the average distance among all possible ties of each consumer.[ 6] These limitations motivate them to conduct scenario-based experiments, in which they investigate the mediating role of perceived homophily and show that geographic proximity to an ambiguous online reviewer increases the likelihood of following the reviewer's opinions, as geographic proximity is used as a cue for perceived similarity. However, the effects of geography are likely to be different in such lab settings due to the lack of familiarity and social ties with online reviewers, which play an important role in persuasion, as the extant literature shows ([13]; [82]; [86]). In addition, while consumers may rely on any available cues to address these concerns of ambiguity and lack of familiarity, lab settings often provide few such cues beyond geography to inform their decisions ([82]). Thus, it remains unclear whether geographic proximity can still play a role in facilitating eWOM influence in real-world settings, where users are familiar with and socially connected to each other and have access to an abundance of available cues.In summary, we empirically investigate whether the geographical distance between individual pairs of familiar disseminators and receivers of organic eWOM in social media plays an important role in driving recipients' subsequent purchase behaviors of physical goods. Table 1 outlines the related studies and additional important differences of our research.GraphTable 1. Overview of Related Studies on the Role of Geography in eWOM. Study (Chronological Order)Research FocusContextSettingWOM TypeSocial TiesFamiliarity with SenderGeographical Distance MeasureAnalysis LevelNotesMain FindingsNaylor, Lamberton, and Norton (2011)How ambiguous reviewers affect persuasionScenario-basedOnline reviewsIndirect; active; artificialNoneNoBinary (same vs. different city)Choice instanceSame vs. different city domicile is one of many reviewer variables manipulated to match that of a lab participant.Consumers assume that reviewers with no identifying information have similar tastes as them, so they are similarly persuaded as they are by reviewers who appear similar to them and more persuaded than by reviewers who appear dissimilar.Aral and Walker (2014)How tie strength and embeddedness affect app adoptionFree Facebook app adoptionSocial media platform (Facebook)Indirect; passive (automated notifications without variation)ObservedFamiliarity with the subject (not sender)Binary (same vs. different town)Notification episodeThe authors note that they ""estimate how relationship-level covariates [same town] are correlated with the extent or impact of influence"" (p. 1363), and the ""results may have limited generalizability to ... cases where there is a significant financial cost to adopting a product"" (p. 1366).Consumers will adopt a free Facebook (movie review) app more quickly the more recent their relationship is and the more embedded they are with an existing user.Packard, Gershoff, and Wooten (2016)How boastful WOM affects persuasionScenario-basedOnline reviewsIndirect; active; artificialNoneNoBinary (nearby vs. distant location)Choice instanceNearby vs. distant location is one of several variables used to manipulate how similar the reviewer appears to a lab participant.Consumers are less persuaded by boasters' WOM when trust cues are low.Chen, Van der Lans, and Phan (2017)How relationship characteristics in a social network affect diffusionMicrofinance program adoptionPotential offline WOM episodeUnobservedSurveysYesNoneAggregateWOM occurs in geographically bounded networks (e.g., village, university), so geographical distance is not material.Social (economical) relationships are the most (least) important drivers of adoption for a microfinance program.Message propagationUniversity network platformDirect; activeObservedYesNoneWOM episodeRelationship intensity (e.g., message volume) is the most important driver of information diffusion in a network.Fossen, Andrews, and Schweidel (2017)How social vs. geographic proximity affect diffusionMessage propagationSocial media platform (Twitter)Direct; activeUnobservedUnobservedBinary (contiguous vs. noncontiguous state)AggregateThe authors note that ""the state-level nature of the data is a limitation"" (p. 249). The study does not control for advertising or other marketing activities and focuses on diffusion.Sociodemographic similarity propagates online message spread. Geographic proximity has no effect when accounting for sociodemographic similarity.Meyners et al. (2017)How geographic proximity affects adoptionCellular service provider adoptionPotential offline WOM episodeUnobservedObserved (aggregate)Yes (aggregate)Average distance from all peer adoptersAggregateThe authors note their field study ""did not have information on either the valence of the signals from the social network or the location of nonadopters"" (p. 63), averages distance across adopters, does not observe WOM, does not control for network externalities, and studies switch behavior. The lab studies have no homophily cues beyond age and gender, and nofamiliarity with or social ties to reviewers.Geographic proximity to adopters of a cellular provider is associated with faster switching to the provider.Scenario-basedOnline reviewsIndirect; active; artificialNoneNoContinuous (miles), categorical (state)Choice instanceGeographic proximity to an ambiguous reviewer increases the likelihood of following the reviewer's recommendation due to perceived homophily.Present studyHow geographical distance affects eWOM effectivenessProduct purchases of significant financial costSocial media platform (Twitter)Direct; activeObservedYesContinuous (miles)WOM episodeOur study has information on nonadopters and social ties; controls for homophily, advertising, and message content; and studies actual product purchases of significant financial cost in a social network where receivers are familiar with the WOM sender.Geographical distance is negatively associated with the likelihood that eWOM influences purchases, above and beyond other effects.  Empirical Setting and Data DescriptionOur empirical setting concerns a large-scale venture of American Express (the service provider) on the microblogging social media platform Twitter. This collaboration introduced a new purchasing service that was seamlessly integrated for two months into the social media platform (Twitter) to leverage users' connections to stimulate eWOM. Specifically, the service enabled consumers to make purchases on the social media platform while simultaneously spreading the word about their purchases to their social media peers (receivers).We further discuss this novel service by describing the data-generating process. In particular, the service provider first posts a short message (tweet) on the platform broadcasting the list of participating merchants and the corresponding products available for purchase. This announcement includes information about the product offerings (e.g., product, respective sale price) and the designated hashtags (i.e., a phrase or word preceded by a hash sign [#]) consumers must use to make a purchase. Consumers must have a microblogging account and sync their service provider account with their microblogging account. Once the service provider broadcasts the products available for purchase, users can purchase these products by posting a tweet that includes the designated hashtag. In addition to the necessary hashtag, consumers can choose to personalize the purchasing tweets that are (automatically) shared with their social media peers. Typically, such messages are posted on the users' social media profiles, and their followers (i.e., those subscribed to their timeline) automatically receive these messages on their own (home) newsfeeds (for an explanation of how we use the variation in message visibility in our main identification strategy, see the ""Econometric Model Identification"" subsection and Figure A1 in the Web Appendix). The social commerce provider tracks the tweets containing the designated hashtag and pairs them with the corresponding product. After the purchase confirmation, the service provider bills the users and ships the product. Empirical DataOur data set includes all the confirmed transactions generated through this purchasing process. Specifically, each transaction in our data set contains the message ID of the purchasing message, the message content, the designated product hashtag, the date and time the message was posted, the corresponding user ID, and whether the message was rendered visible or invisible to each of the user's followers (i.e., included or not included in the follower's newsfeed).For each user, our data set also contains the screen name of the user on the social media platform, when they joined the platform, the set of the user's followers and followees, all the messages users have posted on the platform since they joined, the geolocation of the user, and the self-reported description of the user's profile. Our data set also contains the same information for users who chose not to make a purchase.In addition, our data set contains information about the product offerings. The service provider collaborated with known retailers and offered various products for purchase (e.g., see Figure A2 in the Web Appendix). In particular, the products involve video game consoles and related accessories, electronics and sports equipment (e.g., high-definition tablets, sports and action cameras), general-purpose gift cards, and fashion accessories (e.g., designer bracelets, handbags). These offerings from the social commerce service provider were available for purchase at a reduced price only through the platform (about a 25% discount, yielding an average retail price of $125).Overall, our data set tracks the corresponding purchasing decisions of 132,995 social media users on Twitter with 1.4% of the users purchasing available products. The users are located across the continental United States, as illustrated in Figure 1, and on average follow 996 Twitter users and have 342 followers. Table 2 presents the summary statistics and description of the main variables and Figure 2 shows the corresponding correlations.Graph: Figure 1. Geolocations of users.Graph: Figure 2. Correlation levels among the main variables of interest.GraphTable 2. Descriptive Statistics VariableDescriptionMean/MedianSDMinMaxPurchaseWhether the recipient of the message made a purchase.014.1201Visible messageWhether the message was visible to the recipient.77.4201Geographical distanceGeographical distance between sender and recipient971.08a894.9005,585Number of followersNumber of followers of user342a101,0000376,000Number of followeesNumber of followees of user996a12,6290115,000Reciprocal relationshipWhether the relationship between the users is reciprocal.08.2701Number of interactionsNumber of interactions between users.266.1001,612Sentiment of messageIntensity of message advocacy.21.35−11Personalized messageWhether the message was personalized by the sender.82.3801 1 aWe report the median instead of the mean value.2 Notes: The values of the variables Number of followers, Number of followees, Reciprocal relationship, and Number of interactions correspond to the time of posting the WOM message.We enhanced our data set with a proprietary data set from the ad intelligence company Kantar Media, which includes the local (and national) advertising expenditures of each brand and for each product. We also supplemented our data set with additional information from the American Community Survey five-year estimates of the U.S. Census Bureau regarding local demographics. Empirical MethodologyWe model users' purchase decisions as a function of eWOM message, sender, and recipient as well as relationship characteristics, including observed and latent homophily controls; homophily creates a natural correlation in behaviors that could be incorrectly interpreted as a causal effect (e.g., [10]; [76]). To further control for any unobserved confounds, we use in our research design the variation in the visibility of eWOM messages. Econometric Model SpecificationConsistent with prior literature (e.g., [79]), we use a continuous-time single-failure survival model. In particular, we model how quickly users purchase a product, if any, using a Cox ([39]) proportional hazard model and correcting for censoring of transactions that might have been intended to occur after the observation window ([64]). Specifically, our main estimation equation for the decision of peer i (eWOM recipient) is as follows: λi(t)=λ0(t)exp(VisiblemessageijβM+GeographicaldistanceijβD+Visiblemessageij×GeographicaldistanceijβDM+XijβX+MjβM+DjβDS+RiβR), Graph( 1)where  λi(t)  is the hazard of peer (follower)  i  of consumer  j  to purchase the same product as consumer  j after[ 7] an eWOM message from  j,λ0(t)  represents the baseline hazard,  Visiblemessageij  captures whether the eWOM message of consumer j (sender) was rendered visible to (i.e., included in the home newsfeed of) peer  i  (recipient), and  Geographicaldistanceij  measures the physical distance of i from consumer j in (log) miles. The coefficient of interest is  βDM  and captures the relationship between geographical distance and the effectiveness of eWOM, while the coefficient  βD  accounts for spatially and nonspatially correlated user behaviors and preferences (e.g., homophily) that would have otherwise manifested as the association between geographical distance and the effectiveness of WOM; put simply, our research design leverages the variation in the visibility of eWOM messages, as influence can occur if and only if the eWOM message is rendered visible, whereas correlated user behaviors and preferences are extant even when the eWOM message is nonvisible. We also control for user-relationship (sender–recipient dyad) characteristics, X; message characteristics, M; disseminator characteristics, D; and recipient characteristics, R; as well as product fixed effects and geography (  statei  ) and time (day of message) fixed effects. Model features and machine-learning methodsTo construct the aforementioned user-relationship, message, and disseminator and recipient controls, we employed machine-learning techniques to leverage the vast amount of unstructured and structured data.More specifically, the user-relationship (sender–recipient dyad) characteristics,  X  , include controls for observed and latent pairwise user similarity and tie strength. This set of variables—in addition to the research design—allows us to capture the correlation in latent tastes and preferences between WOM message disseminators and recipients to better distinguish the relationship of interest. In particular, the user similarity between the disseminator and the recipient of a WOM message is measured based on ( 1) the similarity of topics discussed in social media posts using the results of a machine-learning model for natural language processing (NLP), as well as the overlap of the local communities as captured by the ( 2) Jaccard similarity coefficient of followers ([40]) and ( 3) Jaccard similarity coefficient of followees ([40]).The NLP model we employ for this task is the latent Dirichlet allocation (LDA) model ([21]); LDA is a probabilistic generative NLP model that we use to model the user-generated content of each user in our data set (a document in our corpus) as a distribution over topics and every topic as a distribution over words in the English dictionary. We build this model on the corpus of all the messages of the users in our data set as the topics users discuss online reflect their latent interests ([113]); using the complete corpus instead of only the user messages during the observation window improves the inference of the NLP model. In particular, for the implementation of the LDA model, we used 139,850,033 messages in total. We also use a part-of-speech tagger/tokenizer developed specifically for Twitter ([84]) for more accurate tokenization, the removal of stopwords, symbols, typos and uninterpretable words ([97]), and the creation of ngrams, while retaining online-specific textual features (i.e., hashtags, at-mentions, and emoticons) ([92]; [110]). We also use [58] online variational Bayes algorithm to efficiently estimate the LDA model on our corpus. In addition, instead of arbitrarily determining the value of the LDA parameter corresponding to the number of latent topics, we find the natural number of topics present in our corpus using the procedure and measure proposed by [15], evaluated in terms of the Kullback–Leibler ([66]) divergence measure; nonetheless, the findings are not sensitive to the number of topics. Finally, regarding the hyperparameters of our model, we learn an asymmetric prior directly from our data. Beyond capturing the disseminator–recipient similarity with these metrics, we alternatively measure the similarity as a single standardized factor, based on the principal factors method, to avoid any potential multicollinearity; we present both sets of results.In addition, our model specifications include constructs capturing whether the user relationship is reciprocal and (the log of) the number of interactions between the two users ([32]). Finally, we also control for the sociodemographic distance of the users—based on the difference in average age and percentages of male, Black or African American, Hispanic, and Asian-origin residents in the locations of the disseminator and recipient of the message using Census data at the zip code level ([45])—as well as the time zone difference. Overall, the various metrics of user pair similarity capture both observed similarity (e.g., the number of user interactions) and latent similarity (e.g., common latent interests) to further control for potentially unobserved confounds and homophily.The message characteristics,  M  , capture the sentiment of the message (i.e., intensity of WOM advocacy) as well as whether the message was personalized (i.e., explicit rather than implicit advocacy). The sentiment of the message (measured on a continuous scale between −1 and +1) provides a richer metric of the advocacy intensity of the sender compared with other simple metrics, such as lexicon-based scores. The main method we employed uses a publicly available commercial sentiment analysis mechanism based on deep learning ([ 9]). Nonetheless, the results are robust to employing alternative machine learning methods for sentiment analysis.[ 8] Moreover, the message controls also include whether a user account handle is mentioned in the message and whether the sender started the message with a period as the first character; starting a message with a period as the first character affects the visibility of the message and is a common norm among Twitter users when they want to explicitly make a message visible to all users and not only the account mentioned in the message.[ 9] Finally, we also control for advertising expenditures of each brand during our observation period in the local region of the eWOM message recipient expressed in (logarithm of thousands of) U.S. dollars.The disseminator and recipient characteristics,  D  and  R  , include the user (opinion) leadership and expertise measures following the extant literature. We capture these disseminator and recipient characteristics to further control for factors that could bias the true relationship between geographical distance and WOM effectiveness due to omitted individual characteristics and preferences. The user expertise levels are measured on the basis of the standardized similarity of the timeline of a user with the corresponding timelines of the participating vendor and product employing the probabilistic NLP machine-learning model we previously described ([23]; [80]). The motivation for this measure is, for instance, that users who frequently tweet about technological trends and topics similar to those in the social media accounts of the specific product and the corresponding vendor are more likely to be perceived by their social media peers as experts in the area of technological products ([61]). In addition, we further control for correlated user preferences and interests ([26]; [72]) by including in the econometric specifications the latent interests based on the aforementioned LDA model ([ 1]). The user (opinion) leadership is measured based on the additive smoothed ratio ([74]) of followers to followees of the user. The additive smoothed ratio is frequently used in empirical studies to prevent this metric from being oversensitive to small-scale changes in the numbers of followees or followers ([ 1]; [41]). Furthermore, we also control for the number of followers of each user, whether the user has a default profile on the platform, and how many months have elapsed since the user joined the social media platform. Finally, we also control for the age, gender, and income of the users based on the Census data ([45]). Econometric Model IdentificationWe next discuss the research design we use to further distinguish the effect of geographical distance on eWOM effectiveness from unobserved confounds and correlated user behaviors and preferences, such as homophily. Our research design is enabled by a unique feature of the platform that causes some WOM messages to be visible and other messages to be invisible (not included) on the newsfeeds of other peers, as described next.Typically, a message posted by a user on the Twitter platform appears in the newsfeeds of all her followers, as the newsfeeds were not algorithmically curated during this venture. Thus, in our context, whenever a user makes a purchase, her social media peers are exposed to her advocacy toward the product if the purchase is visible in their (home) newsfeeds and their purchasing decisions might, in turn, be affected through such WOM episodes; the specific offers were available for purchase only through Twitter. The research design framework we employ leverages information on whether a message broadcasted on the platform was indeed rendered visible in the newsfeeds of other users or not (see Web Appendix Figure A1).More specifically, on Twitter, a publicly broadcasted message may not be included in the newsfeeds of some followers in instances when a Twitter account handle (if any) appears in a specific position of the message (i.e., if any Twitter account user name following the ""@"" symbol appeared at the very beginning of the message). For instance, if a consumer purchases a product by posting the message ""#BuyActionCamPack @AmericanExpress"" (see Web Appendix Figure A2, Panel A), the message is visible on the newsfeeds of all her followers. However, if the same consumer purchases the same product by posting the message ""@AmericanExpress #BuyActionCamPack"" (see Web Appendix Figure A2, Panel B), then the message is visible only to the social media users who follow both this consumer and the mentioned account.Interestingly, due to the design of Twitter during our observation window, if a consumer initialized the purchase process by clicking on the announcements of the product offerings or a post of another consumer, the handle of the corresponding account (e.g., ""@AmericanExpress"") is automatically prepopulated in the purchasing message. Then, the consumer must click anywhere on the prepopulated field and manually write the purchasing message that includes the required designated hashtag. Notably, if the click happens to be recorded on the left of the prepopulated account handle, the cursor will be placed at the beginning of the field and the consumer can write the purchasing message starting at this position (see Web Appendix Figure A3, Panel A). Thus, if the consumer wants to complete the transaction, she is likely to write a message similar to the one in Web Appendix Figure A2, Panel A, starting immediately with her message, and the tweet will be rendered visible to all of her peers (recipients). Alternatively, if the click happens to be recorded on the right of the prepopulated account handle, the cursor will be placed at the end of the field and the consumer can write the purchasing message starting after the mentioned account handle (see Web Appendix Figure A3, Panel B). Then, if the consumer wants to complete the transaction, she is likely to write a message similar to the one in Web Appendix Figure A2, Panel B, starting immediately with an account handle, and the tweet will not be included in the newsfeeds of her peers (recipients) who do not follow the mentioned account.Thus, the visibility of each WOM message to the peers of a consumer depends on ( 1) whether an account is mentioned (e.g., @AmericanExpress or another account), ( 2) the exact position of the account handle (whether it is at the very beginning of the message or not), ( 3) what social media accounts the consumer's peers follow, ( 4) whether a user click will be recorded more to the left or more to the right of the screen, and ( 5) whether an account handle will be prepopulated by the platform. Thus, the visibility of the messages is affected by platform design factors (e.g., prepopulating accounts) rather than certain user characteristics exploited by a curation algorithm. Importantly, with regard to the visibility of eWOM messages, the recipient does not control the visibility of these messages from the users she follows; this is important because the purchase decisions we study are the recipients' decisions.Because our main identification strategy assumes that the visibility of eWOM messages and the geographical distance of consumers are not endogenous in this study, we also empirically investigate whether the visible and nonvisible message groups differ across the pretreatment variables in our model based on the normalized differences tests ([60]) that provide scale-invariant measures of the size of the differences. The normalized differences range from −.1768 to.1692, indicating that there are no significant differences between the two groups in observable characteristics, further enhancing the validity of the identification strategy; differences of.25 or less indicate a good balance between the two groups ([60]). Similarly, kernel distributions, quantile-quantile plots, and the orthogonality test ([60]) further confirm the validity. We also examine the geographical distance in the same way, reaching the same conclusion.Overall, these unique features of Twitter induce an important variation in the visibility of messages, enabling us to examine the behaviors of peers in treatment (i.e., visible message) and comparison (i.e., nonvisible message) groups in a potential outcomes framework. Thus, differences in purchases between treatment and control groups can be attributed to the corresponding WOM messages and their characteristics, addressing the issue of correlated user behaviors and preferences. In this respect, our research is also relevant to the stream of work that has leveraged the variation in the visibility of advertising messages to estimate the effect of online ads ([ 1]; [48]; [46]).We further enhance our identification strategy using only observations corresponding to dyadic relationships and social media peers who did not receive messages (either visible or invisible) from multiple disseminators ([ 1]; [12]). In addition to taking advantage of this nonintrusive research design, we also avoid any observer biases; the subjects are unaware of being part of the study and, thus, do not alter their behavior in anticipation of the study. Nevertheless, we also control for differences in the pairwise relationships between users by employing an extensive set of variables and fixed effects in our data-rich setting. Table A1 in the Web Appendix presents additional identification strategies. Empirical Results Main ResultsTable 3 presents the results of the main econometric specifications of the eWOM effectiveness model. In particular, Model 1 constitutes our baseline specification as it models eWOM effectiveness based on the constructs of dyadic similarity and relationship strength between the disseminator and recipient of the eWOM message (i.e., pairwise user similarity, reciprocity of users' relationship, and number of user interactions), eWOM message advocacy (i.e., personalized message and sentiment of message), and user characteristics (i.e., expertise and leadership). Then, Model 2 introduces the notion of geographical distance, and Model 3 adds the information of eWOM message visibility and leverages the corresponding variation to further distinguish the relationship between geographical distance and eWOM effectiveness  (βDM)  from correlated behaviors and homophily among users. That is, Model 3 disentangles the relationship between geographical distance and eWOM effectiveness from the correlational effect of geographical distance reported by Model 2; the eWOM influence is transmitted through visible messages only, whereas correlation in user behaviors and preferences (i.e., homophily) is present even with invisible messages. Then, Model 4 further controls for whether a user account was mentioned in the eWOM message and if the message was explicitly made visible while introducing additional disseminator and recipient controls (i.e., number of followers, leadership, default profile, time on the platform, interests, age, gender, and income) and other controls (i.e., the difference in average age and percentage of male, Black or African American, Hispanic, and Asian-origin residents in the locations of the disseminator and recipient of the message, the log of brand advertising expenditures in USD (in 1,000s) in the location of the recipient of the message, and the time zone difference between the locations of the disseminator and recipient of the message). Finally, Model 5 controls for the specific product mentioned in the eWOM message as well as state and day fixed effects.GraphTable 3. Estimation Results of eWOM Effectiveness Model. Model 1Model 2Model 3Model 4Model 5User similarity1.244***1.240***1.239***1.257***1.264***(.009)(.009)(.009)(.010)(.011)Reciprocal relationship5.868***5.378***5.403***4.829***4.527***(.317)(.290)(.291)(.269)(.254)Number of interactions1.015.979.977.987.980(.034)(.033)(.033)(.034)(.035)Sentiment of message1.540***1.614***1.557***1.728***1.556***(.107)(.112)(.111)(.133)(.123)Personalized message1.041***1.046***1.047***1.053***1.041***(.003)(.003)(.004)(.005)(.007)User expertise1.204***1.183***1.184***1.595***1.191**(.030)(.029)(.029)(.130)(.100)User leadership1.006***1.006***1.005***1.011***.998(.001)(.001)(.001)(.002)(.002)Geographical distance.891***.929***.946***.954**(.007)(.017)(.020)(.020)Visible message1.451***1.598***1.608***(.171)(.193)(.188)Visible message × Geographical distance.951**.954**.945***(.019)(.020)(.020)Additional user controlsNoNoNoYesYesAdditional message controlsNoNoNoYesYesAdditional controlsNoNoNoYesYesProduct fixed effectsNoNoNoNoYesGeography fixed effectsNoNoNoNoYesTime fixed effectsNoNoNoNoYesLog-likelihood−22,422.8−22,312.8−22,307.4−22,035.2−21,763.6χ22,763.5−2,983.42,994.23,538.64,081.8N132,955132,955132,955132,955132,955 3 *p < .1.4 **p < .05.5 ***p < .01.6 Notes: eWOM effectiveness analysis. The hazard ratios (HRs) represent the percent increase (HR > 1) or decrease (HR <1) in postpurchase hazard associated with each attribute.According to the results presented in Table 3, we find that the coefficient of the variable capturing the relationship between geographical distance and WOM effectiveness is negative and statistically significant (Visible message × Geographical distance:.945, p < .01, Model 5); all reported coefficients correspond to hazard ratios (HRs) representing the increase (HR > 1) or decrease (HR < 1) in purchase likelihood associated with each attribute. (Table A2 in the Web Appendix also shows the coefficients of the control variables.) Beyond the effect of interest, we also find a negative and statistically significant spatial homophily–based effect of geographical distance (Geographical distance:.954) as well as a positive and statistically significant effect of similarity (homophily). Interestingly, these findings show that despite the ""death of distance"" postulated in the literature ([28]), geographical distance is negatively associated with the effectiveness of eWOM. Thus, our research is the first to establish that geographical distance has a negative relationship with eWOM outcomes even among familiar social media peers, in addition to the previously known homophily-based effect of geographical distance.Moreover, the coefficients of all the other variables are in accordance with what one would expect as well as the extant literature on WOM (e.g., [13]; [20]; [45]). Specifically, we find that the increased user similarity and strength of relationship between users (User similarity: 1.264; Reciprocal relationship: 4.527) as well as more intense WOM advocacy (Sentiment of message: 1.556; Personalized message: 1.041) are associated with higher levels of purchase likelihood after exposure to WOM (Visible message: 1.608). Similarly, users with higher product expertise (User expertise: 1.191) seem more persuasive; thus, their followers are associated with a higher purchase likelihood after being exposed to their advocacy. Economic Significance and Managerial RelevanceThe relationship of interest is also of economic significance, as a decrease of 10 miles in the distance between users accentuates the effectiveness of eWOM by 12.78% based on the aforementioned coefficients; similarly, an increase of 100 miles in the distance corresponds to a decrease of 25.56%, and an increase of 1,000 miles corresponds to a decrease of 38.34%. For instance, for a recipient living in New York City, the relationship is reduced by 24.39% when the eWOM message originates from a sender in Philadelphia, and 38.82% from Miami, compared with the same message from a sender in New York City.We further assess the economic significance of the findings by measuring the out-of-sample performance of the models. Specifically, we use a holdout evaluation scheme with an 80/20 random split of data and evaluate the models in terms of Harrell's C concordance coefficient, which measures the likelihood of correctly ordering survival times for pairs of senders and recipients of eWOM messages; the concordance measure is similar to the Mann–Whitney–Wilcoxon test statistic as well as the area under the receiver operating characteristic curve. The results show that our model achieves a predictive performance of.840. Thus, it outperforms the baseline by a large margin, as the baseline performance corresponds to a value of.5. This statistically significant difference further illustrates the managerial relevance of the findings, as they can enhance seeding and targeting strategies ([57]; [99]).We further quantify the (dollar) value of this increase in out-of-sample performance ([89]). To conduct this calculation ([89]), we use estimates of the cost of targeting (e.g., promoting eWOM messages) and the average product price (Goldfarb and Tucker 2011); the cost of this type of targeting on Twitter is estimated to be $1.35 based on [111], while the average product price in our data set is $125. Combining these data reveals that our model suggests a profit of $.85 per targeted user, which corresponds to a 9% increase over the baseline of not using the information of geographical distance (i.e., $.78), while for random targeting the corresponding profit is only $.008. Potential MechanismOur findings are surprising, as in such an empirical setting the products are not location-specific, there are no transportation fees for consumers, and there is no contracting or potential conflict or ambiguity between senders and recipients of WOM messages ([59]; [79]). Thus, to fully understand our findings, we delve into a likely underlying mechanism of the identified effect and conduct additional analyses that allow us to assess the likelihood of this potential mechanism.We hypothesize that the negative relationship between geographical distance and eWOM effectiveness could be due to the identification processes of social media users. Specifically, a user who resides near the sender of the message is likely to share a common social identity with the sender based on their geographic proximity ([44]; [62]; [83]; [107]) and thus might be more susceptible to WOM influence originating from this (local) sender.[10] Conversely, a recipient who resides farther away from the disseminator of the WOM message is not likely to share the same location-based social identity and thus is less likely to be persuaded ([37]; [38]; [96]) by mere exposure to eWOM advocacy. Location-based social identity activationTo empirically assess this potential underlying mechanism, we first examine the moderating effect of the salience of the geographical distance from the source of WOM. Salience is activating common social identity identification ([44]; [55]; [56]; [68]; [77]; [106]), and thus, we empirically test the likely underlying mechanism by examining the moderating effect of the salience of the sender's location on the impact of geographical distance on the effectiveness of WOM. If the relationship is accentuated when the geographic proximity of the source of the WOM message is more salient, this would provide empirical support for the hypothesized mechanism of common social identity, as the salience of the location—and thus the salience of the geographic proximity—enhances the social identification processes of the recipient ([44]; [55]; [56]; [68]; [77]; [106]). This would also provide additional empirical evidence in favor of the main identification strategy. Alternatively, if more salient location cues attenuate the relationship, this would provide evidence against the hypothesized mechanism.According to the results presented in Table 4, we find a negative and significant moderating effect of the salience of the sender's location on the impact of geographical distance on the effectiveness of WOM; the salience of location variable corresponds to whether the location of the WOM sender is explicitly mentioned in her profile. That is, the relationship between geographic distance and eWOM effectiveness is even more negative when the distance is more salient. This finding indicates that common social identity is a likely mechanism for the identified relationship. The results are robust to alternative econometric specifications.[11]GraphTable 4. Estimation Results of eWOM Effectiveness Model with the Moderating Effect of the Salience of Geographical Distance. Model 1Model 2Model 3Model 4Model 5User similarity1.244***1.240***1.238***1.256***1.260***(.009)(.009)(.009)(.010)(.010)Reciprocal relationship5.868***5.378***5.525***4.881***4.606***(.317)(.290)(.298)(.272)(.258)Number of interactions1.015.979.976.989.969(.034)(.033)(.033)(.034)(.035)Sentiment of message1.540***1.614***1.642***1.781***1.568***(.107)(.112)(.118)(.138)(.124)Personalized message1.041***1.046***1.043***1.050***1.035***(.003)(.003)(.004)(.005)(.008)User expertise1.204***1.183***1.156***1.581***1.172*(.030)(.029)(.028)(.129)(.099)User leadership1.006***1.006***1.004***1.010***.998(.001)(.001)(.001)(.002)(.002)Geographical distance.891***.947***.961*.965*(.007)(.019)(.021)(.021)Visible message1.430***1.588***1.783***(.168)(.192)(.214)Visible message × Geographical distance.949**.950**.918***(.021)(.021)(.020)Visible message × Geographical distance × Salience of location.920***.948***.974*(.011)(.012)(.013)Additional user controlsNoNoNoYesYesAdditional message controlsNoNoNoYesYesAdditional controlsNoNoNoYesYesProduct fixed effectsNoNoNoNoYesGeography fixed effectsNoNoNoNoYesTime fixed effectsNoNoNoNoYesLog-likelihood−22,422.8−22,312.8−22,280.2−22,024.7−21,751.1χ22,763.52,983.43,048.53,559.64,106.8N132,955132,955132,955132,955132,955 7 *p < .1.8 **p < .05.9 ***p < .01.10 Notes: eWOM effectiveness analysis with the moderating effect of salience of geographical distance. The salience of location variable corresponds to whether the location of the disseminator is explicitly mentioned in the profile of the disseminator. Additional table notes as in Table 3. Location-based social identity prominenceWe also examine the likelihood of the hypothesized mechanism in additional ways. For instance, we examine the effectiveness of eWOM under conditions that strengthen the role of geographical location in the social identification process. Specifically, increased political homogeneity in the local area of the recipient of the eWOM message is likely to enhance the importance of the location-based social identity of the recipient as it increases the salience and significance of individuals' social identity due to political entities operating at geographic levels (e.g., precinct, county, state) and the characteristics of the local information environment (e.g., increased number of times an individual is reminded of the local identity, positive perceptions of the local community) ([62]; [91]; [101]). As a result, a pronounced location-based social identity of the recipient is likely to engender biases based on geographical distance, accentuating the relationship. Thus, if the relationship is accentuated when the political homogeneity in the local area of the recipient of the WOM increases, this would provide empirical support for the potential mechanism of social identification. Conversely, the opposite would provide empirical support against this potential mechanism.Based on the results in Table A3 in the Web Appendix, we find a negative and significant moderating role of the political homogeneity in the local area of the recipient; we have collected data from the MIT Election Lab (https://electionlab.mit.edu) on political voting patterns at the precinct level for the 2016 elections and measure political homogeneity on the basis of the percentage of voters that would need to switch from the majority party to the minority party for the two parties to have equal votes. Put simply, the negative relationship between geographical distance and eWOM effectiveness is amplified when location-based social identity might be more pronounced due to increased political homogeneity. This finding lends support to the hypothesized mechanism of social identity. The results are also robust to alternative specifications.Similarly, we also examine the moderating role of exogenous hardships in the local area of the recipient of the WOM message. If there have been significant local community hardships or natural disasters, then geography-based common social identity is likely to be more prominent for the residents of the affected area ([109]). Thus, if the relationship is accentuated when the geographic proximity of the source of the WOM message is combined with local community hardships for the recipient, this would provide additional support for the potential mechanism of common identity; we measure local community hardships using (exogenous) deaths related to extreme weather events in the location of the recipient of the message during the last five years prior to our observation window based on data from the National Oceanic and Atmospheric Administration. According to the results in Web Appendix Table A4, we find that the relationship between geographical distance and the effectiveness of eWOM is even more negative for users for whom location-based social identity is likely more pronounced due to local community hardships, lending empirical support to the hypothesized potential mechanism.[12] Location-based social identity versus other peer effectsWe also examine whether the estimated moderating role of geographical distance is above and beyond other potential peer effects. Table A5 in the Web Appendix presents the corresponding results controlling for both the interaction between user similarity and visible message and the interaction between sociodemographic distance and visible message. Our findings remain highly robust, further illustrating that the estimated moderating effect of geographical distance is above and beyond other peer effects, including actual and perceived homophily. The results are also robust to alternative specifications, such as including additional interactions. Ruling Out Additional Alternative ExplanationsIn addition to the aforementioned evidence and identification strategies, we assess various alternative explanations. Table 5 presents an overview of these, with the main ones discussed next and additional ones discussed in Web Appendix B.GraphTable 5. Alternative Explanations Alternative ExplanationRationaleIdentification StrategyTable(s)Location-RelatedLocation characteristicsLocation-based characteristics may affect purchase likelihoodsMain identification strategy including geography fixed effectsAllSpatially correlated user preferencesUser interests and brand preferences may be spatially correlatedMain identification strategy including latent user interestsAllAdditional brand preference controlsA7Local time-difference effectsTime zone differences are correlated with geographical distance and may relate to differences in users' activities or moodsMain identification strategy including time zone differencesAllLocal weather conditionsLocal weather conditions may affect consumers' activities and moodsAdditional weather controlsA9Small-city effectsGeographic distances are shorter in smaller and more remote locations, where the demand for products sold online might be higher due to potentially limited availability of other productsMain identification strategy including geography fixed effectsAllExclusion of small and remote locationsA8Local marketing effectsLocal marketing effects may affect purchase likelihoodsMain identification strategy including ad controlsAllAdditional ad response controlsA6User-RelatedHomophilyCorrelated behaviors among similar (across observed characteristics) peersMain identification strategy including user similarity controlsAllAdditional brand preferences similarityA7Additional user similarity controlsA11Propensity-score matchingA13Latent or unobserved homophilyCorrelated behaviors among similar (across latent or unobserved characteristics) peersMain identification strategy including user latent similarity controlsAllAdditional latent homophily controlsA10Additional user similarity controlsA11Latent variable modelA14Propensity-score matchingA13User characteristicsUser characteristics may affect purchase likelihoodsMain identification strategy including user controlsAllLocal demographic effectsSociodemographic distance between users may affect purchase likelihoodsMain identification strategy including sociodemographic controlsAllIncome-level effectsIncome levels may affect where users select to live; as such, geographical distance may be correlated with income levelsMain identification strategy including income controlsAllMessage-RelatedMessage contentMessage content characteristics may affect purchase likelihoodsMain identification strategy including message controlsAllPropensity-score matchingA13Nonrandom message visibilityMessage visibility may not be random, despite provided evidenceStatistical tests—Main identification strategy including message visibility controlsAllPropensity-score matchingA13Covariate adjustmentA12Product-RelatedProduct characteristicsProduct characteristics may affect purchase likelihoodsMain identification strategy including product fixed effectsAllMarketing promotionsAdvertising or other marketing activities may affect purchase likelihoodsMain identification strategy including ad controlsAllAdditional ad response controlsA6Other-RelatedUnobserved effectsUnobserved effects correlated with geographical distance of visible eWOM messagesPropensity-score matchingA13Other unobserved time-varying effectsAny other unobserved effects that vary with time and are correlated with geographical distance of visible eWOM messagesLimited time-horizon—Main identification strategy including time fixed effectsAllGeographic distribution of tiesUser might have more geographically proximate than geographically distant tiesMain identification strategyAllModel idiosyncrasiesModel and model specification choices could potentially affect the resultsLogistic regressionA15Alternative specificationsAllSpurious effectsSpurious effects or other statistical artifacts""Placebo"" studiesA16  Local marketing promotion effectsOne may be concerned that the results might be driven by unpaid or organic marketing effects in the local region of the eWOM message recipient. To evaluate this, we supplement our data set with local web search trends for each product from Google Trends ([14]). Table A6 in the Web Appendix presents the corresponding results controlling for both local marketing expenditures and ad response (via search behaviors) of the local audience. The results remain robust, alleviating concerns that local marketing promotion activities drive the results. The results are also robust to alternative specifications, such as using national Google Trends and advertising expenditures or estimating separate models for each potential confound. Local user-preferences effectsWe also examine the robustness of the findings to alternative specifications, such as controlling for homophily based on the overlap in brands that each social media user follows on the platform. Table A7 in the Web Appendix presents the corresponding results. The results remain robust, further corroborating our findings. Small-city effectsAnother potential alternative explanation is that the results are driven by disseminator–recipient pairs located in small and remote locations as in such locations distances are in general shorter and demand for products sold online is higher due to the limited availability of other product alternatives ([21]; [79]). We assess this alternative mechanism by repeating the analysis excluding any observations that correspond to small and remote locations, as determined by the Census (i.e., locale assignments). Table A8 in the Web Appendix presents the results; the results remain robust. Local weather conditions effectsWe also assess the alternative explanation that the results are driven by the local weather conditions affecting the moods and activities of users ([46]; [67]). We assess this potential explanation by controlling for the temperature, precipitation, and sunshine levels in the location of the recipient using data from the National Oceanic and Atmospheric Administration. Web Appendix Table A9 presents the corresponding results; the results remain robust. Robustness Checks and Alternative Identification StrategiesWe also undertake an extensive set of tests to assess the robustness of the results and further strengthen the findings, as discussed next; see Table 5 and Web Appendix B for additional details. Extended econometric specificationsFirst, to enhance the employed identification strategy and examine the robustness of the findings, we further control for latent user characteristics by tapping into the social network structure and recent deep-learning advances. Specifically, we use the method of DeepWalk, a deep-learning method for graphs ([88]), to learn the latent representations of the users and their similarity and further account for both network structure roles and latent user homophily. Table A10 in the Web Appendix presents the corresponding results. The results corroborate our findings. The results also remain robust to employing alternative deep-learning methods, such as the node2vec method ([53]).We also repeat the analysis including multiple user similarity measures. In particular, the similarity measures correspond to the similarity levels between disseminators and recipients based on ( 1) the Jaccard coefficient of their followers, ( 2) the Jaccard coefficient of their followees, ( 3) the topics discussed in social media posts using the results of the LDA model, ( 4) the intrinsic brand and product preferences of the users based on the overlap in brands that each social media user follows on the platform, ( 5) the demographic information at the corresponding geographic locations (i.e., average age and percentages of male, Black or African American, Hispanic, and Asian-origin residents based on Census data), and ( 6) the latent characteristics of the users based on the deep-learning methods for representation learning; in addition to ( 7) the reciprocity of the relationship and ( 8) the number of interactions between the users. Table A11 in the Web Appendix presents the corresponding results; the results remain robust. Alternative identification strategiesWe also examine additional alternative identification strategies to control for any potentially remaining differences between the visible and nonvisible messages; Table A1 in the Web Appendix presents a summary of the different identification strategies. First, we enhance our identification strategy following the covariate adjustment method of [60]. Table A12 in the Web Appendix presents the corresponding results. The results remain robust; the results are also robust to including additional covariate interactions.Moreover, as an alternative identification strategy, we combine propensity-score matching with the main research design. In particular, we model the propensity of each message to be rendered visible using all the variables that describe the users' relationship and the message characteristics as well as the geographical distance between the users.[13] We conduct the matching based on the propensity scores before estimating again the same econometric models (for additional details, refer to the corresponding table notes). For this robustness check, we use one-to-one matching with replacement and a caliper of.05, yielding a standardized mean (median) absolute difference of.009 (.007) across all the variables, which ensures that covariate balance has been successfully achieved ([18]); the density distributions of the propensity scores also indicate significant overlap and common support. As shown in Table A13 in the Web Appendix, the results remain robust. The results are also robust to nearest-neighbor matching with the generalized Mahalanobis distance.Furthermore, as an additional alternative identification strategy, we build latent variable models where the sender–recipient similarity is latent and measured based on the various similarity features. Web Appendix Table A14 shows the corresponding results. Model 1 corresponds to the aforementioned latent variable model, while Model 2 combines the latent variable model with propensity-score matching estimating the model over the matched sample. The results of all the aforementioned alternative models are highly consistent and further corroborate our findings.Finally, as an alternative strategy, to estimate the relationship between geographical distance and eWOM effectiveness, we also use a logit model ([105]) examining whether—rather than how quickly—a user purchases a product. As Web Appendix Table A15 shows, the results remain robust. Falsification testsWe supplement these robustness checks with falsification tests to further assess whether the previous models are picking up spurious effects. As shown in Web Appendix Table A16, the results indicate our findings are not a statistical artifact of the specifications.Overall, the findings remain highly robust to various alternative identification strategies, econometric specifications, robustness checks, and falsification tests. Figure 3 illustrates the corresponding estimated effects across the specifications.Graph: Figure 3. Hazard ratios (HRs) with 95% confidence intervals (whiskers) representing the percentage increase (HR > 1) or decrease (HR < 1) in postpurchase hazard across estimated models. Discussion and ImplicationsIn this study, we investigate the relationship between geographical distance and the effectiveness of eWOM. Specifically, we examine whether the geographical distance between familiar disseminators and receivers of eWOM messages plays an important role—beyond utilitarian reasons and proxying for consumer tastes—in driving recipients' subsequent purchase behaviors. Our results show that the relationship between eWOM and the likelihood that message recipients subsequently also make product purchases significantly strengthens as the spatial proximity between disseminators and receivers grows. Implications for TheoryOur findings help advance understanding of conditions that affect online WOM performance. Many of the characteristics previously shown to impact eWOM outcomes relate to the product, brand, or message ([73]; [85]). We contribute by illustrating the role of the important but often overlooked construct of geographical distance in eWOM effectiveness. In showing how geographical distance is still associated with the effectiveness of online WOM in the absence of geography-specific transaction costs between unambiguous users, we demonstrate how the social force field of geography can tether the potential of eWOM. That is, despite the promise of technology to reduce communication barriers and the proclaimed ""death of distance"" ([29]; [52]), we find that geographic constraints persist online in unexpected ways. Therefore, our results also help address the debate on whether and how geographical distance still matters online ([51]) by showing that it can shape the influence of eWOM.We also contribute to the theory of eWOM examining why geographical distance is associated with eWOM effectiveness. Specifically, we find evidence that social identification may explain why the influence of online WOM is negatively related to the distance between WOM message disseminators and receivers. That is, our results suggest consumers are susceptible to online information and cues related to social identification as they can, in turn, enhance message persuasiveness. Thus, information and cues relating to social identity can be agents of eWOM influence. Whereas much of the literature on the role of geographic distance in e-commerce and other online settings offers economically driven explanations for the impact of geography, our study proposes behavioral bias relating to social identification may be an underlying mechanism that drives the relationship between geographic distance and eWOM. This finding highlights the need for future research to study additional non–economically driven explanations that can induce such biases. Implications for PracticeOur findings have important implications for managers as well. For instance, a controversial argument in the industry is that solely characteristics of the disseminators catalyze the adoption of behaviors and products and thus much of marketing efforts to engineer WOM in social media focus on identifying such characteristics. However, our findings indicate that marketers should expand their focus to take into account the disseminator–recipient pairings and understand that factors pertaining to these pairs can be significantly related to the effectiveness of eWOM. In particular, our results suggest geographical distance matters in online WOM and,, thus marketers can readily take advantage of how geographical distance is associated with eWOM persuasion. Marketers may thus adopt data-driven strategies to selectively promote eWOM episodes according to the proximity of such episodes to each consumer, or to strategically engineer such episodes based on geolocation information. Interestingly, although research has begun to identify pairwise characteristics between senders and receivers that shape the influence of eWOM, such as tie strength and similarity across personality traits ([ 1]; [20]), many of these factors are not readily observable to managers who wish to capitalize on them. The distance between social media users, though, is more easily observable to managers.Beyond promoting or engineering geographically proximate eWOM episodes, marketers may also benefit from promoting and/or engineering episodes containing other social identity cues. The likely connection between eWOM outcomes and social identity suggests that firms may also consider other cues relevant to social identity formation to further boost the success of interpersonal communications and WOM messages, as enhancing social identification may significantly increase message persuasion and user engagement in the online world.Our findings also have important implications for the effective design of viral marketing campaigns and ad content. Specifically, brands may boost the persuasiveness of their marketing campaigns by infusing into their content local cues or other identification triggers to induce consumers' social identification processes. Relatedly, marketers are beginning to leverage users' connections on social networks to develop and deliver marketing communications as part of their social advertising efforts. Our research suggests that they could further improve the effectiveness of these strategies by selecting geographically proximate connections to their targets.Furthermore, going beyond advertising strategies, the implications of our work also provide actionable guidelines for optimizing the delivery of digital content. In particular, our findings can help platforms increase the effectiveness of their content curation and ranking algorithms by incorporating information on content location or source origin and by factoring geolocation into their determination of which user-generated content to disseminate. For instance, content generated by spatially proximate consumers may draw more attention due to identification processes and thereby increase the effectiveness of content provision. In a similar vein, social media platforms may also consider incorporating location information in other functions. For instance, social media platforms may incorporate such information into various other machine-learning algorithms, such as their whom-to-follow recommendations. In addition, our findings could also be used by marketers and platforms to better predict the diffusion of information, products, and user behaviors in social media ([ 5]).Lastly, deepening our understanding of the factors that can attenuate or accentuate the effectiveness of eWOM has important implications that extend to public policies. For instance, revealing how geographic proximity is positively associated with eWOM effectiveness is critical for the development of effective public policies to induce positive behavioral changes, such as voter turnout, civic engagement, and public health actions. Limitations and Future ResearchWhile our work makes important strides in understanding how geographic proximity is related to eWOM effectiveness, we acknowledge certain limitations, which mostly stem from data availability issues. For instance, we examine the relationship between geographical distance and eWOM in a single social media platform because the service provider launched this venture on only one platform. Future research could examine whether the observed relationship manifests differently on other platforms. Moreover, we did not manipulate the visibility of the messages on Twitter because the venture did not alter the functionality of the platform in any way; future research could consider directly manipulating the visibility of the messages. Similarly, we did not manipulate the geographical distance of users from their followers. In addition, while we capture actual purchases in our data, we do not capture other consumer behaviors that could indicate interest in the products, such as online searches, as this type of information was not available to us. It would be interesting for future research to further examine such potential effects. Future research could also further examine and validate the underlying mechanisms. While we provide evidence that social identification may account for the relationship, future work may conduct experiments to verify this. Lastly, we do not observe in our data private communications between individuals due to privacy reasons and ethical concerns. Nevertheless, we hope these limitations provide avenues for future research that can deepen understanding of the critical role geographic proximity plays in eWOM and other online settings. Supplemental Materialsj-pdf-1-jmx-10.1177_00222429211034414 - Supplemental material for Is Distance Really Dead in the Online World? The Moderating Role of Geographical Distance on the Effectiveness of Electronic Word of MouthSupplemental material, sj-pdf-1-jmx-10.1177_00222429211034414 for Is Distance Really Dead in the Online World? The Moderating Role of Geographical Distance on the Effectiveness of Electronic Word of Mouth by Vilma Todri, Panagiotis (Panos) Adamopoulos and Michelle Andrews in Journal of Marketing  "
13,"Leveraging Cofollowership Patterns on Social Media to Identify Brand Alliance Opportunities The use of cobranding and brand extension strategies to access new markets has been a topic of significant interest. However, surprisingly few studies have examined cross-category connections of brands using publicly available digital footprints. In this study, the authors introduce a new, scalable automated approach for identifying potential cobranding and brand extension opportunities using brand networks derived from publicly available Twitter followership data. The digital user–brand relationship, established through followership activity, is regarded as an expression of interest toward the brand. Common followership patterns between brands are then extracted to capture cointerest between those brands' audience. By utilizing the cointerest patterns, the approach aims to derive cross-category brand–brand and brand–category connections, which can serve as important measures for assessing cobranding and extensions opportunities. This article introduces a new construct, transcendence, which measures the extent to which a brand's followers overlap with those of other brands in a new category. The analysis is conducted at different points in time to help managers track shifts in brand transcendence.Keywords: cross-category; brand networks; asymmetry; cobranding; brand extensions; social media; social networks analysis; TwitterCobranding is a brand alliance strategy to bolster reach, awareness, and sales potential by tapping the prospective customers of partnering brands. Many types of cobranding schemes exist in the marketplace, including joint advertising campaigns (e.g., ads depicting the joint consumption of Coca-Cola and McDonald's), cause–brand alliances (e.g., UNICEF and Target), bundling (e.g., streaming deals that include joint Hulu and Spotify subscriptions), and cobranded products (e.g., Louis Vuitton launching an exclusive luggage line for BMW). Cobranding strategies enable brand extensions, with managers leveraging the existing brand names of their partners to enter new markets and categories ([14]). Cobranding is increasingly viewed as a valuable marketing strategy and has been shown to increase awareness, quality, market value, and brand equity ([ 8]; [46]). Although marketers have been leveraging the synergistic benefits of cobranding for decades, surprisingly little empirical research has tried to identify potential cobranding alliances using modern digital approaches. Most of the existing empirical research either uses observations from fast-moving consumer goods categories ([ 8]; [14]; [19]) or conducts analyses within a single category, such as camcorders in [24], car brands in [34], and LED TVs in [42]. Similarly, [36] and [35] use recommendation hyperlinks between Amazon web pages to create a large-scale network of books and demonstrate the value of shared purchasing patterns.Obtaining broader insights into the identification of cobranding opportunities across diverse categories would generate relevant and meaningful information for brand owners. As [43] notes, ""By mashing up two bona fide brands, especially in diverse industries, the impact can be exponential."" For instance, a well-known cobranding deal between Starbucks and Spotify—two seemingly unrelated brands—enabled both brands to cross-promote their products and grow their customer base. By providing premium coffee-shop music, Starbucks incentivized Spotify users to join its loyalty program. In return, Spotify grew its user base through Starbucks' offer of a free coffee upon joining. Having knowledge about relevant cross-category brand connections is crucial to brand owners ([12]); however, there is little or no research on identifying these broader cross-category effects using current digital approaches.This article introduces a new, scalable approach for generating cross-category branding insights using implicit brand networks on social media. The cross-category branding insights are revealed in the form of brand–brand and brand–category connections, which can serve as important measures for assessing cobranding and extensions opportunities. Unlike traditional social networks, which involve explicit interaction between the participating entities,[ 5] edges within a brand network are implicit (or tacit) and arise due to common followership between brands. [45] note the relevance of these tacit connections to decision making. This idea has been studied previously within the domain of collaborative filtering ([45]). Implicit networks, which condense the vast digital interest space of millions of users into a parsimonious form, provide direct insight into the digital ecosystem and are the subject of increasing research attention across domains ([45]). In this study, the cross-category connections of a focal brand in the implicit network are leveraged to help brand managers identify cobranding and extension opportunities.The article introduces a new construct, called ""brand transcendence,"" which is defined in the context of a large ecosystem of brands belonging to different categories. The transcendence of a brand into a new category is the extent to which its followers overlap with those of other brands in the new category. From a managerial perspective, this study provides an automated approach for identifying cross-category cobranding opportunities based on user cointerest, which is measured through overlap of brands' followers on Twitter. Importantly, the cointerest patterns captured through common followership do not necessarily reflect overlapping brand associations or guarantee brand fit, which is traditionally measured using the similarity of brand personality dimensions ([48]). However, such patterns are indicative of common tastes or interests among social media users. Following [39], we consider that the composition of a brand's follower base represents the tastes (and likes) of its audience. Thus, the greater the network overlap between two brands, the greater the similarity in tastes and interests between those brands' audiences. Taking these principles together, we study the transcendence of a brand into a new category based on the extent to which its followers overlap with those of other brands in the new category. Our approach also identifies central brands that have strong and consistent connections within their own category ([ 9]), with ""centrality"" being defined as the extent to which a brand's followers overlap with other brands in its own category.By incorporating directionality into the network edges, we also capture the asymmetric relationships between brand pairs, which help identify brands that may potentially benefit more from a cobranding alliance. We outline how cross-category connections can provide both brand–category and brand–brand insights, depending on a brand's marketing goals (i.e., extension vs. cobranding). For instance, brand–category connections capture the transcendence of brands into new categories and show that certain categories are more viable for extensions than others. Brand–brand connections, in contrast, provide a more granular view of transcendence by revealing the individual brands that are suitable for cobranding. As user–brand relationships on social media may change over time, this article analyzes the brand network in both 2017 and 2020. This helps visualize the fluctuations of brand connections over time and investigate the impact of such fluctuations on cobranding alliances. Understanding whether critical connections with certain brands or prospective categories have waned helps managers promptly identify the problem and take appropriate action. Similarly, identifying new connections that have formed over time illustrates how past marketing actions can impact a brand's transcendence in users' minds.Cross-category connections revealed through the network can be used to both assess the effectiveness of previous marketing campaigns and discover new alliance opportunities. For example, Bud Light's connection to Pepsi reflects the cointerest patterns between the two brands and, thus, affirms the effectiveness of joint marketing campaign led by the two brands previously. Similarly, Sierra Nevada's strong connections with travel and technology brands (e.g., Southwest Airlines, Discovery, SpaceX, Microsoft) highlight strong cointerest with these brands and present new cobranding opportunities that may not yet be known to its owners. We provide examples of both scenarios using information from external industry sources. Another practical application of our method is competitor analysis, which can help managers identify the differentiating connections of brands with respect to their competitors and gauge the type of users their competitors attract.Finally, we validate the findings of our model against external survey ratings and conduct extensive robustness checks, including network simulations, to ensure that our final network estimates are not biased by fake users or bots. Consistently high correlation between our automated approach and external survey ratings affirms the validity of our methodology for identifying cross-category brand–brand and brand–category connections. Overall, the core contribution of this study is a new digital approach to analyzing audiences' interests across a broad brand ecosystem. The cross-category insights generated by this approach can help researchers and practitioners identify nontraditional branding opportunities that are difficult to infer from traditional survey-based approaches. From a managerial perspective, our brand network can efficiently and cost-effectively generate cross-category insights, given that most of the data collection and network analyses are automated. Furthermore, as our approach uses information that is publicly available on social media, it is easily scalable to a large number of brands, with the resulting network structures reflecting the preferences of a diverse set of users. In the next section, we discuss relevant studies in the marketing literature and describe how our work contributes to the field. Conceptual Motivations Social Networks for Cobranding and ExtensionsResearchers regard cobranding as a source of competitive advantage that helps brands differentiate themselves, gain consumer trust, acquire new channels of distribution, and enter new markets ([47]). Brand extensions (i.e., leveraging the existing brand's name to enter into new categories) are another widely adopted strategy for firms entering new markets ([ 1]). Brand extensions provide greater quality assurance to customers who are familiar with the original brand, reduce the costs of distribution, and increase the efficiency of promotional expenditure ([ 1]). Both brand extensions and cobranding strengthen the focal brand and reinforce customers' value perceptions of the new product ([20]).Naturally, identifying underlying brand-to-brand connections on the basis of users' cointerests may be a key that enables brand managers to discover potential cobranding and extension opportunities. Most studies in this domain have used surveys ([ 5]). Although collecting input from prescreened participants is desirable, recruiting and maintaining a pool of such participants may be unfeasible due to cost or other constraints ([11]). Recent advances in social network analysis have enabled a wide range of scalable solutions that go beyond conventional market research methods. Although previous studies have considered the identification of brand-to-brand connections based on digital user traces, such research has been restricted to brand (or product) relationships within a single category. For example, [34] focus mainly on intracategory connections to create competitive market structures for car brands. Using survey approaches, [13] obtain intracategory maps for centrality and distinctiveness. Finally, [42] develop mapping methods to visualize large market structures within a single category (i.e., LED TVs).The marketing literature acknowledges the importance of cross-category brand connections for generating extensions, licensing, and cobranding deals ([22]; [41]). However, only limited empirical work has been done in this area. This article introduces an automated, scalable approach for identifying cross-category brand cointerest patterns by leveraging the cofollowership data on Twitter. The use of common followership data on Twitter in our analysis follows the recent work of Culotta and Cutler (2016). However, while [11] aimed to derive perceptual attribute ratings from Twitter followership data, the goal of this work is to investigate asymmetric cross-category brand transcendence over time. Further, unlike Culotta and Cutler, our large-scale network approach does not require any supervised knowledge on exemplars and uses categorical affiliations of brands to infer brand perceptions on transcendence and centrality. Specifically, the inclusion of network-derived measures enables us to study both within-category competition and across-category complementarity between brands. Table 1 presents the unique contribution of this research compared with previous network studies.GraphTable 1. Comparison of This Study with Previous Network Studies in Marketing. StudyFocal ContributionData SourceOutputAsymmetrySurvey ValidationAddresses Bots/Fake Users OnlineCompetitor AnalysisAnalysis Presented over TimeNetzer et al. (2012)Create competitive structure maps using text mining and network analysisComentions on online discussion forumMarket structures within a categoryNoYesNoNoNoCulotta and Cutler (2016)Propose a methodology for inferring attribute-specific brand perceptionsCofollowership data on TwitterPerceptual maps for a set of predefined exemplarsNoYesNoNoNoRingel and Skiera (2016)Develop mapping methods for visualizing complex market structuresConsideration sets from online searchMapping solutions for large complex market structures within a category of >1,000 productsYes, using conditional probabilityNoYes, eliminating implausible clickstreamsYesNoCurrent studyUse implicit brand networks to infer asymmetric cross-category brand connections over time.Cofollowership data on Twitter over timeCross-category connections maps at two levels: brand–category brand–brandYes, using conditional probabilityYesYes, using network rewiringYesYes  Cofollowership Patterns for Identifying Cobranding and Brand ExtensionsOur approach to identifying cobranding and brand extension opportunities harnesses the digital cofollowership patterns between brands. Survey research has shown that users follow brands on social media with the intention of purchasing a product or learning more about their favorite brands ([31]; [40]). Aspirations can also motivate consumers to follow brands ([ 3]). This digital user–brand relationship, which is established through followership activity, can be interpreted as an expression of affinity for the brand ([27]; [33]). Alternatively, this relationship can be viewed through the lens of homophily, meaning that people tend to associate with those who are similar to them in socially significant ways ([32]). This is further supported by consumer research studies ([ 6]; [10]), which show a strong relationship between a brand's image and characteristics and the identities of its followers.Similarly, [39] find that the composition of one's follower base represents the tastes (and likes) of their audience. Thus, the more network overlap (i.e., common followers) between two entities, the greater the similarity of tastes and interests among those entities' audiences ([39]). [ 2] find that common friends (i.e., common mutual followers) have a positive effect on the adoption of an application on Facebook. Similarly, we expect that brands that share a high number of followers on Twitter have a user composition that represents similar tastes or interests (e.g., the partnership between GoPro and Red Bull, which leveraged the shared affinities of their common followers: action, adventure, and fearlessness). Given that individuals primarily follow a brand because they like its products and that most followers are customers ([40]), brands having more common followers implies that their customers may have complementary consumption patterns (e.g., the Starbucks–Spotify partnership, which facilitated the complementary consumption of Spotify music and Starbucks coffee). Building on these theoretical principles, we study the cobranding candidates of a focal brand based on the extent to which its followers overlap with the followers of another given brand (and/or category) of interest. Brand Transcendence and CentralitySome of the greatest brands in the world have defied category norms and transcended their initial market boundaries ([23]). For example, in 1995, Amazon positioned itself as ""Earth's biggest bookstore,"" and its success with books enabled it to transcend its origins to become a leader in e-commerce. Although all brands theoretically operate within their categorical boundaries, such boundaries are often considered malleable ([ 7]). Important cobranding and extension opportunities can be missed if managers are not aware of connections that are relevant to brands in other categories ([ 5]). The brand network provides a solution to this problem by relying on a brand's social connections on Twitter to infer category-specific brand connections.At a high level, our proposed algorithm extracts the category-specific connections of a brand by exploiting the overlap in brand followers on Twitter. Whereas some brands may possess strong connections within their own category, others may have diverse connections across new categories. This article introduces a new construct, transcendence, which measures the extent to which a brand's followers overlap with those of other brands in a new category. The transcendence of a nonsports brand along any given category—for example, say sports—is based on the extent to which its followers overlap with those of other brands in the sports category. Further, to measure the connections not shared by the overall brand category, we calculate ""net transcendence"" as the deviation of a brand's own idiosyncratic connections from its category average. Net transcendence is more informative than raw transcendence because it ignores the cross-category connections that are generic to the category and identifies those that are intrinsic to the brand itself.Lastly, in addition to transcendence, there are brands that possess strong connections within their own category. These brands can be viewed as central. The concept of centrality (or typicality) bears direct relation to a brand's probability of recall, consideration, and choice among consumers' minds ([28]). Such central brands are those that come first to consumers' mind and serve as reference points in their categories ([13]). In the next section, we describe how the brand network is generated from followership patterns on Twitter and lay out important network details. The Network Mining MethodologyThe key contribution of this article is the introduction of an automated framework for inferring cross-category branding insights using implicit brand networks derived from social media. With their ability to provide a direct digital window into the interests of millions of social media users, implicit brand networks can help mangers identify nontraditional branding opportunities that would otherwise be hard to perceive. In this section, we generate implicit brand networks using brand communities on Twitter and outline important network details. DataDrawing from the notion that the social signal of ""who follows a brand"" provides a strong reflection of brand image ([11]), we use a set of 507 brands' Twitter accounts as the basis for our analysis. We select the most active Twitter brand accounts based on followership data from the social media directory FanPageList.com. We use Twitter's public application programming interface to collect the brands' lists of followers for 2017 and 2020. We manually verify that all Twitter handles correspond to the official brand account. Overall, the data set consists of brands from many major categories: airlines, luxury goods, retail, automotive, sports, technology, dining, food and beverages, lodging, media, travel, cruises, and beer. Each brand is assigned to a specific category based on the basic or superordinate category-level analyses ([28]).[ 6] To prevent bots or spam accounts from influencing the network analysis, all Twitter brand accounts included in the analysis are manually audited using the audience intelligence website SparkToro.[ 7] Furthermore, as we discuss in the section on robustness checks, we conduct network simulations to ensure that our final network estimates are not biased by such bots. Network GenerationThe next step is to extract the common followers between all brand pairs. The raw brand network is a weighted edge list, defined as 〈bi, bj, wij〉, where bi and bj are individual brands or nodes and wij is the common followers between those brands. If Fi and Fj represent the list of Twitter accounts following brands bi and bj, then an edge between two nodes is created if Fi ∩ Fj > 0. Alternatively, the weighted edge list can be represented as a weighted adjacency matrix Aij where Aij=wij0}ifbrandiandbrandjareconnectedotherwise. GraphOverall, we extract two brand networks: one for 2017 and one for 2020. The original brand networks are highly dense, with common followers between almost all pairs of brands. The numbers of common followers vary from a few hundred to more than a million users. Although it is possible to work with such dense networks, valuable information may be lost due to the redundancy generated by the large number of connections ([44]). Further, connections based on too few followers may not indicate significant connectivity. Given the wide heterogeneity in raw edge weights (i.e., the numbers of common followers), we next aim to extract the truly relevant brand–brand connections. Network filteringA common way to extract a relevant network structure is by applying a global threshold to remove the edges with weights below a particular cutoff. This, however, can destroy the multiscale properties of the brand network. Instead, we use a disparity filter ([44]), which is a filtering algorithm for multiscale networks, to obtain a reduced but more meaningful representation of the network. This method preserves the important edges present at all scales by locally identifying the statistically relevant weights at the node level. The statistically relevant edges (at a given significance level; e.g., α) represent a significant deviation from a null model of uniform randomness. Thus, smaller brands with fewer common followers are not ignored during the network reduction process. Although the current analysis focuses on Twitter brand accounts with between a few thousand and more than a million followers, this method could also be applied to smaller brands with fewer than 1,000 followers. Following [44], we use the commonly specified significance level α = .05 to extract the important connections in the brand network. The filtered networks for 2017 and 2020 consist of roughly 14,000 edges between brands. Although we use the disparity filter to obtain a filtered representation of the original network, there are alternative information-filtering algorithms available in the network science literature, including the global threshold and global statistical significance filters. In Web Appendix A, we revisit these alternative methods and discuss our rationale for choosing the disparity filter. Asymmetric normalizationBrand community sizes can vary both within and across categories. Brands with large brand communities (e.g., Chanel, Microsoft, Starbucks) tend to have more common followers than those with smaller communities. The normalization of edge weights is required to account for this variance. Thus, we use the conditional probability measure ([42]) to compute new network weights that not only normalize the effects of brand size but also account for asymmetry between brand pairs. Asymmetry between brand pairs may occur when the degree of connection between any two brands is unequal (i.e., the connection from A to B is not equal to the connection from B to A) ([15]). Ignoring the directionality of brand connections can lead to inaccurate estimates of consumer brand knowledge ([16]). We observe many cases of associative asymmetry in our brand network and use conditional probability to account for such scenarios. For instance, Figure 1 shows the cross-category connection between Starbucks and Stella Artois. A large percentage of Stella Artois fans are interested in Starbucks, and the outgoing directional strength is almost.20. However, fewer Starbucks fans are interested in Stella Artois, and the outgoing directional strength is comparatively much lower, at.0009. Incorporating directionality in the network reveals this crucial information, which is not visible in a simple, undirected, weighted network. Mathematically, the conditional probability measure calculates the normalized edges between any two brands A and B as P(A∩B)=|A∩B||A|, Graphwhere the numerator  |A∩B|  is the number of common followers between brands A and B and the denominator  |A|  is the number of followers of the focal brand.Graph: Figure 1. Calculating asymmetry between brand pairs.Figure 2 shows the entire brand network structure for 2020 using the dimensional reduction algorithm, t-distributed stochastic neighbor embedding (t-SNE). The t-SNE algorithm yields a two-dimensional undirected representation of the brand network, with the distance between brands in the t-SNE space being proportional to the mean conditional probabilities between brands. The colors of the brands correspond to their category affiliation. Interestingly, while most automotive brands in Figure 2 are well-distanced from other nonautomotive brands, Tesla is positioned in the technology category. A similar pattern is observed for certain retail brands such as Adidas and Reebok, which are closer to the sports group than to other brands in their category. Individual brand constructs on transcendence, as described in the next section, help reveal specific cross-category connections for a given brand.MAP: Figure 2. t-SNE map of the undirected brand network.[ 8] Measures of Brand Transcendence and CentralityThis section outlines the process of identifying centrality and transcendence by exploiting the connections of a brand within the network. For any given brand, the first step is to disentangle its connections across the main categories: airlines, luxury goods, retail, automotive, sports, technology, dining, food and beverages, lodging, media, travel, cruises, and beer. The algorithm then computes the weighted out-degree centrality of a brand across these categories. In weighted networks, out-degree centrality or node strength is commonly calculated as the sum of weights emanating from a focal node to all its connections ([ 4]). However, to account for the strength of edge weights and the number of connections of a focal node, [37] propose a new measure of weighted degree centrality: Ci=di(1−α)×wiα, Graph( 1)where  di  is the degree of the focal node (i.e., the number of connections),  wi  is the node strength (i.e., the sum of the weighted connections), and  α  is the tuning parameter from 0 to 1. For example, following [37], the transcendence of a given nonsports brand into the sports category is calculated as the function of its number and strength of outgoing connections to all other brands in the sports category.More formally, the set of brands in the network can be represented as B, where any individual brand  b∈B  . Brand categories, G, are subsets of B, such as G ⊆ B.[ 9] The transcendence of focal brand b onto a new category G is evaluated as tbG=∑k∈Gdb,k(1−α)×∑k∈Gwb,kα|G|, Graph( 2)where  ∑k∈Gdb,k  is the number of outgoing edges from brand b to all k brands in category G,  ∑k∈Gwb,k  is the sum of weighted edges from brand b to all k brands in category G, and  |G|  gives the number of brands in category G. Dividing by the total number of brands in a category,  |G|  , helps ensure that large categories with many brands do not dominate the analysis. Following [37], we set  α  to.5 to place equal importance on a brand's number of connections and the weight of those connections.Furthermore, considering that brands whose followers tend to follow many other brands may inflate the network constructs, we divide our transcendence construct by the total degree of a brand in the network (i.e., its number of connections to other brands). Intuitively, brands whose followers tend to follow many brands have a higher degree than brands whose followers follow fewer brands. Thus, in the transcendence construct, the sum of the numerator increases with each new connection of a brand in the network. With the objective of normalizing for brands that inherently have higher aggregate transcendence due to higher degree in the network, we divide our final transcendence construct by the total degree of a brand. Thus, the final transcendence construct becomes tbG=∑k∈Gdb,k(1−α)×∑k∈Gwb,kα|G|×db, Graph( 3)where the additional variable in the denominator,  db,  is the total number of connections of a brand in the network.Furthermore, in our transcendence construct, brand b's connections within its own category  Gp  relate to its centrality (  tbGp  ): the higher the strength of these connections, the more central the brand is in its own category. As noted previously, the notion of centrality is directly related to a brand's probability of being recalled, considered, and chosen by consumers ([28]). We multiply the centrality construct by the size of the focal brand's community (i.e., its number of followers) to account for the brand's popularity among users. Thus, the final centrality construct is tbGp×fbmax(fbi∈Gp), Graph( 4)where  tbGp  is brand b's connections within its own category  Gp  and  fb  is its number of followers. Given that the values of  fb  vary from a few hundred to more than a million users, we use its scaled value. Given a set of nonoverlapping categories G1, G2, ..., Gp, the transcendence of brand b across p categories is a 1 × p-dimensional vector: tb=[tbG1tbG2tbG3⋯tbGp]. Graph( 5)The transcendence vector of a brand can also be analyzed with respect to its competitors in the category. The 1 × p-dimensional vector  tb  can be further extended into an n × p matrix, where n rows represent brands (i.e.,  b1,b2...,bn  ) and p columns represent transcendence across the p categories, as shown in Figure 3.Graph: Figure 3. The transcendence matrix, tbG for n brands across p categories.The average category connections (i.e., connections emanating from one category  Gi  to another category  Gp  ) are then calculated as follows: tGiGp=∑b∈GitbGp|Gi|, Graph( 6)where  i≠p  . This formula measures the average transcendence of brands in one category (for example,  Gi  ) into another category  Gp.  To separate a brand's unique connections (  tbGp  ) from its category average (  tGiGp  ), we calculate the net transcendence of brand b into category  Gp  as follows: t_netbGp=tbGp−tGiGp, Graph( 7)where  b∈Gi  and  i≠p  . A positive value for  t_netbGp  indicates that the brand's transcendence is above the category average, while a negative value indicates that its transcendence is below average. As in the raw transcendence vector  tbGp  , the net transcendence vector of a brand b across p categories is t_netb=[t_netbG1t_netbG2t_netbG3...t_netbGp]. Graph( 8)The 1 × p-dimensional vector  t_netb  can be further extended into an n × p matrix, where rows represent n brands in a category and p columns represent the net transcendence of brands across the p categories, as shown in Figure 4.Graph: Figure 4. Net transcendence matrix of n brands across p categories.Figure 4 provides a more comprehensive view of the competitive landscape of a particular brand by highlighting its cross-category connections as well as those of its competitors. In the next section, we discuss the results of these analyses and identify their key managerial implications. Results and Managerial ImplicationsDepending on the business objective, the category-specific connections generated by the brand network can be visualized on two different levels: brand–category and brand–brand. Using examples from the automotive and beer categories, in this section we present our results on these two levels and discuss the managerial implications of our findings. First, we note the brand–category connections of automotive brands and identify the categories suitable for brand extensions. Second, we focus on brand–brand connections and discuss how asymmetry can be leveraged to attain more nuanced insights into the expected benefits of cobranding. Third, we highlight the network's ability to capture changes that occurred between 2017 and 2020, given that individual brand–brand connections may change over time. Finally, we discuss how the brand network described in this article can help managers identify the differentiating category-specific connections of brands with respect to their competitors and gauge the type of users their competitors attract. Brand–Category Connections: Transcendence and CentralityTo identify the brand–category connections of automotive brands in 2020, we study the net transcendence matrix,  t_netbGp  , as shown in Figure 5. All column values have been scaled, with positive values shown in red coloring and negative values shown in blue on the heatmap. For every brand (n)–category (p) relationship in the heatmap, values closer to dark red indicate a stronger perceived relationship between a brand and category. The stronger the relationship between the brand and category, the greater the user cointerest between that brand and category. As discussed previously, the cointerest patterns captured through the analysis of common followership do not necessarily guarantee brand fit, which is typically based on the similarity of brand personality dimensions ([48]). Instead, values in the transcendence matrix reveal cointerest patterns that enable managers to explore potential extension and cobranding opportunities that are difficult to infer from traditional survey-based approaches. For instance, the audience of the car brand Mercedes has strong cointerest with the luxury, technology, retail, and sports categories, suggesting that extensions may be possible in these categories.Graph: Figure 5. Net transcendence matrix, t_netbG, reflecting brand–category connections of the automotive brands (2020).Similarly, there may be brands that, despite having low net transcendence into different categories, have strong connections with their group, making them central in their own category. For example, Toyota and Dodge are highly central to the automotive category, although their net transcendence across categories is low. Tesla, in contrast, has high net transcendence into the technology category, despite having low centrality in the automotive group. We also observe that some car brands with high net transcendence across multiple categories have moderately low centrality in their group (e.g., Audi, Mercedes, Tesla, and Lamborghini). However, brands such as Chevrolet and Ford share strong cointerest in the automotive category and also have moderate net transcendence into beer, dining, and sports. Thus, centrality and transcendence are not mutually exclusive, and a brand may be perceived as both central and transcendent, depending on its connections in the network. Brand–Brand Connections: Leveraging Asymmetry for Cobranding InsightsThe brand network developed in this study can also help managers obtain a more granular view of transcendence by identifying brand–brand connections across categories. The different levels of analysis (i.e., brand–category and brand–brand) offered by the network can help managers understand why certain cobranding opportunities are more promising than others. Further, the network's analysis of the asymmetry between brand pairs can help identify brands which may potentially benefit more from a cobranding alliance. To identify strong, relevant cobranding candidates for a focal brand, we only consider brand–brand connections in categories where the net transcendence of the brand is positive. This ensures that the identified brand connection is not generic to the category but rather is intrinsic to the brand itself. For instance, the net transcendence of Mercedes into the luxury goods and retail categories is positive, meaning that, on average, its connections with luxury and retail goods are relatively higher than those of other car brands. Thus, for Mercedes, brands in the luxury and retail categories are considered suitable candidates for cobranding.Figure 6 shows the brand–brand connections of Mercedes with brands in the luxury and retail categories. Some of Mercedes' strongest connections are with Louis Vuitton, Nike, Tissot, and Chanel, which highlights these brands' potential for alliances with Mercedes. However, given the asymmetrical nature of these relationships, the benefits gained through such alliances may not always be equal. For instance, the asymmetrical connection between Mercedes and Tissot reflects that a greater proportion of Tissot's audience is interested in Mercedes than vice versa. This indicates that there may be a greater potential benefit for Tissot from such an alliance. These results on asymmetry can provide additional insights to brand managers of both Mercedes and Tissot when evaluating potential cobranding candidates.Graph: Figure 6. Top 20 brand–brand connections of Mercedes with brands in the luxury and retail categories using the Fruchterman–Reingold (1991) layout.Indeed, prior strategic alliance literature suggests that unequal spillover benefits can be expected from asymmetrical brand alliances ([21]). However, the main findings of [21] suggest that even though the magnitude of financial gains in asymmetrical alliances is not equal, it is not a win–lose partnership but rather a win–win or a shareholder value-adding alliance for both the larger and smaller partner firms.[10] Similarly, in the case of Mercedes, even though the expected benefits may not be equal for the asymmetrical relationships (e.g., Mercedes–Chanel, Mercedes–Tissot, Mercedes–Rolex), future deals can be still beneficial to both brands. Although the smaller brand, Tissot, may achieve greater gains from this asymmetrical alliance (e.g., by having a greater proportion of its audience interested in Mercedes), the larger brand, Mercedes, may still gain access to a niche audience that may not be a part of its current demographic. Capturing Shifts in TranscendenceA brand's transcendence in the network, which arises from common consumer interest, may not be static. Brands may, for various reasons, wish to shift their transcendence to new categories in search of new alliances or cobranding opportunities. Our network can track such shifts in transcendence, allowing brand managers to better assess the effectiveness of their marketing actions and identify emerging or waning categories for future brand alliances.Figure 7 shows the change in net transcendence of car brands into the technology category. The dynamic plots for other categories can be analyzed similarly. Interestingly, between 2017 and 2020, the technology connections of many car brands, including Honda, Jeep, Chrysler, Acura, and Chevrolet, decrease. However, some brands, including Tesla, Lamborghini, and Infiniti, show a steep rise in their connections with technology. This may be related to, for example, Infiniti's plans to go all-electric in 2021 and use intelligent technologies to reflect its new ethos ([30]). Thus, our brand network-based methodology can be used to assess the effectiveness of a brand's marketing campaign and showcase how marketing actions can impact the brand's transcendence in users' minds.Graph: Figure 7. Change in net transcendence over time.The network's ability to highlight shifts in brand transcendence over time can be of vital use to managers. The emergence of new connections with specific categories over time (e.g., Infiniti's increasing transcendence into the technology category) provides insight into the effectiveness of brands' marketing campaigns and affirms the possibility of future extensions in those categories. Similarly, the waning of a brand's connections with specific categories (e.g., Jeep's decreasing transcendence into the technology category) allows that company to identify potential problems and take appropriate action to address them. The brand network can also help managers investigate issues in more detail by uncovering the specific cross-category brand–brand connections that have diminished over time. Change in a brand's net transcendence into a category can be caused by several factors, including joint ads, new alliances, embedded promotions, or other external events. Although the method in this study does not examine the causes for such shifts, it provides managers with timely intelligence on the subject. Future marketing studies could build on this work to further investigate the causes for changes in brand transcendence over time. Competitor AnalysisThis subsection discusses how the category-specific brand connections revealed through the transcendence matrix may not only allow managers to understand the position of their brands in consumers' minds but also help distinguish them from their competitors. Figure 8 shows the net transcendence,  t_netbGp  , of two beer brands, Bud Light and Sierra Nevada, into different categories. Whereas Bud Light has high transcendence into food and dining, Sierra Nevada has high transcendence into travel, airlines, and technology. Regarding centrality, Bud Light outperforms Sierra Nevada, with stronger connections within the beer category. Thus, whereas the former brand is positioned strongly among beer and food enthusiasts, the latter brand resonates more with technology and travel enthusiasts. This type of analysis can help brand managers identify the differentiating connections of their brands with respect to their competitors and also gauge the type of users their competitors attract.Graph: Figure 8. Net transcendence matrix, t_netbG, of Bud Light and Sierra Nevada.Brand managers can obtain richer insights into the cointerest patterns of their competing brands by examining the individual brand–brand connections in the different categories. For example, Bud Light is connected to more food, beverage, and dining brands (e.g., Pepsi, Coca-Cola, McDonald's, Subway, Taco Bell), while Sierra Nevada is mostly connected to airline, travel, and technology brands (e.g., Southwest Airlines, Discovery, SpaceX, Amazon, Netflix). As one might expect, some brand–brand connections can reflect previous marketing activities (e.g., joint advertisements or promotions, collaborations, licensing deals). In such cases, our brand network–based methodology enables managers to measure the effectiveness of a marketing campaign and showcases how marketing actions can impact a brand's transcendence. For example, Bud Light's connection with Pepsi reflects strong cofollowership patterns between the two brands, affirming the effectiveness of their earlier joint marketing campaign.[11] Alternatively, brand–brand connections can highlight potential new cobranding or alliance opportunities that were previously unknown to brand managers. For instance, Bud Light's strong connections with McDonald's and Taco Bell highlight strong cointerest between these brands, suggesting untapped cobranding opportunities. Similarly, Sierra Nevada could leverage the technology and travel interests of its fans, as revealed by the network, to partner with relevant travel brands such as Southwest Airlines, SpaceX, Discovery, Amazon, and Netflix. In the next section, we validate our results against external survey ratings and test the reliability of our findings. Survey ValidationTo validate the effectiveness of our methodology, we compare the network ratings from our automated approach with directly elicited survey ratings. The survey was conducted through Amazon Mechanical Turk, which is a reliable source for conducting social sciences research ([11]). The survey respondents were asked to report their income, age, and gender to account for any demographic influence in the sample. The participants were required to be located in United States and be over 18 years old. To ensure high-quality responses, a prior task approval rate of 95% was required for all survey respondents. The brands were grouped by sector, and four separate surveys, consisting of 250 participants each, were conducted to validate the brand–category and brand–brand connections of beer and automotive brands. Next, we discuss our survey findings along with several robustness checks. Validating Brand–Brand ConnectionsIn this subsection, we examine whether the cobranding candidates identified by the network are also perceived by consumers to be such candidates. The network edge weights between brands are intended to reflect consumers' perceptions of the brands that could be paired for cobranding; thus, such a relationship should be reflected in the survey responses. For this validation, we select the five most-followed brands in the beer and automotive categories. Then, for each brand, we select nine cobranding candidates: ( 1) the top three cross-category cobranding opportunities (i.e., brands), as identified by the network, ( 2) the top three most-followed brands in the sample that are not included in part 1, and ( 3) three randomly drawn brands that are not included in parts 1 and 2.For each focal brand, we ask the respondents to rate its cobranding candidates on a scale of 1 (""less likely to go together"") to 10 (""highly likely to go together"") according to how strongly they can be paired with the focal brand. The survey scores were then correlated with the outgoing edge weights from the focal brands to their cobranding candidates in the brand network. For every survey question, the brand order was randomized, and attention filters were included to identify invalid responses. To identify loyal fans, participants were separately requested to select their favorite auto and beer brands from the list. Details of the survey and the corresponding descriptive statistics are included in Web Appendix B. ResultsTable 2 shows the Pearson correlation coefficients between the survey and network scores. Overall, the survey measures correlate well with the network estimates, with the survey's top-three-box score[12] achieving an average correlation of.67 with the network estimates. The overall correlation between average survey ratings and network constructs is.65.GraphTable 2. Pearson Correlation Coefficients of the Survey Estimates with the Network Constructs. CategoryBrandsr (Mean)r (Top Three Box)AutomotiveTesla.89.88Mercedes-Benz.63.54BMW.76.76Audi.93.93Ford.62.52BeerMiller Lite.64.66Sierra Nevada.53.55Bud Light.56.51Budweiser.53.55Coors Light.57.57Average correlation coefficient (r).67.65 In addition, we compute the correlations between network scores and the survey ratings of users who rate a specific brand as their favorite. When using only data from fans, the overall correlation coefficient increases to.70 for average survey ratings and.71 for the top-three-box survey ratings. Finally, we examine the scatter plots more closely to better understand the circumstances in which the network cobranding candidates align well with the survey responses. For example, Figure 9, Panel A, shows the cobranding candidates for Audi as suggested by the network, together with the corresponding survey ratings. The top three cobranding candidates suggested by the network (i.e., Microsoft, Nike, and Intel) also receive very high ratings from the survey respondents. Brands with low network connectivity with Audi (i.e., Lays, Forever21, and ABC) also receive lower ratings from the survey respondents. These results reaffirm the previous findings that network connectivity patterns between brands are useful marketing metrics for consumers' perceptions of which brands are likely to pair well together.Graph: Figure 9. Network versus survey estimates for brand–brand connections of Audi and Budweiser.Figure 9, Panel B, shows the cobranding candidates for Budweiser, as suggested by the network and survey responses. Some top cobranding candidates suggested by the network, including the NFL and Pepsi, also receive high ratings from the survey respondents. However, despite its strong connection to Budweiser in the network, Starbucks receives low ratings from the survey respondents. Although the strong network connectivity between Starbucks and Budweiser is not directly perceived by survey responders, it may indirectly reflect the complementary taste interests of coffee and beer drinkers, which Starbucks previously leveraged to launch a line of beer-like coffee drinks ([26]; [38]). Similarly, Budweiser has a lower network connectivity score with the soccer brand FIFA than the corresponding survey rating. On further investigation, whereas Budweiser's U.S. Twitter account, which was used in our survey validation, has low network connectivity with FIFA, the brand's global Twitter account is very strongly connected to FIFA. This suggests that brand managers should apply domain knowledge and managerial judgment to explore alternative cobranding candidates based on their market of interest. Using domain customization, brand managers can query the brand network to include brand accounts that best suit their target market and conduct a more tailored network analysis to identify where the proposed cobranding opportunity may work well.The preceding analysis also highlights some of the limitations of survey-based validation. First, consumer responses to direct questions on brand perceptions are based on the respondents' existing notions of brand extendibility and may be confounded by prior user experiences ([25]). Second, asking consumers about their overall perceptions using direct rating scales may not reveal novel or unique brand extensions or cobranding ideas; rather, it may simply facilitate the testing of known concepts ([ 5]). The brand network, in contrast, leverages the cofollowership patterns of millions of Twitter users across a broad brand ecosystem to reveal cross-category cobranding ideas that may not be intuitive to consumers but, in hindsight, are effective. Overall, with an average correlation of.71 between network scores and fans' survey ratings, the validation results suggest that the automated network-based approach enables managers to quickly and inexpensively identify cobranding and brand extension ideas that would otherwise be difficult to anticipate. Validating Brand–Category TranscendenceNext, we validate whether the transcendence measures derived from the brand network align with consumers' perceptions of the brands. For both beer and automotive brands, consumer ratings along the luxury and technology categories were elicited. On a scale of 1 (""least likely"") to 5 (""most likely""), participants were asked to rate the focal brands (e.g., Heineken) according to how strongly they associated them with a new category (i.e., luxury goods and technology). Further, to identify brands with strong centrality within their own group, participants were asked to rate the focal brands, on a scale of 1 (""least likely"") to 5 (""most likely""), according to how strongly they believed them to be central in their own category. Finally, the average survey ratings for each brand across the luxury and technology categories are compared with the brand transcendence constructs obtained using the network measures. Similarly, the average survey rating for centrality is compared with the centrality construct obtained using the network measures. We also calculate the top-two-box score for each brand to assess the proportion of people who rate a brand very highly (i.e., a score of 4 or 5). Details of the survey and the corresponding descriptive statistics are included in Web Appendix B. ResultsThe Pearson correlation coefficients between the survey results and network constructs are listed in Table 3. Overall, the survey measures correlate well with the network estimates, with the top-two-box survey scores and mean survey scores achieving an average correlation of.63 with the network estimates.GraphTable 3. Pearson Correlation Coefficients of the Survey Estimates with the Network Constructs. CategoryConstructsr (Mean)r (Top Two Box)AutomotiveTranscendence (luxury).59.61Transcendence (technology).66.67Centrality.58.61BeerTranscendence (luxury).52.53Transcendence (technology).72.69Centrality.71.70Average correlation coefficient (r).63.63 Figure 10 includes the scatter plots for survey versus network measures. Given that the network and survey estimates are measured in different units, the plots have been scaled to 0–1 for easier interpretation. Overall, the scatter points are well distributed along the best fit line, with few outliers. As we discuss next, we then conduct a series of tests to ensure that the network accurately captures the shifts in connections over time. Overall, our general findings pass these tests, supporting the future use of implicit brand networks in marketing research.Graph: Figure 10. Scatter plots of network versus survey estimates for automotive and beer brands. Shift in connections from 2017 to 2020The ""Results"" subsection discusses the brand network's ability to capture shifts in brand transcendence over time. We now test whether the waning of certain connections between 2017 and 2020 in the network is supported by the survey responses. To do this, we first identify the connections between brands and categories that exist in the 2017 network but decline in the 2020 network. Figure 11 illustrates the filtered cases for automotive brands and the corresponding survey results. Panel A shows that brands such as Mazda, Mini, Buick, Chrysler, and GMC all have connections with the luxury category in 2017. According to [17] in Forbes, at the time, the Mazda 2017 CX-9 Signature model was considered the most luxurious vehicle produced by Mazda to date. The author mentions that ""Mazda has never been considered a luxury brand, but maybe it's time to reconsider that classification"" ([17]). However, the results from the brand network in 2020 show that Mazda does not retain its connection with luxury. This is further validated by the survey participants, who also rate Mazda as very weakly associated with luxury. There is a similar pattern in Panel B, in which brands such as Dodge, Chevrolet, Jeep, Honda, and Chrysler show a significant drop in their connections with technology category between 2017 and 2020. This change is also reflected in the survey responses.Graph: Figure 11. Shift in transcendence of car brands from 2017 to 2020. Random connections rejected by survey participantsNext, we test whether survey respondents also reject random connections that do not exist in either 2017 or 2020. To do so, we filter the cases where connections between brands and categories are absent in both the 2017 and 2020 networks and compare them with the survey responses.[13] We find that the average survey ratings are below 2 (out of 5) for most brand–category connections not existing in either network (i.e., 2017 and 2020). Addressing demographic bias on TwitterStudies that mine brand perceptions from social media sources must consider the extent to which brand followers on social media represent the general population. It is also important to consider whether certain Twitter brands accounts are more appealing to a specific audience (e.g., young people, men). Recent studies have reported that Twitter followership data successfully captures attribute-specific consumer perceptions beyond demographic similarities ([11]). We investigate this issue further by comparing the survey ratings, which were provided by users of different demographics, with the transcendence values obtained from the brand network. Table 4 lists the correlation values between the network estimates and income-specific survey ratings for the transcendence of automotive brands into the technology category. For most income groups in the survey, there are adequately high correlations with the network transcendence constructs. Results for all the remaining survey demographic groups (i.e., age and gender) are included in Web Appendix D. We observe adequately high correlations between the demographic-specific survey ratings and the brand network constructs. This affirms that the overall brand network estimates are not heavily influenced by the demographics of Twitter users.GraphTable 4. Income-Specific Survey Correlations with the Network Constructs. Automotive Brands' Transcendence to TechnologyIncomeSurvey-Based Measure (≤$29,999)Survey-Based Measure ($30,000–$59,999)Survey-Based Measure ($60,000–$99,999)Survey-Based Measure ($100,000–$149,999)Survey-Based Measure (≥$150,000)Survey-based measure (≤$29,999)1.00Survey-based measure ($30,000–$59,999).961.00Survey-based measure ($60,000–$99,999).97.981.00Survey-based measure ($100,000–$149,999).93.96.941.00Survey-based measure (≥$150,000).93.91.94.881.00Network-based measure.67.62.72.59.74  Sensitivity of transcendence constructs to network rewiringThe presence of Twitter bots may inflate the number of common followers between brands, which can, in turn, lead to inaccurate network estimates of transcendence and centrality. In this section, we conduct multiple network simulations by repeatedly rewiring the edges to test whether the original brand network structure remains reasonably stable. We incrementally rewire the cofollowers from any random pair of edges and reperform the entire analysis. As Figure 12 shows, the rewiring stage involves the addition and removal of 5% of the cofollowers of any random pair of edges in the network, continuing until 50% of the cofollowership patterns have been altered. In each iteration, once the network rewiring is complete, the algorithm reruns the entire analysis (i.e., it applies the disparity filter to identify statistically significant edges, normalizes the edge weights, and calculates the transcendence across categories).Graph: Figure 12. Process flow for each iteration.Figure 13 illustrates the results of the simulations for the automotive brands and compares the transcendence values obtained after rewiring the network with the original values. The purpose of the test is to ensure that the original network estimates hold for small rewiring changes (i.e., that significant rewiring is needed to yield completely different network estimates). For all plots, the rewired network estimates correlate highly with the original estimates until a large percentage of the network (>30%) is rewired. This demonstrates that the brand network structure is not sensitive to small underlying changes that may occur due to bots.Graph: Figure 13. Correlation of post rewiring transcendence values with original transcendence values for automotive brands. ConclusionDespite its relevance to various marketing decisions (such as cobranding and brand extensions), the identification of cross-category insights across a broad brand ecosystem is currently understudied in the marketing literature. This article uses implicit brand networks to identify the category-specific connections of brands and their competitors by exploiting the overlap in brand followers on Twitter. We introduce a new construct, transcendence, that measures the extent to which a brand shares cointerest with other brands in different categories. Depending on a firm's marketing objectives (i.e., their focus on extensions vs. cobranding), the transcendence of a brand can be studied at different levels: brand–category or brand–brand. These different levels of analysis can help managers identify viable cobranding opportunities.Furthermore, we leverage the concept of asymmetry between brand pairs to provide more nuanced insights into possible cobranding opportunities and determine which brand can potentially benefit more from a cobranding alliance. We conducted the analysis over time to track shifts in brand transcendence, allowing brand managers to both assess the effectiveness of existing marketing strategies and identify new alliance opportunities. To ensure the reliability of our proposed methodology, we validate our findings against external survey ratings and conduct extensive robustness checks, including network simulations, to ensure that our final network estimates are not biased by Twitter bots.From a methodological standpoint, the implicit brand networks utilized in this article condense the high-dimensional interest space of millions of brand followers into a parsimonious form that is more amenable to research and business applications. The readily accessible artifact, which is obtained with little human intervention in the processing of the underlying data, allows managers to efficiently infer cross-category branding insights in a scalable way. Compared with extant digital approaches that rely on extensive preprocessing, this straightforward automated approach enables practitioners to readily obtain the cointerest patterns of brands with respect to their competitors and gauge the types of users that their competitors attract. More specifically, given its automated data collection and network analyses, the brand network can act as an effective business intelligence tool for the identification of cobranding and extension opportunities across a broad ecosystem of brands.Overall, our approach offers several benefits to marketers. It also highlights avenues for future research. First, although our analyses use Twitter brand communities, it would be interesting to compare similar communities on Facebook and Instagram. Brand networks on different social media platforms may vary based on factors such as user demographics, category, platform characteristics, or a brand's marketing strategy. Although consistent brand connections across different platforms can provide additional validity to findings of this study, meaningful insights may also be gleaned if substantial differences are observed. Such differences may, for example, stem from a brand's tailored marketing efforts on a specific platform. Using brand networks to track the effectiveness of such efforts can be beneficial to brand owners. Differing user demographics across platforms may also have an impact on brand network structures. Though this study did not identify substantial differences between the demographic-specific survey ratings and the transcendence values obtained from the brand network, future research could examine platform-specific brand networks to obtain richer insights. Second, future research could consider how to distinguish the content on brand pages that may affect consumers' decisions to follow brands, including promoted content on a brand's page, multichannel advertising across platforms (e.g., email, Facebook), and the use of trending topics or sponsored tweets.Third, the analysis in this article relies on a brand's followers at a given point in time. Twitter does not provide data on when a user starts or stops following an account. The article's analysis of two different periods highlights the potential for our method to examine how transcendence changes over time. Because most aspects of the data collection and network analyses in this approach can be automated, brand managers could collect followership information at more regular intervals to examine changes in transcendence more frequently. Fourth, while our study relies on validation from two categories, future studies can consider expanding the survey-based validation for broader set of brands across multiple categories. However, for such validation, it is important to consider that certain cross-category cobranding candidates, revealed through the cofollowership patterns on social media, may not always be intuitive to survey respondents, though in hindsight they make sense and work. In conjunction with the brand network results, marketers should apply domain knowledge and managerial judgment to explore different extension and cobranding opportunities that may work best for the brand.Fifth, it is also important for brand managers to consider that Twitter users from around the globe are free to follow any account(s) of a brand (which can include global or country-specific accounts). As the data collection and network analyses can be largely automated, marketers can create custom brand networks to include Twitter brand accounts (country-specific and/or global) that best suit their market of interest. Domain customization can help managers conduct a more targeted network analysis of where the proposed cobranding opportunity may work best. Lastly, although our approach relies on cofollowership patterns to identify cobranding opportunities, we do not investigate the drivers of common followership on Twitter and the extent to which these drivers lead to network overlap between brands. The reasons that users cofollow brands on Twitter are varied and complex, with many unobservable factors possibly at play. Industry research by Nielsen ([29]) indicates that 55% of Twitter users say they follow a brand because they like it, followed by 52% of users who want to keep up-to-date on the latest promotions and offers posted by the brand. There are various other reasons that users cofollow multiple brands on social media; thus, future research could use the brand network described in this article to investigate and better understand the drivers of cofollowership between brands on social media.Overall, this work offers a new approach for researchers and practitioners interested in automatically monitoring cross-category brand connections over time. Network-based methods for brand management are relatively new and present many opportunities for future research. The methods introduced in this article provide a foundation for marketing researchers interested in leveraging implicit brand networks to gain richer insights into consumers and brands. Supplemental Materialsj-pdf-1-jmx-10.1177_00222429221083668 - Supplemental material for Leveraging Cofollowership Patterns on Social Media to Identify Brand Alliance OpportunitiesSupplemental material, sj-pdf-1-jmx-10.1177_00222429221083668 for Leveraging Cofollowership Patterns on Social Media to Identify Brand Alliance Opportunities by Pankhuri Malhotra and Siddhartha Bhattacharyya in Journal of Marketing  "
13,"Leveraging Cofollowership Patterns on Social Media to Identify Brand Alliance Opportunities The use of cobranding and brand extension strategies to access new markets has been a topic of significant interest. However, surprisingly few studies have examined cross-category connections of brands using publicly available digital footprints. In this study, the authors introduce a new, scalable automated approach for identifying potential cobranding and brand extension opportunities using brand networks derived from publicly available Twitter followership data. The digital user–brand relationship, established through followership activity, is regarded as an expression of interest toward the brand. Common followership patterns between brands are then extracted to capture cointerest between those brands' audience. By utilizing the cointerest patterns, the approach aims to derive cross-category brand–brand and brand–category connections, which can serve as important measures for assessing cobranding and extensions opportunities. This article introduces a new construct, transcendence, which measures the extent to which a brand's followers overlap with those of other brands in a new category. The analysis is conducted at different points in time to help managers track shifts in brand transcendence.Keywords: cross-category; brand networks; asymmetry; cobranding; brand extensions; social media; social networks analysis; TwitterCobranding is a brand alliance strategy to bolster reach, awareness, and sales potential by tapping the prospective customers of partnering brands. Many types of cobranding schemes exist in the marketplace, including joint advertising campaigns (e.g., ads depicting the joint consumption of Coca-Cola and McDonald's), cause–brand alliances (e.g., UNICEF and Target), bundling (e.g., streaming deals that include joint Hulu and Spotify subscriptions), and cobranded products (e.g., Louis Vuitton launching an exclusive luggage line for BMW). Cobranding strategies enable brand extensions, with managers leveraging the existing brand names of their partners to enter new markets and categories ([14]). Cobranding is increasingly viewed as a valuable marketing strategy and has been shown to increase awareness, quality, market value, and brand equity ([ 8]; [46]). Although marketers have been leveraging the synergistic benefits of cobranding for decades, surprisingly little empirical research has tried to identify potential cobranding alliances using modern digital approaches. Most of the existing empirical research either uses observations from fast-moving consumer goods categories ([ 8]; [14]; [19]) or conducts analyses within a single category, such as camcorders in [24], car brands in [34], and LED TVs in [42]. Similarly, [36] and [35] use recommendation hyperlinks between Amazon web pages to create a large-scale network of books and demonstrate the value of shared purchasing patterns.Obtaining broader insights into the identification of cobranding opportunities across diverse categories would generate relevant and meaningful information for brand owners. As [43] notes, ""By mashing up two bona fide brands, especially in diverse industries, the impact can be exponential."" For instance, a well-known cobranding deal between Starbucks and Spotify—two seemingly unrelated brands—enabled both brands to cross-promote their products and grow their customer base. By providing premium coffee-shop music, Starbucks incentivized Spotify users to join its loyalty program. In return, Spotify grew its user base through Starbucks' offer of a free coffee upon joining. Having knowledge about relevant cross-category brand connections is crucial to brand owners ([12]); however, there is little or no research on identifying these broader cross-category effects using current digital approaches.This article introduces a new, scalable approach for generating cross-category branding insights using implicit brand networks on social media. The cross-category branding insights are revealed in the form of brand–brand and brand–category connections, which can serve as important measures for assessing cobranding and extensions opportunities. Unlike traditional social networks, which involve explicit interaction between the participating entities,[ 5] edges within a brand network are implicit (or tacit) and arise due to common followership between brands. [45] note the relevance of these tacit connections to decision making. This idea has been studied previously within the domain of collaborative filtering ([45]). Implicit networks, which condense the vast digital interest space of millions of users into a parsimonious form, provide direct insight into the digital ecosystem and are the subject of increasing research attention across domains ([45]). In this study, the cross-category connections of a focal brand in the implicit network are leveraged to help brand managers identify cobranding and extension opportunities.The article introduces a new construct, called ""brand transcendence,"" which is defined in the context of a large ecosystem of brands belonging to different categories. The transcendence of a brand into a new category is the extent to which its followers overlap with those of other brands in the new category. From a managerial perspective, this study provides an automated approach for identifying cross-category cobranding opportunities based on user cointerest, which is measured through overlap of brands' followers on Twitter. Importantly, the cointerest patterns captured through common followership do not necessarily reflect overlapping brand associations or guarantee brand fit, which is traditionally measured using the similarity of brand personality dimensions ([48]). However, such patterns are indicative of common tastes or interests among social media users. Following [39], we consider that the composition of a brand's follower base represents the tastes (and likes) of its audience. Thus, the greater the network overlap between two brands, the greater the similarity in tastes and interests between those brands' audiences. Taking these principles together, we study the transcendence of a brand into a new category based on the extent to which its followers overlap with those of other brands in the new category. Our approach also identifies central brands that have strong and consistent connections within their own category ([ 9]), with ""centrality"" being defined as the extent to which a brand's followers overlap with other brands in its own category.By incorporating directionality into the network edges, we also capture the asymmetric relationships between brand pairs, which help identify brands that may potentially benefit more from a cobranding alliance. We outline how cross-category connections can provide both brand–category and brand–brand insights, depending on a brand's marketing goals (i.e., extension vs. cobranding). For instance, brand–category connections capture the transcendence of brands into new categories and show that certain categories are more viable for extensions than others. Brand–brand connections, in contrast, provide a more granular view of transcendence by revealing the individual brands that are suitable for cobranding. As user–brand relationships on social media may change over time, this article analyzes the brand network in both 2017 and 2020. This helps visualize the fluctuations of brand connections over time and investigate the impact of such fluctuations on cobranding alliances. Understanding whether critical connections with certain brands or prospective categories have waned helps managers promptly identify the problem and take appropriate action. Similarly, identifying new connections that have formed over time illustrates how past marketing actions can impact a brand's transcendence in users' minds.Cross-category connections revealed through the network can be used to both assess the effectiveness of previous marketing campaigns and discover new alliance opportunities. For example, Bud Light's connection to Pepsi reflects the cointerest patterns between the two brands and, thus, affirms the effectiveness of joint marketing campaign led by the two brands previously. Similarly, Sierra Nevada's strong connections with travel and technology brands (e.g., Southwest Airlines, Discovery, SpaceX, Microsoft) highlight strong cointerest with these brands and present new cobranding opportunities that may not yet be known to its owners. We provide examples of both scenarios using information from external industry sources. Another practical application of our method is competitor analysis, which can help managers identify the differentiating connections of brands with respect to their competitors and gauge the type of users their competitors attract.Finally, we validate the findings of our model against external survey ratings and conduct extensive robustness checks, including network simulations, to ensure that our final network estimates are not biased by fake users or bots. Consistently high correlation between our automated approach and external survey ratings affirms the validity of our methodology for identifying cross-category brand–brand and brand–category connections. Overall, the core contribution of this study is a new digital approach to analyzing audiences' interests across a broad brand ecosystem. The cross-category insights generated by this approach can help researchers and practitioners identify nontraditional branding opportunities that are difficult to infer from traditional survey-based approaches. From a managerial perspective, our brand network can efficiently and cost-effectively generate cross-category insights, given that most of the data collection and network analyses are automated. Furthermore, as our approach uses information that is publicly available on social media, it is easily scalable to a large number of brands, with the resulting network structures reflecting the preferences of a diverse set of users. In the next section, we discuss relevant studies in the marketing literature and describe how our work contributes to the field. Conceptual Motivations Social Networks for Cobranding and ExtensionsResearchers regard cobranding as a source of competitive advantage that helps brands differentiate themselves, gain consumer trust, acquire new channels of distribution, and enter new markets ([47]). Brand extensions (i.e., leveraging the existing brand's name to enter into new categories) are another widely adopted strategy for firms entering new markets ([ 1]). Brand extensions provide greater quality assurance to customers who are familiar with the original brand, reduce the costs of distribution, and increase the efficiency of promotional expenditure ([ 1]). Both brand extensions and cobranding strengthen the focal brand and reinforce customers' value perceptions of the new product ([20]).Naturally, identifying underlying brand-to-brand connections on the basis of users' cointerests may be a key that enables brand managers to discover potential cobranding and extension opportunities. Most studies in this domain have used surveys ([ 5]). Although collecting input from prescreened participants is desirable, recruiting and maintaining a pool of such participants may be unfeasible due to cost or other constraints ([11]). Recent advances in social network analysis have enabled a wide range of scalable solutions that go beyond conventional market research methods. Although previous studies have considered the identification of brand-to-brand connections based on digital user traces, such research has been restricted to brand (or product) relationships within a single category. For example, [34] focus mainly on intracategory connections to create competitive market structures for car brands. Using survey approaches, [13] obtain intracategory maps for centrality and distinctiveness. Finally, [42] develop mapping methods to visualize large market structures within a single category (i.e., LED TVs).The marketing literature acknowledges the importance of cross-category brand connections for generating extensions, licensing, and cobranding deals ([22]; [41]). However, only limited empirical work has been done in this area. This article introduces an automated, scalable approach for identifying cross-category brand cointerest patterns by leveraging the cofollowership data on Twitter. The use of common followership data on Twitter in our analysis follows the recent work of Culotta and Cutler (2016). However, while [11] aimed to derive perceptual attribute ratings from Twitter followership data, the goal of this work is to investigate asymmetric cross-category brand transcendence over time. Further, unlike Culotta and Cutler, our large-scale network approach does not require any supervised knowledge on exemplars and uses categorical affiliations of brands to infer brand perceptions on transcendence and centrality. Specifically, the inclusion of network-derived measures enables us to study both within-category competition and across-category complementarity between brands. Table 1 presents the unique contribution of this research compared with previous network studies.GraphTable 1. Comparison of This Study with Previous Network Studies in Marketing. StudyFocal ContributionData SourceOutputAsymmetrySurvey ValidationAddresses Bots/Fake Users OnlineCompetitor AnalysisAnalysis Presented over TimeNetzer et al. (2012)Create competitive structure maps using text mining and network analysisComentions on online discussion forumMarket structures within a categoryNoYesNoNoNoCulotta and Cutler (2016)Propose a methodology for inferring attribute-specific brand perceptionsCofollowership data on TwitterPerceptual maps for a set of predefined exemplarsNoYesNoNoNoRingel and Skiera (2016)Develop mapping methods for visualizing complex market structuresConsideration sets from online searchMapping solutions for large complex market structures within a category of >1,000 productsYes, using conditional probabilityNoYes, eliminating implausible clickstreamsYesNoCurrent studyUse implicit brand networks to infer asymmetric cross-category brand connections over time.Cofollowership data on Twitter over timeCross-category connections maps at two levels: brand–category brand–brandYes, using conditional probabilityYesYes, using network rewiringYesYes  Cofollowership Patterns for Identifying Cobranding and Brand ExtensionsOur approach to identifying cobranding and brand extension opportunities harnesses the digital cofollowership patterns between brands. Survey research has shown that users follow brands on social media with the intention of purchasing a product or learning more about their favorite brands ([31]; [40]). Aspirations can also motivate consumers to follow brands ([ 3]). This digital user–brand relationship, which is established through followership activity, can be interpreted as an expression of affinity for the brand ([27]; [33]). Alternatively, this relationship can be viewed through the lens of homophily, meaning that people tend to associate with those who are similar to them in socially significant ways ([32]). This is further supported by consumer research studies ([ 6]; [10]), which show a strong relationship between a brand's image and characteristics and the identities of its followers.Similarly, [39] find that the composition of one's follower base represents the tastes (and likes) of their audience. Thus, the more network overlap (i.e., common followers) between two entities, the greater the similarity of tastes and interests among those entities' audiences ([39]). [ 2] find that common friends (i.e., common mutual followers) have a positive effect on the adoption of an application on Facebook. Similarly, we expect that brands that share a high number of followers on Twitter have a user composition that represents similar tastes or interests (e.g., the partnership between GoPro and Red Bull, which leveraged the shared affinities of their common followers: action, adventure, and fearlessness). Given that individuals primarily follow a brand because they like its products and that most followers are customers ([40]), brands having more common followers implies that their customers may have complementary consumption patterns (e.g., the Starbucks–Spotify partnership, which facilitated the complementary consumption of Spotify music and Starbucks coffee). Building on these theoretical principles, we study the cobranding candidates of a focal brand based on the extent to which its followers overlap with the followers of another given brand (and/or category) of interest. Brand Transcendence and CentralitySome of the greatest brands in the world have defied category norms and transcended their initial market boundaries ([23]). For example, in 1995, Amazon positioned itself as ""Earth's biggest bookstore,"" and its success with books enabled it to transcend its origins to become a leader in e-commerce. Although all brands theoretically operate within their categorical boundaries, such boundaries are often considered malleable ([ 7]). Important cobranding and extension opportunities can be missed if managers are not aware of connections that are relevant to brands in other categories ([ 5]). The brand network provides a solution to this problem by relying on a brand's social connections on Twitter to infer category-specific brand connections.At a high level, our proposed algorithm extracts the category-specific connections of a brand by exploiting the overlap in brand followers on Twitter. Whereas some brands may possess strong connections within their own category, others may have diverse connections across new categories. This article introduces a new construct, transcendence, which measures the extent to which a brand's followers overlap with those of other brands in a new category. The transcendence of a nonsports brand along any given category—for example, say sports—is based on the extent to which its followers overlap with those of other brands in the sports category. Further, to measure the connections not shared by the overall brand category, we calculate ""net transcendence"" as the deviation of a brand's own idiosyncratic connections from its category average. Net transcendence is more informative than raw transcendence because it ignores the cross-category connections that are generic to the category and identifies those that are intrinsic to the brand itself.Lastly, in addition to transcendence, there are brands that possess strong connections within their own category. These brands can be viewed as central. The concept of centrality (or typicality) bears direct relation to a brand's probability of recall, consideration, and choice among consumers' minds ([28]). Such central brands are those that come first to consumers' mind and serve as reference points in their categories ([13]). In the next section, we describe how the brand network is generated from followership patterns on Twitter and lay out important network details. The Network Mining MethodologyThe key contribution of this article is the introduction of an automated framework for inferring cross-category branding insights using implicit brand networks derived from social media. With their ability to provide a direct digital window into the interests of millions of social media users, implicit brand networks can help mangers identify nontraditional branding opportunities that would otherwise be hard to perceive. In this section, we generate implicit brand networks using brand communities on Twitter and outline important network details. DataDrawing from the notion that the social signal of ""who follows a brand"" provides a strong reflection of brand image ([11]), we use a set of 507 brands' Twitter accounts as the basis for our analysis. We select the most active Twitter brand accounts based on followership data from the social media directory FanPageList.com. We use Twitter's public application programming interface to collect the brands' lists of followers for 2017 and 2020. We manually verify that all Twitter handles correspond to the official brand account. Overall, the data set consists of brands from many major categories: airlines, luxury goods, retail, automotive, sports, technology, dining, food and beverages, lodging, media, travel, cruises, and beer. Each brand is assigned to a specific category based on the basic or superordinate category-level analyses ([28]).[ 6] To prevent bots or spam accounts from influencing the network analysis, all Twitter brand accounts included in the analysis are manually audited using the audience intelligence website SparkToro.[ 7] Furthermore, as we discuss in the section on robustness checks, we conduct network simulations to ensure that our final network estimates are not biased by such bots. Network GenerationThe next step is to extract the common followers between all brand pairs. The raw brand network is a weighted edge list, defined as 〈bi, bj, wij〉, where bi and bj are individual brands or nodes and wij is the common followers between those brands. If Fi and Fj represent the list of Twitter accounts following brands bi and bj, then an edge between two nodes is created if Fi ∩ Fj > 0. Alternatively, the weighted edge list can be represented as a weighted adjacency matrix Aij where Aij=wij0}ifbrandiandbrandjareconnectedotherwise. GraphOverall, we extract two brand networks: one for 2017 and one for 2020. The original brand networks are highly dense, with common followers between almost all pairs of brands. The numbers of common followers vary from a few hundred to more than a million users. Although it is possible to work with such dense networks, valuable information may be lost due to the redundancy generated by the large number of connections ([44]). Further, connections based on too few followers may not indicate significant connectivity. Given the wide heterogeneity in raw edge weights (i.e., the numbers of common followers), we next aim to extract the truly relevant brand–brand connections. Network filteringA common way to extract a relevant network structure is by applying a global threshold to remove the edges with weights below a particular cutoff. This, however, can destroy the multiscale properties of the brand network. Instead, we use a disparity filter ([44]), which is a filtering algorithm for multiscale networks, to obtain a reduced but more meaningful representation of the network. This method preserves the important edges present at all scales by locally identifying the statistically relevant weights at the node level. The statistically relevant edges (at a given significance level; e.g., α) represent a significant deviation from a null model of uniform randomness. Thus, smaller brands with fewer common followers are not ignored during the network reduction process. Although the current analysis focuses on Twitter brand accounts with between a few thousand and more than a million followers, this method could also be applied to smaller brands with fewer than 1,000 followers. Following [44], we use the commonly specified significance level α = .05 to extract the important connections in the brand network. The filtered networks for 2017 and 2020 consist of roughly 14,000 edges between brands. Although we use the disparity filter to obtain a filtered representation of the original network, there are alternative information-filtering algorithms available in the network science literature, including the global threshold and global statistical significance filters. In Web Appendix A, we revisit these alternative methods and discuss our rationale for choosing the disparity filter. Asymmetric normalizationBrand community sizes can vary both within and across categories. Brands with large brand communities (e.g., Chanel, Microsoft, Starbucks) tend to have more common followers than those with smaller communities. The normalization of edge weights is required to account for this variance. Thus, we use the conditional probability measure ([42]) to compute new network weights that not only normalize the effects of brand size but also account for asymmetry between brand pairs. Asymmetry between brand pairs may occur when the degree of connection between any two brands is unequal (i.e., the connection from A to B is not equal to the connection from B to A) ([15]). Ignoring the directionality of brand connections can lead to inaccurate estimates of consumer brand knowledge ([16]). We observe many cases of associative asymmetry in our brand network and use conditional probability to account for such scenarios. For instance, Figure 1 shows the cross-category connection between Starbucks and Stella Artois. A large percentage of Stella Artois fans are interested in Starbucks, and the outgoing directional strength is almost.20. However, fewer Starbucks fans are interested in Stella Artois, and the outgoing directional strength is comparatively much lower, at.0009. Incorporating directionality in the network reveals this crucial information, which is not visible in a simple, undirected, weighted network. Mathematically, the conditional probability measure calculates the normalized edges between any two brands A and B as P(A∩B)=|A∩B||A|, Graphwhere the numerator  |A∩B|  is the number of common followers between brands A and B and the denominator  |A|  is the number of followers of the focal brand.Graph: Figure 1. Calculating asymmetry between brand pairs.Figure 2 shows the entire brand network structure for 2020 using the dimensional reduction algorithm, t-distributed stochastic neighbor embedding (t-SNE). The t-SNE algorithm yields a two-dimensional undirected representation of the brand network, with the distance between brands in the t-SNE space being proportional to the mean conditional probabilities between brands. The colors of the brands correspond to their category affiliation. Interestingly, while most automotive brands in Figure 2 are well-distanced from other nonautomotive brands, Tesla is positioned in the technology category. A similar pattern is observed for certain retail brands such as Adidas and Reebok, which are closer to the sports group than to other brands in their category. Individual brand constructs on transcendence, as described in the next section, help reveal specific cross-category connections for a given brand.MAP: Figure 2. t-SNE map of the undirected brand network.[ 8] Measures of Brand Transcendence and CentralityThis section outlines the process of identifying centrality and transcendence by exploiting the connections of a brand within the network. For any given brand, the first step is to disentangle its connections across the main categories: airlines, luxury goods, retail, automotive, sports, technology, dining, food and beverages, lodging, media, travel, cruises, and beer. The algorithm then computes the weighted out-degree centrality of a brand across these categories. In weighted networks, out-degree centrality or node strength is commonly calculated as the sum of weights emanating from a focal node to all its connections ([ 4]). However, to account for the strength of edge weights and the number of connections of a focal node, [37] propose a new measure of weighted degree centrality: Ci=di(1−α)×wiα, Graph( 1)where  di  is the degree of the focal node (i.e., the number of connections),  wi  is the node strength (i.e., the sum of the weighted connections), and  α  is the tuning parameter from 0 to 1. For example, following [37], the transcendence of a given nonsports brand into the sports category is calculated as the function of its number and strength of outgoing connections to all other brands in the sports category.More formally, the set of brands in the network can be represented as B, where any individual brand  b∈B  . Brand categories, G, are subsets of B, such as G ⊆ B.[ 9] The transcendence of focal brand b onto a new category G is evaluated as tbG=∑k∈Gdb,k(1−α)×∑k∈Gwb,kα|G|, Graph( 2)where  ∑k∈Gdb,k  is the number of outgoing edges from brand b to all k brands in category G,  ∑k∈Gwb,k  is the sum of weighted edges from brand b to all k brands in category G, and  |G|  gives the number of brands in category G. Dividing by the total number of brands in a category,  |G|  , helps ensure that large categories with many brands do not dominate the analysis. Following [37], we set  α  to.5 to place equal importance on a brand's number of connections and the weight of those connections.Furthermore, considering that brands whose followers tend to follow many other brands may inflate the network constructs, we divide our transcendence construct by the total degree of a brand in the network (i.e., its number of connections to other brands). Intuitively, brands whose followers tend to follow many brands have a higher degree than brands whose followers follow fewer brands. Thus, in the transcendence construct, the sum of the numerator increases with each new connection of a brand in the network. With the objective of normalizing for brands that inherently have higher aggregate transcendence due to higher degree in the network, we divide our final transcendence construct by the total degree of a brand. Thus, the final transcendence construct becomes tbG=∑k∈Gdb,k(1−α)×∑k∈Gwb,kα|G|×db, Graph( 3)where the additional variable in the denominator,  db,  is the total number of connections of a brand in the network.Furthermore, in our transcendence construct, brand b's connections within its own category  Gp  relate to its centrality (  tbGp  ): the higher the strength of these connections, the more central the brand is in its own category. As noted previously, the notion of centrality is directly related to a brand's probability of being recalled, considered, and chosen by consumers ([28]). We multiply the centrality construct by the size of the focal brand's community (i.e., its number of followers) to account for the brand's popularity among users. Thus, the final centrality construct is tbGp×fbmax(fbi∈Gp), Graph( 4)where  tbGp  is brand b's connections within its own category  Gp  and  fb  is its number of followers. Given that the values of  fb  vary from a few hundred to more than a million users, we use its scaled value. Given a set of nonoverlapping categories G1, G2, ..., Gp, the transcendence of brand b across p categories is a 1 × p-dimensional vector: tb=[tbG1tbG2tbG3⋯tbGp]. Graph( 5)The transcendence vector of a brand can also be analyzed with respect to its competitors in the category. The 1 × p-dimensional vector  tb  can be further extended into an n × p matrix, where n rows represent brands (i.e.,  b1,b2...,bn  ) and p columns represent transcendence across the p categories, as shown in Figure 3.Graph: Figure 3. The transcendence matrix, tbG for n brands across p categories.The average category connections (i.e., connections emanating from one category  Gi  to another category  Gp  ) are then calculated as follows: tGiGp=∑b∈GitbGp|Gi|, Graph( 6)where  i≠p  . This formula measures the average transcendence of brands in one category (for example,  Gi  ) into another category  Gp.  To separate a brand's unique connections (  tbGp  ) from its category average (  tGiGp  ), we calculate the net transcendence of brand b into category  Gp  as follows: t_netbGp=tbGp−tGiGp, Graph( 7)where  b∈Gi  and  i≠p  . A positive value for  t_netbGp  indicates that the brand's transcendence is above the category average, while a negative value indicates that its transcendence is below average. As in the raw transcendence vector  tbGp  , the net transcendence vector of a brand b across p categories is t_netb=[t_netbG1t_netbG2t_netbG3...t_netbGp]. Graph( 8)The 1 × p-dimensional vector  t_netb  can be further extended into an n × p matrix, where rows represent n brands in a category and p columns represent the net transcendence of brands across the p categories, as shown in Figure 4.Graph: Figure 4. Net transcendence matrix of n brands across p categories.Figure 4 provides a more comprehensive view of the competitive landscape of a particular brand by highlighting its cross-category connections as well as those of its competitors. In the next section, we discuss the results of these analyses and identify their key managerial implications. Results and Managerial ImplicationsDepending on the business objective, the category-specific connections generated by the brand network can be visualized on two different levels: brand–category and brand–brand. Using examples from the automotive and beer categories, in this section we present our results on these two levels and discuss the managerial implications of our findings. First, we note the brand–category connections of automotive brands and identify the categories suitable for brand extensions. Second, we focus on brand–brand connections and discuss how asymmetry can be leveraged to attain more nuanced insights into the expected benefits of cobranding. Third, we highlight the network's ability to capture changes that occurred between 2017 and 2020, given that individual brand–brand connections may change over time. Finally, we discuss how the brand network described in this article can help managers identify the differentiating category-specific connections of brands with respect to their competitors and gauge the type of users their competitors attract. Brand–Category Connections: Transcendence and CentralityTo identify the brand–category connections of automotive brands in 2020, we study the net transcendence matrix,  t_netbGp  , as shown in Figure 5. All column values have been scaled, with positive values shown in red coloring and negative values shown in blue on the heatmap. For every brand (n)–category (p) relationship in the heatmap, values closer to dark red indicate a stronger perceived relationship between a brand and category. The stronger the relationship between the brand and category, the greater the user cointerest between that brand and category. As discussed previously, the cointerest patterns captured through the analysis of common followership do not necessarily guarantee brand fit, which is typically based on the similarity of brand personality dimensions ([48]). Instead, values in the transcendence matrix reveal cointerest patterns that enable managers to explore potential extension and cobranding opportunities that are difficult to infer from traditional survey-based approaches. For instance, the audience of the car brand Mercedes has strong cointerest with the luxury, technology, retail, and sports categories, suggesting that extensions may be possible in these categories.Graph: Figure 5. Net transcendence matrix, t_netbG, reflecting brand–category connections of the automotive brands (2020).Similarly, there may be brands that, despite having low net transcendence into different categories, have strong connections with their group, making them central in their own category. For example, Toyota and Dodge are highly central to the automotive category, although their net transcendence across categories is low. Tesla, in contrast, has high net transcendence into the technology category, despite having low centrality in the automotive group. We also observe that some car brands with high net transcendence across multiple categories have moderately low centrality in their group (e.g., Audi, Mercedes, Tesla, and Lamborghini). However, brands such as Chevrolet and Ford share strong cointerest in the automotive category and also have moderate net transcendence into beer, dining, and sports. Thus, centrality and transcendence are not mutually exclusive, and a brand may be perceived as both central and transcendent, depending on its connections in the network. Brand–Brand Connections: Leveraging Asymmetry for Cobranding InsightsThe brand network developed in this study can also help managers obtain a more granular view of transcendence by identifying brand–brand connections across categories. The different levels of analysis (i.e., brand–category and brand–brand) offered by the network can help managers understand why certain cobranding opportunities are more promising than others. Further, the network's analysis of the asymmetry between brand pairs can help identify brands which may potentially benefit more from a cobranding alliance. To identify strong, relevant cobranding candidates for a focal brand, we only consider brand–brand connections in categories where the net transcendence of the brand is positive. This ensures that the identified brand connection is not generic to the category but rather is intrinsic to the brand itself. For instance, the net transcendence of Mercedes into the luxury goods and retail categories is positive, meaning that, on average, its connections with luxury and retail goods are relatively higher than those of other car brands. Thus, for Mercedes, brands in the luxury and retail categories are considered suitable candidates for cobranding.Figure 6 shows the brand–brand connections of Mercedes with brands in the luxury and retail categories. Some of Mercedes' strongest connections are with Louis Vuitton, Nike, Tissot, and Chanel, which highlights these brands' potential for alliances with Mercedes. However, given the asymmetrical nature of these relationships, the benefits gained through such alliances may not always be equal. For instance, the asymmetrical connection between Mercedes and Tissot reflects that a greater proportion of Tissot's audience is interested in Mercedes than vice versa. This indicates that there may be a greater potential benefit for Tissot from such an alliance. These results on asymmetry can provide additional insights to brand managers of both Mercedes and Tissot when evaluating potential cobranding candidates.Graph: Figure 6. Top 20 brand–brand connections of Mercedes with brands in the luxury and retail categories using the Fruchterman–Reingold (1991) layout.Indeed, prior strategic alliance literature suggests that unequal spillover benefits can be expected from asymmetrical brand alliances ([21]). However, the main findings of [21] suggest that even though the magnitude of financial gains in asymmetrical alliances is not equal, it is not a win–lose partnership but rather a win–win or a shareholder value-adding alliance for both the larger and smaller partner firms.[10] Similarly, in the case of Mercedes, even though the expected benefits may not be equal for the asymmetrical relationships (e.g., Mercedes–Chanel, Mercedes–Tissot, Mercedes–Rolex), future deals can be still beneficial to both brands. Although the smaller brand, Tissot, may achieve greater gains from this asymmetrical alliance (e.g., by having a greater proportion of its audience interested in Mercedes), the larger brand, Mercedes, may still gain access to a niche audience that may not be a part of its current demographic. Capturing Shifts in TranscendenceA brand's transcendence in the network, which arises from common consumer interest, may not be static. Brands may, for various reasons, wish to shift their transcendence to new categories in search of new alliances or cobranding opportunities. Our network can track such shifts in transcendence, allowing brand managers to better assess the effectiveness of their marketing actions and identify emerging or waning categories for future brand alliances.Figure 7 shows the change in net transcendence of car brands into the technology category. The dynamic plots for other categories can be analyzed similarly. Interestingly, between 2017 and 2020, the technology connections of many car brands, including Honda, Jeep, Chrysler, Acura, and Chevrolet, decrease. However, some brands, including Tesla, Lamborghini, and Infiniti, show a steep rise in their connections with technology. This may be related to, for example, Infiniti's plans to go all-electric in 2021 and use intelligent technologies to reflect its new ethos ([30]). Thus, our brand network-based methodology can be used to assess the effectiveness of a brand's marketing campaign and showcase how marketing actions can impact the brand's transcendence in users' minds.Graph: Figure 7. Change in net transcendence over time.The network's ability to highlight shifts in brand transcendence over time can be of vital use to managers. The emergence of new connections with specific categories over time (e.g., Infiniti's increasing transcendence into the technology category) provides insight into the effectiveness of brands' marketing campaigns and affirms the possibility of future extensions in those categories. Similarly, the waning of a brand's connections with specific categories (e.g., Jeep's decreasing transcendence into the technology category) allows that company to identify potential problems and take appropriate action to address them. The brand network can also help managers investigate issues in more detail by uncovering the specific cross-category brand–brand connections that have diminished over time. Change in a brand's net transcendence into a category can be caused by several factors, including joint ads, new alliances, embedded promotions, or other external events. Although the method in this study does not examine the causes for such shifts, it provides managers with timely intelligence on the subject. Future marketing studies could build on this work to further investigate the causes for changes in brand transcendence over time. Competitor AnalysisThis subsection discusses how the category-specific brand connections revealed through the transcendence matrix may not only allow managers to understand the position of their brands in consumers' minds but also help distinguish them from their competitors. Figure 8 shows the net transcendence,  t_netbGp  , of two beer brands, Bud Light and Sierra Nevada, into different categories. Whereas Bud Light has high transcendence into food and dining, Sierra Nevada has high transcendence into travel, airlines, and technology. Regarding centrality, Bud Light outperforms Sierra Nevada, with stronger connections within the beer category. Thus, whereas the former brand is positioned strongly among beer and food enthusiasts, the latter brand resonates more with technology and travel enthusiasts. This type of analysis can help brand managers identify the differentiating connections of their brands with respect to their competitors and also gauge the type of users their competitors attract.Graph: Figure 8. Net transcendence matrix, t_netbG, of Bud Light and Sierra Nevada.Brand managers can obtain richer insights into the cointerest patterns of their competing brands by examining the individual brand–brand connections in the different categories. For example, Bud Light is connected to more food, beverage, and dining brands (e.g., Pepsi, Coca-Cola, McDonald's, Subway, Taco Bell), while Sierra Nevada is mostly connected to airline, travel, and technology brands (e.g., Southwest Airlines, Discovery, SpaceX, Amazon, Netflix). As one might expect, some brand–brand connections can reflect previous marketing activities (e.g., joint advertisements or promotions, collaborations, licensing deals). In such cases, our brand network–based methodology enables managers to measure the effectiveness of a marketing campaign and showcases how marketing actions can impact a brand's transcendence. For example, Bud Light's connection with Pepsi reflects strong cofollowership patterns between the two brands, affirming the effectiveness of their earlier joint marketing campaign.[11] Alternatively, brand–brand connections can highlight potential new cobranding or alliance opportunities that were previously unknown to brand managers. For instance, Bud Light's strong connections with McDonald's and Taco Bell highlight strong cointerest between these brands, suggesting untapped cobranding opportunities. Similarly, Sierra Nevada could leverage the technology and travel interests of its fans, as revealed by the network, to partner with relevant travel brands such as Southwest Airlines, SpaceX, Discovery, Amazon, and Netflix. In the next section, we validate our results against external survey ratings and test the reliability of our findings. Survey ValidationTo validate the effectiveness of our methodology, we compare the network ratings from our automated approach with directly elicited survey ratings. The survey was conducted through Amazon Mechanical Turk, which is a reliable source for conducting social sciences research ([11]). The survey respondents were asked to report their income, age, and gender to account for any demographic influence in the sample. The participants were required to be located in United States and be over 18 years old. To ensure high-quality responses, a prior task approval rate of 95% was required for all survey respondents. The brands were grouped by sector, and four separate surveys, consisting of 250 participants each, were conducted to validate the brand–category and brand–brand connections of beer and automotive brands. Next, we discuss our survey findings along with several robustness checks. Validating Brand–Brand ConnectionsIn this subsection, we examine whether the cobranding candidates identified by the network are also perceived by consumers to be such candidates. The network edge weights between brands are intended to reflect consumers' perceptions of the brands that could be paired for cobranding; thus, such a relationship should be reflected in the survey responses. For this validation, we select the five most-followed brands in the beer and automotive categories. Then, for each brand, we select nine cobranding candidates: ( 1) the top three cross-category cobranding opportunities (i.e., brands), as identified by the network, ( 2) the top three most-followed brands in the sample that are not included in part 1, and ( 3) three randomly drawn brands that are not included in parts 1 and 2.For each focal brand, we ask the respondents to rate its cobranding candidates on a scale of 1 (""less likely to go together"") to 10 (""highly likely to go together"") according to how strongly they can be paired with the focal brand. The survey scores were then correlated with the outgoing edge weights from the focal brands to their cobranding candidates in the brand network. For every survey question, the brand order was randomized, and attention filters were included to identify invalid responses. To identify loyal fans, participants were separately requested to select their favorite auto and beer brands from the list. Details of the survey and the corresponding descriptive statistics are included in Web Appendix B. ResultsTable 2 shows the Pearson correlation coefficients between the survey and network scores. Overall, the survey measures correlate well with the network estimates, with the survey's top-three-box score[12] achieving an average correlation of.67 with the network estimates. The overall correlation between average survey ratings and network constructs is.65.GraphTable 2. Pearson Correlation Coefficients of the Survey Estimates with the Network Constructs. CategoryBrandsr (Mean)r (Top Three Box)AutomotiveTesla.89.88Mercedes-Benz.63.54BMW.76.76Audi.93.93Ford.62.52BeerMiller Lite.64.66Sierra Nevada.53.55Bud Light.56.51Budweiser.53.55Coors Light.57.57Average correlation coefficient (r).67.65 In addition, we compute the correlations between network scores and the survey ratings of users who rate a specific brand as their favorite. When using only data from fans, the overall correlation coefficient increases to.70 for average survey ratings and.71 for the top-three-box survey ratings. Finally, we examine the scatter plots more closely to better understand the circumstances in which the network cobranding candidates align well with the survey responses. For example, Figure 9, Panel A, shows the cobranding candidates for Audi as suggested by the network, together with the corresponding survey ratings. The top three cobranding candidates suggested by the network (i.e., Microsoft, Nike, and Intel) also receive very high ratings from the survey respondents. Brands with low network connectivity with Audi (i.e., Lays, Forever21, and ABC) also receive lower ratings from the survey respondents. These results reaffirm the previous findings that network connectivity patterns between brands are useful marketing metrics for consumers' perceptions of which brands are likely to pair well together.Graph: Figure 9. Network versus survey estimates for brand–brand connections of Audi and Budweiser.Figure 9, Panel B, shows the cobranding candidates for Budweiser, as suggested by the network and survey responses. Some top cobranding candidates suggested by the network, including the NFL and Pepsi, also receive high ratings from the survey respondents. However, despite its strong connection to Budweiser in the network, Starbucks receives low ratings from the survey respondents. Although the strong network connectivity between Starbucks and Budweiser is not directly perceived by survey responders, it may indirectly reflect the complementary taste interests of coffee and beer drinkers, which Starbucks previously leveraged to launch a line of beer-like coffee drinks ([26]; [38]). Similarly, Budweiser has a lower network connectivity score with the soccer brand FIFA than the corresponding survey rating. On further investigation, whereas Budweiser's U.S. Twitter account, which was used in our survey validation, has low network connectivity with FIFA, the brand's global Twitter account is very strongly connected to FIFA. This suggests that brand managers should apply domain knowledge and managerial judgment to explore alternative cobranding candidates based on their market of interest. Using domain customization, brand managers can query the brand network to include brand accounts that best suit their target market and conduct a more tailored network analysis to identify where the proposed cobranding opportunity may work well.The preceding analysis also highlights some of the limitations of survey-based validation. First, consumer responses to direct questions on brand perceptions are based on the respondents' existing notions of brand extendibility and may be confounded by prior user experiences ([25]). Second, asking consumers about their overall perceptions using direct rating scales may not reveal novel or unique brand extensions or cobranding ideas; rather, it may simply facilitate the testing of known concepts ([ 5]). The brand network, in contrast, leverages the cofollowership patterns of millions of Twitter users across a broad brand ecosystem to reveal cross-category cobranding ideas that may not be intuitive to consumers but, in hindsight, are effective. Overall, with an average correlation of.71 between network scores and fans' survey ratings, the validation results suggest that the automated network-based approach enables managers to quickly and inexpensively identify cobranding and brand extension ideas that would otherwise be difficult to anticipate. Validating Brand–Category TranscendenceNext, we validate whether the transcendence measures derived from the brand network align with consumers' perceptions of the brands. For both beer and automotive brands, consumer ratings along the luxury and technology categories were elicited. On a scale of 1 (""least likely"") to 5 (""most likely""), participants were asked to rate the focal brands (e.g., Heineken) according to how strongly they associated them with a new category (i.e., luxury goods and technology). Further, to identify brands with strong centrality within their own group, participants were asked to rate the focal brands, on a scale of 1 (""least likely"") to 5 (""most likely""), according to how strongly they believed them to be central in their own category. Finally, the average survey ratings for each brand across the luxury and technology categories are compared with the brand transcendence constructs obtained using the network measures. Similarly, the average survey rating for centrality is compared with the centrality construct obtained using the network measures. We also calculate the top-two-box score for each brand to assess the proportion of people who rate a brand very highly (i.e., a score of 4 or 5). Details of the survey and the corresponding descriptive statistics are included in Web Appendix B. ResultsThe Pearson correlation coefficients between the survey results and network constructs are listed in Table 3. Overall, the survey measures correlate well with the network estimates, with the top-two-box survey scores and mean survey scores achieving an average correlation of.63 with the network estimates.GraphTable 3. Pearson Correlation Coefficients of the Survey Estimates with the Network Constructs. CategoryConstructsr (Mean)r (Top Two Box)AutomotiveTranscendence (luxury).59.61Transcendence (technology).66.67Centrality.58.61BeerTranscendence (luxury).52.53Transcendence (technology).72.69Centrality.71.70Average correlation coefficient (r).63.63 Figure 10 includes the scatter plots for survey versus network measures. Given that the network and survey estimates are measured in different units, the plots have been scaled to 0–1 for easier interpretation. Overall, the scatter points are well distributed along the best fit line, with few outliers. As we discuss next, we then conduct a series of tests to ensure that the network accurately captures the shifts in connections over time. Overall, our general findings pass these tests, supporting the future use of implicit brand networks in marketing research.Graph: Figure 10. Scatter plots of network versus survey estimates for automotive and beer brands. Shift in connections from 2017 to 2020The ""Results"" subsection discusses the brand network's ability to capture shifts in brand transcendence over time. We now test whether the waning of certain connections between 2017 and 2020 in the network is supported by the survey responses. To do this, we first identify the connections between brands and categories that exist in the 2017 network but decline in the 2020 network. Figure 11 illustrates the filtered cases for automotive brands and the corresponding survey results. Panel A shows that brands such as Mazda, Mini, Buick, Chrysler, and GMC all have connections with the luxury category in 2017. According to [17] in Forbes, at the time, the Mazda 2017 CX-9 Signature model was considered the most luxurious vehicle produced by Mazda to date. The author mentions that ""Mazda has never been considered a luxury brand, but maybe it's time to reconsider that classification"" ([17]). However, the results from the brand network in 2020 show that Mazda does not retain its connection with luxury. This is further validated by the survey participants, who also rate Mazda as very weakly associated with luxury. There is a similar pattern in Panel B, in which brands such as Dodge, Chevrolet, Jeep, Honda, and Chrysler show a significant drop in their connections with technology category between 2017 and 2020. This change is also reflected in the survey responses.Graph: Figure 11. Shift in transcendence of car brands from 2017 to 2020. Random connections rejected by survey participantsNext, we test whether survey respondents also reject random connections that do not exist in either 2017 or 2020. To do so, we filter the cases where connections between brands and categories are absent in both the 2017 and 2020 networks and compare them with the survey responses.[13] We find that the average survey ratings are below 2 (out of 5) for most brand–category connections not existing in either network (i.e., 2017 and 2020). Addressing demographic bias on TwitterStudies that mine brand perceptions from social media sources must consider the extent to which brand followers on social media represent the general population. It is also important to consider whether certain Twitter brands accounts are more appealing to a specific audience (e.g., young people, men). Recent studies have reported that Twitter followership data successfully captures attribute-specific consumer perceptions beyond demographic similarities ([11]). We investigate this issue further by comparing the survey ratings, which were provided by users of different demographics, with the transcendence values obtained from the brand network. Table 4 lists the correlation values between the network estimates and income-specific survey ratings for the transcendence of automotive brands into the technology category. For most income groups in the survey, there are adequately high correlations with the network transcendence constructs. Results for all the remaining survey demographic groups (i.e., age and gender) are included in Web Appendix D. We observe adequately high correlations between the demographic-specific survey ratings and the brand network constructs. This affirms that the overall brand network estimates are not heavily influenced by the demographics of Twitter users.GraphTable 4. Income-Specific Survey Correlations with the Network Constructs. Automotive Brands' Transcendence to TechnologyIncomeSurvey-Based Measure (≤$29,999)Survey-Based Measure ($30,000–$59,999)Survey-Based Measure ($60,000–$99,999)Survey-Based Measure ($100,000–$149,999)Survey-Based Measure (≥$150,000)Survey-based measure (≤$29,999)1.00Survey-based measure ($30,000–$59,999).961.00Survey-based measure ($60,000–$99,999).97.981.00Survey-based measure ($100,000–$149,999).93.96.941.00Survey-based measure (≥$150,000).93.91.94.881.00Network-based measure.67.62.72.59.74  Sensitivity of transcendence constructs to network rewiringThe presence of Twitter bots may inflate the number of common followers between brands, which can, in turn, lead to inaccurate network estimates of transcendence and centrality. In this section, we conduct multiple network simulations by repeatedly rewiring the edges to test whether the original brand network structure remains reasonably stable. We incrementally rewire the cofollowers from any random pair of edges and reperform the entire analysis. As Figure 12 shows, the rewiring stage involves the addition and removal of 5% of the cofollowers of any random pair of edges in the network, continuing until 50% of the cofollowership patterns have been altered. In each iteration, once the network rewiring is complete, the algorithm reruns the entire analysis (i.e., it applies the disparity filter to identify statistically significant edges, normalizes the edge weights, and calculates the transcendence across categories).Graph: Figure 12. Process flow for each iteration.Figure 13 illustrates the results of the simulations for the automotive brands and compares the transcendence values obtained after rewiring the network with the original values. The purpose of the test is to ensure that the original network estimates hold for small rewiring changes (i.e., that significant rewiring is needed to yield completely different network estimates). For all plots, the rewired network estimates correlate highly with the original estimates until a large percentage of the network (>30%) is rewired. This demonstrates that the brand network structure is not sensitive to small underlying changes that may occur due to bots.Graph: Figure 13. Correlation of post rewiring transcendence values with original transcendence values for automotive brands. ConclusionDespite its relevance to various marketing decisions (such as cobranding and brand extensions), the identification of cross-category insights across a broad brand ecosystem is currently understudied in the marketing literature. This article uses implicit brand networks to identify the category-specific connections of brands and their competitors by exploiting the overlap in brand followers on Twitter. We introduce a new construct, transcendence, that measures the extent to which a brand shares cointerest with other brands in different categories. Depending on a firm's marketing objectives (i.e., their focus on extensions vs. cobranding), the transcendence of a brand can be studied at different levels: brand–category or brand–brand. These different levels of analysis can help managers identify viable cobranding opportunities.Furthermore, we leverage the concept of asymmetry between brand pairs to provide more nuanced insights into possible cobranding opportunities and determine which brand can potentially benefit more from a cobranding alliance. We conducted the analysis over time to track shifts in brand transcendence, allowing brand managers to both assess the effectiveness of existing marketing strategies and identify new alliance opportunities. To ensure the reliability of our proposed methodology, we validate our findings against external survey ratings and conduct extensive robustness checks, including network simulations, to ensure that our final network estimates are not biased by Twitter bots.From a methodological standpoint, the implicit brand networks utilized in this article condense the high-dimensional interest space of millions of brand followers into a parsimonious form that is more amenable to research and business applications. The readily accessible artifact, which is obtained with little human intervention in the processing of the underlying data, allows managers to efficiently infer cross-category branding insights in a scalable way. Compared with extant digital approaches that rely on extensive preprocessing, this straightforward automated approach enables practitioners to readily obtain the cointerest patterns of brands with respect to their competitors and gauge the types of users that their competitors attract. More specifically, given its automated data collection and network analyses, the brand network can act as an effective business intelligence tool for the identification of cobranding and extension opportunities across a broad ecosystem of brands.Overall, our approach offers several benefits to marketers. It also highlights avenues for future research. First, although our analyses use Twitter brand communities, it would be interesting to compare similar communities on Facebook and Instagram. Brand networks on different social media platforms may vary based on factors such as user demographics, category, platform characteristics, or a brand's marketing strategy. Although consistent brand connections across different platforms can provide additional validity to findings of this study, meaningful insights may also be gleaned if substantial differences are observed. Such differences may, for example, stem from a brand's tailored marketing efforts on a specific platform. Using brand networks to track the effectiveness of such efforts can be beneficial to brand owners. Differing user demographics across platforms may also have an impact on brand network structures. Though this study did not identify substantial differences between the demographic-specific survey ratings and the transcendence values obtained from the brand network, future research could examine platform-specific brand networks to obtain richer insights. Second, future research could consider how to distinguish the content on brand pages that may affect consumers' decisions to follow brands, including promoted content on a brand's page, multichannel advertising across platforms (e.g., email, Facebook), and the use of trending topics or sponsored tweets.Third, the analysis in this article relies on a brand's followers at a given point in time. Twitter does not provide data on when a user starts or stops following an account. The article's analysis of two different periods highlights the potential for our method to examine how transcendence changes over time. Because most aspects of the data collection and network analyses in this approach can be automated, brand managers could collect followership information at more regular intervals to examine changes in transcendence more frequently. Fourth, while our study relies on validation from two categories, future studies can consider expanding the survey-based validation for broader set of brands across multiple categories. However, for such validation, it is important to consider that certain cross-category cobranding candidates, revealed through the cofollowership patterns on social media, may not always be intuitive to survey respondents, though in hindsight they make sense and work. In conjunction with the brand network results, marketers should apply domain knowledge and managerial judgment to explore different extension and cobranding opportunities that may work best for the brand.Fifth, it is also important for brand managers to consider that Twitter users from around the globe are free to follow any account(s) of a brand (which can include global or country-specific accounts). As the data collection and network analyses can be largely automated, marketers can create custom brand networks to include Twitter brand accounts (country-specific and/or global) that best suit their market of interest. Domain customization can help managers conduct a more targeted network analysis of where the proposed cobranding opportunity may work best. Lastly, although our approach relies on cofollowership patterns to identify cobranding opportunities, we do not investigate the drivers of common followership on Twitter and the extent to which these drivers lead to network overlap between brands. The reasons that users cofollow brands on Twitter are varied and complex, with many unobservable factors possibly at play. Industry research by Nielsen ([29]) indicates that 55% of Twitter users say they follow a brand because they like it, followed by 52% of users who want to keep up-to-date on the latest promotions and offers posted by the brand. There are various other reasons that users cofollow multiple brands on social media; thus, future research could use the brand network described in this article to investigate and better understand the drivers of cofollowership between brands on social media.Overall, this work offers a new approach for researchers and practitioners interested in automatically monitoring cross-category brand connections over time. Network-based methods for brand management are relatively new and present many opportunities for future research. The methods introduced in this article provide a foundation for marketing researchers interested in leveraging implicit brand networks to gain richer insights into consumers and brands. Supplemental Materialsj-pdf-1-jmx-10.1177_00222429221083668 - Supplemental material for Leveraging Cofollowership Patterns on Social Media to Identify Brand Alliance OpportunitiesSupplemental material, sj-pdf-1-jmx-10.1177_00222429221083668 for Leveraging Cofollowership Patterns on Social Media to Identify Brand Alliance Opportunities by Pankhuri Malhotra and Siddhartha Bhattacharyya in Journal of Marketing  "
14,"Leveraging Creativity in Charity Marketing: The Impact of Engaging in Creative Activities on Subsequent Donation Behavior Charities are constantly looking for new and more effective ways to engage potential donors in order to secure the resources needed to deliver services. The current work demonstrates that creative activities are one way for marketers to meet this challenge. Field and lab studies find that engaging potential donors in creative activities positively influences their donation behaviors (i.e., the likelihood of donation and the monetary amount donated). Importantly, the observed effects are shown to be context independent: they hold even when potential donors engage in creative activities unrelated to the focal cause of the charity (or the charitable organization itself). The findings suggest that engaging in a creative activity enhances the felt autonomy of the participant, thus inducing a positive affective state, which in turn leads to higher donation behaviors. Positive affect is demonstrated to enhance donation behaviors due to perceptions of donation impact and a desire for mood maintenance. However, the identified effects emerge only when one engages in a creative activity—not when the activity is noncreative, or when only the concept of creativity itself is made salient.Keywords: autonomy; creativity; donation; positive affectCharitable organizations exist to support a wide variety of causes, such as helping malnourished children, caring for the homeless, supporting animal welfare, and meeting environmental concerns, to name a few. The success of these organizations in supporting their causes largely depends on the donations they secure. According to the [58], approximately 1.56 million registered nonprofit organizations exist in the United States, together raising an estimated $390 billion in donations annually. Despite these large numbers, fundraising remains a major challenge for such organizations, with approximately 45% of charities unable to secure the required level of resources needed to deliver their services ([59]).In light of this, it is not surprising that marketers at these organizations seek more effective ways to solicit donations, often utilizing nontraditional approaches and fundraising events (e.g., ice-cream socials, silent auctions, trivia nights) to engage potential donors ([11]). For example, in 2014, the ALS Association invited people around the globe to participate in its ""Ice Bucket Challenge"" to increase awareness of ALS, raising approximately $220 million from individual donors in the process (Holan 2014). Bloodwater.org devised the ""Real Game of Thrones"" campaign, which called on people to participate through Twitter and used a combination of pop culture, humor, and bathroom puns to raise money to build latrines throughout Africa. This creative campaign successfully raised enough money in 24 hours to build 21 latrines in Rwanda. Cookies for Kids, another charitable organization, sponsors creative charity events each year such as cookie swap parties, where participants decorate cookies and swap recipes to raise donations. These fundraising anecdotes suggest that charities are defining new ways of engaging potential donors, while raising questions about which types of activities most effectively enhance donation behaviors.The current work meets this challenge by examining how engaging potential donors in creative activities can positively influence their propensity to donate money to a charitable cause. We argue that engaging in a creative activity induces a positive affective state, which in turn increases both the likelihood and amount of monetary donation made to the charitable organization. While prior work has independently examined links between creativity and positive affect (e.g., [ 9]; [47]), as well as between positive affect and helpfulness (e.g., [ 2]; [ 3]), we provide a deeper understanding of why and how creativity leads to enhanced donation behaviors. Specifically, we show that the link between creativity and positive affect is driven by the sense of autonomy that is induced by engaging in a creative activity (i.e., an attempt to create something novel). Further, by identifying the roles that desire for mood maintenance and perceived donation impact play, we provide insight into why the positive affect resulting from a creative activity leads to enhanced donation behavior.The current research makes several important contributions. From a practical perspective, this research offers a simple and effective way for marketers to improve their donation appeals; it suggests that engaging potential donors in a creative activity enhances subsequent donation behavior. This recommended approach provides a real opportunity for charity marketers to increase the efficacy of their fundraising campaigns. At the theoretical level, the present work advances the marketing and charity literature streams in several ways. First, we demonstrate the positive effect of creative engagement on donation behavior. To our knowledge, no research thus far has examined whether and how engaging in creative activities can impact an individual's subsequent donation behavior toward a charitable organization. Second, we explicate the reasons and conditions that drive the relationship between creativity and donation behavior. We demonstrate that it is the act of actually engaging in a creative activity—rather than simple priming or making the concept of creativity salient—that drives the effect. Further, while prior work has consistently shown that a sense of autonomy can facilitate creativity (e.g., [18]), we demonstrate that engaging in a creative activity also heightens one's sense of autonomy, which in turn induces positive affect. As noted previously, we also highlight that the positive affect experienced during a creative activity bolsters desire for mood maintenance and perceived donation impact, thereby enhancing the likelihood of donation and the monetary amount donated. Finally, we find evidence that the positive effect of engaging in a creative activity on monetary donation is context independent. That is, engaging potential donors in a creative activity not directly related to the charitable cause or organization still has a positive influence on their donation behavior. Thus, the current work not only offers marketers a way to build effective donation campaigns but also provides a deeper theoretical understanding of the relationship between creativity and donation behavior. Conceptual Framework Donation BehaviorResearchers have explored many facets of donation behavior, from the demographic and socioeconomic determinants of donation ([ 6]; [10]; [40]) to the extent to which other factors—such as motivation, psychological characteristics, and social cognition—can affect donation ([34]). In addition, prior research has proposed and examined various marketing strategies and tactics used to increase donations. For example, using public recognition ([73]), taking advantage of price promotions ([78]), designing more attractive appeals ([51]), expressing one's identity ([61]), and using positioning to enhance the effectiveness of the charity ([74]) have all been investigated.More relevant to the current research, recent work has also started to examine the merits of engaging potential donors in different types of activities and tasks before soliciting donations. For example, [62] examined the influence of a storytelling event in the crowdfunding context, finding that direct (vs. indirect) storytelling positively affects customer engagement and donation likelihood. In contrast with more traditional donation requests ([69]), some charities are utilizing physical activities (e.g., walks and runs [[37]], sporting events [[36]], silent auctions [[41]], ice cream socials, trivia nights [[11]]) as precursors to the donation solicitation. Despite the initial academic interest in these tactics, the effectiveness of such approaches has been understudied in the literature, and reporting has shown mixed results. For example, while [37] have argued that positive fundraising outcomes result from physical activity events (e.g., running activities, golf tournaments), [75] did not find a positive relationship between sports activities and charitable event outcomes. The current work aims to add to the literature in this regard by validating the use of activities to increase the likelihood and amount of donation contributions. Specifically, we examine the impact of engaging potential donors in creative activities. Creative Engagement, Autonomy, and Positive AffectActivities involving creation of an output span a continuum ranging from routine tasks, such as simply copying a given design, to highly creative activities, such as creating an original work of art ([18]). Within this context, we argue that the inherent characteristic of creative engagement, which is differentiated from priming or simple salience of creativity and/or creativity-related concepts, is that an individual must engage, physically or mentally, in an activity requiring the production of something novel (i.e., the activity leans to one side of the continuum referenced previously). For example, actively generating an original cookie design would lead to creative engagement, but copying a cookie design or simply being primed by the concept of creativity (e.g., through exposure to creative stimuli) would not.Importantly, we propose that engaging in a creative activity induces positive affect for the creator. In support of this notion, liberal arts literature finds that engaging in creative activities to generate novel outputs (e.g., music composition, visual arts, creative writing) can bring about positive thoughts and feelings ([66]). Relatedly, [ 9] show that engaging in a divergent creative task induces higher levels of positive affect. Results reported in the psychology literature also support these findings. [16], while explicating the construct of flow, interviewed people who engage in creative work on a regular basis (i.e., artists and musicians) and found that these individuals often experience positive affect and happiness when creating something original. [15] confirm these findings in an experimental lab setting and show that engagement in a creative activity induces a state of flow, leading to higher positive affect.Why does engaging in creative activities lead to positive affect? One potential driver of this positive relationship is a heightened sense of autonomy (i.e., having a sense of choice and freedom from external control; [24]; [52]; [63]), attained by engaging in a creative activity.[ 5] By definition, an attempt to generate a creative output requires one to actively recognize remote associations between broad and distant concepts and then combine these loosely connected ideas and concepts in a novel fashion ([20]; [31], [32]; [69]). Such a process requires and encourages one to think freely and make different combinations and choices without being constrained by norms and rules ([ 4]; [30]; [43]). Thus, the process associated with creative generation should manifest a sense of choice and freedom (i.e., autonomy), which we contend induces positive affect.Prior work offers initial support for this proposition. As we have discussed, engaging in a creative activity induces a state of flow, which then leads to higher positive affect ([15]); notably, empirical work has shown the state of flow to be associated with a sense of autonomy ([49]). Similarly, [18] found that being involved in a creative activity can enhance experienced enjoyment, but only when the activity imparts a sense of autonomy. [47] conducted a daily-diary study following the routine of 1,042 hobby musicians and found that the participants reported higher positive affect on the days they engaged in music composition and performance. Importantly, the authors found that this relationship was driven by satisfaction of one's needs for autonomy. Finally, [46] found that creative generation, such as the production of visual art, significantly reduced cortisol levels (a biomarker and proxy measure of stress in humans) and increased feelings of relaxation, pleasantness, and enjoyment. Their work shows that such art making is associated with the experience of being free from constraints (i.e., the sense of autonomy).Given this discussion, we argue that engagement in creative activities heightens one's sense of autonomy, which in turn leads to positive affect. We further propose that the positive affect induced by participation in a creatively engaging activity will lead to enhanced donation behavior. We elaborate on this prediction in the following subsection. Positive Affect and Donation BehaviorFindings reported in the extant literature offer compelling evidence that being in a positive affective state enhances donation behavior ([ 1]; [17]; [26]; [42]; [44]; [60]). While prior work has consistently demonstrated a positive relationship between positive affect and donation behavior, it offers disparate explanations for this relationship ([ 7]). Indeed, we recognize that the relationship between positive affect and donation behavior is likely to be multiply determined, and we therefore identify three mechanisms that are most relevant to the context of creative engagement in question.Perhaps the most common explanation for the clear link between positive affect and helping behavior derives from the mood maintenance model. This line of reasoning proposes that people tend to maintain positive mood states ([ 7]; [33]). Thus, individuals tend to help more when in a positive affective state, because doing so enables them to prolong said state ([12]; [45]). In the context of our work, this suggests that the positive affect realized by participating in a creative activity can best be maintained when a subsequent behavior, such as helping others through enhanced donation behavior, also fosters positive feelings ([ 7]).A second potential mechanism derives from the social aspects associated with positive affect. Indeed, it has been argued that being in a positive affective state can directly influence one's perceived social connectedness ([38]; [39]). As such, the positive affect defined by participation in a creative activity is likely to enhance the value of creating and maintaining social connections ([13]; [25]). Further, valuing social connections has been shown to enhance feelings of care and concern toward others ([ 8]), which should subsequently enhance the donation behaviors of the individual.Finally, prior work reports that positive affect can also boost self-efficacy and/or the perceived impact of one's actions ([ 5]; [64]). That is, experiencing positive affect can lead to the belief that one's actions are more efficacious, thus creating a heightened expectancy of positive outcomes ([53]). Thus, we contend that positive affect can enhance the perceived impact of one's potential donation and, in turn, raise the likelihood and amount of one's donation behavior ([14]). In our empirical work, we explicitly test the proposed chain of effects (i.e., engagement in a creative activity → autonomy → positive affect → enhanced donation behaviors), and the three aforementioned potential mechanisms underlying the impact of positive affect on donation behavior. Summarizing our arguments, we hypothesize the following: H1:  Engaging in a creative activity (vs. an activity that does not provide an opportunity for novel creation) leads to enhanced donation behaviors (i.e., the likelihood of donation and the monetary amount donated). H2a:  The relationship between engaging in a creative activity and donation behavior is serially mediated by autonomy and positive affect. H2b:  The influence of positive affect on donation behavior is driven by (a) mood maintenance, (b) social connection, and/or (c) perceived donation impact. Overview of StudiesWe utilized a combination of field studies and controlled lab experiments to test our hypotheses. First, we conducted a pilot study in collaboration with a nonprofit organization as an initial test of our focal hypothesis and found support for the prediction that engaging in a creative activity enhances donation behavior (H1). Study 1, conducted in a controlled lab setting, replicated the initial pilot study findings, thus reconfirming support for H1. Study 2, a quasi-field experiment, shows that engaging in a creative activity increases both the likelihood and amount of monetary donations, whereas simply priming the concept of creativity does not (H1). Study 3 tested the full serial mediation prediction (H2a) by demonstrating that creative engagement induces a sense of autonomy, which in turn heightens positive affect, leading to higher donation behavior. Study 4 tested H2b, showing that the path from positive affect to donation behavior is indeed multiply determined. In every study, we report all experimental conditions and measures as collected and disclose any eliminated data points when applicable. The sample size was predetermined for each study based on current experimental norms but varied within an acceptable range depending on actual participant sign-ups. Study 3, a supplementary study (follow-up to Study 3 reported in the Web Appendix), and Study 4 were preregistered on aspredicted.org (see respective studies for details). Pilot Study: Creativity and Monetary Donations—A Field ExperimentTo gain an initial understanding of whether engaging in a creative (as compared with neutral) activity enhances monetary donation, we collaborated with a registered U.S. nonprofit organization operating an animal shelter in a small city in the Southwestern United States (population 46,000 according to 2010 U.S. Census data). Every year, employees of this charity produce T-shirt designs that are printed and used as giveaways or sold in fundraising activities. To test our focal hypothesis, the charity agreed to open the T-shirt design activity to the public and use it as a fundraising event. The T-shirt design campaign was launched by the charity via its social media platform, inviting members of the public (i.e., potential donors) to create T-shirt designs as part of a donation appeal. The charity had set two overarching guidelines for this T-shirt design campaign: ( 1) the submitted designs were to follow the theme of ""Rescue 2020"" and ( 2) the charity's logo had to be part of the design. The charity managed the entire event.Relevant to our prediction (H1), we manipulated the opportunity to be creative (vs. not) within the T-shirt design campaign. While participants in both conditions had to develop a T-shirt design reflecting the charity's yearly theme and including its logo, those in the creativity condition were invited to develop an innovative T-shirt design and were explicitly instructed to be creative while doing so. Those in the neutral condition were not specifically instructed to be creative. Once participants submitted their designs, the charity presented them with a donation appeal that included a link to the donation page. At the end of the campaign period, the charity forwarded us the designs along with the corresponding donation amounts, having removed donors' identifying information (for additional methodological details, see Web Appendix A.1).To examine the relationship between creativity and donation behavior, we first coded the participants who did not donate as 0 and those who donated (any amount greater than zero) as 1. We then conducted a binary logistic regression analysis testing the effect of engaging in a creative T-shirt design activity on donation rate (i.e., the percentage of participants who donated to the charity). We found that a significantly higher percentage of people in the creativity condition (34.48%) donated, as compared with the percentage of people in the neutral condition (12.20%; χ2 = 4.97, p = .026). Next, we examined the effect of creative engagement on the amount of money that was donated. A Shapiro–Wilk test ([65]) indicated that the donation amount data was not normally distributed (p <.001). Thus, in accordance with prior research ([57]; [61]), we used a nonparametric Mann–Whitney U test for the analysis. We found that the average donation amount made by participants in the creativity condition (M = 7.07, SD = 13.06) was significantly higher than that of participants in the neutral condition (M = 1.10, SD = 3.26; U = 739.50, p = .016; for additional results, see Web Appendix A.2).By collaborating with a registered charity and assessing actual donation behaviors, we found initial evidence showing that including a creative activity as part of a donation appeal can be an effective approach to enhance donation behaviors (i.e., both higher donation rates and amounts). Interestingly, one could argue that the creativity of the generated output (i.e., the T-shirt design) may also have impacted the donation behavior. To test this alternative explanation, we asked two trained research assistants (employed within the domains of creativity and advertising, respectively) to rate each T-shirt design on its creativity (1 = ""not at all creative,"" and 7 = ""very creative""). Both raters were blind to the conditions and hypothesis. Validating our manipulation, a one-way analysis of variance (ANOVA) showed that the designs produced in the creative condition (M = 3.62, SD = .80) were rated as significantly more creative than those in the neutral condition (M = 3.05, SD = .98; F( 1, 68) = 6.72, p = .01). However, we did not observe a significant relationship between rated creativity of the generated designs and the donation rate (B = .04, t < 1) or the donation amount (B = .83, t < 1). We conducted a similar analysis using only the creative condition, where natural variability in output creativity may occur (even though everyone was instructed to be creative). Again, we did not find a significant relationship between the creativity of the generated designs and the donation rate (B = −.15, t = −1.34, p = .19) or the donation amount (B = −3.07, t < 1). Importantly, our conceptualization argues that it is the act of engaging in a creativity activity that leads to enhanced donation behavior, not the level of creative output achieved. Subsequent studies replicate this finding, consistently demonstrating that creative engagement itself, rather than the creativity of the generated outcome, positively impacts donation behavior (for brevity, these findings are reported in the respective Web Appendices).This pilot study provided initial evidence of the proposed effect, but as a real-life field study conducted in collaboration with a third party, it is not without limitations (dependence on the charity's social media platform for sampling, the messaging guidelines set by the charity, etc.; for discussion, see Web Appendix A.3). As such, our first study aimed to replicate these initial findings observed in the field in a more controlled lab setting (i.e., provide a more robust test of H1). Study 1: Creativity and Monetary DonationsStudy 1 used a donation context inspired and adapted from a real-life social enterprise known as Elephant Parade. This organization invites everyday consumers to create/paint their own elephant toy using an ""Artbox Kit"" (containing a small white clay elephant and a variety of colors) in return for a monetary donation. The proceeds are subsequently used for elephant welfare and conservation projects worldwide. MethodEighty-nine undergraduate students (49 women; Mage = 20.04 years, SD = 1.27 years) at a large North American university completed this study in exchange for course credit. To begin, participants were checked in and assigned to a designated computer desk, each of which was equipped with a small donation box (see Web Appendix B.1) and a white envelope containing $2 in quarters (i.e., eight quarters). The donation box was labeled with an Elephant Parade sticker and had a slit on the top. Four quarters were left in each donation box, creating the impression that the study administrator would not be able to tell if the participant donated or not, thus reducing any demand effects and obligation to donate. Participants were told that, in addition to the course credit, they would receive $2 (in an envelope on their desks) as a token of appreciation for their participation.The experiment adopted a one-way design in which participants were assigned to complete either a creative or neutral activity, randomized by session (i.e., we ran only one condition per session). A drawing activity (inspired by Elephant Parade's clay elephant painting) was used to induce the focal manipulation. Participants were told that the researchers wanted to put their minds at ease before the study commenced and would therefore like them to engage in a coloring activity. All participants were given a sheet of paper with a picture of an elephant (see Web Appendix B.2) and asked to color it. Those in the creative condition received a box of Crayola markers in ten different colors and were told to be as creative as possible while coloring and decorating their elephant pictures. They were also told to use any number and variety of colors they liked for the task. In contrast, those in the neutral condition received gray crayon markers only, and were told to simply color the elephant picture. In both conditions, participants were asked to spend no more than five minutes on the coloring activity. The elephant coloring task, though based on a real-life activity (i.e., Elephant Parade donation protocol), mimics a widely used creativity task in the literature: the alien task ([72]). In these types of activities, creative thinking encourages people to violate standard characteristics of a stereotypical object (e.g., an elephant, as in our study; [48]; [50]). A stereotypical elephant picture would be colored gray, whereas a nonstereotypical elephant picture would be multicolored.Next, participants were presented with the donation opportunity and informed that the researchers were helping raise money for the nonprofit organization Elephant Parade. Participants read a short description and donation appeal from Elephant Parade (see Web Appendix B.3), and were asked if they would like to contribute; they could donate any amount of the participation money (eight quarters) they wished, and put it in the donation box. The number of quarters each participant donated served as the key dependent variable. Finally, all participants provided their demographic information (age and gender) and were debriefed before being dismissed. (Gender and age were captured in all studies. However, no effects were observed for these variables in either this or any other study. For the sake of brevity, we do not discuss them further.) After each session, the research assistants removed the quarters from the donation boxes and recorded the number of donated quarters (i.e., the total number of quarters in the box minus the four quarters initially placed in each donation box). Results Preliminary analysesThe elephant designs were rated on creativity (1 = ""not at all creative,"" and 7 = ""very creative"") by a research assistant who was blind to the conditions and hypothesis (for sample designs, see Web Appendix B.4). A one-way ANOVA confirmed that the designs produced in the creative condition (M = 4.24, SD = 1.61) were significantly more creative than those produced in the neutral condition (M = 1.51, SD = .95; F( 1, 87) = 97.41, p <.001). Donation behaviorsFirst, we explored whether there was any difference between the two conditions on the donation rate (i.e., the percentage of participants who donated to the Elephant Parade foundation) and found that a significantly higher percentage of participants (80.95%) in the creative condition—as compared with those in the neutral condition (55.32%)—donated (χ2 = 6.83, p = .009). Next, we analyzed the effect of activity type on donation amount, which was assessed by the number of quarters donated to the Elephant Parade after completing either the creative or neutral activity. The donation data did not meet the normal distribution criteria (Shapiro–Wilk test: p <.001; [65]); therefore, a nonparametric Mann–Whitney U test was again used for the analysis. We found that those who completed the creative activity (M = 4.50 quarters, SD = 3.19) donated a significantly higher number of quarters than those who completed the neutral activity (M = 2.98 quarters, SD = 3.54; U = 720, p = .022). DiscussionThe obtained results provide support for our focal hypothesis (H1), namely, that engaging in a creative activity enhances donation behavior, in terms of both the likelihood of donation (i.e., donation rate) and the donation amount. The study utilized a creative activity adapted from a real-life charity and assessed donation behavior through real monetary donations. It demonstrated that engaging potential donors in creative activities, before soliciting them for donations, can be an effective way to enhance donation behavior.One potential criticism of this study could be the different number of colors provided in the creative versus neutral conditions. However, this procedure was necessary to manipulate creativity within the context of the study. Offering a variety of colors provides participants with an opportunity for creativity, that is, to think outside the box and beyond the stereotypical characteristics of an elephant (i.e., all gray). The sole use of gray crayon markers in the neutral condition, in contrast, conforms to the stereotypical characteristics of an elephant and curtails creative opportunity. To address this potential limitation, in future studies we adopt contexts in which we can provide the same materials to participants in both conditions.In both the pilot study and Study 1, the creative activity was directly related to the charitable cause, thereby raising a question about the generalizability of the effect—that is, whether the observed effect is domain specific or whether it holds when the creative activity is independent of the donation context. We explore this possibility in Study 2. In addition, our studies do not delineate whether the obtained results were observed because participants engaged in a creative activity (as hypothesized), or simply because the concept of creativity was salient for the participants in the creativity condition. In other words, is it necessary to actually engage in a creative activity, or can mere exposure to the concept of creativity also enhance donation behaviors? Prior research has shown that priming creativity (making creativity salient without engagement) can influence cognitive processing, thereby affecting people's propensity to engage in dishonest behaviors ([27]). We examine this question in the next study. Study 2: Creative Stimuli Versus Creative ActivityStudy 2 was aimed to discern whether engagement in a creative activity is needed to obtain the identified effects, or rather, if exposure to a simple creative prime would suffice. To this end, we added another focal condition to the experimental design used in Study 1: this time participants were exposed to creative stimuli only, with no opportunity to participate in a creative activity. Interestingly, this condition mimics the default strategy of many charitable organizations, in which potential donors are presented solely with a donation appeal (without an opportunity for active engagement). In addition, to test the context-independent nature of the effect, the creativity activity was kept independent from the donation context (i.e., the charitable cause). Finally, we conducted the study in a real-life setting; we followed a format used by baked goods company C. Krueger's, which hosts a holiday charity event wherein customers are invited to decorate cookies and make purchases. For our study, two booths were set up in the lobby of a university building with high foot traffic, featuring large signs advertising a cookie decoration event sponsored by the charity ChildHelp. Passersby were invited to participate in the event and decorate a cookie before being solicited for a monetary donation. MethodOne hundred seventy adults (82 women; Mage = 21.09 years, SD = 2.47 years) agreed to participate in the event and were assigned to one of the three treatment conditions: creative engagement, creative exposure without engagement, or neutral engagement. The conditions were randomized and rotated by the hour. Once passersby agreed to participate in the event, they were told they would receive $2 as a token of appreciation for participating in the event. They were given a white envelope containing eight quarters and asked to sign a form indicating receipt of the money. The signing process was necessary for participants to feel ownership of the money they had earned, before being solicited for donations later in the study. Prior research has shown that signing one's name increases this sense of ownership ([22]; [68]).Next, one of the ""staff members"" guided individual participants to a table bearing a plain cookie on a paper plate, four different icing colors, and a spatula. They were also handed an event participation instruction sheet, which served as our key manipulation. Each instruction sheet had the ChildHelp Foundation logo at the top, with ""ChildHelp Foundation Annual Charity Event"" printed underneath (see Web Appendix C.1). The task manipulation for the two engagement conditions (i.e., cookie decoration) was adapted from [18]. In the creative engagement condition, participants were told that this was an annual charity event hosted by the ChildHelp Foundation and, as part of the event, we wanted them to decorate their cookie in the most creative manner possible using the provided materials. Those in the neutral engagement condition were given a picture of a routinely (i.e., noncreatively) decorated cookie (see Web Appendix C.2) and asked to ice the cookie as shown in the picture, using the provided materials. Those in the creative condition had the freedom to use their imagination and creativity to come up with a novel cookie design, thereby promoting creative engagement. However, in the neutral condition, participants were simply asked to copy the noncreative cookie as depicted, negating any potential creative engagement process. In the creative exposure (without engagement) condition, in keeping with prior research showing that exposure to creative images can make the concept of creativity accessible ([76]), we simply showed participants three creative cookie designs (see Web Appendix C.3) and asked them to choose the most creative one.To assess whether our manipulation made the concept of creativity salient, we conducted a separate online study. In this study, participants were randomly assigned to complete one of the three treatment condition tasks used in the main study (creative engagement, neutral engagement, or creative exposure without engagement). They were then presented with two types of measures that captured the salience of creativity implicitly and explicitly. The obtained results showed that, as anticipated, the concept of creativity was equally salient for both the creative engagement and creative exposure conditions, and both were significantly higher than the neutral condition (for study details and complete results, see Web Appendix C.4).Next, in the main study, all participants were given a manila envelope with a survey featuring a donation appeal and some questions about the cookie event. Each envelope was marked with a unique identification number to enable us to match participants' survey responses, donation amounts, and their assigned condition. All participants were presented with a donation appeal from the ChildHelp Foundation: a nonsectarian, nonpolitical, registered charity dedicated to helping children living in distress in North America and overseas. Furthermore, participants were told that if they decided to contribute, they could put the quarters they wanted to donate in the manila envelope and leave it in the box beside their table. Lastly, to gain initial insights into the underlying process, we captured exploratory measures of participants' positive affective state and their perceived donation impact (for details, see Web Appendix C.5). At the end of the study, the participants in the two engagement conditions were invited to take their cookie with them, while those in the creative exposure without engagement condition were given a cookie at the end of the study (for consistency with the other two conditions). ResultsWe first examined the donation rate by calculating the percentage of participants who donated in each condition. A logistic regression revealed a significant difference in the donation rates across the three conditions (χ2( 2) = 12.26, p = .002). A significantly higher percentage of participants in the creative engagement condition (81.03%) donated, compared with both those in the creative exposure (without engagement) condition (50.88%; χ2( 1) = 11.01, p = .001) and those in the neutral condition (61.82%; χ2( 1) = 4.98, p = .026). We observed no difference between the latter two conditions (p = .244). Next, we assessed the donation amount, with the number of donated quarters serving as our key dependent variable. A Shapiro–Wilk test ([65]) indicated that our data were not normally distributed (p <.001); therefore, we used the nonparametric Kruskal–Wallis test for the analysis. The obtained results revealed a significant overall main effect of the activity type on monetary donation (H( 2) = 9.8, p = .007). Pairwise comparisons showed that those in the creative engagement condition (M = 6.10 quarters, SD = 3.30) donated significantly more quarters than both those who were in the creative exposure (without engagement) (M = 4.04 quarters, SD = 3.97, p = .003) and neutral engagement (M = 4.49 quarters, SD = 3.85, p = .021) conditions. There was no difference between the latter two conditions (p = .52). DiscussionIn this study, we created a charity event in which individuals participated in different activities before receiving a donation solicitation. In line with our predictions, we found that participating in an activity that enabled creative engagement (i.e., creatively decorating a cookie) enhanced donation behavior (as compared with those who either reproduced a routine cookie design or were merely exposed to creative cookie designs). Importantly, the obtained results showed that donation behavior is only enhanced when participants actually engage in a creative activity, not simply when the notion of creativity is made salient. Further, the creative activity utilized in this study was independent of the charity cause, thereby demonstrating the context-independent nature of the effect.Our findings, so far, provide consistent evidence for the hypothesized relationship between creative engagement and donation behavior. In the following studies, we extend our examination to understand the underlying process through which creative activity and higher donation behavior are connected. In particular, we examine the mediating role of positive affect in this relationship. In addition, in Study 3 we also test the role of autonomy as a driver of creative engagement's impact on positive affect, which consequently influences donation behavior. Study 4 then explores why positive affect has such a significant impact on donation behavior. Finally, given the null findings in the creative exposure condition (on donation behavior), we dropped this condition from subsequent studies. Study 3: Testing the Role of Autonomy and Positive AffectStudy 3 was conducted to fully test H2a and identify the underlying process through which creative engagement impacts donation behavior. In particular, we tested our prediction that engaging in a creative activity heightens one's sense of autonomy, which in turn induces positive affect, leading to higher donation behavior. This study was preregistered on aspredicted.org (https://aspredicted.org/blind.php?x=ag5ci3). MethodTwo hundred adults (117 women; Mage = 32.76 years, SD = 11.51 years) recruited from the online platform Prolific completed this study in exchange for a small monetary compensation. At the outset, participants were told that in addition to their regular compensation, they would also receive $1 as a thank-you bonus for completing the study, with an opportunity to spend this money later if they chose to. They were further informed that the study was being conducted in collaboration with the charitable organization ChildHelp Foundation and were provided with information about the organization (see Web Appendix D.1). Next, all participants were told that the ChildHelp Foundation runs an annual charitable event, wherein individuals are invited to participate in various tasks. However, given the COVID-19 pandemic, this year's event would be virtual, and the organization needed help planning the function. Thus, the organization was inviting them to participate in this study as if they were actual donors participating in the event.The activity type manipulation used an idea generation task, mimicking the ""Think outside the cereal box"" campaign Kellogg launched several years ago. In the creative condition, participants were told that as part of its annual charity event, the ChildHelp Foundation was inviting them to ""think outside the cereal box"" and generate a fun and creative way to use Froot Loops cereal, besides eating it for breakfast. Further, participants were told to be as creative as possible and use their imagination to generate an innovative way to use Froot Loops cereal. In the neutral condition, participants were asked to ""think about the cereal"" and share a traditional way of how they eat the Froot Loops cereal (for detailed instructions, see Web Appendix D.2).Once participants completed the Froot Loops task, we assessed their donation behavior, sense of autonomy, and affective state. To capture participants' donation behavior, they were told that the ChildHelp Foundation was seeking donations, and they could help by donating part or all of their $1 bonus (in multiples of $.10) to the charity. All participants were then provided with a scale from $0 to $1 in increments of $.10 to indicate their donation amounts.Sense of autonomy was measured by adapting established measures defined in the literature (i.e., [18]; [56]). Specifically, participants were asked to indicate how they felt while generating their ideas during the Froot Loops task: ( 1) ""To what extent did you feel you had autonomy in generating your ideas during the Froot Loops task?,"" ( 2) ""To what extent did you feel you had freedom in coming up with your ideas for the Froot Loops task?,"" ( 3) ""How free did you feel in generating your ideas for the Froot Loops task?,"" and ( 4) ""How much did you feel you were able to express yourself when generating your ideas for the Froot Loops task?"" (1 = ""not at all,"" and 7 = ""very much""). Next, to measure positive affect, they were asked to think back to the Froot Loops activity and indicate how they felt during this activity on 11 items adapted from [19] and [18]. Specifically, all participants reported how they felt while completing the Froot Loops activity: ( 1) 1 = ""very negative,"" and 7 = ""very positive""; ( 2) 1 = ""very unpleasant,"" and 7 = ""very pleasant""; ( 3) 1 = ""not at all nice,"" and 7 = ""very nice""; and ( 4) 1 = ""very bad,"" and 7 = ""very good."" This was followed by seven items anchored with 1 = ""not at all,"" and 7 = ""very much,"" asking ( 5) how positive they felt during the Froot Loops activity, ( 6) the extent to which they enjoyed the Froot Loops activity, ( 7) the extent to which they had a good time during the Froot Loops activity, ( 8) how much fun the Froot Loops activity was, ( 9) how satisfied they felt during the Froot Loops activity, (10) how pleasurable the Froot Loops activity was, and (11) how exciting the Froot Loops activity was. Results Preliminary analysesA trained research assistant blind to hypothesis and condition rated the creativity of the generated Froot Loops ideas. As we expected, a main effect of creativity emerged such that those in the creative condition (M = 3.89, SD = 1.29) generated more creative ideas than those in the neutral condition (M = 1.24, SD = .81; F( 1, 198) = 301.96, p <.001, Cohen's d = 2.46). Donation behaviorsWe first examined the effect of engaging in the creative (vs. neutral) activity on the donation rate. A binary logistic regression revealed that a significantly higher percentage of participants in the creative condition (63.37%) donated money, compared with the percentage of participants in the neutral condition (45.45%; χ2( 1) = 6.50, p = .01). Next, we examined the difference between the creative and neutral conditions' donation amounts. As in previous studies, a Shapiro–Wilk test ([65]) indicated that the data were not normally distributed (p <.001); thus, we used the nonparametric Mann–Whitney U test for the analysis. Replicating the results from the previous studies, those who engaged in the creative (M = 41.49¢, SD = 41.60¢) versus the neutral (M = 27.07¢, SD = 37.04¢) activity donated significantly more money (U = 6,034.50, p = .007). Process measuresFactor analysis showed that all four items used to capture participants' sense of autonomy loaded onto the same factor; therefore, we averaged them to create a sense of autonomy index (α = .92). A one-way ANOVA revealed that those in the creative condition (M = 6.05, SD = 1.02) reported a significantly higher sense of autonomy than those in the neutral condition (M = 4.94, SD = 1.73; F( 1, 198) = 30.61, p <.001, Cohen's d = .78). In addition, factor analysis showed that the 11 items used to capture participants' positive affective state loaded onto the same factor, and we therefore averaged them to create a positive affect index (α = .96). A one-way ANOVA revealed that those in the creative condition (M = 5.24, SD = 1.24) reported a significantly higher positive affective state than those in the neutral condition (M = 4.52, SD = 1.31; F( 1, 198) = 16.15, p <.001, Cohen's d = .56). Mediation analysisTo test the potential underlying process paths, we first examined the mediation effect of positive affect on the creative engagement/donation rate relationship. A bias-corrected bootstrap confidence interval obtained by resampling the data 10,000 times (with activity type as the independent variable, positive affect as the mediator, and the donation rate as the dependent variable) did not include zero (β = .31, SE = .12, bias-corrected 95% confidence interval [CI] = [.11,.59]), indicating a significant indirect (i.e., mediation) effect. Next, we conducted a serial mediation analysis to understand the role of autonomy in this relationship. Serial mediation (Model 6, [35]) conducted with activity type as the independent variable, sense of autonomy and positive affect as the serial mediators (in that order), and the donation rate as the dependent variable together revealed the presence of a significant indirect effect (β = .21, SE = .09, bias-corrected 95% CI = [.07,.41]).We also conducted the same mediation analyses for the donation amount. A bias-corrected bootstrap confidence interval obtained by resampling the data 10,000 times (with activity type as the independent variable, positive affect as the mediator, and the donation amount as the dependent variable) did not include zero (β = 4.71, SE = 1.99, bias-corrected 95% CI = [1.32, 9.02]), indicating a significant indirect (i.e., mediation) effect. Next, we conducted a serial mediation analyses to understand the role of sense of autonomy. A serial mediation (Model 6, [35]) conducted with activity type as the independent variable, sense of autonomy and positive affect as the serial mediators (in that order), and the donation amount as the dependent variable together revealed the presence of a significant indirect effect (β = 2.99, SE = 1.48, bias-corrected 95% CI = [.29, 6.10]). DiscussionStudy 3 results replicated the findings from the previous studies and showed that engaging in a creative (as compared with neutral) activity enhances donation behaviors (H1). Further, the findings from this study highlighted the underlying processes through which creative engagement affects monetary donation. As we hypothesized, creative (vs. neutral) engagement induces a higher positive affect, which in turn leads to enhanced donation behavior. Importantly, we found that sense of autonomy drives the relationship between creative engagement and positive affect (H2a is fully supported). To further confirm the role of autonomy in this relationship, we conducted a supplementary study in which we directly manipulated the sense of autonomy felt by the participant. Here, we showed that when felt autonomy is mitigated, the positive effect of creative engagement on positive affect (and, in turn, the donation behavior) is attenuated (for the details of this supplementary study, see Web Appendix E).In the next study, we further explicate the underlying process through which creative engagement impacts donation behavior by examining the pathways through which positive affect leads to higher donation behavior. Study 4: Exploring Why Positive Affect Impacts Donation BehaviorWe conducted Study 4 to test H2b and provide additional insight into the underlying process through which creative engagement impacts donation behavior. In particular, we assessed the possible role of mood maintenance, social connection, and perceived donation impact in driving positive affect's influence on donation behavior outcomes. This study was also preregistered on aspredicted.org (https://aspredicted.org/blind.php?x=j8rj2w). MethodTwo hundred adults (109 women; Mage = 34.84 years, SD = 12.91 years) recruited from the online platform Prolific completed this study in exchange for a small monetary compensation. At the outset, participants were told that in addition to their regular compensation, they would also receive $1 as a thank-you bonus for completing the study, with an opportunity to spend this money later if they chose to. They were further informed that the study was being conducted in collaboration with the charitable organization Healthier Tomorrow and were provided with information about the organization (see Web Appendix F.1). Next, all participants were told that Healthier Tomorrow runs an annual charitable event, wherein individuals are invited to participate in various tasks. However, given the COVID-19 pandemic, this year's event was going to be virtual, and the organization needed help planning the function. Thus, the organization wanted to invite them to participate in this study as if they were actual donors participating in the event.Next, participants were randomly presented with either the creative or neutral version of the event and asked to create (reproduce) a T-shirt design. Those in the creative condition were specifically asked to design an innovative T-shirt and be as creative as possible (for detailed instructions and the sample designs produced by participants, see Web Appendix F.2). In the neutral condition, participants were simply provided with a generic T-shirt design and asked to reproduce it, thus negating any potential creative engagement (for detailed instruction and the T-shirt design provided to the participants, see Web Appendix F.3). Participants were then directed to the T-shirt customization website (customink.com) to complete the design activity. Once participants had finished creating (reproducing) their designs they were asked to save them with a unique ID provided in the survey and then use the save/share function in the T-shirt customization website to email their design to a designated email address created for the study.Once participants completed and submitted information about their designs, we assessed their donation behavior, affective state, mood maintenance, social connection, and perceived donation impact. To capture participants' donation behavior, we told them that Healthier Tomorrow was seeking donations, and they could contribute by donating part or all of their $1 bonus (in multiples of $.10) to the charity. All participants were then provided with a scale from $0 to $1 in increments of $.10 to indicate their donation amounts.To assess participants' positive affective state, we asked them to think back to the T-shirt design task and indicate how they felt during this activity, on the same 11 items used in Study 3 (adapted from [19]] and [18]]). Participants responded to a mood-maintenance measure adapted from [23], where they were asked to think about their donation decision and indicate their agreement with the following statements on seven-point scales (1 = ""not at all,"" and 7 = ""very much""): ""I thought ..."" ( 1) ""I would feel good about myself if I donate,"" ( 2) ""donating will make me feel good,"" and ( 3) ""if I donate it would be a personally rewarding experience."" To measure social connection, we adapted items from [ 8] to suit the context of our study. Participants responded to the following items on seven-point scales (1 = ""not at all,"" and 7 = ""very much""): ""To what extent did you feel ..."" ( 1) ""closer to Healthier Tomorrow,"" ( 2) ""connected to Healthier Tomorrow,"" and ( 3) ""that completing the design task affected the way you think about the relationship with Healthier Tomorrow."" We measured participants' perceived donation impact by means of three items adapted from [14] and [67]. These items specifically asked the participants how much they thought their donation could ( 1) make a positive difference, ( 2) be valuable, and ( 3) do a lot of good (1 = ""not at all,"" and 7 = ""very much""). Results Preliminary analysisTen participants did not complete the T-shirt design activity and were excluded from the analysis (including these participants in the analysis does not change the significance or pattern of results; see Web Appendix F.4). Donation behaviorsWe first examined the effect of engaging in the creative (vs. neutral) activity on the donation rate. A binary logistic regression revealed that a significantly higher percentage of participants in the creative condition (47.87%) donated money, compared with the percentage of participants in the neutral condition (27.08%; χ2( 1) = 8.85, p = .003). Next, we examined the difference in donation amounts between the creative and neutral conditions. As in previous studies, a Shapiro–Wilk test ([65]) indicated that the data were not normally distributed (p <.001); thus, we used the nonparametric Mann–Whitney U test for the analysis. In a replication of the results from the previous studies, those who engaged in the creative (M = 22.34¢, SD = 31.81¢) versus the neutral (M = 11.25¢, SD = 24.63¢) activity donated significantly more money (U = 5,517, p = .002). Process measureFactor analysis showed that all 11 items used to capture participants' positive affective state loaded onto the same factor, and we therefore averaged them to create a positive affect index (α = .97). A one-way ANOVA revealed that those in the creative condition (M = 5.32, SD = 1.32) reported a significantly higher positive affective state than those in the neutral condition (M = 4.72, SD = 1.40; F( 1, 188) = 9.26, p = .003, Cohen's d = .45). Mediation analysisAs an initial step, we examined the mediation effect of positive affect on the creative engagement/donation rate relationship. A bias-corrected bootstrap confidence interval obtained by resampling the data 10,000 times (with activity type as the independent variable, positive affect as the mediator, and donation rate as the dependent variable) did not include zero (β = .19, SE = .11, bias-corrected 95% CI = [.03,.48]), indicating a presence of a significant indirect (i.e., mediation) effect.Next, we examined the pathways through which positive affect, as induced by engaging in creative activity, impacts donation rate. We tested a sequential-parallel mediation model with creative engagement as the independent variable, positive affect as the first mediator, three factors (mood maintenance, perceived donation impact, and social connection) as a second set of mediators in parallel, and donation rate as the dependent variable (see Figure 1) using structural equation modeling (for statistics for each path in the model, see Table 1). This model is very similar to a serial mediation model; however, no order is assumed among the second set of mediators (i.e., mood maintenance, perceived donation impact, and social connection) ([21]). We used bootstrapping procedures to compute 95% CIs by generating 10,000 resamples. The results indicated significant serial indirect effects through positive affect and mood maintenance (β = .031, SE = .014, bias-corrected 95% CI = [.011,.068], p = .001) and positive affect and perceived donation impact (β = .031, SE = .014, bias-corrected 95% CI = [.010,.067], p = .001). However, the serial indirect effect of positive affect and social connection on the creative engagement and donation rate relationship was not significant (β = .005, SE = .012, bias-corrected 95% CI = [−.015,.035], p = .57). Interestingly, with positive affect in the model, creative engagement did not directly impact any of the second set of three mediators, thereby demonstrating the importance of positive affect in the conceptualization. Positive affect positively influenced all three potential mediators, but only mood maintenance and perceived donation impact significantly impacted donation rate. Thus, the positive affect experienced during a creative activity bolstered the desire for mood maintenance and the perceived donation impact, which in turn enhanced donation behaviors.Graph: Figure 1. Sequential-Parallel Mediation Model (Study 4).GraphTable 1. Sequential-Parallel Mediation Model (Study 4). PathPredictorOutcomePath Estimates (Standardized)SE95% CIa1Activity typePositive affect.22.20[.08,.35]a2Activity typeMood maintenance−.14.21[−.27, −. 01]a3Activity typePerceived donation impact.09.23[−.05,.23]a4Activity typeSocial connection.09.19[−.03,.21]b′1Positive affectDonation rate−.13.03[−.29,.02]Donation amount−.141.98[−.28,.002]b2Mood maintenanceDonation rate.31.02[.14,.46]Donation amount.251.32[.06,.42]b3Perceived donation impactDonation rate.37.02[.20,.52]Donation amount.321.16[.17,.46]b4Social connectionDonation rate.04.02[−.13,.21]Donation amount.041.43[−.16,.23]c′Activity typeDonation rate.19.06[.05,.32]Donation amount.173.81[.04,.29]d1Positive affectMood maintenance.51.07[.38,.61]d2Positive affectPerceived donation impact.41.08[.28,.53]d3Positive affectSocial connection.63.07[.52,.72] 1 Notes: Path analysis assessing the effect of creative (vs. noncreative) engagement on donation rate/amount through positive affect, mood maintenance, perceived donation impact, and social connection (estimates and 95% CIs for individual paths).A similar analysis was conducted for donation amount. First, a bias-corrected bootstrap confidence interval obtained by resampling the data 10,000 times (with activity type as the independent variable, positive affect as the mediator, and the donation amount as the dependent variable) did not include zero (β = 1.80, SE = 1.13, bias-corrected 95% CI = [.04, 4.44]), indicating a significant indirect (i.e., mediation) effect of positive affect. Next, to examine the pathways through which positive affect impacts donation, we used structural equation modeling to test the hypothesized process model, with donation amount as the dependent variable (for statistics for each path in the model, see Table 1). Ninety-five percent confidence intervals obtained by generating 10,000 bootstrap resamples indicated significant serial indirect effects through positive affect and mood maintenance (β = 1.51, SE = .78, bias-corrected 95% CI = [.40, 3.72], p = .004) and positive affect and perceived donation impact (β = 1.60, SE = .76, bias-corrected 95% CI = [.53, 3.65], p = .001). However, similar to what we observed for the donation rate, the serial indirect effect of creative engagement through positive affect and social connection on donation amount was not significant (β = .33, SE = .81, bias-corrected 95% CI = [−1.06, 2.26], p = .58). DiscussionThe Study 4 results replicated the findings from the previous studies and showed that engaging in a creative (vs. neutral) activity induces positive affect, which in turn enhances donation behaviors. Importantly, this study further examined the underlying process, providing understanding of how positive affect influences donation behavior. We found that the path from positive affect to donation behavior was multiply determined. Indeed, perceived donation impact and mood maintenance were both shown to be drivers of the affect/donation relationship. Interestingly, we did not find evidence for social connection as a mechanism that triggered the identified effects. Thus, it appears that attributions flowing from the positive experience of creative engagement are more at play in defining enhanced donation behavior, and efforts to maintain positivity also spill over into the donation outcomes. Future research should continue to explore these identified mechanisms and discern within which charitable contexts they are most applicable and effective. General DiscussionThe current research examined the relationship between creativity and donation behavior. A pilot study and four subsequent experiments demonstrated that engaging in a creative activity induces a sense of autonomy, which leads to a positive affective state, which in turn results in enhanced donation behaviors (i.e., the likelihood of donation and the monetary amount donated; for a summary of all study results, see Table 2). We further showed that the positive affect experienced by the creator leads to enhanced donation behavior, due to perceptions of increased donation impact and an effort to maintain the resulting positive mood.GraphTable 2. Summary of Study Results. Donation AmountDonation RateUnits DonatedTask EngagementTask EngagementStudyCreativeNeutralNoneCreativeNeutralNonePilot StudyUSD ($)7.07a(13.06)1.10a(3.26)—34.48%a12.20%a—Study 1Number of quarters (maximum 8)4.50a(3.19)2.98a(3.54)—80.95%b55.32%b—Study 2Number of quarters (maximum 8)6.10a b(3.30)4.49a c(3.85)4.04b c(3.97)81.03%a b61.82%a c50.88%b cStudy 3USD (¢)41.49b(41.60)27.07b(37.04)63.37%b45.45%bSupplementary study (follow-up to Study 3)USD ($)Control12.52ab(7.58)8.43ab(7.10)87.88%ad75.41%cdAutonomy inhibited9.60ac(8.30)11.84ac(7.72)70.77%ac80.33%cStudy 4USD (¢)22.34b(31.81)11.25b(24.63)47.87%b27.08%b 2 Notes: Standard deviations are in parentheses. The contrasts are identified with superscript notation: ap ≤.05, bp ≤.01, cp >.1, dp ≤.1. Implications for PracticeOur work was motivated by the documented fact that charitable organizations often struggle to find effective ways to engage donors and solicit donations ([59]). Thus, a central contribution of our research is in confirming that engaging potential donors through creativity can meet this challenge by increasing engagement and enhancing donation behaviors. Substantively, we can recommend incorporating creative activities into fundraising campaigns and charity events as a viable marketing strategy. Indeed, creative activities can be implemented through social media platforms (as exemplified in our pilot study) or in person during charity events and solicitations (as exemplified in Study 2). Current industry practices suggest that some charities have begun testing this approach (i.e., engaging potential donors through creative activities before soliciting them for donations). For example, Roots and Shoots (a charitable organization that supports environmental, conservation, and humanitarian issues) regularly posts a variety of gamified challenges on its website and invites potential donors to participate. Many of these challenges encourage people to incorporate creativity in defining their solutions (e.g., for a ""World Chimpanzee Day Challenge,"" people were asked to design and submit a creative communication graphic to spread awareness about chimpanzee protection).To gain additional insights on practitioners' points of view (concerning our proposed strategy), we sent an email to 220 charities nationwide, inviting them to participate in a short survey. The survey asked three questions that measured the usefulness and applicability of this donation strategy. The first question assessed whether the charity had previously used a creative activity as a preface to a donation request. The second question asked whether, if presented with evidence that engaging donors through a creative activity increases monetary donation, they would implement this strategy in their donation campaigns (1 = ""not very likely,"" and 7 = ""very likely""). The final question assessed how feasible they thought it would be to implement such a strategy (1 = ""not very feasible,"" and 7 = ""very feasible""). We obtained 29 responses from the surveyed national charities (13% response rate). Interestingly, 45% of the charities mentioned that they have previously used a creative activity as a preface to a donation request—showing that our research validates a tactic already in use by some charities today. Most importantly, charities indicated they would definitely be willing to implement this strategy in their donation campaigns (M = 6.55, SD = .69; t(28) = 20.04, p <.001, compared with the midpoint) and thought it would be feasible to implement such a strategy (M = 5.38, SD = 1.68; t(28) = 4.43, p <.001, compared with the midpoint). Though a small sample, these results are encouraging and affirm that utilizing creative activities in charity campaigns is both highly relevant and feasible in the marketplace. Theoretical ContributionsThe current work also provides several theoretical contributions to the field. First, we advance the marketing and charity literature streams by identifying that positive affect experienced during a creative activity is a key mechanism that bolsters subsequent donation behaviors. Second, we offer a deeper understanding of why engaging in a creative activity leads to higher donation behavior through positive affect. Specifically, we show that creative engagement enhances a sense of autonomy, which in turn induces positive affect, which then positively impacts donation behavior. In addition, the relationship between positive affect and enhanced donation behavior is shown to be multiply determined. We identify two specific mechanisms that link affect and behavior: namely, the positive attributions of the impact of one's donation and the mood maintenance tendency of the participant. Third, we establish that the act of creativity itself (not just being primed with creativity as a construct) is a necessary condition to achieve beneficial donation outcomes. Finally, we confirm that the creative activity employed need not relate directly to the organization and/or charitable cause underlying the sought-after donation behavior. This is important both theoretically and practically, as it establishes generalizability in our findings and provides more freedom to charities in defining the type of creativity activity appropriate for their donation campaigns.More broadly, the current research adds to prior work demonstrating the consequences of engaging in creative thinking tasks. While a significant amount of research has been devoted to studying various factors and cognitive processes that impact creativity ([28]; [54]; [55]), much less attention has been paid to the implications and outcomes of being creative ([27]; [71]). Our research shows that there is value in understanding what implications creativity may have for subsequent consumption behaviors. Building up our understanding of the importance of creativity is especially significant in today's consumption environment, where customers are increasingly provided with opportunities to engage in creative activities, from participating in crowdsourcing platforms (e.g., MyStarbucksIdea.com, ideas.lego.com) to engaging in customization processes (e.g., NikeID, Casetify customized phone cases). Limitations and Future ResearchLimitations inherent to our research approach open up several avenues for additional investigation. First, research should be directed toward developing a better understanding of the generalizability of the effects we identify. Although we demonstrated that a creative activity does not have to be specific to the charity in question to provide a positive outcome, we did not assess a broad range of charities and donation appeals. To this end, we conducted a preliminary study examining the impact of the inherent history of the charity (i.e., whether the charity was well-established; for study details; see Web Appendix G) on donation behavior. Here, we found that creative engagement indeed led to enhanced donation behavior, but only when the charity was newly established. When the charity was well-established, the donation behavior was enhanced irrespective of the type of activity utilized in the appeal. Additional research is needed to better explore this potential boundary condition and, more generally, to define other contextual factors that might moderate the effects we have documented here.Second, most of the creative activities tested in this research involved artistry and design (e.g., cookie decorating, T-shirt design, coloring). It remains to be seen whether other forms of creativity could produce similar effects. Indeed, we believe that the effects identified in this research are likely to be observed for any enjoyable creative activities that encourage people to explore and think freely. However, we conjecture that the positive effects defined herein may be attenuated if the creative activity is more convergent in nature (e.g., identifying the one right solution). While we did not directly test the effect of a convergent creative activity on donation behavior, prior research has found that engaging in convergent creative tasks may not lead to a positive affective state ([ 9]). Future research should explore this possibility and outline the breadth of creative activities that are effective precursors to enhanced donation behaviors.Another interesting research question arising from our work concerns the identified difference between creative engagement and creative priming (on subsequent behaviors). We found that engaging in creative activities leads to higher donation behavior, but exposure to creative stimuli does not have a parallel effect. Previous research ([27]), however, has shown that creative priming can influence cognitive processes. Thus, it is important to further distinguish between creative engagement and exposure to creative materials and understand how they differentially impact subsequent behaviors. While both creative engagement and creative priming may influence cognition, perhaps only creative engagement can induce a positive affective state. Future research could further clarify the differences between these stimuli.Finally, future research should continue to build understanding as to when creativity leads to positive (vs. negative) outcomes. Indeed, prior research has shown both positive and negative implications for creative thinking. For example, creativity has been shown to help overcome the burden of secrecy ([29]) and to enhance one's tendency to take a target's perspective ([77]). On the negative side, previous research has found that creativity can lead to dishonesty (e.g., [27]) and enhance unhealthy choices ([31]). We find an opportunity for future work in building on these initial studies to better understand where creativity can influence downstream consumption behaviors. Indeed, we hope that future research will expand on our findings and further investigate the outcomes of creativity for individuals, charities, nonprofit organizations, and the broader marketplace at hand. Supplemental Materialsj-appendix-10.1177_00222429211037587 - Supplemental material for Leveraging Creativity in Charity Marketing: The Impact of Engaging in Creative Activities on Subsequent Donation BehaviorSupplemental material, sj-appendix-10.1177_00222429211037587 for Leveraging Creativity in Charity Marketing: The Impact of Engaging in Creative Activities on Subsequent Donation Behavior by Lidan Xu, Ravi Mehta and Darren W. Dahl in Journal of Marketing  "
14,"Leveraging Creativity in Charity Marketing: The Impact of Engaging in Creative Activities on Subsequent Donation Behavior Charities are constantly looking for new and more effective ways to engage potential donors in order to secure the resources needed to deliver services. The current work demonstrates that creative activities are one way for marketers to meet this challenge. Field and lab studies find that engaging potential donors in creative activities positively influences their donation behaviors (i.e., the likelihood of donation and the monetary amount donated). Importantly, the observed effects are shown to be context independent: they hold even when potential donors engage in creative activities unrelated to the focal cause of the charity (or the charitable organization itself). The findings suggest that engaging in a creative activity enhances the felt autonomy of the participant, thus inducing a positive affective state, which in turn leads to higher donation behaviors. Positive affect is demonstrated to enhance donation behaviors due to perceptions of donation impact and a desire for mood maintenance. However, the identified effects emerge only when one engages in a creative activity—not when the activity is noncreative, or when only the concept of creativity itself is made salient.Keywords: autonomy; creativity; donation; positive affectCharitable organizations exist to support a wide variety of causes, such as helping malnourished children, caring for the homeless, supporting animal welfare, and meeting environmental concerns, to name a few. The success of these organizations in supporting their causes largely depends on the donations they secure. According to the [58], approximately 1.56 million registered nonprofit organizations exist in the United States, together raising an estimated $390 billion in donations annually. Despite these large numbers, fundraising remains a major challenge for such organizations, with approximately 45% of charities unable to secure the required level of resources needed to deliver their services ([59]).In light of this, it is not surprising that marketers at these organizations seek more effective ways to solicit donations, often utilizing nontraditional approaches and fundraising events (e.g., ice-cream socials, silent auctions, trivia nights) to engage potential donors ([11]). For example, in 2014, the ALS Association invited people around the globe to participate in its ""Ice Bucket Challenge"" to increase awareness of ALS, raising approximately $220 million from individual donors in the process (Holan 2014). Bloodwater.org devised the ""Real Game of Thrones"" campaign, which called on people to participate through Twitter and used a combination of pop culture, humor, and bathroom puns to raise money to build latrines throughout Africa. This creative campaign successfully raised enough money in 24 hours to build 21 latrines in Rwanda. Cookies for Kids, another charitable organization, sponsors creative charity events each year such as cookie swap parties, where participants decorate cookies and swap recipes to raise donations. These fundraising anecdotes suggest that charities are defining new ways of engaging potential donors, while raising questions about which types of activities most effectively enhance donation behaviors.The current work meets this challenge by examining how engaging potential donors in creative activities can positively influence their propensity to donate money to a charitable cause. We argue that engaging in a creative activity induces a positive affective state, which in turn increases both the likelihood and amount of monetary donation made to the charitable organization. While prior work has independently examined links between creativity and positive affect (e.g., [ 9]; [47]), as well as between positive affect and helpfulness (e.g., [ 2]; [ 3]), we provide a deeper understanding of why and how creativity leads to enhanced donation behaviors. Specifically, we show that the link between creativity and positive affect is driven by the sense of autonomy that is induced by engaging in a creative activity (i.e., an attempt to create something novel). Further, by identifying the roles that desire for mood maintenance and perceived donation impact play, we provide insight into why the positive affect resulting from a creative activity leads to enhanced donation behavior.The current research makes several important contributions. From a practical perspective, this research offers a simple and effective way for marketers to improve their donation appeals; it suggests that engaging potential donors in a creative activity enhances subsequent donation behavior. This recommended approach provides a real opportunity for charity marketers to increase the efficacy of their fundraising campaigns. At the theoretical level, the present work advances the marketing and charity literature streams in several ways. First, we demonstrate the positive effect of creative engagement on donation behavior. To our knowledge, no research thus far has examined whether and how engaging in creative activities can impact an individual's subsequent donation behavior toward a charitable organization. Second, we explicate the reasons and conditions that drive the relationship between creativity and donation behavior. We demonstrate that it is the act of actually engaging in a creative activity—rather than simple priming or making the concept of creativity salient—that drives the effect. Further, while prior work has consistently shown that a sense of autonomy can facilitate creativity (e.g., [18]), we demonstrate that engaging in a creative activity also heightens one's sense of autonomy, which in turn induces positive affect. As noted previously, we also highlight that the positive affect experienced during a creative activity bolsters desire for mood maintenance and perceived donation impact, thereby enhancing the likelihood of donation and the monetary amount donated. Finally, we find evidence that the positive effect of engaging in a creative activity on monetary donation is context independent. That is, engaging potential donors in a creative activity not directly related to the charitable cause or organization still has a positive influence on their donation behavior. Thus, the current work not only offers marketers a way to build effective donation campaigns but also provides a deeper theoretical understanding of the relationship between creativity and donation behavior. Conceptual Framework Donation BehaviorResearchers have explored many facets of donation behavior, from the demographic and socioeconomic determinants of donation ([ 6]; [10]; [40]) to the extent to which other factors—such as motivation, psychological characteristics, and social cognition—can affect donation ([34]). In addition, prior research has proposed and examined various marketing strategies and tactics used to increase donations. For example, using public recognition ([73]), taking advantage of price promotions ([78]), designing more attractive appeals ([51]), expressing one's identity ([61]), and using positioning to enhance the effectiveness of the charity ([74]) have all been investigated.More relevant to the current research, recent work has also started to examine the merits of engaging potential donors in different types of activities and tasks before soliciting donations. For example, [62] examined the influence of a storytelling event in the crowdfunding context, finding that direct (vs. indirect) storytelling positively affects customer engagement and donation likelihood. In contrast with more traditional donation requests ([69]), some charities are utilizing physical activities (e.g., walks and runs [[37]], sporting events [[36]], silent auctions [[41]], ice cream socials, trivia nights [[11]]) as precursors to the donation solicitation. Despite the initial academic interest in these tactics, the effectiveness of such approaches has been understudied in the literature, and reporting has shown mixed results. For example, while [37] have argued that positive fundraising outcomes result from physical activity events (e.g., running activities, golf tournaments), [75] did not find a positive relationship between sports activities and charitable event outcomes. The current work aims to add to the literature in this regard by validating the use of activities to increase the likelihood and amount of donation contributions. Specifically, we examine the impact of engaging potential donors in creative activities. Creative Engagement, Autonomy, and Positive AffectActivities involving creation of an output span a continuum ranging from routine tasks, such as simply copying a given design, to highly creative activities, such as creating an original work of art ([18]). Within this context, we argue that the inherent characteristic of creative engagement, which is differentiated from priming or simple salience of creativity and/or creativity-related concepts, is that an individual must engage, physically or mentally, in an activity requiring the production of something novel (i.e., the activity leans to one side of the continuum referenced previously). For example, actively generating an original cookie design would lead to creative engagement, but copying a cookie design or simply being primed by the concept of creativity (e.g., through exposure to creative stimuli) would not.Importantly, we propose that engaging in a creative activity induces positive affect for the creator. In support of this notion, liberal arts literature finds that engaging in creative activities to generate novel outputs (e.g., music composition, visual arts, creative writing) can bring about positive thoughts and feelings ([66]). Relatedly, [ 9] show that engaging in a divergent creative task induces higher levels of positive affect. Results reported in the psychology literature also support these findings. [16], while explicating the construct of flow, interviewed people who engage in creative work on a regular basis (i.e., artists and musicians) and found that these individuals often experience positive affect and happiness when creating something original. [15] confirm these findings in an experimental lab setting and show that engagement in a creative activity induces a state of flow, leading to higher positive affect.Why does engaging in creative activities lead to positive affect? One potential driver of this positive relationship is a heightened sense of autonomy (i.e., having a sense of choice and freedom from external control; [24]; [52]; [63]), attained by engaging in a creative activity.[ 5] By definition, an attempt to generate a creative output requires one to actively recognize remote associations between broad and distant concepts and then combine these loosely connected ideas and concepts in a novel fashion ([20]; [31], [32]; [69]). Such a process requires and encourages one to think freely and make different combinations and choices without being constrained by norms and rules ([ 4]; [30]; [43]). Thus, the process associated with creative generation should manifest a sense of choice and freedom (i.e., autonomy), which we contend induces positive affect.Prior work offers initial support for this proposition. As we have discussed, engaging in a creative activity induces a state of flow, which then leads to higher positive affect ([15]); notably, empirical work has shown the state of flow to be associated with a sense of autonomy ([49]). Similarly, [18] found that being involved in a creative activity can enhance experienced enjoyment, but only when the activity imparts a sense of autonomy. [47] conducted a daily-diary study following the routine of 1,042 hobby musicians and found that the participants reported higher positive affect on the days they engaged in music composition and performance. Importantly, the authors found that this relationship was driven by satisfaction of one's needs for autonomy. Finally, [46] found that creative generation, such as the production of visual art, significantly reduced cortisol levels (a biomarker and proxy measure of stress in humans) and increased feelings of relaxation, pleasantness, and enjoyment. Their work shows that such art making is associated with the experience of being free from constraints (i.e., the sense of autonomy).Given this discussion, we argue that engagement in creative activities heightens one's sense of autonomy, which in turn leads to positive affect. We further propose that the positive affect induced by participation in a creatively engaging activity will lead to enhanced donation behavior. We elaborate on this prediction in the following subsection. Positive Affect and Donation BehaviorFindings reported in the extant literature offer compelling evidence that being in a positive affective state enhances donation behavior ([ 1]; [17]; [26]; [42]; [44]; [60]). While prior work has consistently demonstrated a positive relationship between positive affect and donation behavior, it offers disparate explanations for this relationship ([ 7]). Indeed, we recognize that the relationship between positive affect and donation behavior is likely to be multiply determined, and we therefore identify three mechanisms that are most relevant to the context of creative engagement in question.Perhaps the most common explanation for the clear link between positive affect and helping behavior derives from the mood maintenance model. This line of reasoning proposes that people tend to maintain positive mood states ([ 7]; [33]). Thus, individuals tend to help more when in a positive affective state, because doing so enables them to prolong said state ([12]; [45]). In the context of our work, this suggests that the positive affect realized by participating in a creative activity can best be maintained when a subsequent behavior, such as helping others through enhanced donation behavior, also fosters positive feelings ([ 7]).A second potential mechanism derives from the social aspects associated with positive affect. Indeed, it has been argued that being in a positive affective state can directly influence one's perceived social connectedness ([38]; [39]). As such, the positive affect defined by participation in a creative activity is likely to enhance the value of creating and maintaining social connections ([13]; [25]). Further, valuing social connections has been shown to enhance feelings of care and concern toward others ([ 8]), which should subsequently enhance the donation behaviors of the individual.Finally, prior work reports that positive affect can also boost self-efficacy and/or the perceived impact of one's actions ([ 5]; [64]). That is, experiencing positive affect can lead to the belief that one's actions are more efficacious, thus creating a heightened expectancy of positive outcomes ([53]). Thus, we contend that positive affect can enhance the perceived impact of one's potential donation and, in turn, raise the likelihood and amount of one's donation behavior ([14]). In our empirical work, we explicitly test the proposed chain of effects (i.e., engagement in a creative activity → autonomy → positive affect → enhanced donation behaviors), and the three aforementioned potential mechanisms underlying the impact of positive affect on donation behavior. Summarizing our arguments, we hypothesize the following: H1:  Engaging in a creative activity (vs. an activity that does not provide an opportunity for novel creation) leads to enhanced donation behaviors (i.e., the likelihood of donation and the monetary amount donated). H2a:  The relationship between engaging in a creative activity and donation behavior is serially mediated by autonomy and positive affect. H2b:  The influence of positive affect on donation behavior is driven by (a) mood maintenance, (b) social connection, and/or (c) perceived donation impact. Overview of StudiesWe utilized a combination of field studies and controlled lab experiments to test our hypotheses. First, we conducted a pilot study in collaboration with a nonprofit organization as an initial test of our focal hypothesis and found support for the prediction that engaging in a creative activity enhances donation behavior (H1). Study 1, conducted in a controlled lab setting, replicated the initial pilot study findings, thus reconfirming support for H1. Study 2, a quasi-field experiment, shows that engaging in a creative activity increases both the likelihood and amount of monetary donations, whereas simply priming the concept of creativity does not (H1). Study 3 tested the full serial mediation prediction (H2a) by demonstrating that creative engagement induces a sense of autonomy, which in turn heightens positive affect, leading to higher donation behavior. Study 4 tested H2b, showing that the path from positive affect to donation behavior is indeed multiply determined. In every study, we report all experimental conditions and measures as collected and disclose any eliminated data points when applicable. The sample size was predetermined for each study based on current experimental norms but varied within an acceptable range depending on actual participant sign-ups. Study 3, a supplementary study (follow-up to Study 3 reported in the Web Appendix), and Study 4 were preregistered on aspredicted.org (see respective studies for details). Pilot Study: Creativity and Monetary Donations—A Field ExperimentTo gain an initial understanding of whether engaging in a creative (as compared with neutral) activity enhances monetary donation, we collaborated with a registered U.S. nonprofit organization operating an animal shelter in a small city in the Southwestern United States (population 46,000 according to 2010 U.S. Census data). Every year, employees of this charity produce T-shirt designs that are printed and used as giveaways or sold in fundraising activities. To test our focal hypothesis, the charity agreed to open the T-shirt design activity to the public and use it as a fundraising event. The T-shirt design campaign was launched by the charity via its social media platform, inviting members of the public (i.e., potential donors) to create T-shirt designs as part of a donation appeal. The charity had set two overarching guidelines for this T-shirt design campaign: ( 1) the submitted designs were to follow the theme of ""Rescue 2020"" and ( 2) the charity's logo had to be part of the design. The charity managed the entire event.Relevant to our prediction (H1), we manipulated the opportunity to be creative (vs. not) within the T-shirt design campaign. While participants in both conditions had to develop a T-shirt design reflecting the charity's yearly theme and including its logo, those in the creativity condition were invited to develop an innovative T-shirt design and were explicitly instructed to be creative while doing so. Those in the neutral condition were not specifically instructed to be creative. Once participants submitted their designs, the charity presented them with a donation appeal that included a link to the donation page. At the end of the campaign period, the charity forwarded us the designs along with the corresponding donation amounts, having removed donors' identifying information (for additional methodological details, see Web Appendix A.1).To examine the relationship between creativity and donation behavior, we first coded the participants who did not donate as 0 and those who donated (any amount greater than zero) as 1. We then conducted a binary logistic regression analysis testing the effect of engaging in a creative T-shirt design activity on donation rate (i.e., the percentage of participants who donated to the charity). We found that a significantly higher percentage of people in the creativity condition (34.48%) donated, as compared with the percentage of people in the neutral condition (12.20%; χ2 = 4.97, p = .026). Next, we examined the effect of creative engagement on the amount of money that was donated. A Shapiro–Wilk test ([65]) indicated that the donation amount data was not normally distributed (p <.001). Thus, in accordance with prior research ([57]; [61]), we used a nonparametric Mann–Whitney U test for the analysis. We found that the average donation amount made by participants in the creativity condition (M = 7.07, SD = 13.06) was significantly higher than that of participants in the neutral condition (M = 1.10, SD = 3.26; U = 739.50, p = .016; for additional results, see Web Appendix A.2).By collaborating with a registered charity and assessing actual donation behaviors, we found initial evidence showing that including a creative activity as part of a donation appeal can be an effective approach to enhance donation behaviors (i.e., both higher donation rates and amounts). Interestingly, one could argue that the creativity of the generated output (i.e., the T-shirt design) may also have impacted the donation behavior. To test this alternative explanation, we asked two trained research assistants (employed within the domains of creativity and advertising, respectively) to rate each T-shirt design on its creativity (1 = ""not at all creative,"" and 7 = ""very creative""). Both raters were blind to the conditions and hypothesis. Validating our manipulation, a one-way analysis of variance (ANOVA) showed that the designs produced in the creative condition (M = 3.62, SD = .80) were rated as significantly more creative than those in the neutral condition (M = 3.05, SD = .98; F( 1, 68) = 6.72, p = .01). However, we did not observe a significant relationship between rated creativity of the generated designs and the donation rate (B = .04, t < 1) or the donation amount (B = .83, t < 1). We conducted a similar analysis using only the creative condition, where natural variability in output creativity may occur (even though everyone was instructed to be creative). Again, we did not find a significant relationship between the creativity of the generated designs and the donation rate (B = −.15, t = −1.34, p = .19) or the donation amount (B = −3.07, t < 1). Importantly, our conceptualization argues that it is the act of engaging in a creativity activity that leads to enhanced donation behavior, not the level of creative output achieved. Subsequent studies replicate this finding, consistently demonstrating that creative engagement itself, rather than the creativity of the generated outcome, positively impacts donation behavior (for brevity, these findings are reported in the respective Web Appendices).This pilot study provided initial evidence of the proposed effect, but as a real-life field study conducted in collaboration with a third party, it is not without limitations (dependence on the charity's social media platform for sampling, the messaging guidelines set by the charity, etc.; for discussion, see Web Appendix A.3). As such, our first study aimed to replicate these initial findings observed in the field in a more controlled lab setting (i.e., provide a more robust test of H1). Study 1: Creativity and Monetary DonationsStudy 1 used a donation context inspired and adapted from a real-life social enterprise known as Elephant Parade. This organization invites everyday consumers to create/paint their own elephant toy using an ""Artbox Kit"" (containing a small white clay elephant and a variety of colors) in return for a monetary donation. The proceeds are subsequently used for elephant welfare and conservation projects worldwide. MethodEighty-nine undergraduate students (49 women; Mage = 20.04 years, SD = 1.27 years) at a large North American university completed this study in exchange for course credit. To begin, participants were checked in and assigned to a designated computer desk, each of which was equipped with a small donation box (see Web Appendix B.1) and a white envelope containing $2 in quarters (i.e., eight quarters). The donation box was labeled with an Elephant Parade sticker and had a slit on the top. Four quarters were left in each donation box, creating the impression that the study administrator would not be able to tell if the participant donated or not, thus reducing any demand effects and obligation to donate. Participants were told that, in addition to the course credit, they would receive $2 (in an envelope on their desks) as a token of appreciation for their participation.The experiment adopted a one-way design in which participants were assigned to complete either a creative or neutral activity, randomized by session (i.e., we ran only one condition per session). A drawing activity (inspired by Elephant Parade's clay elephant painting) was used to induce the focal manipulation. Participants were told that the researchers wanted to put their minds at ease before the study commenced and would therefore like them to engage in a coloring activity. All participants were given a sheet of paper with a picture of an elephant (see Web Appendix B.2) and asked to color it. Those in the creative condition received a box of Crayola markers in ten different colors and were told to be as creative as possible while coloring and decorating their elephant pictures. They were also told to use any number and variety of colors they liked for the task. In contrast, those in the neutral condition received gray crayon markers only, and were told to simply color the elephant picture. In both conditions, participants were asked to spend no more than five minutes on the coloring activity. The elephant coloring task, though based on a real-life activity (i.e., Elephant Parade donation protocol), mimics a widely used creativity task in the literature: the alien task ([72]). In these types of activities, creative thinking encourages people to violate standard characteristics of a stereotypical object (e.g., an elephant, as in our study; [48]; [50]). A stereotypical elephant picture would be colored gray, whereas a nonstereotypical elephant picture would be multicolored.Next, participants were presented with the donation opportunity and informed that the researchers were helping raise money for the nonprofit organization Elephant Parade. Participants read a short description and donation appeal from Elephant Parade (see Web Appendix B.3), and were asked if they would like to contribute; they could donate any amount of the participation money (eight quarters) they wished, and put it in the donation box. The number of quarters each participant donated served as the key dependent variable. Finally, all participants provided their demographic information (age and gender) and were debriefed before being dismissed. (Gender and age were captured in all studies. However, no effects were observed for these variables in either this or any other study. For the sake of brevity, we do not discuss them further.) After each session, the research assistants removed the quarters from the donation boxes and recorded the number of donated quarters (i.e., the total number of quarters in the box minus the four quarters initially placed in each donation box). Results Preliminary analysesThe elephant designs were rated on creativity (1 = ""not at all creative,"" and 7 = ""very creative"") by a research assistant who was blind to the conditions and hypothesis (for sample designs, see Web Appendix B.4). A one-way ANOVA confirmed that the designs produced in the creative condition (M = 4.24, SD = 1.61) were significantly more creative than those produced in the neutral condition (M = 1.51, SD = .95; F( 1, 87) = 97.41, p <.001). Donation behaviorsFirst, we explored whether there was any difference between the two conditions on the donation rate (i.e., the percentage of participants who donated to the Elephant Parade foundation) and found that a significantly higher percentage of participants (80.95%) in the creative condition—as compared with those in the neutral condition (55.32%)—donated (χ2 = 6.83, p = .009). Next, we analyzed the effect of activity type on donation amount, which was assessed by the number of quarters donated to the Elephant Parade after completing either the creative or neutral activity. The donation data did not meet the normal distribution criteria (Shapiro–Wilk test: p <.001; [65]); therefore, a nonparametric Mann–Whitney U test was again used for the analysis. We found that those who completed the creative activity (M = 4.50 quarters, SD = 3.19) donated a significantly higher number of quarters than those who completed the neutral activity (M = 2.98 quarters, SD = 3.54; U = 720, p = .022). DiscussionThe obtained results provide support for our focal hypothesis (H1), namely, that engaging in a creative activity enhances donation behavior, in terms of both the likelihood of donation (i.e., donation rate) and the donation amount. The study utilized a creative activity adapted from a real-life charity and assessed donation behavior through real monetary donations. It demonstrated that engaging potential donors in creative activities, before soliciting them for donations, can be an effective way to enhance donation behavior.One potential criticism of this study could be the different number of colors provided in the creative versus neutral conditions. However, this procedure was necessary to manipulate creativity within the context of the study. Offering a variety of colors provides participants with an opportunity for creativity, that is, to think outside the box and beyond the stereotypical characteristics of an elephant (i.e., all gray). The sole use of gray crayon markers in the neutral condition, in contrast, conforms to the stereotypical characteristics of an elephant and curtails creative opportunity. To address this potential limitation, in future studies we adopt contexts in which we can provide the same materials to participants in both conditions.In both the pilot study and Study 1, the creative activity was directly related to the charitable cause, thereby raising a question about the generalizability of the effect—that is, whether the observed effect is domain specific or whether it holds when the creative activity is independent of the donation context. We explore this possibility in Study 2. In addition, our studies do not delineate whether the obtained results were observed because participants engaged in a creative activity (as hypothesized), or simply because the concept of creativity was salient for the participants in the creativity condition. In other words, is it necessary to actually engage in a creative activity, or can mere exposure to the concept of creativity also enhance donation behaviors? Prior research has shown that priming creativity (making creativity salient without engagement) can influence cognitive processing, thereby affecting people's propensity to engage in dishonest behaviors ([27]). We examine this question in the next study. Study 2: Creative Stimuli Versus Creative ActivityStudy 2 was aimed to discern whether engagement in a creative activity is needed to obtain the identified effects, or rather, if exposure to a simple creative prime would suffice. To this end, we added another focal condition to the experimental design used in Study 1: this time participants were exposed to creative stimuli only, with no opportunity to participate in a creative activity. Interestingly, this condition mimics the default strategy of many charitable organizations, in which potential donors are presented solely with a donation appeal (without an opportunity for active engagement). In addition, to test the context-independent nature of the effect, the creativity activity was kept independent from the donation context (i.e., the charitable cause). Finally, we conducted the study in a real-life setting; we followed a format used by baked goods company C. Krueger's, which hosts a holiday charity event wherein customers are invited to decorate cookies and make purchases. For our study, two booths were set up in the lobby of a university building with high foot traffic, featuring large signs advertising a cookie decoration event sponsored by the charity ChildHelp. Passersby were invited to participate in the event and decorate a cookie before being solicited for a monetary donation. MethodOne hundred seventy adults (82 women; Mage = 21.09 years, SD = 2.47 years) agreed to participate in the event and were assigned to one of the three treatment conditions: creative engagement, creative exposure without engagement, or neutral engagement. The conditions were randomized and rotated by the hour. Once passersby agreed to participate in the event, they were told they would receive $2 as a token of appreciation for participating in the event. They were given a white envelope containing eight quarters and asked to sign a form indicating receipt of the money. The signing process was necessary for participants to feel ownership of the money they had earned, before being solicited for donations later in the study. Prior research has shown that signing one's name increases this sense of ownership ([22]; [68]).Next, one of the ""staff members"" guided individual participants to a table bearing a plain cookie on a paper plate, four different icing colors, and a spatula. They were also handed an event participation instruction sheet, which served as our key manipulation. Each instruction sheet had the ChildHelp Foundation logo at the top, with ""ChildHelp Foundation Annual Charity Event"" printed underneath (see Web Appendix C.1). The task manipulation for the two engagement conditions (i.e., cookie decoration) was adapted from [18]. In the creative engagement condition, participants were told that this was an annual charity event hosted by the ChildHelp Foundation and, as part of the event, we wanted them to decorate their cookie in the most creative manner possible using the provided materials. Those in the neutral engagement condition were given a picture of a routinely (i.e., noncreatively) decorated cookie (see Web Appendix C.2) and asked to ice the cookie as shown in the picture, using the provided materials. Those in the creative condition had the freedom to use their imagination and creativity to come up with a novel cookie design, thereby promoting creative engagement. However, in the neutral condition, participants were simply asked to copy the noncreative cookie as depicted, negating any potential creative engagement process. In the creative exposure (without engagement) condition, in keeping with prior research showing that exposure to creative images can make the concept of creativity accessible ([76]), we simply showed participants three creative cookie designs (see Web Appendix C.3) and asked them to choose the most creative one.To assess whether our manipulation made the concept of creativity salient, we conducted a separate online study. In this study, participants were randomly assigned to complete one of the three treatment condition tasks used in the main study (creative engagement, neutral engagement, or creative exposure without engagement). They were then presented with two types of measures that captured the salience of creativity implicitly and explicitly. The obtained results showed that, as anticipated, the concept of creativity was equally salient for both the creative engagement and creative exposure conditions, and both were significantly higher than the neutral condition (for study details and complete results, see Web Appendix C.4).Next, in the main study, all participants were given a manila envelope with a survey featuring a donation appeal and some questions about the cookie event. Each envelope was marked with a unique identification number to enable us to match participants' survey responses, donation amounts, and their assigned condition. All participants were presented with a donation appeal from the ChildHelp Foundation: a nonsectarian, nonpolitical, registered charity dedicated to helping children living in distress in North America and overseas. Furthermore, participants were told that if they decided to contribute, they could put the quarters they wanted to donate in the manila envelope and leave it in the box beside their table. Lastly, to gain initial insights into the underlying process, we captured exploratory measures of participants' positive affective state and their perceived donation impact (for details, see Web Appendix C.5). At the end of the study, the participants in the two engagement conditions were invited to take their cookie with them, while those in the creative exposure without engagement condition were given a cookie at the end of the study (for consistency with the other two conditions). ResultsWe first examined the donation rate by calculating the percentage of participants who donated in each condition. A logistic regression revealed a significant difference in the donation rates across the three conditions (χ2( 2) = 12.26, p = .002). A significantly higher percentage of participants in the creative engagement condition (81.03%) donated, compared with both those in the creative exposure (without engagement) condition (50.88%; χ2( 1) = 11.01, p = .001) and those in the neutral condition (61.82%; χ2( 1) = 4.98, p = .026). We observed no difference between the latter two conditions (p = .244). Next, we assessed the donation amount, with the number of donated quarters serving as our key dependent variable. A Shapiro–Wilk test ([65]) indicated that our data were not normally distributed (p <.001); therefore, we used the nonparametric Kruskal–Wallis test for the analysis. The obtained results revealed a significant overall main effect of the activity type on monetary donation (H( 2) = 9.8, p = .007). Pairwise comparisons showed that those in the creative engagement condition (M = 6.10 quarters, SD = 3.30) donated significantly more quarters than both those who were in the creative exposure (without engagement) (M = 4.04 quarters, SD = 3.97, p = .003) and neutral engagement (M = 4.49 quarters, SD = 3.85, p = .021) conditions. There was no difference between the latter two conditions (p = .52). DiscussionIn this study, we created a charity event in which individuals participated in different activities before receiving a donation solicitation. In line with our predictions, we found that participating in an activity that enabled creative engagement (i.e., creatively decorating a cookie) enhanced donation behavior (as compared with those who either reproduced a routine cookie design or were merely exposed to creative cookie designs). Importantly, the obtained results showed that donation behavior is only enhanced when participants actually engage in a creative activity, not simply when the notion of creativity is made salient. Further, the creative activity utilized in this study was independent of the charity cause, thereby demonstrating the context-independent nature of the effect.Our findings, so far, provide consistent evidence for the hypothesized relationship between creative engagement and donation behavior. In the following studies, we extend our examination to understand the underlying process through which creative activity and higher donation behavior are connected. In particular, we examine the mediating role of positive affect in this relationship. In addition, in Study 3 we also test the role of autonomy as a driver of creative engagement's impact on positive affect, which consequently influences donation behavior. Study 4 then explores why positive affect has such a significant impact on donation behavior. Finally, given the null findings in the creative exposure condition (on donation behavior), we dropped this condition from subsequent studies. Study 3: Testing the Role of Autonomy and Positive AffectStudy 3 was conducted to fully test H2a and identify the underlying process through which creative engagement impacts donation behavior. In particular, we tested our prediction that engaging in a creative activity heightens one's sense of autonomy, which in turn induces positive affect, leading to higher donation behavior. This study was preregistered on aspredicted.org (https://aspredicted.org/blind.php?x=ag5ci3). MethodTwo hundred adults (117 women; Mage = 32.76 years, SD = 11.51 years) recruited from the online platform Prolific completed this study in exchange for a small monetary compensation. At the outset, participants were told that in addition to their regular compensation, they would also receive $1 as a thank-you bonus for completing the study, with an opportunity to spend this money later if they chose to. They were further informed that the study was being conducted in collaboration with the charitable organization ChildHelp Foundation and were provided with information about the organization (see Web Appendix D.1). Next, all participants were told that the ChildHelp Foundation runs an annual charitable event, wherein individuals are invited to participate in various tasks. However, given the COVID-19 pandemic, this year's event would be virtual, and the organization needed help planning the function. Thus, the organization was inviting them to participate in this study as if they were actual donors participating in the event.The activity type manipulation used an idea generation task, mimicking the ""Think outside the cereal box"" campaign Kellogg launched several years ago. In the creative condition, participants were told that as part of its annual charity event, the ChildHelp Foundation was inviting them to ""think outside the cereal box"" and generate a fun and creative way to use Froot Loops cereal, besides eating it for breakfast. Further, participants were told to be as creative as possible and use their imagination to generate an innovative way to use Froot Loops cereal. In the neutral condition, participants were asked to ""think about the cereal"" and share a traditional way of how they eat the Froot Loops cereal (for detailed instructions, see Web Appendix D.2).Once participants completed the Froot Loops task, we assessed their donation behavior, sense of autonomy, and affective state. To capture participants' donation behavior, they were told that the ChildHelp Foundation was seeking donations, and they could help by donating part or all of their $1 bonus (in multiples of $.10) to the charity. All participants were then provided with a scale from $0 to $1 in increments of $.10 to indicate their donation amounts.Sense of autonomy was measured by adapting established measures defined in the literature (i.e., [18]; [56]). Specifically, participants were asked to indicate how they felt while generating their ideas during the Froot Loops task: ( 1) ""To what extent did you feel you had autonomy in generating your ideas during the Froot Loops task?,"" ( 2) ""To what extent did you feel you had freedom in coming up with your ideas for the Froot Loops task?,"" ( 3) ""How free did you feel in generating your ideas for the Froot Loops task?,"" and ( 4) ""How much did you feel you were able to express yourself when generating your ideas for the Froot Loops task?"" (1 = ""not at all,"" and 7 = ""very much""). Next, to measure positive affect, they were asked to think back to the Froot Loops activity and indicate how they felt during this activity on 11 items adapted from [19] and [18]. Specifically, all participants reported how they felt while completing the Froot Loops activity: ( 1) 1 = ""very negative,"" and 7 = ""very positive""; ( 2) 1 = ""very unpleasant,"" and 7 = ""very pleasant""; ( 3) 1 = ""not at all nice,"" and 7 = ""very nice""; and ( 4) 1 = ""very bad,"" and 7 = ""very good."" This was followed by seven items anchored with 1 = ""not at all,"" and 7 = ""very much,"" asking ( 5) how positive they felt during the Froot Loops activity, ( 6) the extent to which they enjoyed the Froot Loops activity, ( 7) the extent to which they had a good time during the Froot Loops activity, ( 8) how much fun the Froot Loops activity was, ( 9) how satisfied they felt during the Froot Loops activity, (10) how pleasurable the Froot Loops activity was, and (11) how exciting the Froot Loops activity was. Results Preliminary analysesA trained research assistant blind to hypothesis and condition rated the creativity of the generated Froot Loops ideas. As we expected, a main effect of creativity emerged such that those in the creative condition (M = 3.89, SD = 1.29) generated more creative ideas than those in the neutral condition (M = 1.24, SD = .81; F( 1, 198) = 301.96, p <.001, Cohen's d = 2.46). Donation behaviorsWe first examined the effect of engaging in the creative (vs. neutral) activity on the donation rate. A binary logistic regression revealed that a significantly higher percentage of participants in the creative condition (63.37%) donated money, compared with the percentage of participants in the neutral condition (45.45%; χ2( 1) = 6.50, p = .01). Next, we examined the difference between the creative and neutral conditions' donation amounts. As in previous studies, a Shapiro–Wilk test ([65]) indicated that the data were not normally distributed (p <.001); thus, we used the nonparametric Mann–Whitney U test for the analysis. Replicating the results from the previous studies, those who engaged in the creative (M = 41.49¢, SD = 41.60¢) versus the neutral (M = 27.07¢, SD = 37.04¢) activity donated significantly more money (U = 6,034.50, p = .007). Process measuresFactor analysis showed that all four items used to capture participants' sense of autonomy loaded onto the same factor; therefore, we averaged them to create a sense of autonomy index (α = .92). A one-way ANOVA revealed that those in the creative condition (M = 6.05, SD = 1.02) reported a significantly higher sense of autonomy than those in the neutral condition (M = 4.94, SD = 1.73; F( 1, 198) = 30.61, p <.001, Cohen's d = .78). In addition, factor analysis showed that the 11 items used to capture participants' positive affective state loaded onto the same factor, and we therefore averaged them to create a positive affect index (α = .96). A one-way ANOVA revealed that those in the creative condition (M = 5.24, SD = 1.24) reported a significantly higher positive affective state than those in the neutral condition (M = 4.52, SD = 1.31; F( 1, 198) = 16.15, p <.001, Cohen's d = .56). Mediation analysisTo test the potential underlying process paths, we first examined the mediation effect of positive affect on the creative engagement/donation rate relationship. A bias-corrected bootstrap confidence interval obtained by resampling the data 10,000 times (with activity type as the independent variable, positive affect as the mediator, and the donation rate as the dependent variable) did not include zero (β = .31, SE = .12, bias-corrected 95% confidence interval [CI] = [.11,.59]), indicating a significant indirect (i.e., mediation) effect. Next, we conducted a serial mediation analysis to understand the role of autonomy in this relationship. Serial mediation (Model 6, [35]) conducted with activity type as the independent variable, sense of autonomy and positive affect as the serial mediators (in that order), and the donation rate as the dependent variable together revealed the presence of a significant indirect effect (β = .21, SE = .09, bias-corrected 95% CI = [.07,.41]).We also conducted the same mediation analyses for the donation amount. A bias-corrected bootstrap confidence interval obtained by resampling the data 10,000 times (with activity type as the independent variable, positive affect as the mediator, and the donation amount as the dependent variable) did not include zero (β = 4.71, SE = 1.99, bias-corrected 95% CI = [1.32, 9.02]), indicating a significant indirect (i.e., mediation) effect. Next, we conducted a serial mediation analyses to understand the role of sense of autonomy. A serial mediation (Model 6, [35]) conducted with activity type as the independent variable, sense of autonomy and positive affect as the serial mediators (in that order), and the donation amount as the dependent variable together revealed the presence of a significant indirect effect (β = 2.99, SE = 1.48, bias-corrected 95% CI = [.29, 6.10]). DiscussionStudy 3 results replicated the findings from the previous studies and showed that engaging in a creative (as compared with neutral) activity enhances donation behaviors (H1). Further, the findings from this study highlighted the underlying processes through which creative engagement affects monetary donation. As we hypothesized, creative (vs. neutral) engagement induces a higher positive affect, which in turn leads to enhanced donation behavior. Importantly, we found that sense of autonomy drives the relationship between creative engagement and positive affect (H2a is fully supported). To further confirm the role of autonomy in this relationship, we conducted a supplementary study in which we directly manipulated the sense of autonomy felt by the participant. Here, we showed that when felt autonomy is mitigated, the positive effect of creative engagement on positive affect (and, in turn, the donation behavior) is attenuated (for the details of this supplementary study, see Web Appendix E).In the next study, we further explicate the underlying process through which creative engagement impacts donation behavior by examining the pathways through which positive affect leads to higher donation behavior. Study 4: Exploring Why Positive Affect Impacts Donation BehaviorWe conducted Study 4 to test H2b and provide additional insight into the underlying process through which creative engagement impacts donation behavior. In particular, we assessed the possible role of mood maintenance, social connection, and perceived donation impact in driving positive affect's influence on donation behavior outcomes. This study was also preregistered on aspredicted.org (https://aspredicted.org/blind.php?x=j8rj2w). MethodTwo hundred adults (109 women; Mage = 34.84 years, SD = 12.91 years) recruited from the online platform Prolific completed this study in exchange for a small monetary compensation. At the outset, participants were told that in addition to their regular compensation, they would also receive $1 as a thank-you bonus for completing the study, with an opportunity to spend this money later if they chose to. They were further informed that the study was being conducted in collaboration with the charitable organization Healthier Tomorrow and were provided with information about the organization (see Web Appendix F.1). Next, all participants were told that Healthier Tomorrow runs an annual charitable event, wherein individuals are invited to participate in various tasks. However, given the COVID-19 pandemic, this year's event was going to be virtual, and the organization needed help planning the function. Thus, the organization wanted to invite them to participate in this study as if they were actual donors participating in the event.Next, participants were randomly presented with either the creative or neutral version of the event and asked to create (reproduce) a T-shirt design. Those in the creative condition were specifically asked to design an innovative T-shirt and be as creative as possible (for detailed instructions and the sample designs produced by participants, see Web Appendix F.2). In the neutral condition, participants were simply provided with a generic T-shirt design and asked to reproduce it, thus negating any potential creative engagement (for detailed instruction and the T-shirt design provided to the participants, see Web Appendix F.3). Participants were then directed to the T-shirt customization website (customink.com) to complete the design activity. Once participants had finished creating (reproducing) their designs they were asked to save them with a unique ID provided in the survey and then use the save/share function in the T-shirt customization website to email their design to a designated email address created for the study.Once participants completed and submitted information about their designs, we assessed their donation behavior, affective state, mood maintenance, social connection, and perceived donation impact. To capture participants' donation behavior, we told them that Healthier Tomorrow was seeking donations, and they could contribute by donating part or all of their $1 bonus (in multiples of $.10) to the charity. All participants were then provided with a scale from $0 to $1 in increments of $.10 to indicate their donation amounts.To assess participants' positive affective state, we asked them to think back to the T-shirt design task and indicate how they felt during this activity, on the same 11 items used in Study 3 (adapted from [19]] and [18]]). Participants responded to a mood-maintenance measure adapted from [23], where they were asked to think about their donation decision and indicate their agreement with the following statements on seven-point scales (1 = ""not at all,"" and 7 = ""very much""): ""I thought ..."" ( 1) ""I would feel good about myself if I donate,"" ( 2) ""donating will make me feel good,"" and ( 3) ""if I donate it would be a personally rewarding experience."" To measure social connection, we adapted items from [ 8] to suit the context of our study. Participants responded to the following items on seven-point scales (1 = ""not at all,"" and 7 = ""very much""): ""To what extent did you feel ..."" ( 1) ""closer to Healthier Tomorrow,"" ( 2) ""connected to Healthier Tomorrow,"" and ( 3) ""that completing the design task affected the way you think about the relationship with Healthier Tomorrow."" We measured participants' perceived donation impact by means of three items adapted from [14] and [67]. These items specifically asked the participants how much they thought their donation could ( 1) make a positive difference, ( 2) be valuable, and ( 3) do a lot of good (1 = ""not at all,"" and 7 = ""very much""). Results Preliminary analysisTen participants did not complete the T-shirt design activity and were excluded from the analysis (including these participants in the analysis does not change the significance or pattern of results; see Web Appendix F.4). Donation behaviorsWe first examined the effect of engaging in the creative (vs. neutral) activity on the donation rate. A binary logistic regression revealed that a significantly higher percentage of participants in the creative condition (47.87%) donated money, compared with the percentage of participants in the neutral condition (27.08%; χ2( 1) = 8.85, p = .003). Next, we examined the difference in donation amounts between the creative and neutral conditions. As in previous studies, a Shapiro–Wilk test ([65]) indicated that the data were not normally distributed (p <.001); thus, we used the nonparametric Mann–Whitney U test for the analysis. In a replication of the results from the previous studies, those who engaged in the creative (M = 22.34¢, SD = 31.81¢) versus the neutral (M = 11.25¢, SD = 24.63¢) activity donated significantly more money (U = 5,517, p = .002). Process measureFactor analysis showed that all 11 items used to capture participants' positive affective state loaded onto the same factor, and we therefore averaged them to create a positive affect index (α = .97). A one-way ANOVA revealed that those in the creative condition (M = 5.32, SD = 1.32) reported a significantly higher positive affective state than those in the neutral condition (M = 4.72, SD = 1.40; F( 1, 188) = 9.26, p = .003, Cohen's d = .45). Mediation analysisAs an initial step, we examined the mediation effect of positive affect on the creative engagement/donation rate relationship. A bias-corrected bootstrap confidence interval obtained by resampling the data 10,000 times (with activity type as the independent variable, positive affect as the mediator, and donation rate as the dependent variable) did not include zero (β = .19, SE = .11, bias-corrected 95% CI = [.03,.48]), indicating a presence of a significant indirect (i.e., mediation) effect.Next, we examined the pathways through which positive affect, as induced by engaging in creative activity, impacts donation rate. We tested a sequential-parallel mediation model with creative engagement as the independent variable, positive affect as the first mediator, three factors (mood maintenance, perceived donation impact, and social connection) as a second set of mediators in parallel, and donation rate as the dependent variable (see Figure 1) using structural equation modeling (for statistics for each path in the model, see Table 1). This model is very similar to a serial mediation model; however, no order is assumed among the second set of mediators (i.e., mood maintenance, perceived donation impact, and social connection) ([21]). We used bootstrapping procedures to compute 95% CIs by generating 10,000 resamples. The results indicated significant serial indirect effects through positive affect and mood maintenance (β = .031, SE = .014, bias-corrected 95% CI = [.011,.068], p = .001) and positive affect and perceived donation impact (β = .031, SE = .014, bias-corrected 95% CI = [.010,.067], p = .001). However, the serial indirect effect of positive affect and social connection on the creative engagement and donation rate relationship was not significant (β = .005, SE = .012, bias-corrected 95% CI = [−.015,.035], p = .57). Interestingly, with positive affect in the model, creative engagement did not directly impact any of the second set of three mediators, thereby demonstrating the importance of positive affect in the conceptualization. Positive affect positively influenced all three potential mediators, but only mood maintenance and perceived donation impact significantly impacted donation rate. Thus, the positive affect experienced during a creative activity bolstered the desire for mood maintenance and the perceived donation impact, which in turn enhanced donation behaviors.Graph: Figure 1. Sequential-Parallel Mediation Model (Study 4).GraphTable 1. Sequential-Parallel Mediation Model (Study 4). PathPredictorOutcomePath Estimates (Standardized)SE95% CIa1Activity typePositive affect.22.20[.08,.35]a2Activity typeMood maintenance−.14.21[−.27, −. 01]a3Activity typePerceived donation impact.09.23[−.05,.23]a4Activity typeSocial connection.09.19[−.03,.21]b′1Positive affectDonation rate−.13.03[−.29,.02]Donation amount−.141.98[−.28,.002]b2Mood maintenanceDonation rate.31.02[.14,.46]Donation amount.251.32[.06,.42]b3Perceived donation impactDonation rate.37.02[.20,.52]Donation amount.321.16[.17,.46]b4Social connectionDonation rate.04.02[−.13,.21]Donation amount.041.43[−.16,.23]c′Activity typeDonation rate.19.06[.05,.32]Donation amount.173.81[.04,.29]d1Positive affectMood maintenance.51.07[.38,.61]d2Positive affectPerceived donation impact.41.08[.28,.53]d3Positive affectSocial connection.63.07[.52,.72] 1 Notes: Path analysis assessing the effect of creative (vs. noncreative) engagement on donation rate/amount through positive affect, mood maintenance, perceived donation impact, and social connection (estimates and 95% CIs for individual paths).A similar analysis was conducted for donation amount. First, a bias-corrected bootstrap confidence interval obtained by resampling the data 10,000 times (with activity type as the independent variable, positive affect as the mediator, and the donation amount as the dependent variable) did not include zero (β = 1.80, SE = 1.13, bias-corrected 95% CI = [.04, 4.44]), indicating a significant indirect (i.e., mediation) effect of positive affect. Next, to examine the pathways through which positive affect impacts donation, we used structural equation modeling to test the hypothesized process model, with donation amount as the dependent variable (for statistics for each path in the model, see Table 1). Ninety-five percent confidence intervals obtained by generating 10,000 bootstrap resamples indicated significant serial indirect effects through positive affect and mood maintenance (β = 1.51, SE = .78, bias-corrected 95% CI = [.40, 3.72], p = .004) and positive affect and perceived donation impact (β = 1.60, SE = .76, bias-corrected 95% CI = [.53, 3.65], p = .001). However, similar to what we observed for the donation rate, the serial indirect effect of creative engagement through positive affect and social connection on donation amount was not significant (β = .33, SE = .81, bias-corrected 95% CI = [−1.06, 2.26], p = .58). DiscussionThe Study 4 results replicated the findings from the previous studies and showed that engaging in a creative (vs. neutral) activity induces positive affect, which in turn enhances donation behaviors. Importantly, this study further examined the underlying process, providing understanding of how positive affect influences donation behavior. We found that the path from positive affect to donation behavior was multiply determined. Indeed, perceived donation impact and mood maintenance were both shown to be drivers of the affect/donation relationship. Interestingly, we did not find evidence for social connection as a mechanism that triggered the identified effects. Thus, it appears that attributions flowing from the positive experience of creative engagement are more at play in defining enhanced donation behavior, and efforts to maintain positivity also spill over into the donation outcomes. Future research should continue to explore these identified mechanisms and discern within which charitable contexts they are most applicable and effective. General DiscussionThe current research examined the relationship between creativity and donation behavior. A pilot study and four subsequent experiments demonstrated that engaging in a creative activity induces a sense of autonomy, which leads to a positive affective state, which in turn results in enhanced donation behaviors (i.e., the likelihood of donation and the monetary amount donated; for a summary of all study results, see Table 2). We further showed that the positive affect experienced by the creator leads to enhanced donation behavior, due to perceptions of increased donation impact and an effort to maintain the resulting positive mood.GraphTable 2. Summary of Study Results. Donation AmountDonation RateUnits DonatedTask EngagementTask EngagementStudyCreativeNeutralNoneCreativeNeutralNonePilot StudyUSD ($)7.07a(13.06)1.10a(3.26)—34.48%a12.20%a—Study 1Number of quarters (maximum 8)4.50a(3.19)2.98a(3.54)—80.95%b55.32%b—Study 2Number of quarters (maximum 8)6.10a b(3.30)4.49a c(3.85)4.04b c(3.97)81.03%a b61.82%a c50.88%b cStudy 3USD (¢)41.49b(41.60)27.07b(37.04)63.37%b45.45%bSupplementary study (follow-up to Study 3)USD ($)Control12.52ab(7.58)8.43ab(7.10)87.88%ad75.41%cdAutonomy inhibited9.60ac(8.30)11.84ac(7.72)70.77%ac80.33%cStudy 4USD (¢)22.34b(31.81)11.25b(24.63)47.87%b27.08%b 2 Notes: Standard deviations are in parentheses. The contrasts are identified with superscript notation: ap ≤.05, bp ≤.01, cp >.1, dp ≤.1. Implications for PracticeOur work was motivated by the documented fact that charitable organizations often struggle to find effective ways to engage donors and solicit donations ([59]). Thus, a central contribution of our research is in confirming that engaging potential donors through creativity can meet this challenge by increasing engagement and enhancing donation behaviors. Substantively, we can recommend incorporating creative activities into fundraising campaigns and charity events as a viable marketing strategy. Indeed, creative activities can be implemented through social media platforms (as exemplified in our pilot study) or in person during charity events and solicitations (as exemplified in Study 2). Current industry practices suggest that some charities have begun testing this approach (i.e., engaging potential donors through creative activities before soliciting them for donations). For example, Roots and Shoots (a charitable organization that supports environmental, conservation, and humanitarian issues) regularly posts a variety of gamified challenges on its website and invites potential donors to participate. Many of these challenges encourage people to incorporate creativity in defining their solutions (e.g., for a ""World Chimpanzee Day Challenge,"" people were asked to design and submit a creative communication graphic to spread awareness about chimpanzee protection).To gain additional insights on practitioners' points of view (concerning our proposed strategy), we sent an email to 220 charities nationwide, inviting them to participate in a short survey. The survey asked three questions that measured the usefulness and applicability of this donation strategy. The first question assessed whether the charity had previously used a creative activity as a preface to a donation request. The second question asked whether, if presented with evidence that engaging donors through a creative activity increases monetary donation, they would implement this strategy in their donation campaigns (1 = ""not very likely,"" and 7 = ""very likely""). The final question assessed how feasible they thought it would be to implement such a strategy (1 = ""not very feasible,"" and 7 = ""very feasible""). We obtained 29 responses from the surveyed national charities (13% response rate). Interestingly, 45% of the charities mentioned that they have previously used a creative activity as a preface to a donation request—showing that our research validates a tactic already in use by some charities today. Most importantly, charities indicated they would definitely be willing to implement this strategy in their donation campaigns (M = 6.55, SD = .69; t(28) = 20.04, p <.001, compared with the midpoint) and thought it would be feasible to implement such a strategy (M = 5.38, SD = 1.68; t(28) = 4.43, p <.001, compared with the midpoint). Though a small sample, these results are encouraging and affirm that utilizing creative activities in charity campaigns is both highly relevant and feasible in the marketplace. Theoretical ContributionsThe current work also provides several theoretical contributions to the field. First, we advance the marketing and charity literature streams by identifying that positive affect experienced during a creative activity is a key mechanism that bolsters subsequent donation behaviors. Second, we offer a deeper understanding of why engaging in a creative activity leads to higher donation behavior through positive affect. Specifically, we show that creative engagement enhances a sense of autonomy, which in turn induces positive affect, which then positively impacts donation behavior. In addition, the relationship between positive affect and enhanced donation behavior is shown to be multiply determined. We identify two specific mechanisms that link affect and behavior: namely, the positive attributions of the impact of one's donation and the mood maintenance tendency of the participant. Third, we establish that the act of creativity itself (not just being primed with creativity as a construct) is a necessary condition to achieve beneficial donation outcomes. Finally, we confirm that the creative activity employed need not relate directly to the organization and/or charitable cause underlying the sought-after donation behavior. This is important both theoretically and practically, as it establishes generalizability in our findings and provides more freedom to charities in defining the type of creativity activity appropriate for their donation campaigns.More broadly, the current research adds to prior work demonstrating the consequences of engaging in creative thinking tasks. While a significant amount of research has been devoted to studying various factors and cognitive processes that impact creativity ([28]; [54]; [55]), much less attention has been paid to the implications and outcomes of being creative ([27]; [71]). Our research shows that there is value in understanding what implications creativity may have for subsequent consumption behaviors. Building up our understanding of the importance of creativity is especially significant in today's consumption environment, where customers are increasingly provided with opportunities to engage in creative activities, from participating in crowdsourcing platforms (e.g., MyStarbucksIdea.com, ideas.lego.com) to engaging in customization processes (e.g., NikeID, Casetify customized phone cases). Limitations and Future ResearchLimitations inherent to our research approach open up several avenues for additional investigation. First, research should be directed toward developing a better understanding of the generalizability of the effects we identify. Although we demonstrated that a creative activity does not have to be specific to the charity in question to provide a positive outcome, we did not assess a broad range of charities and donation appeals. To this end, we conducted a preliminary study examining the impact of the inherent history of the charity (i.e., whether the charity was well-established; for study details; see Web Appendix G) on donation behavior. Here, we found that creative engagement indeed led to enhanced donation behavior, but only when the charity was newly established. When the charity was well-established, the donation behavior was enhanced irrespective of the type of activity utilized in the appeal. Additional research is needed to better explore this potential boundary condition and, more generally, to define other contextual factors that might moderate the effects we have documented here.Second, most of the creative activities tested in this research involved artistry and design (e.g., cookie decorating, T-shirt design, coloring). It remains to be seen whether other forms of creativity could produce similar effects. Indeed, we believe that the effects identified in this research are likely to be observed for any enjoyable creative activities that encourage people to explore and think freely. However, we conjecture that the positive effects defined herein may be attenuated if the creative activity is more convergent in nature (e.g., identifying the one right solution). While we did not directly test the effect of a convergent creative activity on donation behavior, prior research has found that engaging in convergent creative tasks may not lead to a positive affective state ([ 9]). Future research should explore this possibility and outline the breadth of creative activities that are effective precursors to enhanced donation behaviors.Another interesting research question arising from our work concerns the identified difference between creative engagement and creative priming (on subsequent behaviors). We found that engaging in creative activities leads to higher donation behavior, but exposure to creative stimuli does not have a parallel effect. Previous research ([27]), however, has shown that creative priming can influence cognitive processes. Thus, it is important to further distinguish between creative engagement and exposure to creative materials and understand how they differentially impact subsequent behaviors. While both creative engagement and creative priming may influence cognition, perhaps only creative engagement can induce a positive affective state. Future research could further clarify the differences between these stimuli.Finally, future research should continue to build understanding as to when creativity leads to positive (vs. negative) outcomes. Indeed, prior research has shown both positive and negative implications for creative thinking. For example, creativity has been shown to help overcome the burden of secrecy ([29]) and to enhance one's tendency to take a target's perspective ([77]). On the negative side, previous research has found that creativity can lead to dishonesty (e.g., [27]) and enhance unhealthy choices ([31]). We find an opportunity for future work in building on these initial studies to better understand where creativity can influence downstream consumption behaviors. Indeed, we hope that future research will expand on our findings and further investigate the outcomes of creativity for individuals, charities, nonprofit organizations, and the broader marketplace at hand. Supplemental Materialsj-appendix-10.1177_00222429211037587 - Supplemental material for Leveraging Creativity in Charity Marketing: The Impact of Engaging in Creative Activities on Subsequent Donation BehaviorSupplemental material, sj-appendix-10.1177_00222429211037587 for Leveraging Creativity in Charity Marketing: The Impact of Engaging in Creative Activities on Subsequent Donation Behavior by Lidan Xu, Ravi Mehta and Darren W. Dahl in Journal of Marketing  "
15,"Measuring the Real-Time Stock Market Impact of Firm-Generated Content Firms increasingly follow an ""always on"" philosophy, publishing multiple pieces of firm-generated content (FGC) every day. Current methodologies used in marketing are unfit to unbiasedly capture the impact of FGC disseminated intermittently throughout the day on stock markets characterized by ultra-high-frequency trading. They also neither distinguish between the permanent (i.e., long-term) and temporary (i.e., short-term) price impacts nor identify FGC attributes capable of generating these price impacts. In this study, the authors define price impact as the impact on the variance of stock price. Employing a market microstructure approach to exploit the variance of high-frequency changes in stock price, the authors estimate the permanent and temporary price impacts of the firm-generated Twitter content of S&P 500 information technology firms. The authors find that firm-generated tweets induce both permanent and temporary price impacts, which are linked to tweet attributes of valence and subject matter. Tweets reflecting only valence or subject matter concerning consumer or competitor orientation result in temporary price impacts, whereas those embodying both attributes generate permanent price impacts. Negative-valence tweets about competitors generate the largest permanent price impacts. Building on these findings, the authors offer suggestions to marketing managers regarding the design of intraday FGC.Keywords: real-time marketing; microstructure; high-frequency data; firm-generated content; TwitterWith U.S. firms investing in excess of $37 billion in 2020 ([69]), social media is one of the most pervasive communication channels used by marketers ([ 5]; [36]). The result of this investment is the creation of corporate social media accounts that support firm-generated content (FGC), defined as a firm's communications disseminated through its own online communication tools ([45]). Many firms have adopted an ""always on"" approach in their social media marketing, disseminating multiple pieces of FGC throughout the day. Figure 1 illustrates the high-frequency approach to FGC dissemination using the example of information technology (IT) firms' activity on Twitter. Each piece of FGC is characterized by its attributes (e.g., Figure 1 shows FGC's valence and subject matter as key attributes),[ 5] and the timestamps show the dissemination time for each piece of FGC. Each piece of FGC and its timestamp can be accurately recorded to the second and mapped against the corresponding timestamp of trading activity that takes place at ultra-high frequency, that is, at subsecond intervals (Hasbrouck and Saar 2013). As a result of these high-frequency activities, large volumes of intraday data emerge. For example, an S&P 500 IT firm can issue in excess of 6,000 tweets in a given month, and trading in a single firm's stock often yields well over 10 million trading-related messages (e.g., quotes, cancellations, transactions) during the same interval (see Web Appendix A).Graph: Figure 1. A high-frequency approach to FGC dissemination on the example of IT firms' activity on Twitter.Current marketing methodologies are challenged when analyzing high-frequency data because the trading data, which is used in capturing the impact of FGC on firm value, is characterized by unequal time intervals. Low-frequency event studies using end-of-day price measures and time series analysis, such as standard vector autoregressive (VAR) models, are unable to effectively address these problems (see Web Appendix B). These methods rely on discrete and uniform time intervals that do not align with the time intervals associated with high-frequency trading data, which encapsulate the intraday evolution of firm stock price. Often, these methods aggregate trading data to regular intervals, which can lead to the elimination of upwards of 99% of intraday trading observations in studies employing end-of-day price data (see Web Appendix C). The frequency of FGC as an intraday variable and the effects of other non-FGC events potentially further bias low-frequency analyses that employ standard low-frequency analytical methods. Furthermore, the richness of such an assessment and marketing researchers' understanding of ""always on"" strategies are compromised unless the research identifies both the short- and long-term impacts of this form of marketing ([26]). Consequently, the findings derived from current examinations lack detailed ex post insights on the impact of FGC generated at intraday frequencies, which leaves marketing managers unable to effectively design future intraday marketing resource allocation strategies ([41]).Employing the market microstructure approach, which relies on ultra-high-frequency trading data analysis, we investigate the stock price impact of FGC where price impact is defined as the impact on the variance of stock price. This approach of estimating price impact of FGC as impact on the variance of stock price rather than on changes in the level of stock price is driven by both methodological and theoretical necessity. Although estimating level changes in stock price as a result of an event such as FGC dissemination could be approached from the perspective of computing simple price impact measures, when working with high-frequency data, this approach is inadequate for at least two reasons. First, simple price impacts are misleading when trades in stock markets are serially correlated. Second, in the presence of transient impacts, as is the case in this study (e.g., we capture price impact at second-by-second intervals), simple price impacts rely on getting the timing just right, which is methodologically unfeasible when investigating large data sets, as is the case here. Our methodological approach is in line with market microstructure theory, addresses the previously outlined issues, and is consistent with the microstructure literature ([11]; [65]). Using this approach and assessing S&P 500 IT firms' Twitter activity, we contribute to the marketing literature at three levels.First, by using high-frequency data, this is the first study to document the subminute impact of individual pieces of FGC disseminated during the day; therefore, the insights presented are unlikely to be affected by the confounding effects that the use of end-of-day data is susceptible to. We estimate the price impact of FGC at the second and minute levels by computing the variance of fast-paced (e.g., subsecond-by-subsecond) intraday changes in stock price as they occur in financial markets ([11]; [12]; [42]), thereby demonstrating the instantaneous impact of FGC. We obtain the variance of intraday changes in price through state-space modeling with Kalman filtering. By doing so, we contribute to the emerging stream of marketing research studying ( 1) the impact of FGC on firm financial outcomes ([ 8]; [15]) and ( 2) real-time marketing ([64]), and we respond to research priorities established by the Marketing Science Institute (2018), emphasizing the need to help marketers ""get marketing right"" by providing insights into the instantaneous impact of FGC.Second, using the microstructure perspective, we reveal both the permanent and temporary price impact of FGC as new forms of FGC impact on firm-level performance. In the process, we address [26] call for research capable of identifying both the short- and long-term financial impact of a marketing activity, which has thus far remained difficult to quantify. By being able to distinguish between temporary and permanent price impacts at the fine-grained level of analysis, marketing managers can demonstrate both the short- and long-term impacts of FGC on firm financial performance ([53]). This, in turn, will allow them to overcome short-termism in marketing and improve long-term growth initiatives ([18]; [58]).Finally, to support intraday actionable FGC design, we examine the extent to which key attributes of FGC, including content valence and subject matter, influence the occurrence of permanent and temporary impacts of FGC on price. Although FGC valence and subject matter have been examined by previous research (e.g., [20]; [27]; [36]) and are recognized as key components of marketing excellence ([39]), what constitutes the ""right content"" is largely unknown according to research priorities recently published by the Marketing Science Institute (2020). We show that FGC reflecting only one of the attributes of valence (positive or negative) or subject matter (consumer or competitor orientation) generates temporary price impact, whereas FGC that incorporates both valence and subject matter is associated with permanent price impacts on stock price and thus correlates with long-term firm performance.Using a two-stage least squares (2SLS) estimation framework to examine S&P 500 IT firms' Twitter activity, we find that negative- or positive-valence tweets are consistently linked with a reduction in permanent price impact and an increase in temporary price impact. Similar findings are obtained when examining tweets that only reflect a consumer or competitor subject matter, although the reduction in permanent price impact and increase in temporary price impact they elicit are of smaller magnitudes. These findings indicate that tweets reflecting only valence (positive or negative) or subject matter (consumer or competitor orientation) result in temporary price impacts, which is commonly associated with the incorporation of noise into the price discovery process ([60]). This type of effect has not been studied previously in the marketing literature; however, given that it is a source of uncertainty in the value of firms that can increase a firm's cost of capital ([17]), it demands attention. Employing the market microstructure approach to exploit the variance of high-frequency changes in stock price, this is the first study that reveals tweets' temporary price impacts and identifies tweet attributes that elicit such short-term impacts on price.We further find that tweets that embody both attributes—valence and subject matter—generate permanent price impacts; however, this impact varies according to the type of valence and subject matter. The results evidence the importance of interaction effects between tweet valence and subject matter in generating a higher permanent price impact. The average negative- and positive-valence tweet, when viewed through the lens of a consumer or competitor orientation, generates a permanent price impact, and a competitor-oriented tweet with a negative valence is likely to have the highest permanent price impact. This is a crucial finding from the perspectives of marketing practice and intraday social media marketing strategy design because valence, as a singular attribute, is associated with decreasing permanent price impact. Our research shows that information-rich tweets that include both variance and subject matter can result in permanent price impacts, and it demonstrates investors' ability to act on information contained in FGC at subsecond levels (Hendershott et al. 2011; [65]).To illustrate the relevance and magnitude of these findings, in Figure 2, tweets A and B are characterized by negative and positive valence, respectively, and tweets C and D reflect only consumer and competitor orientation, respectively. In line with our research findings, the permanent price impacts associated with these tweets are more than three standard deviations lower than the average permanent price impact estimates for all the 153,041 tweets in our sample and are therefore below the 10th percentile of the estimates. In contrast, tweets E to H reflect varying combinations of both valence and subject matter (consumer or competitor orientation). Consistent with our findings, we show that these tweets generate permanent price impact estimates ranked above the 90th percentile in our sample of tweet trades' permanent price impact estimates. The temporary and permanent impact estimates for the average tweet are as large as 279 and 187 times, respectively, what we document for the average regular intraday transaction in our sample.Graph: Figure 2. Examples of tweets characterized by valence and subject matter (consumer and competitor orientation). Theoretical Background FGC and High-Frequency Trading DataFirms increasingly use social media because it provides greater reach and can be less costly than traditional channels for FGC dissemination ([45]). FGC is often posted several times a day ([41]) and serves as a valuable source of high-frequency marketing data that can offer insights into the growth potential of a firm ([18]). With the advancement of data collection tools ([79]), marketing researchers can now record each piece of FGC and create large data sets depicting FGC attributes and their dissemination time. Recorded with accuracy to the second, FGC can then be mapped against the corresponding trading activity that takes place at subsecond intervals and used to study its financial impact. However, measuring the impact of FGC sampled at intraday levels requires marketing researchers to be able to utilize high-frequency trading data with observations occurring at unequal time intervals.The market microstructure approach to estimating price impact offers marketing researchers tools to, piece-by-piece, algorithmically link FGC to time-specific trading activity at a fine-grained level of analysis (e.g., subseconds, seconds, minutes). Unlike symmetrical asset pricing models, market microstructure recognizes that a firm's stock price is only informationally efficient to the extent that it reflects all available and relevant information ([21]). A firm's stock price, while reflecting information, is also distorted by noise generated by (temporary) non-information-based factors. Such factors can include trading frictions due to low levels of liquidity, defined as the ability to trade large quantities of a firm's stock quickly with little or no price impact ([ 2]; [28]), or the activity of traders who lack adequate information regarding the value of a stock, known as ""uninformed traders"" in the market microstructure literature ([24]; [47]). Estimating the proportion of stock price driven by information (relevant to the value of a firm) and the proportion driven by noise is a critical aspect of the analyses many studies conduct in the market microstructure literature (see Web Appendix D). However, this holistic view of both temporary and permanent price impacts is often missing from marketing research. The market microstructure approach allows marketing researchers to estimate the price impact of FGC at high frequencies and to identify both types of price impacts. A crucial element in such analysis is knowing the ""event time"" (i.e., timestamp), which refers to the time at which an event occurs, such as the FGC dissemination time. By deploying time series models to estimate changes in the components of price at high-frequency intervals (e.g., seconds) and then linking the FGC timestamp to the components, the instantaneous impact of FGC on firm stock price can be estimated. FGC's Impact on Firm OutcomesExtant research has primarily focused on linking FGC with consumer behavior ([15]; [13]; [36]; [45]; [55]; [72]) and firm performance ([ 8]; [15]; [64]) (see Web Appendix E). With the focus on firm performance, [15], [ 8], and, most recently, [64] show that FGC affects firm value. [15] document an indirect effect of FGC volume on shareholder value measured using abnormal returns and idiosyncratic risks. [ 8] were the first to demonstrate the direct impact of humorous FGC on firm value as measured by abnormal stock market returns. To demonstrate these impacts, they employed an event study that estimated abnormal returns and VAR modeling. However, these methods, as deployed, focus on daily activity, which can result in aggregation bias and misevaluation of FGC's impact on firm value ([64]). Moreover, the richness of such an assessment is compromised because short- and long-term impacts of FGC are not estimated ([26]; [58]). Finally, current marketing methods do not examine FGC attributes at a fine level of granularity (i.e., intraday frequencies), preventing marketing managers from moving beyond a ""throw it on the wall and see what sticks"" strategy ([37], p. 47) in the design and dissemination of intraday FGC ([36]). High-Frequency Approach to FGC AnalysisThe market microstructure approach responds to calls for more powerful methodological approaches that allow marketing researchers to harness the potential of rich data sources and develop insights capable of advancing theory and informing contemporary marketing practice (e.g., [18]; [36]; [49]; [79]). The fine-grained level of analysis available using a market microstructure approach overcomes the limitations of low-frequency methodologies, such as VAR and daily event studies, to study FGC and its impact on firm value (i.e., it estimates the variance in firm stock price following FGC dissemination). Utilizing high-frequency intraday data, it adds richness to the assessment of FGC's financial impacts by distinguishing between permanent and temporary price impacts. Temporary and permanent price impactsTemporary price impacts are short-term impacts that result in momentary changes in the price of a stock before it returns to its pre-event (e.g., pre-FGC) value, and they are often the result of uninformed trader activity (see Web Appendix D). Uninformed trader activity could be driven by several factors; for example, it could be linked to investor uncertainty about the relevance of information ([34]; [38]) or trading friction due to liquidity constraints ([ 2]; [16]). Ignoring temporary price impacts can lead to misunderstanding the total impact of FGC, with prior research suggesting that temporary price impacts result in larger transaction costs ([14]) and firm cost of capital ([17]). In contrast, an event (e.g., FGC) can generate a permanent price impact and result in the price attaining an enduring new value after the event. This occurs when the event provides information that updates informed investor/trader expectations related to a firm's long-term performance ([52]). Importantly, the microstructure approach also supports intraday actionability by assessing how the attributes of information signaled by these events influence temporary and permanent price impacts. A state-space decomposition of firm stock priceConsistent with the market microstructure literature, this study estimates the permanent and temporary price impact of FGC by first conducting a state-space decomposition of firm stock price into its efficient (permanent) and inefficient/noise (temporary) components and then linking the changes in these components to individual pieces of FGC. State-space modeling is a tool for modeling an observed variable as the sum of unobserved variables ([35]), and it is commonly used for the decomposition of price ([11]; [35]; [57]; [65]). Due to its efficiency when applied to ultra-high-frequency data like stock price movements, the state-space modeling approach for decomposing price has significant economic and methodological advantages over other commonly used methods ([31]) such as VAR models.An assumption underlying a standard VAR model is that data are sampled at regular frequencies, as variables at time t are regressed on variables dated at t − 1, t − 2, and so on. However, FGC and intraday trading data are often sampled at unequal time intervals, which suggests there would be many instances of missing variables in a model calibrated on regular time intervals ([63]). The modeling of such data using VAR requires the alignment of variables misaligned in time either downward, by aggregating the data to a lower frequency, or upward, by interpolating the high-frequency data with heuristic rules such as polynomial fillings. Downward alignment eliminates potentially valuable information in the high-frequency data. Data aggregation is problematic ([68]), as it can alter the lag order of autoregressive moving average models ([ 1]), reduce the efficiency of the parameter estimation and forecast ([74]), affect Granger causality and cointegration among component variables ([54]), and induce spurious instantaneous causality ([10]). Upward alignment has also been deemed inefficient and dubious ([61]) because a VAR approach assumes that the model specifies the high-frequency data-generating process. However, interpolation is not based on the multivariate model that generates the data but instead on heuristic rules, which, at a minimum, inevitably incorporate noise into the data and distort it.State-space modeling offers a solution to the irregular frequency challenge inherent in intraday transaction data ([62]). Specifically, the use of state-space modeling with a Kalman filter in maximum likelihood estimation of parameter estimates ensures maximum efficiency in dealing with unequal time intervals or irregular frequency in data. The use of a Kalman filter accounts for changes across periods of analysis with missing observations. This is a critical consideration in the use of state-space modeling for decomposing high-frequency time series because standard approaches do not deal with the ""missing observations"" caused by unequal data intervals. For example, estimating a standard autoregressive framework implies truncation of the lag structure and could potentially discount valuable information in high-frequency data. Using the Kalman filter facilitates the decomposition of any realized change in the time series (e.g., variance in the stock prices), such that the permanent or temporary component at any interval is estimated using all past, present, and future observations in the series. Thus, the purpose of filtering is to ensure that estimates are updated with the introduction of every additional observation ([19]). Heterogeneously informed traders and FGC attributesWith the estimation of FGC's temporary and permanent price impacts, marketing researchers can explore how FGC attributes influence the occurrence of these two types of price impact, which are driven by the existence of heterogeneously informed trading agents in financial markets ([29]; [60]). Thus, how information events, such as FGC, are observed and deciphered vary significantly between the two main groups of agents in financial markets: the informed and uninformed traders/investors (for a discussion on how the activities of informed and uninformed traders drive the asymmetric effects of information events in financial markets, see Web Appendix D). The valence and subject matter of FGC are attributes that should provide information signals to (informed) investors and thus generate a permanent price impact. This is because FGC subject matter (consumer and competitor orientation) relate to a firm's competitive advantage ([46]; [48]), which is not often public and can be difficult to observe because it is embedded in a firm's culture ([23]). The role of valence has also been documented in the literature ([71]; [76]), with the impact of negative valence appearing to be stronger than that of positive valence and thus more commonly associated with permanent price impacts ([75]). There is also reason to expect that FGC valence may interact with FGC subject matter and induce a permanent price impact. The basis for this expectation comes from a branch of signaling theory recognizing that signal recipients combine information signals to make more informed decisions ([ 6]; [73]).Although ample evidence suggests that FGC valence and subject matter may generate a permanent price impact, note that the price impact of FGC cannot occur without trading in financial markets. Trading activity incorporates the information and/or noise content of an event (e.g., FGC) into price. Therefore, because trading agents in financial markets are heterogeneously informed due to how they observe and decipher the information content of events, their trading activities also generate varied price impacts. Specifically, a permanent price impact will arise as a result of trading activity by traders/investors who have been able to correctly decipher the information content of FGC—these are the informed traders. Conversely, the trading activity of those unable to decipher the information content of FGC (i.e., uninformed traders) will only induce temporary price impacts ([24]) because their trading activity is uncorrelated with firm value ([ 4]; [29]). Accordingly, FGC that incorporates valence and subject matter can be associated with both permanent and temporary price impacts simply because of heterogeneously informed trading agents. The trading activity of informed traders thus contributes to the efficient component of price (i.e., permanent price impact), which is driven by information, whereas the activity of uninformed traders incorporates noise (i.e., temporary price impact), which is uncorrelated with firm-relevant information. S&P 500 IT Firms' Use of TwitterWe examine the instantaneous stock market impact of FGC by studying S&P 500 IT firms' activity on Twitter. Twitter is a social media communication channel characterized by ""fast-paced and short-lived information flows"" ([50], p. 177), from which deep insights can be derived if appropriate methods are developed ([79]). In addition, with 92% of firms tweeting multiple times a day ([10]), Twitter FGC is an example of high-frequency intraday marketing data. Finally, the Securities and Exchange Commission's Regulation Fair Disclosure recognizes Twitter FGC as potentially carrying ""useful"" information for investors. For these reasons, Twitter provides a suitable context to study. We study the IT sector because IT firms are often considered early adopters of trends ([ 7]). The IT sector provides a comprehensive sample of firms disseminating multiple pieces of FGC throughout the day (see Web Appendix F). It is a major driver of economic activity, with the leading five IT firms in the United States accounting for more than 22% of the S&P 500 ([30]). Globally, the IT sector is valued at $11.5 trillion, representing over 15.5% of the global gross domestic product ([14]). Finally, the diverse consumer base of IT firms is useful for characterizing the relevance of FGC subject matter (consumer and competitor orientation) and its interaction effects with valence. A review of 10-K filings for each firm shows that 7% of the sample consists of firms marketing solely in B2C markets, 72% solely in B2B markets, and 21% selling in both B2C and B2B markets. Data Set ConstructionWe obtained a sample of tweets using an application programming interface (API) to access Twitter data. In line with previous research ([50]; [77]) and following [16], we employed the API to access tweets from corporate accounts for S&P 500 IT firms. In total, we obtained 153,041 firm-generated tweets from 64 firms, which we then used in our analysis. On average, this is 2,391.2 tweets per firm over our sample period spanning January 2013 to August 2018. It should be noted that these are tweets that fall within the limits of Twitter's API in terms of the maximum number of tweets that can be accessed over a given time period. Among the IT firms we initially selected, we eliminated seven because either they did not have established corporate Twitter accounts or Twitter's API limited access to their data. Firms included in the sample engaged in high-frequency intraday marketing activity. On average, they generated a minimum of 1.07 to a maximum of 37.03 tweets a day, with the total average equaling 4.42 tweets per firm per day (see Web Appendix F), which confirms the appropriateness of the selected sample. Table 1 shows the sample of ten S&P 500 IT firms generating the highest number of tweets per day.GraphTable 1. Twitter Data Sample. S&P 500 IT FirmNumber of TweetsNumber of Tweet DaysMinimum Number of Tweets per DayMaximum Number of Tweets per DayAverage Number of Tweets per DaySingle Tweet Days (%)Number of Tweets ExcludedaNumber of Days ExcludedaRed Hat3,102204215515.1302025DXC Technology2,23915533014.35058CA3,05223216913.09.26027Cognizant3,09429413210.48.653436Oracle3,18733111059.59.852024F5 Networks3,0413552328.5401530Gartner3,2293931858.19.771624FLIR Systems3,2284131307.791.332927ANSYS3,1414251427.371.972220PAYCHEX3,0724281687.161.461030 2 a Excluded due to excessive return volatility.3 Notes: Table 1 reports the frequency statistics for a sample of tweets generated between January 8, 2013, and August 17, 2018, for S&P 500 IT firms with stocks included in the S&P 500 Index. The table shows the ten firms with the highest average number of tweets per day. Web Appendix F provides the full sample of 64 S&P 500 IT firms.We recorded each tweet with a timestamp to the nearest millisecond.[ 6] We then used these timestamps to obtain corresponding ultra-high-frequency stock trading activity data from the Thomson Reuters Tick History v2 database in Datascope for each tweet in the sample (see Table 2). This stock trading data supplemented the Twitter data set. Our data set included data for the trading days between January 2013 and August 2018. After performing data cleaning using the criteria consistent with that of [19] and [32], the stock trading data included 8,177,183,865 instances of trading activity or messages (i.e., quotes, cancellations, and transactions), which includes 520,356,393 transactions and 7,656,827,472 orders.[ 7]GraphTable 2. Trading Data Descriptive Statistics MessagesTransactionsOrdersBefore Cleaning8,182,063,205522,403,1787,659,660,027After Cleaning8,177,183,865520,356,3937,656,827,472Percentage of Data Removed After Data Cleaning.06%.39%.04% 1 Notes: Table 2 reports precleaning and postcleaning statistics for the number of messages (quotes, cancellations, and transactions) generated for 64 S&P 500 IT firms between January 8, 2013, and August 17, 2018. Data cleaning is completed by following Chordia, Roll and Subrahmanyam (2001) and Ibikunle (2015), and each individual message is measured U.S. currency.After excluding days (and tweets generated on these days) with comparatively high levels of price volatility, the descriptive statistics show that the average time between trades is 7.159 seconds and the mean number of tweets per firm during the sample period is 2,377.22. The mean number of tweets per day is 54.03, and the mean number of tweets per day per firm is.844 (see Table 3 for details).GraphTable 3. Descriptive Statistics of Tweet Activity. Mean Number of TweetsMinimum Number of TweetsMaximum Number of TweetsTweets per Day per Firm.844.006.79Tweets per Firm2,377.2230.003,040Tweets per Day54.030.00353.00 4 Notes: Table 3 reports precleaning and postcleaning statistics for the number of messages (quotes, cancellations, and transactions) generated for 64 S&P 500 IT firms between January 8, 2013, and August 17, 2018. Each individual message is measured U.S. currency. Investigating the Permanent and Temporary Price Impact of TweetsTo investigate tweets' permanent and temporary price impacts respectively, we first used state-space modeling to estimate the permanent and temporary components of price at a given time interval using trading observations within that time interval.[ 8] The primary interval of interest was one second; however, we estimated for one minute as well for robustness. Next, we linked these estimates to firms' tweet activity using tweets' timestamps, which were labeled to the second. Thereafter, we estimated the impact of each tweet on the temporary and permanent components of price by estimating the corresponding absolute change in the components following each tweet as the respective temporary and permanent price impacts. The methodological steps are outlined next. Step 1 (model characterization)The first step involved modeling price as the sum of a nonstationary permanent (information-driven) component and a stationary temporary (noise) component.[ 9] In this step, the only relevant observations were the prices of the 520,356,393 transactions obtained from the Thomson Reuters Tick History v2 database. These prices were defined as the prices of stocks at intraday periods and intervals. In its simplest form, the structure of the state-space model for price, a multiple of S stock prices, T intraday periods, and N intervals, is expressed as follows: vs,t,τ=ms,t,τ+is,t,τ, Graph( 1)and ms,t,τ=ms,t,τ−1+us,t,τ, Graph( 2)where vs,t,τ=ln(ps,t,τ), Graph( 3)for s = 1,...,S,  τ   = 1,...,T, and t = 1,...,N;  τ  and t index event and clock times, respectively ([56]); and an event occurs when a transaction is recorded. Thus, T = 520,356,393 and N equals the number of one-second or one-minute intervals during a stock trading day.  ps,t,τ  is the price of stock s at interval t and period  τ  ,  ms,t,τ  is a nonstationary permanent component of the price of stock s at interval t and period  τ  ,  is,t,τ  is a stationary transitory component of the price of stock s at interval t and period  τ  , and  us,t,τ  is an idiosyncratic disturbance error in the permanent price component of stock s at interval t and period  τ  .  is,t,τ  and  us,t,τ  are assumed to be mutually uncorrelated and normally distributed.[10]The model captured in Equations 1–3 is a special case of the general state-space representation. The standard state-space model is formulated for a vector of time series  vt  with a frequency/time interval  t  , and this is given by (for simplicity, we temporarily ignore the stock notation  s  and period  τ  ): vt=Wtδ+Ztmt+it,mt+1=Dtmt+Rtut,t=1,...,N, Graph( 4)where disturbances  it∼N(0,It)  and  ut∼N(0,Ut)  are mutually and serially uncorrelated. The initial state vector  m1∼N(a,P)  is also uncorrelated with the disturbances. The mean vector  a  and variance matrix  P  are usually implied by the dynamic process for  mt  in Equation 4 ([57]). The remaining terms,  Wt,Zt,Dt,Rt,It  and  Ut  , are system matrices and are generally assumed to be fixed for  t=1,...,N  . The elements of these system matrices are usually known; however, some elements that are functions of the fixed parameter vector need to be estimated. Equations 1 and 2 can be represented as the state-space Equation 4 by choosing  vt  as a single time series (this is the stock price series in this study), where  Wt=0  ,  Zt=Dt=Rt=1  ,  It=σ2i  , and  Ut=σ2u  . We note that  σ2i  and  σ2u  vary for each frequency  t  for  t=1,..,N  . Unlike standard variable decomposition approaches, this model naturally deals with irregular frequency/missing observation issues because the Kalman filter is used for its estimation, which is critical in a high-frequency analysis.[11] Step 2 (model outputs)The structure of the model shows that only changes in  us,t,τ  (now reinstating the stock notation  s  and period  τ  ) affect price permanently, and  is,t,τ  is temporary because its effects are transient and hold no significance for long-term firm performance. This is because this model decomposes price into two parts. The first,  us,t,τ  , captures smoothed (constant) changes in price, which are driven by informed trading activity, while the second captures irregular changes in price, which deviate from the smoothed evolution and are therefore driven by uninformed trading activity (noise or friction in the pricing process). By using maximum likelihood (constructed using the Kalman filter), we estimated  σs,t2u  (i.e., permanent component) and  σs,t2i  (i.e., temporary component), where t is equal to either one second or one minute. Specifically, we first partitioned our sample into one-second and one-minute (clock) intervals, and then estimated  σs,t2u  and  σs,t2i  for these intervals by using the prices at different event periods (  τ  ) during the intervals. This suggests that, as in [57], our permanent and temporary components (  σs,t2u  and  σs,t2i  ), as estimated using the state-space model, were time variant (see Table 4 in Menkveld, Koopman, and Lucas [2007, p. 220]). We imposed the time-variant structure to be consistent with the time intervals studied in subsequent multivariate regressions, which is in line with [11], who also compute time-variant permanent and transitory components of price.GraphTable 4. Permanent and Temporary Components of Price: Descriptive Statistics. Price ComponentMeanMedianStandard DeviationMinimumMaximumTemporary price component (σs,t2i).011.008.009.000.297Permanent price component (σs,t2u).055.010.048.000.644 5 Notes: Table 4 reports descriptive statistics for the permanent and temporary impact of tweeting.  σs,t2i  and  σs,t2u  are the respective estimates of the temporary and permanent components of the price for firm stock s at interval t, estimated by maximum likelihood (constructed using the Kalman filter). Step 3 (estimation with the Kalman filter)We used the Kalman filter to evaluate the conditional mean and variances of the state vector  mt  (ignoring the stock notation  s  and period  τ  ) given past observations  Vt−1={v1,...,vt-1}  :  at|t−1=E(mt|Vt−1)  ,  Pt|t−1=var(mt|Vt−1)  ,  t=1,...,N.  To initialize the Kalman filter, we also had  a1|0=a  and  P1|0=P  , where  m1∼N(a,P)  . This initialization only works if  mt  is a stationary process. However, as in our case,  mt  is often not a stationary process because it is obtained from stock price series, which are inherently nonstationary given the rational expectation of economic growth over time. Therefore, ""diffuse initialization"" (i.e., infinite variance distribution; see [44]) is used and estimated by numerically maximizing the log-likelihood. This is evaluated with the Kalman filter due to prediction error decomposition. According to the structure of the state-space model, our estimated outputs,  σs,t2u  and  σs,t2i  , were modeled as variances of permanent and temporary components of price, respectively.  σs,t2u  is a proxy for information reflected in the price (i.e., the permanent component of price), and  σs,t2i  is a proxy for noise reflected in the price (i.e., the temporary component of price). Stock prices should only experience permanent movements because of the arrival of new information; thus, we would expect  σs,t2u  to be higher than  σs,t2i  . The two estimated coefficients are variances; therefore, the coefficient encapsulating information  (σs,t2u)  , which is the primary driver of price from an efficient market perspective, should be larger.  σs,t2i  captures frictions/noise and should therefore have a lower value.[12] Step 4 (linking σ s , t 2 u and σ s , t 2 i to tweets)Our empirical framework required linking an individual intraday tweet to a corresponding trade/transaction with price pt in our sample. We call each tweet-linked trade a ""tweet-trade,"" and t, in this case, corresponds to both trade and time. Accordingly, we designated a trade in the stock of a firm as a ""tweet-trade"" if it was the first trade to occur immediately after a tweet in our sample and if it occurred within 60 seconds of the tweet. For robustness, we varied this threshold but find our inferences to be consistent if the threshold is reduced to 30 and 45 seconds, suggesting that the link between tweets and trading of firms' stock is not merely coincidental. The tweet-trade's time of occurrence allowed us to link a tweet to a corresponding pair of  σs,t2u  and  σs,t2i  , which we estimated for the one-second and one-minute intervals covered by our sample period, including those with no tweet-trades. Thus, each second and minute in our sample has a corresponding set of  σs,t2u  and  σs,t2i  . We could therefore determine the information reflected in the price (i.e., price efficiency) and noise contained in the price at every second or minute. This information allowed us to estimate whether the change in both components was occasioned by the posting of a tweet. Table 4 presents the descriptive statistics for the one-minute intervals, including tweet-trades.  σs,t2u  is higher than  σs,t2i  , which is consistent with our expectation that most of the observed tweets at time t reflect fundamental information rather than frictions or transitory components of price. This is also in line with microstructure literature ([11]; [35]; [57]; [65]). Step 5 (estimating changes in σ s , t 2 u and σ s , t 2 i following a tweet)The next step in our analysis was determining how a tweet/tweet-trade changes the composition of price with regard to  σs,t2u  and  σs,t2i  , which is required in further analysis when examining the impact of tweet valence and subject matter (i.e., consumer and competitor orientation). We linked each tweet-trade to a pair of  σs,t2u  and  σs,t2i  using the tweet-trade timestamps at the second level and then computing 30-second percentage absolute changes for both  σs,t2u  and  σs,t2i  . The changes in  σs,t2u  and  σs,t2i  following a tweet are designated as  Δσs,t2u  (permanent price impact) and  Δσs,t2i  (temporary price impact), respectively (45- and 60-second percentage changes are also computed for robustness)[13]: Δσs,t2u=|σs,t+30s2u−σs,t−12uσs,t−12u|, Graph( 5) Δσs,t2i=|σs,t+30s2i−σs,t−12iσs,t−12i|. Graph( 6)Thereafter, we also constructed  Δσs,t2u  and  Δσs,t2i  for each non-tweet-trade in our sample. Using these measures, we constructed daily ratios of the impact of a non-tweet-trade relative to that of an average tweet-trade in stock s during day d. We then tested the null that the mean daily ratios in stock s equal 1 on average across our sample period by using their standard errors for statistical inference. We expected to reject the null if the tweet-trades, on average, generated a larger or lower price impact than all the trades on an average day.[14] We present the results of the hypothesis testing in Table 5. The ratios employed in the analysis were winsorized at.5 and 99.5 percentiles within each stock. This statistical approach was consistent with prior marketing research ([ 9]), and it allowed us to eliminate outliers or extreme values and improve the chance of obtaining statistically significant estimates. Winsorization was also necessary due to the inherent noisiness of high-frequency trading data used in estimating  Δσs,t2u  and  Δσs,t2i  .GraphTable 5. Ratios of the Price Impact of Tweet-Trades to the Price Impact of Other Trades. Price Impactt60-Second Threshold45-Second Threshold30-Second ThresholdTemporary price impact (Δσs,t2i)279.67***(7.51)230.12***(9.58)222.55***(10.23)Permanent price impact (Δσs,t2u)146.83***(4.33)178.87***(3.21)189.04***(5.17) 6 ***Statistical significance at the.01 level.7 Notes: The t-statistics testing the null that the ratios equal 1 are presented in parentheses.  Δσs,t2i  and  Δσs,t2u  , which correspond to the temporary price impact and permanent price impact for stock s at time/interval t, are obtained from  σs,t2i  and  σs,t2u  (defined in Table 4). The ratios of  Δσs,t2u  and  Δσs,t2i  for each tweet-trade to the  Δσs,t2u  and  Δσs,t2i  for the other trades during an average trading day are then computed. The mean ratios are presented in Table 5 and their corresponding t-statistics are reported in parentheses.The estimates in Table 5 show that, on average, tweet-trades generate larger permanent and temporary intraday price impacts than non-tweet-trades. All the estimates are statistically significant at the.01 level, thus refuting the null hypothesis that there is no difference between the impact generated by tweet-trades and other trades. The price impact of a tweet-trade is 150–300 times larger than that of the average trade. Using the 60-second threshold,  Δσs,t2i  , which corresponds to the temporary price impact generated by the average tweet-trade, we find that the average tweet-trade has 279.67 times the impact of the average trade, suggesting that tweets result in large but momentary movements of price. This finding suggests that FGC generates temporary effects that can induce increases in the cost of trading a firm's stock and the firm's cost of capital ([14]; [17]).  Δσs,t2u  , the permanent impact of the average tweet-trade, is about 146.83 times larger than the average trade's permanent impact, suggesting that FGC can cause investors to update their expectations about a firm's future performance, which in turn leads to price movement. Overall, the analysis indicates that, on average, tweet-trades occurring in the wake of a potentially information-laden tweet substantially impact stock prices both permanently and temporarily relative to non-tweet trading activity.Estimating the effects of tweet-trades within subminute to minute windows addresses methodological issues associated with the occurrence of confounding events. Therefore, to a very high level of accuracy, we can attribute estimated temporary and permanent price impacts to the observed FGC. Given the fine-grained level of analysis, it is highly unlikely that any other relevant event could be driving the effects we capture. The sampling at high-frequency intervals also raises the question of whether investors and other trading agents could digest and act on the contents of tweets within the price impact windows we examine. Addressing this question requires an understanding of the nature of trading in financial markets today, especially in the case of highly traded stocks such as the S&P 500 stocks in our sample. Today's markets are dominated by algorithmic traders (commonly known as ""algos"") capable of digesting and acting on information in FGC (e.g., tweets) within the windows we examine in our analysis. The effects of this speed of activity are evidenced by the findings of [65], who, using S&P 500 stock data, show that information arriving in U.S. markets is exploited within seconds and that this activity is driven by algorithmic trading.Although all the ratios are statistically significant and suggest that FGC influences the permanent and temporary components of price, the obvious question is how economically meaningful tweet-trades are compared with other events that impact stock price. To answer this question, we conducted further analysis to examine the corresponding ratios of other large-impact non-tweet-trades in the same period by computing ratios similar to the ones presented in Table 5. This involved substituting a permanent price impact measure for each tweet-trade with that of other trades generating price impacts corresponding to one standard deviation or more above the daily mean in each stock. The obtained average ratios for the three thresholds are 7.9, 5.2, and 1.3 for the 30-, 45-, and 60-second windows, respectively. The inference drawn from this analysis is that the information content of tweet-trades is several times higher than that of the average non-tweet, high-impact trade. In comparing the temporary price impacts associated with the same trades with those of the tweet-trades, we find that tweet-trade ratios are again several times higher. This suggests that tweet-trades tend to be noisier than other trades associated with a more permanent price impact, and this provides a basis for demonstrating to marketers the significance of the relatively high levels of both permanent and temporary price impacts that can be generated in financial markets with the use of tweets. A robustness comparative analysis based on [22] is consistent with the presented findings (see Web Appendix G). Investigating the Temporary and Permanent Price Impacts of Tweet Valence and Subject MatterTo add intraday actionability, we used  Δσs,t2u  and  Δσs,t2i  , which encapsulate the permanent and temporary impacts of intraday tweets on firm value, as dependent variables to determine how tweet valence and subject matter (consumer and competitor orientation) influence the impact of tweets on stock price. To investigate whether tweet valence and subject matter drive the price impact of tweet-trades, we estimate Equation 7: PriceImpacts,t=αs+βt+γ1consumers,t+γ2competitors,t+γ3consumer*−ves,t+γ4competitor*−ves,t+γ5consumer*+ves,t+γ6competitor*+ves,t+γ7−ves,t+γ8+ves,t+∑k=17φkCk,s,t+ϵs,t, Graph( 7)where  PriceImpacts,t  corresponds to  Δσs,t2u  or  Δσs,t2i  , respectively, for a tweet-trade t in stock s.  αs  and  βt  are stock and time fixed effects. We used the VADER rule-based algorithm ([40]) to determine the valence of the tweets. VADER outperforms other commonly used benchmark methodologies such as Linguistic Inquiry and Word Count, Affective Norms for English Words, and the machine learning algorithm support vector machine in the literature as well as in our robustness tests. We also utilized [66] library and followed their method for measuring the competitor (  competitors,t  ) and consumer  (consumers,t)  orientation for each tweet, which is in line with [ 3] and [78]. Consumer and competitor subject matter are dummy variables that equal 1 when a tweet-trade's content is about consumer and/or competitors. We also studied the interaction effects of these attributes.  competitor*+ves,t  and  competitor*−ves,t  refer to positive-valence tweets related to competitors and negative-valence tweets related to competitors, respectively, for a tweet-trade t in stock s, and  consumer*+ves,t  and  consumer*−ves,t  refer to positive-valence tweets related to consumers and negative-valence tweets related to consumers, respectively, for a tweet-trade t in stock s.To avoid omitted variable bias and to ensure completeness, the model also includes  Ck,s,t  , which reflects a vector of known determinants of price impact based on past research in the market microstructure literature, as well as the natural logarithm of the number of an account's followers at the time of a tweet-trade t's tweet (  #followerss,t  ).  Ck,s,t  includes the natural logarithm of trading volume (  lnvolumes,t  ), the natural logarithm of average trade size (  lntradesizes,t  ), volatility (  volatilitys,t  ), effective spread (  Effectivespreads,t  ), the natural logarithm of a high-frequency trading proxy (  HFTs,t  ), and order imbalance (  OIBs,t  ). We measured trading volume as the dollar volume of transactions executed in stock s prior to a corresponding tweet-trade t. We computed average trade size as the trading volume prior to tweet-trade t divided by the number of transactions just prior to a corresponding tweet-trade t in stock s.  volatilitys,t  is the standard deviation of midpoint dollar price returns from the start of the trading day up to the trade just before the corresponding tweet-trade t in stock s.  Effectivespreads,t  (in basis points) was computed as twice the absolute value of the last trade price less the prevailing price midpoint prior to the corresponding tweet-trade t in stock s divided by the prevailing price midpoint. Price midpoint is the average of the prevailing best bid and ask prices.  HFTs,t  is the ratio of the number of messages (quotes, cancellations, and transactions) to actual transactions from the start of the trading day until prior to a corresponding tweet-trade t in stock s. Finally,  OIBs,t  is the ratio of the difference between the number of sell and buy orders and the average of both from the start of the trading day until prior to a corresponding tweet-trade t in stock s. To eliminate outliers in the data caused by the characteristic noisiness of high-frequency trading data, all variables are winsorized at.5 and 99.5 percentiles within each stock.We estimated Equation 7 using both panel least squares and 2SLS instrumental variable (IV) estimation approaches. Panel-corrected standard errors were computed to obtain heteroskedasticity and autocorrelation robust standard errors. We performed the IV estimation to account for the likelihood of endogeneity due to selection bias caused by a firm's decision regarding whether to use Twitter ([25]). The IV approach we employed was based on approaches adopted by an increasing number of studies in the marketing literature ([80]). For a given firm in our sample of S&P 500 IT firms, our approach involved first identifying the firms in the same two-digit Standard Industrial Classification that had sent a corresponding tweet on the previous or same day as the firm and then estimating the mean value of the potentially endogenous variables (consumer and competitor orientation) for these firms. The mean estimates were employed as an instrument for the firm in question. This variable met the requirements for an instrument because price impacts observed in the other firms' stocks were unlikely to be driven by the focal firm's tweets and, at the same time, tweeting activity has been shown to be correlated for firms in similar industries. In each of the first-stage regressions, we regressed each of the consumer and competitor variables separately on the corresponding IVs and the control variables defined previously for each firm/stock and obtained the F-statistics as tests of the null of weak instruments. The fitted values for each of the measures from the first-stage regressions were then employed as the variables in place of the consumer and competitor orientation variables in the second-stage regressions.The first-stage F-statistics, testing the null of weak instruments, show that our IV model does not suffer weak instrument issues. The test statistic is higher than the threshold of 10 needed for 2SLS inferences to be reliable when instrumenting for endogenous variables ([70]). We also conducted further tests to examine the instruments' relevance and the validity of the overidentifying restrictions in the IV regressions. The Cragg–Donald and Kleibergen–Paap Lagrange multiplier statistics we obtained reject the nulls of weak instruments and underidentification according to the [33] critical values, respectively. Essentially, these test the null hypothesis that the instruments we used have insufficient explanatory power to predict the endogenous variables in the model for identification of the parameters. All the p-values we obtained in the Sargan χ2 test also indicate that we cannot reject the null that the overidentifying restrictions are valid. All the 2SLS estimates for Equation 7 are presented in Table 6, and the results of the panel least squares estimations are presented in Web Appendix H.GraphTable 6. The Relationship Between Permanent and Temporary Price Impact and Tweet Valence and Orientation. VariablesPermanent Price Impact(Δσs,t2u)Temporary Price Impact(Δσs,t2i)Key Findingsconsumers,t−.007(1.06).009**(2.39)Consumer-related tweets are, on average, associated with a larger temporary price impact relative to other tweets.competitors,t−.034**(2.03).026**(2.46)Competitor-related tweets are, on average, associated with a larger temporary price impact and lower permanent price impact relative to other tweets.−ves,t−.063**(2.37).032**(2.46)Tweets with only negative or only positive valence are associated with increasing temporary price impact and decreasing permanent price impact.+ves,t−.108***(3.11).033**(2.20)consumers,t*−ves,t.047**(1.97).072**(2.50)Tweets reflecting both valence and subject matter are associated with increases in both permanent and temporary price impact. The increase in permanent price impact contrasts the decrease in permanent price impact that tweets with only valence or only subject matter are associated with.Except for tweets reflecting negative valence and consumer orientation (consumers,t*−ves,t), permanent price impact is more pronounced than temporary price impact.competitors,t*−ves,t.606***(3.69).011**(2.09)consumers,t*+ves,t.088**(2.10).019**(2.21)competitors,t*+ves,t.220***(4.88).077**(2.43)lnvolumes,t−.039***(-4.51)−.034***(-6.68)Increases in firm stock trading activity are linked with reductions in both permanent and temporary price impacts.lntradesizes,t.089***(6.64).061***(7.25)Larger firm stock trade sizes induce larger permanent and temporary price impacts.volatilitys,t−.123***(−3.62).015**(2.13)Firm stock volatility is linked with increases in temporary price impact and decreases in permanent price impact.Effectivespreads,t.241**(2.66).014**(2.06)Deterioration in firm stock liquidity is associated with increases in permanent and temporary price impacts.lnHFTs,t−.000(−.26)−.021***(−3.83)Algorithmic and high-frequency trading is linked with decreases in temporary price impact. Its effect on permanent price impact is benign.OIBs,t−.371***(−6.89).046**(2.39)Order imbalance is linked with reductions in permanent price impact and increases in temporary price impact.ln#followerss,t−.082**(−2.43).037**(2.43)The number of followers of a firm's twitter account amplifies the propensity for tweets to generate larger temporary price impact and reduce permanent price impact.R2¯.35.49Observations139,997139,997Kleibergen–Paap LM31.32***110.24***Tests the null hypothesis that the employed instruments have insufficient explanatory power to predict the endogenous variables in the model for identification of the parameter.Cragg–Donald79.08***88.66***Tests the same null hypothesis as the Kleibergen–Paap LM test.Sargan's χ2p-value.37.46Tests the null hypothesis that the overidentifying restrictions are valid. 8 **p <.05.9 ***p <.01.10 Notes: Table 6 reports 2SLS estimated coefficients for Equation 7. Standard errors are robust to heteroscedasticity and autocorrelation, coefficients are multiplied by 107, and t-statistics are reported in parentheses. LM = Lagrange multiplier.The results presented in Table 6 show the importance of tweet valence and subject matter in determining the permanent and temporary impacts of tweets on firm value. The existence of permanent and temporary price impacts associated with tweet attributes supports the signal theory perspective ([43]) and shows that investors pay attention to the tweet attributes of valence and subject matter. The estimates of permanent price impact for  γ7  and  γ8  are negative and statistically significant (−.063, p < .05, and −.108, p < .01, respectively). This suggests that tweets displaying only positive or only negative valence are linked to less permanent impacts in stock price. The positive and statistically significant  γ7  and  γ8  estimates of the temporary price impact estimation also indicate that such tweets are linked to increasing temporary price impact (.032, p < .05, and <.033, p < .05, respectively), and they suggest that tweet valence generally contributes more noise to stock price than stock-relevant information. The findings reinforce the role of positive and negative valence FGCs and their impact on firm value ([75]; [76]).With respect to tweet subject matter, only tweets conveying information about competitors generate statistically significant permanent price impacts (−.034, p < .05). Therefore, on average, tweets about a firm's competitors generate lower permanent price impact relative to other tweets. Conversely, the positive and statistically significant estimates for  γ2  for temporary price impact (.026, p < .05) show that these types of tweets are more likely to contribute to the noise component of price; in other words, they generate a larger temporary price impact than other tweets, on average. Thus, tweets conveying competitor orientation appear to result in a lower permanent price impact, suggesting that this form of subject matter is comparatively less impactful and relevant to investors' expectations about a firm's future performance ([48]). Notably, tweets about consumers do not yield any permanent price impact that is statistically different from that of other tweets and, thus, by themselves do not appear to offer a signal capable of causing investors to permanently update their firm performance expectations. Consumer-related tweets, similar to those about competitors, also generate more temporary price impact than other tweets on average, which suggests that their potential for inducing noise in stock price is higher than that of the average tweet in our sample. The  γ1  and  γ2  estimates of the temporary price impact are positive and statistically significant (.009, p < .05, and.026, p < .05, respectively). This finding implies that, as is the case with valence, tweets reflecting only competitor or only consumer orientation generate noise in the price discovery or trading processes and lower permanent price impact.Inferring from information-based market microstructure models ([24]; [47]), the more information about a firm investors observe, the more they become informed about the valuation of the firm. In line with this expectation, the interaction variables we include in Equation 7 should yield positive estimates for the  Δσs,t2u  estimations. As we expected, all the  γ3  ,  γ4  ,  γ5  , and  γ6  estimates of permanent price impact are positive and statistically significant (respectively,.047, p < .05;.606, p < .01;.088, p < .05; and.220, p < .01), even though, as already stated,  γ1  ,  γ2  ,  γ7  , and  γ8  are negative and statistically significant (except for  γ1  ). Thus, increases in both negative and positive valence, when viewed through the lens of subject matter, are linked with increased permanent price impact. These estimates show that tweet valence, when contextualized by subject matter or vice versa, is seen by investors/traders as firm-relevant information. In the context of these findings, the incorporation of valence and subject matter into FGC can yield increases in permanent price impact.Furthermore, the findings suggest that tweets about competitors with a negative valence are likely to have the highest permanent price impacts (.606, p < .001). This finding is crucial from the perspective of marketing practice and intraday social media marketing strategy design because valence and competitor subject matter as singular attributes of FGC are independently associated with decreasing permanent price impact. The findings underscore the view that investors seek additional information (i.e., information beyond what they already have) when making trading decisions ([ 6]; [73]) and that they operate according to classical market microstructure models. For example, [47] and [24] emphasize the crucial importance of information for price discovery in financial markets. This also confirms [51] findings that information from microblogging platforms, such as Twitter, impact investors' decisions.To illustrate the relevance of these findings, Figure 3 presents tweets A and B as examples of FGC characterized by negative and positive valence, respectively, but not containing any subject matter related to competitor or consumer orientation. Consistent with our findings, the permanent price impact estimates for the tweet trades corresponding to both tweets are more than three standard deviations lower than the average permanent price impact estimate and are thus below the tenth percentile of the estimates. The estimates for the negative and positive tweets' tweet-trades are.0017% and.0035%, respectively. In contrast with A and B, tweets C, D, and E reflect varying combinations of both valence and subject matter. Our findings suggest that these tweets should generate significant permanent price impact, and indeed, the permanent price impact estimates for the tweet-trades corresponding to tweets C, D, and E are above the 90th percentile in our sample of tweet-trades' permanent price impact estimates. The estimates are 3.74%, 2.84%, and 1.32% for tweets C, D, and E, respectively.Graph: Figure 3. Examples of tweets generating temporary and permanent price impacts.The effects of the tweet attributes we studied on temporary price impact,  Δσs,t2i  , also deserve attention. The results suggest that the relationship between valence and temporary price impact is generally magnified when combined with subject matter. For example, the  γ7  and  γ8  estimates, which capture the relationship between  Δσs,t2i  on the one hand and  −ves,t  and  +ves,t  on the other, are positive and statistically significant (.032, p < .05, and <.033, p < .05, respectively), while the estimates for  γ3  and  γ6  , which capture the relationship between  Δσs,t2i  on the one hand and  consumers,t*−ves,t  and  competitors,t*+ves,t  on the other, are also positive and statistically significant (.072, p < .05, and <.077, p < .05, respectively). The latter set of estimates is at least two times larger than the former. The overall implication of these positive and statistically significant coefficient estimates related to temporary price impacts is that, although tweets reflecting both valence and subject matter are likely to generate permanent price impact, these attributes may also be associated with increased temporary price impact. Thus, on average, tweets inject noise (uncertainty) into the prices of stocks traded in financial markets.In conclusion, the estimates presented in Table 6 highlight the relevance of tweet attributes for the price discovery process in financial markets and reinforce the importance of studying the multifaceted nature of FGC ([45]). We find that tweets, as with many events observed in relation to trading in financial markets, generate both permanent and temporary price impacts. However, whereas tweets containing singular attributes—either positive or negative valence or either consumer or competitor orientation—readily inject noise into the price discovery process and thus generate temporary price impact, those that include more than one attribute generate permanent price impact and thus generally enhance the efficiency of the price discovery process. DiscussionIn this research, we examine the real-time impact of FGC on the variance of firms' stock price. In the current fast-paced online communication landscape, marketers must understand the financial impact of firms' ""always on"" marketing ([64]). The assessment of FGC's financial impacts, however, is in an early stage ([ 8]; [15]; [64]). This research contributes to this emerging stream of marketing research and addresses multiple calls for new methods that are able to develop real-time insights from online data ([ 5]; [49]; [59]; [79]). By employing the market microstructure approach to study S&P 500 IT firms' Twitter activity, this study contributes to marketing literature and practice. Research ContributionsThis study offers several implications for marketing research. First, aligning with the work by [15], [ 8], and [64], it advances understanding of FGC's financial impact by providing an assessment of FGC's impact on the variance of stock price in real time (i.e., seconds). By employing a market microstructure approach, we show how to algorithmically link individual pieces of FGC to time-specific trading activity at a fine-grained level of analysis. In the process, we demonstrate the limitations of low-frequency methodologies such as daily event studies, which are subject to aggregation bias and may yield biased estimates of the impact of FGC on firms' financial outcomes, while offering an alternative and more robust method of analysis for studying intraday marketing activity. In our examination of the impact of FGC on variance, we fully utilized high-frequency transaction data characterized by unequal time intervals and demonstrate how to retain data that otherwise would have been eliminated in studies that use end-of-day stock price. By doing so, we provide marketing researchers with a new approach that allows them to harness the potential of online data.Second, we distinguish between FGC's temporary and permanent price impacts. Specifically, we show that FGC impacts investor expectations related to a firm's future performance, thus generating permanent price impact, and it also injects uncertainty about a firm's value into the firm's stock price, thus inducing temporary price impact. Our research, therefore, adds a new perspective to the marketing literature stream on the financial impact of FGC. This assessment of FGC's temporary and permanent price impacts adds richness to the examination of marketing's financial impact and enables the quantifying of long- and short-term financial impacts of marketing activity.Finally, this research has implications for the design of intraday marketing strategies. By examining FGC valence and subject matter (consumer and competitor orientation), we advance a growing body of research documenting the complex nature through which marketing signals impact financial markets and firm financial outcomes. We show that, by themselves, FGC valence and subject matter are more prone to injecting uncertainty about a firm's stock price into the market, and thus, they generate temporary price impacts rather than permanently changing investors' and traders' beliefs about firm value. Used together, FGC valence and subject matter both hold statistically significant and economically meaningful relevance for price discovery in financial markets. In other words, they can influence investors' expectations related to firms' future performance and thus result in permanent price impacts. Recent research by [ 6] provides evidence of interactions between marketing signals, and our research shows that the interaction between FGC valence and subject matter can also impact firm stock price. Managerial ImplicationsThus far, firms have struggled to demonstrate financial accountability regarding FGC's impact on firm value ([15]; [45]) or provide evidence of its immediate contribution to their financial outcomes ([53]; [58]). We provide marketing managers with evidence of FGC's impact on variance in firms' stock price. Specifically, we show that tweets can generate both permanent and temporary price impacts. By manipulating tweet attributes, such as valence and subject matter, marketing managers can design Twitter content to generate varying degrees of permanent or temporary impact. From a market quality perspective, firm managers should prefer tweets that generate a permanent price impact, and our research provides some useful indications about how to achieve this outcome. We show that tweets expressing degrees of positive or negative valence regarding either consumers or competitors generate a permanent price impact. We therefore encourage marketing managers to design information-rich tweets that both ( 1) focus on consumers or competitors and ( 2) communicate valence. Our results suggest that firms should utilize valence and subject matter in their tweets if they would like their stock to be more informative with respect to their value. Our analysis suggests that tweets about competitors with a negative valence are likely to have the highest permanent price impacts. Thus, by using permanent price impact as a metric to evaluate the longer-term impact of tweets, social media managers can design campaigns that have a sustainable impact on firm financial outcomes. The design recommendations from this study complement [41] work on social media content scheduling, as well as [64], which addresses real-time social media marketing and provides firms with information regarding which tweets to disseminate during the day for long-term effectiveness. We recognize that not all intraday tweets will, nor should they, have permanent impacts on firms' stock price. Some tweets are aimed at the creation of social media buzz, which is related to the temporary price impacts we examined in this study. Firms can indeed achieve social media buzz by tweeting, as our findings reveal that tweets, in aggregate, mostly generate temporary price impacts. We urge caution, however, because temporary price impacts are linked with larger transaction costs ([14]) and increases in firm cost of capital ([17]). This suggests that the benefits of designing tweets to generate buzz and incorporate information into stock price must be carefully managed. To support marketing managers in their intraday social media strategy design, Table 7 is designed as a set of insights based on our findings.GraphTable 7. Suggested Insights for Marketing Managers. Permanent Price ImpactTemporary Price ImpactResearch FindingsRecommendationExpected OutcomeInteraction EffectsPermanent Price ImpactTemporary Price ImpactResearch FindingsValencePositive valence Positive and negative valence-only FGC contributes to more noise in a firm's stock price.Add subject matter (e.g., competitor orientation such as ""competition,"" ""peer"")Increased permanent price impactPositive valence and competitor orientation Interaction of valence and subject matter increases/generates permanent price impact and amplifies temporary price impact. Permanent price impact is more pronounced than temporary price impact. The financial implication of these outcomes is a reduction in transaction and firm capital costs.Negative valenceAdd subject matter (e.g., consumer orientation such as ""customer,"" ""consumer,"" ""buyer"")Increased permanent price impactNegative valence and consumer orientationSubject MatterConsumer orientationSubject matter-only FGC contributes to the noise component in a firm's stock price.Add valence (e.g., positive valence such as ""help,"" ""solution,"" ""best"")Increased permanent price impactPositive valence and consumer orientationCompetitor orientationAdd valence (e.g., negative valence such as ""attack,"" ""stop,"" ""threat"")Increased permanent price impactNegative valence and competitor orientation 11 = no price impact; = negative impact on stock price component; = positive impact on stock price component; = increased positive impact on stock price component. LimitationsWe conclude by encouraging future research to address the limitations of our empirical study. One potential limitation of our analysis is its focus on firms in the IT sector. We recognize that these findings may not apply to other sectors. Future research could extend our analysis to other sectors to confirm whether similar price impacts hold. Second, the impact of tweets could depend on whether Twitter was the first source through which a firm released an important piece of news. For example, tweets could have been published in response to a competitor's tweet. In some cases, a firm's tweet could lead to a number of successive tweets, in which case the subsequent tweets might not be as impactful as the first. We do not discount the possibility that there could be some carryover or dampening effect in such situations. We note that, if this is the case, it would be highly unlikely for the magnitude of the effects we observe to occur, especially given the granular level of analysis that our market microstructure approach entails. Third, future work could explore high-frequency data generated by firms' use of FGC other than tweets, such as Facebook posts, where it has been reported firms post up to 80 times a day (Hutchinson 2018). It would be interesting to see whether the effect of FGC across social media platforms is consistent or if it varies. In addition to social media, it would be useful to examine firms' use of other online communication tools, such as webpages and blogging platforms. Researchers could also explore various types of FGC, including video content, as well as its characteristics, including emotions ([72]). As [36] show, there is an array of online marketing communication practices, and future research could therefore study the ""echoverse"" at a fine-grained level of analysis. Finally, we note that researchers can apply market microstructure to study user-generated content (UGC) in future research. We welcome future research that addresses the following questions: What is the real-time impact of UGC on firm value? What are the UGC attributes capable of generating permanent and temporary price impacts? Are these attributes the same as for FGC, or do they differ? Our research highlights the importance of interaction effects when examining the impact of FGC attributes on firm value; therefore, investigating the optimal mix of UGC attributes capable of generating temporary and permanent price impacts should be an interesting endeavor. Supplemental Materialsj-pdf-1-jmx-10.1177_00222429211042848 - Supplemental material for Measuring the Real-Time Stock Market Impact of Firm-Generated ContentSupplemental material, sj-pdf-1-jmx-10.1177_00222429211042848 for Measuring the Real-Time Stock Market Impact of Firm-Generated Content by Ewelina Lacka, D. Eric Boyd, Gbenga Ibikunle and P.K. Kannan in Journal of Marketing  "
15,"Measuring the Real-Time Stock Market Impact of Firm-Generated Content Firms increasingly follow an ""always on"" philosophy, publishing multiple pieces of firm-generated content (FGC) every day. Current methodologies used in marketing are unfit to unbiasedly capture the impact of FGC disseminated intermittently throughout the day on stock markets characterized by ultra-high-frequency trading. They also neither distinguish between the permanent (i.e., long-term) and temporary (i.e., short-term) price impacts nor identify FGC attributes capable of generating these price impacts. In this study, the authors define price impact as the impact on the variance of stock price. Employing a market microstructure approach to exploit the variance of high-frequency changes in stock price, the authors estimate the permanent and temporary price impacts of the firm-generated Twitter content of S&P 500 information technology firms. The authors find that firm-generated tweets induce both permanent and temporary price impacts, which are linked to tweet attributes of valence and subject matter. Tweets reflecting only valence or subject matter concerning consumer or competitor orientation result in temporary price impacts, whereas those embodying both attributes generate permanent price impacts. Negative-valence tweets about competitors generate the largest permanent price impacts. Building on these findings, the authors offer suggestions to marketing managers regarding the design of intraday FGC.Keywords: real-time marketing; microstructure; high-frequency data; firm-generated content; TwitterWith U.S. firms investing in excess of $37 billion in 2020 ([69]), social media is one of the most pervasive communication channels used by marketers ([ 5]; [36]). The result of this investment is the creation of corporate social media accounts that support firm-generated content (FGC), defined as a firm's communications disseminated through its own online communication tools ([45]). Many firms have adopted an ""always on"" approach in their social media marketing, disseminating multiple pieces of FGC throughout the day. Figure 1 illustrates the high-frequency approach to FGC dissemination using the example of information technology (IT) firms' activity on Twitter. Each piece of FGC is characterized by its attributes (e.g., Figure 1 shows FGC's valence and subject matter as key attributes),[ 5] and the timestamps show the dissemination time for each piece of FGC. Each piece of FGC and its timestamp can be accurately recorded to the second and mapped against the corresponding timestamp of trading activity that takes place at ultra-high frequency, that is, at subsecond intervals (Hasbrouck and Saar 2013). As a result of these high-frequency activities, large volumes of intraday data emerge. For example, an S&P 500 IT firm can issue in excess of 6,000 tweets in a given month, and trading in a single firm's stock often yields well over 10 million trading-related messages (e.g., quotes, cancellations, transactions) during the same interval (see Web Appendix A).Graph: Figure 1. A high-frequency approach to FGC dissemination on the example of IT firms' activity on Twitter.Current marketing methodologies are challenged when analyzing high-frequency data because the trading data, which is used in capturing the impact of FGC on firm value, is characterized by unequal time intervals. Low-frequency event studies using end-of-day price measures and time series analysis, such as standard vector autoregressive (VAR) models, are unable to effectively address these problems (see Web Appendix B). These methods rely on discrete and uniform time intervals that do not align with the time intervals associated with high-frequency trading data, which encapsulate the intraday evolution of firm stock price. Often, these methods aggregate trading data to regular intervals, which can lead to the elimination of upwards of 99% of intraday trading observations in studies employing end-of-day price data (see Web Appendix C). The frequency of FGC as an intraday variable and the effects of other non-FGC events potentially further bias low-frequency analyses that employ standard low-frequency analytical methods. Furthermore, the richness of such an assessment and marketing researchers' understanding of ""always on"" strategies are compromised unless the research identifies both the short- and long-term impacts of this form of marketing ([26]). Consequently, the findings derived from current examinations lack detailed ex post insights on the impact of FGC generated at intraday frequencies, which leaves marketing managers unable to effectively design future intraday marketing resource allocation strategies ([41]).Employing the market microstructure approach, which relies on ultra-high-frequency trading data analysis, we investigate the stock price impact of FGC where price impact is defined as the impact on the variance of stock price. This approach of estimating price impact of FGC as impact on the variance of stock price rather than on changes in the level of stock price is driven by both methodological and theoretical necessity. Although estimating level changes in stock price as a result of an event such as FGC dissemination could be approached from the perspective of computing simple price impact measures, when working with high-frequency data, this approach is inadequate for at least two reasons. First, simple price impacts are misleading when trades in stock markets are serially correlated. Second, in the presence of transient impacts, as is the case in this study (e.g., we capture price impact at second-by-second intervals), simple price impacts rely on getting the timing just right, which is methodologically unfeasible when investigating large data sets, as is the case here. Our methodological approach is in line with market microstructure theory, addresses the previously outlined issues, and is consistent with the microstructure literature ([11]; [65]). Using this approach and assessing S&P 500 IT firms' Twitter activity, we contribute to the marketing literature at three levels.First, by using high-frequency data, this is the first study to document the subminute impact of individual pieces of FGC disseminated during the day; therefore, the insights presented are unlikely to be affected by the confounding effects that the use of end-of-day data is susceptible to. We estimate the price impact of FGC at the second and minute levels by computing the variance of fast-paced (e.g., subsecond-by-subsecond) intraday changes in stock price as they occur in financial markets ([11]; [12]; [42]), thereby demonstrating the instantaneous impact of FGC. We obtain the variance of intraday changes in price through state-space modeling with Kalman filtering. By doing so, we contribute to the emerging stream of marketing research studying ( 1) the impact of FGC on firm financial outcomes ([ 8]; [15]) and ( 2) real-time marketing ([64]), and we respond to research priorities established by the Marketing Science Institute (2018), emphasizing the need to help marketers ""get marketing right"" by providing insights into the instantaneous impact of FGC.Second, using the microstructure perspective, we reveal both the permanent and temporary price impact of FGC as new forms of FGC impact on firm-level performance. In the process, we address [26] call for research capable of identifying both the short- and long-term financial impact of a marketing activity, which has thus far remained difficult to quantify. By being able to distinguish between temporary and permanent price impacts at the fine-grained level of analysis, marketing managers can demonstrate both the short- and long-term impacts of FGC on firm financial performance ([53]). This, in turn, will allow them to overcome short-termism in marketing and improve long-term growth initiatives ([18]; [58]).Finally, to support intraday actionable FGC design, we examine the extent to which key attributes of FGC, including content valence and subject matter, influence the occurrence of permanent and temporary impacts of FGC on price. Although FGC valence and subject matter have been examined by previous research (e.g., [20]; [27]; [36]) and are recognized as key components of marketing excellence ([39]), what constitutes the ""right content"" is largely unknown according to research priorities recently published by the Marketing Science Institute (2020). We show that FGC reflecting only one of the attributes of valence (positive or negative) or subject matter (consumer or competitor orientation) generates temporary price impact, whereas FGC that incorporates both valence and subject matter is associated with permanent price impacts on stock price and thus correlates with long-term firm performance.Using a two-stage least squares (2SLS) estimation framework to examine S&P 500 IT firms' Twitter activity, we find that negative- or positive-valence tweets are consistently linked with a reduction in permanent price impact and an increase in temporary price impact. Similar findings are obtained when examining tweets that only reflect a consumer or competitor subject matter, although the reduction in permanent price impact and increase in temporary price impact they elicit are of smaller magnitudes. These findings indicate that tweets reflecting only valence (positive or negative) or subject matter (consumer or competitor orientation) result in temporary price impacts, which is commonly associated with the incorporation of noise into the price discovery process ([60]). This type of effect has not been studied previously in the marketing literature; however, given that it is a source of uncertainty in the value of firms that can increase a firm's cost of capital ([17]), it demands attention. Employing the market microstructure approach to exploit the variance of high-frequency changes in stock price, this is the first study that reveals tweets' temporary price impacts and identifies tweet attributes that elicit such short-term impacts on price.We further find that tweets that embody both attributes—valence and subject matter—generate permanent price impacts; however, this impact varies according to the type of valence and subject matter. The results evidence the importance of interaction effects between tweet valence and subject matter in generating a higher permanent price impact. The average negative- and positive-valence tweet, when viewed through the lens of a consumer or competitor orientation, generates a permanent price impact, and a competitor-oriented tweet with a negative valence is likely to have the highest permanent price impact. This is a crucial finding from the perspectives of marketing practice and intraday social media marketing strategy design because valence, as a singular attribute, is associated with decreasing permanent price impact. Our research shows that information-rich tweets that include both variance and subject matter can result in permanent price impacts, and it demonstrates investors' ability to act on information contained in FGC at subsecond levels (Hendershott et al. 2011; [65]).To illustrate the relevance and magnitude of these findings, in Figure 2, tweets A and B are characterized by negative and positive valence, respectively, and tweets C and D reflect only consumer and competitor orientation, respectively. In line with our research findings, the permanent price impacts associated with these tweets are more than three standard deviations lower than the average permanent price impact estimates for all the 153,041 tweets in our sample and are therefore below the 10th percentile of the estimates. In contrast, tweets E to H reflect varying combinations of both valence and subject matter (consumer or competitor orientation). Consistent with our findings, we show that these tweets generate permanent price impact estimates ranked above the 90th percentile in our sample of tweet trades' permanent price impact estimates. The temporary and permanent impact estimates for the average tweet are as large as 279 and 187 times, respectively, what we document for the average regular intraday transaction in our sample.Graph: Figure 2. Examples of tweets characterized by valence and subject matter (consumer and competitor orientation). Theoretical Background FGC and High-Frequency Trading DataFirms increasingly use social media because it provides greater reach and can be less costly than traditional channels for FGC dissemination ([45]). FGC is often posted several times a day ([41]) and serves as a valuable source of high-frequency marketing data that can offer insights into the growth potential of a firm ([18]). With the advancement of data collection tools ([79]), marketing researchers can now record each piece of FGC and create large data sets depicting FGC attributes and their dissemination time. Recorded with accuracy to the second, FGC can then be mapped against the corresponding trading activity that takes place at subsecond intervals and used to study its financial impact. However, measuring the impact of FGC sampled at intraday levels requires marketing researchers to be able to utilize high-frequency trading data with observations occurring at unequal time intervals.The market microstructure approach to estimating price impact offers marketing researchers tools to, piece-by-piece, algorithmically link FGC to time-specific trading activity at a fine-grained level of analysis (e.g., subseconds, seconds, minutes). Unlike symmetrical asset pricing models, market microstructure recognizes that a firm's stock price is only informationally efficient to the extent that it reflects all available and relevant information ([21]). A firm's stock price, while reflecting information, is also distorted by noise generated by (temporary) non-information-based factors. Such factors can include trading frictions due to low levels of liquidity, defined as the ability to trade large quantities of a firm's stock quickly with little or no price impact ([ 2]; [28]), or the activity of traders who lack adequate information regarding the value of a stock, known as ""uninformed traders"" in the market microstructure literature ([24]; [47]). Estimating the proportion of stock price driven by information (relevant to the value of a firm) and the proportion driven by noise is a critical aspect of the analyses many studies conduct in the market microstructure literature (see Web Appendix D). However, this holistic view of both temporary and permanent price impacts is often missing from marketing research. The market microstructure approach allows marketing researchers to estimate the price impact of FGC at high frequencies and to identify both types of price impacts. A crucial element in such analysis is knowing the ""event time"" (i.e., timestamp), which refers to the time at which an event occurs, such as the FGC dissemination time. By deploying time series models to estimate changes in the components of price at high-frequency intervals (e.g., seconds) and then linking the FGC timestamp to the components, the instantaneous impact of FGC on firm stock price can be estimated. FGC's Impact on Firm OutcomesExtant research has primarily focused on linking FGC with consumer behavior ([15]; [13]; [36]; [45]; [55]; [72]) and firm performance ([ 8]; [15]; [64]) (see Web Appendix E). With the focus on firm performance, [15], [ 8], and, most recently, [64] show that FGC affects firm value. [15] document an indirect effect of FGC volume on shareholder value measured using abnormal returns and idiosyncratic risks. [ 8] were the first to demonstrate the direct impact of humorous FGC on firm value as measured by abnormal stock market returns. To demonstrate these impacts, they employed an event study that estimated abnormal returns and VAR modeling. However, these methods, as deployed, focus on daily activity, which can result in aggregation bias and misevaluation of FGC's impact on firm value ([64]). Moreover, the richness of such an assessment is compromised because short- and long-term impacts of FGC are not estimated ([26]; [58]). Finally, current marketing methods do not examine FGC attributes at a fine level of granularity (i.e., intraday frequencies), preventing marketing managers from moving beyond a ""throw it on the wall and see what sticks"" strategy ([37], p. 47) in the design and dissemination of intraday FGC ([36]). High-Frequency Approach to FGC AnalysisThe market microstructure approach responds to calls for more powerful methodological approaches that allow marketing researchers to harness the potential of rich data sources and develop insights capable of advancing theory and informing contemporary marketing practice (e.g., [18]; [36]; [49]; [79]). The fine-grained level of analysis available using a market microstructure approach overcomes the limitations of low-frequency methodologies, such as VAR and daily event studies, to study FGC and its impact on firm value (i.e., it estimates the variance in firm stock price following FGC dissemination). Utilizing high-frequency intraday data, it adds richness to the assessment of FGC's financial impacts by distinguishing between permanent and temporary price impacts. Temporary and permanent price impactsTemporary price impacts are short-term impacts that result in momentary changes in the price of a stock before it returns to its pre-event (e.g., pre-FGC) value, and they are often the result of uninformed trader activity (see Web Appendix D). Uninformed trader activity could be driven by several factors; for example, it could be linked to investor uncertainty about the relevance of information ([34]; [38]) or trading friction due to liquidity constraints ([ 2]; [16]). Ignoring temporary price impacts can lead to misunderstanding the total impact of FGC, with prior research suggesting that temporary price impacts result in larger transaction costs ([14]) and firm cost of capital ([17]). In contrast, an event (e.g., FGC) can generate a permanent price impact and result in the price attaining an enduring new value after the event. This occurs when the event provides information that updates informed investor/trader expectations related to a firm's long-term performance ([52]). Importantly, the microstructure approach also supports intraday actionability by assessing how the attributes of information signaled by these events influence temporary and permanent price impacts. A state-space decomposition of firm stock priceConsistent with the market microstructure literature, this study estimates the permanent and temporary price impact of FGC by first conducting a state-space decomposition of firm stock price into its efficient (permanent) and inefficient/noise (temporary) components and then linking the changes in these components to individual pieces of FGC. State-space modeling is a tool for modeling an observed variable as the sum of unobserved variables ([35]), and it is commonly used for the decomposition of price ([11]; [35]; [57]; [65]). Due to its efficiency when applied to ultra-high-frequency data like stock price movements, the state-space modeling approach for decomposing price has significant economic and methodological advantages over other commonly used methods ([31]) such as VAR models.An assumption underlying a standard VAR model is that data are sampled at regular frequencies, as variables at time t are regressed on variables dated at t − 1, t − 2, and so on. However, FGC and intraday trading data are often sampled at unequal time intervals, which suggests there would be many instances of missing variables in a model calibrated on regular time intervals ([63]). The modeling of such data using VAR requires the alignment of variables misaligned in time either downward, by aggregating the data to a lower frequency, or upward, by interpolating the high-frequency data with heuristic rules such as polynomial fillings. Downward alignment eliminates potentially valuable information in the high-frequency data. Data aggregation is problematic ([68]), as it can alter the lag order of autoregressive moving average models ([ 1]), reduce the efficiency of the parameter estimation and forecast ([74]), affect Granger causality and cointegration among component variables ([54]), and induce spurious instantaneous causality ([10]). Upward alignment has also been deemed inefficient and dubious ([61]) because a VAR approach assumes that the model specifies the high-frequency data-generating process. However, interpolation is not based on the multivariate model that generates the data but instead on heuristic rules, which, at a minimum, inevitably incorporate noise into the data and distort it.State-space modeling offers a solution to the irregular frequency challenge inherent in intraday transaction data ([62]). Specifically, the use of state-space modeling with a Kalman filter in maximum likelihood estimation of parameter estimates ensures maximum efficiency in dealing with unequal time intervals or irregular frequency in data. The use of a Kalman filter accounts for changes across periods of analysis with missing observations. This is a critical consideration in the use of state-space modeling for decomposing high-frequency time series because standard approaches do not deal with the ""missing observations"" caused by unequal data intervals. For example, estimating a standard autoregressive framework implies truncation of the lag structure and could potentially discount valuable information in high-frequency data. Using the Kalman filter facilitates the decomposition of any realized change in the time series (e.g., variance in the stock prices), such that the permanent or temporary component at any interval is estimated using all past, present, and future observations in the series. Thus, the purpose of filtering is to ensure that estimates are updated with the introduction of every additional observation ([19]). Heterogeneously informed traders and FGC attributesWith the estimation of FGC's temporary and permanent price impacts, marketing researchers can explore how FGC attributes influence the occurrence of these two types of price impact, which are driven by the existence of heterogeneously informed trading agents in financial markets ([29]; [60]). Thus, how information events, such as FGC, are observed and deciphered vary significantly between the two main groups of agents in financial markets: the informed and uninformed traders/investors (for a discussion on how the activities of informed and uninformed traders drive the asymmetric effects of information events in financial markets, see Web Appendix D). The valence and subject matter of FGC are attributes that should provide information signals to (informed) investors and thus generate a permanent price impact. This is because FGC subject matter (consumer and competitor orientation) relate to a firm's competitive advantage ([46]; [48]), which is not often public and can be difficult to observe because it is embedded in a firm's culture ([23]). The role of valence has also been documented in the literature ([71]; [76]), with the impact of negative valence appearing to be stronger than that of positive valence and thus more commonly associated with permanent price impacts ([75]). There is also reason to expect that FGC valence may interact with FGC subject matter and induce a permanent price impact. The basis for this expectation comes from a branch of signaling theory recognizing that signal recipients combine information signals to make more informed decisions ([ 6]; [73]).Although ample evidence suggests that FGC valence and subject matter may generate a permanent price impact, note that the price impact of FGC cannot occur without trading in financial markets. Trading activity incorporates the information and/or noise content of an event (e.g., FGC) into price. Therefore, because trading agents in financial markets are heterogeneously informed due to how they observe and decipher the information content of events, their trading activities also generate varied price impacts. Specifically, a permanent price impact will arise as a result of trading activity by traders/investors who have been able to correctly decipher the information content of FGC—these are the informed traders. Conversely, the trading activity of those unable to decipher the information content of FGC (i.e., uninformed traders) will only induce temporary price impacts ([24]) because their trading activity is uncorrelated with firm value ([ 4]; [29]). Accordingly, FGC that incorporates valence and subject matter can be associated with both permanent and temporary price impacts simply because of heterogeneously informed trading agents. The trading activity of informed traders thus contributes to the efficient component of price (i.e., permanent price impact), which is driven by information, whereas the activity of uninformed traders incorporates noise (i.e., temporary price impact), which is uncorrelated with firm-relevant information. S&P 500 IT Firms' Use of TwitterWe examine the instantaneous stock market impact of FGC by studying S&P 500 IT firms' activity on Twitter. Twitter is a social media communication channel characterized by ""fast-paced and short-lived information flows"" ([50], p. 177), from which deep insights can be derived if appropriate methods are developed ([79]). In addition, with 92% of firms tweeting multiple times a day ([10]), Twitter FGC is an example of high-frequency intraday marketing data. Finally, the Securities and Exchange Commission's Regulation Fair Disclosure recognizes Twitter FGC as potentially carrying ""useful"" information for investors. For these reasons, Twitter provides a suitable context to study. We study the IT sector because IT firms are often considered early adopters of trends ([ 7]). The IT sector provides a comprehensive sample of firms disseminating multiple pieces of FGC throughout the day (see Web Appendix F). It is a major driver of economic activity, with the leading five IT firms in the United States accounting for more than 22% of the S&P 500 ([30]). Globally, the IT sector is valued at $11.5 trillion, representing over 15.5% of the global gross domestic product ([14]). Finally, the diverse consumer base of IT firms is useful for characterizing the relevance of FGC subject matter (consumer and competitor orientation) and its interaction effects with valence. A review of 10-K filings for each firm shows that 7% of the sample consists of firms marketing solely in B2C markets, 72% solely in B2B markets, and 21% selling in both B2C and B2B markets. Data Set ConstructionWe obtained a sample of tweets using an application programming interface (API) to access Twitter data. In line with previous research ([50]; [77]) and following [16], we employed the API to access tweets from corporate accounts for S&P 500 IT firms. In total, we obtained 153,041 firm-generated tweets from 64 firms, which we then used in our analysis. On average, this is 2,391.2 tweets per firm over our sample period spanning January 2013 to August 2018. It should be noted that these are tweets that fall within the limits of Twitter's API in terms of the maximum number of tweets that can be accessed over a given time period. Among the IT firms we initially selected, we eliminated seven because either they did not have established corporate Twitter accounts or Twitter's API limited access to their data. Firms included in the sample engaged in high-frequency intraday marketing activity. On average, they generated a minimum of 1.07 to a maximum of 37.03 tweets a day, with the total average equaling 4.42 tweets per firm per day (see Web Appendix F), which confirms the appropriateness of the selected sample. Table 1 shows the sample of ten S&P 500 IT firms generating the highest number of tweets per day.GraphTable 1. Twitter Data Sample. S&P 500 IT FirmNumber of TweetsNumber of Tweet DaysMinimum Number of Tweets per DayMaximum Number of Tweets per DayAverage Number of Tweets per DaySingle Tweet Days (%)Number of Tweets ExcludedaNumber of Days ExcludedaRed Hat3,102204215515.1302025DXC Technology2,23915533014.35058CA3,05223216913.09.26027Cognizant3,09429413210.48.653436Oracle3,18733111059.59.852024F5 Networks3,0413552328.5401530Gartner3,2293931858.19.771624FLIR Systems3,2284131307.791.332927ANSYS3,1414251427.371.972220PAYCHEX3,0724281687.161.461030 2 a Excluded due to excessive return volatility.3 Notes: Table 1 reports the frequency statistics for a sample of tweets generated between January 8, 2013, and August 17, 2018, for S&P 500 IT firms with stocks included in the S&P 500 Index. The table shows the ten firms with the highest average number of tweets per day. Web Appendix F provides the full sample of 64 S&P 500 IT firms.We recorded each tweet with a timestamp to the nearest millisecond.[ 6] We then used these timestamps to obtain corresponding ultra-high-frequency stock trading activity data from the Thomson Reuters Tick History v2 database in Datascope for each tweet in the sample (see Table 2). This stock trading data supplemented the Twitter data set. Our data set included data for the trading days between January 2013 and August 2018. After performing data cleaning using the criteria consistent with that of [19] and [32], the stock trading data included 8,177,183,865 instances of trading activity or messages (i.e., quotes, cancellations, and transactions), which includes 520,356,393 transactions and 7,656,827,472 orders.[ 7]GraphTable 2. Trading Data Descriptive Statistics MessagesTransactionsOrdersBefore Cleaning8,182,063,205522,403,1787,659,660,027After Cleaning8,177,183,865520,356,3937,656,827,472Percentage of Data Removed After Data Cleaning.06%.39%.04% 1 Notes: Table 2 reports precleaning and postcleaning statistics for the number of messages (quotes, cancellations, and transactions) generated for 64 S&P 500 IT firms between January 8, 2013, and August 17, 2018. Data cleaning is completed by following Chordia, Roll and Subrahmanyam (2001) and Ibikunle (2015), and each individual message is measured U.S. currency.After excluding days (and tweets generated on these days) with comparatively high levels of price volatility, the descriptive statistics show that the average time between trades is 7.159 seconds and the mean number of tweets per firm during the sample period is 2,377.22. The mean number of tweets per day is 54.03, and the mean number of tweets per day per firm is.844 (see Table 3 for details).GraphTable 3. Descriptive Statistics of Tweet Activity. Mean Number of TweetsMinimum Number of TweetsMaximum Number of TweetsTweets per Day per Firm.844.006.79Tweets per Firm2,377.2230.003,040Tweets per Day54.030.00353.00 4 Notes: Table 3 reports precleaning and postcleaning statistics for the number of messages (quotes, cancellations, and transactions) generated for 64 S&P 500 IT firms between January 8, 2013, and August 17, 2018. Each individual message is measured U.S. currency. Investigating the Permanent and Temporary Price Impact of TweetsTo investigate tweets' permanent and temporary price impacts respectively, we first used state-space modeling to estimate the permanent and temporary components of price at a given time interval using trading observations within that time interval.[ 8] The primary interval of interest was one second; however, we estimated for one minute as well for robustness. Next, we linked these estimates to firms' tweet activity using tweets' timestamps, which were labeled to the second. Thereafter, we estimated the impact of each tweet on the temporary and permanent components of price by estimating the corresponding absolute change in the components following each tweet as the respective temporary and permanent price impacts. The methodological steps are outlined next. Step 1 (model characterization)The first step involved modeling price as the sum of a nonstationary permanent (information-driven) component and a stationary temporary (noise) component.[ 9] In this step, the only relevant observations were the prices of the 520,356,393 transactions obtained from the Thomson Reuters Tick History v2 database. These prices were defined as the prices of stocks at intraday periods and intervals. In its simplest form, the structure of the state-space model for price, a multiple of S stock prices, T intraday periods, and N intervals, is expressed as follows: vs,t,τ=ms,t,τ+is,t,τ, Graph( 1)and ms,t,τ=ms,t,τ−1+us,t,τ, Graph( 2)where vs,t,τ=ln(ps,t,τ), Graph( 3)for s = 1,...,S,  τ   = 1,...,T, and t = 1,...,N;  τ  and t index event and clock times, respectively ([56]); and an event occurs when a transaction is recorded. Thus, T = 520,356,393 and N equals the number of one-second or one-minute intervals during a stock trading day.  ps,t,τ  is the price of stock s at interval t and period  τ  ,  ms,t,τ  is a nonstationary permanent component of the price of stock s at interval t and period  τ  ,  is,t,τ  is a stationary transitory component of the price of stock s at interval t and period  τ  , and  us,t,τ  is an idiosyncratic disturbance error in the permanent price component of stock s at interval t and period  τ  .  is,t,τ  and  us,t,τ  are assumed to be mutually uncorrelated and normally distributed.[10]The model captured in Equations 1–3 is a special case of the general state-space representation. The standard state-space model is formulated for a vector of time series  vt  with a frequency/time interval  t  , and this is given by (for simplicity, we temporarily ignore the stock notation  s  and period  τ  ): vt=Wtδ+Ztmt+it,mt+1=Dtmt+Rtut,t=1,...,N, Graph( 4)where disturbances  it∼N(0,It)  and  ut∼N(0,Ut)  are mutually and serially uncorrelated. The initial state vector  m1∼N(a,P)  is also uncorrelated with the disturbances. The mean vector  a  and variance matrix  P  are usually implied by the dynamic process for  mt  in Equation 4 ([57]). The remaining terms,  Wt,Zt,Dt,Rt,It  and  Ut  , are system matrices and are generally assumed to be fixed for  t=1,...,N  . The elements of these system matrices are usually known; however, some elements that are functions of the fixed parameter vector need to be estimated. Equations 1 and 2 can be represented as the state-space Equation 4 by choosing  vt  as a single time series (this is the stock price series in this study), where  Wt=0  ,  Zt=Dt=Rt=1  ,  It=σ2i  , and  Ut=σ2u  . We note that  σ2i  and  σ2u  vary for each frequency  t  for  t=1,..,N  . Unlike standard variable decomposition approaches, this model naturally deals with irregular frequency/missing observation issues because the Kalman filter is used for its estimation, which is critical in a high-frequency analysis.[11] Step 2 (model outputs)The structure of the model shows that only changes in  us,t,τ  (now reinstating the stock notation  s  and period  τ  ) affect price permanently, and  is,t,τ  is temporary because its effects are transient and hold no significance for long-term firm performance. This is because this model decomposes price into two parts. The first,  us,t,τ  , captures smoothed (constant) changes in price, which are driven by informed trading activity, while the second captures irregular changes in price, which deviate from the smoothed evolution and are therefore driven by uninformed trading activity (noise or friction in the pricing process). By using maximum likelihood (constructed using the Kalman filter), we estimated  σs,t2u  (i.e., permanent component) and  σs,t2i  (i.e., temporary component), where t is equal to either one second or one minute. Specifically, we first partitioned our sample into one-second and one-minute (clock) intervals, and then estimated  σs,t2u  and  σs,t2i  for these intervals by using the prices at different event periods (  τ  ) during the intervals. This suggests that, as in [57], our permanent and temporary components (  σs,t2u  and  σs,t2i  ), as estimated using the state-space model, were time variant (see Table 4 in Menkveld, Koopman, and Lucas [2007, p. 220]). We imposed the time-variant structure to be consistent with the time intervals studied in subsequent multivariate regressions, which is in line with [11], who also compute time-variant permanent and transitory components of price.GraphTable 4. Permanent and Temporary Components of Price: Descriptive Statistics. Price ComponentMeanMedianStandard DeviationMinimumMaximumTemporary price component (σs,t2i).011.008.009.000.297Permanent price component (σs,t2u).055.010.048.000.644 5 Notes: Table 4 reports descriptive statistics for the permanent and temporary impact of tweeting.  σs,t2i  and  σs,t2u  are the respective estimates of the temporary and permanent components of the price for firm stock s at interval t, estimated by maximum likelihood (constructed using the Kalman filter). Step 3 (estimation with the Kalman filter)We used the Kalman filter to evaluate the conditional mean and variances of the state vector  mt  (ignoring the stock notation  s  and period  τ  ) given past observations  Vt−1={v1,...,vt-1}  :  at|t−1=E(mt|Vt−1)  ,  Pt|t−1=var(mt|Vt−1)  ,  t=1,...,N.  To initialize the Kalman filter, we also had  a1|0=a  and  P1|0=P  , where  m1∼N(a,P)  . This initialization only works if  mt  is a stationary process. However, as in our case,  mt  is often not a stationary process because it is obtained from stock price series, which are inherently nonstationary given the rational expectation of economic growth over time. Therefore, ""diffuse initialization"" (i.e., infinite variance distribution; see [44]) is used and estimated by numerically maximizing the log-likelihood. This is evaluated with the Kalman filter due to prediction error decomposition. According to the structure of the state-space model, our estimated outputs,  σs,t2u  and  σs,t2i  , were modeled as variances of permanent and temporary components of price, respectively.  σs,t2u  is a proxy for information reflected in the price (i.e., the permanent component of price), and  σs,t2i  is a proxy for noise reflected in the price (i.e., the temporary component of price). Stock prices should only experience permanent movements because of the arrival of new information; thus, we would expect  σs,t2u  to be higher than  σs,t2i  . The two estimated coefficients are variances; therefore, the coefficient encapsulating information  (σs,t2u)  , which is the primary driver of price from an efficient market perspective, should be larger.  σs,t2i  captures frictions/noise and should therefore have a lower value.[12] Step 4 (linking σ s , t 2 u and σ s , t 2 i to tweets)Our empirical framework required linking an individual intraday tweet to a corresponding trade/transaction with price pt in our sample. We call each tweet-linked trade a ""tweet-trade,"" and t, in this case, corresponds to both trade and time. Accordingly, we designated a trade in the stock of a firm as a ""tweet-trade"" if it was the first trade to occur immediately after a tweet in our sample and if it occurred within 60 seconds of the tweet. For robustness, we varied this threshold but find our inferences to be consistent if the threshold is reduced to 30 and 45 seconds, suggesting that the link between tweets and trading of firms' stock is not merely coincidental. The tweet-trade's time of occurrence allowed us to link a tweet to a corresponding pair of  σs,t2u  and  σs,t2i  , which we estimated for the one-second and one-minute intervals covered by our sample period, including those with no tweet-trades. Thus, each second and minute in our sample has a corresponding set of  σs,t2u  and  σs,t2i  . We could therefore determine the information reflected in the price (i.e., price efficiency) and noise contained in the price at every second or minute. This information allowed us to estimate whether the change in both components was occasioned by the posting of a tweet. Table 4 presents the descriptive statistics for the one-minute intervals, including tweet-trades.  σs,t2u  is higher than  σs,t2i  , which is consistent with our expectation that most of the observed tweets at time t reflect fundamental information rather than frictions or transitory components of price. This is also in line with microstructure literature ([11]; [35]; [57]; [65]). Step 5 (estimating changes in σ s , t 2 u and σ s , t 2 i following a tweet)The next step in our analysis was determining how a tweet/tweet-trade changes the composition of price with regard to  σs,t2u  and  σs,t2i  , which is required in further analysis when examining the impact of tweet valence and subject matter (i.e., consumer and competitor orientation). We linked each tweet-trade to a pair of  σs,t2u  and  σs,t2i  using the tweet-trade timestamps at the second level and then computing 30-second percentage absolute changes for both  σs,t2u  and  σs,t2i  . The changes in  σs,t2u  and  σs,t2i  following a tweet are designated as  Δσs,t2u  (permanent price impact) and  Δσs,t2i  (temporary price impact), respectively (45- and 60-second percentage changes are also computed for robustness)[13]: Δσs,t2u=|σs,t+30s2u−σs,t−12uσs,t−12u|, Graph( 5) Δσs,t2i=|σs,t+30s2i−σs,t−12iσs,t−12i|. Graph( 6)Thereafter, we also constructed  Δσs,t2u  and  Δσs,t2i  for each non-tweet-trade in our sample. Using these measures, we constructed daily ratios of the impact of a non-tweet-trade relative to that of an average tweet-trade in stock s during day d. We then tested the null that the mean daily ratios in stock s equal 1 on average across our sample period by using their standard errors for statistical inference. We expected to reject the null if the tweet-trades, on average, generated a larger or lower price impact than all the trades on an average day.[14] We present the results of the hypothesis testing in Table 5. The ratios employed in the analysis were winsorized at.5 and 99.5 percentiles within each stock. This statistical approach was consistent with prior marketing research ([ 9]), and it allowed us to eliminate outliers or extreme values and improve the chance of obtaining statistically significant estimates. Winsorization was also necessary due to the inherent noisiness of high-frequency trading data used in estimating  Δσs,t2u  and  Δσs,t2i  .GraphTable 5. Ratios of the Price Impact of Tweet-Trades to the Price Impact of Other Trades. Price Impactt60-Second Threshold45-Second Threshold30-Second ThresholdTemporary price impact (Δσs,t2i)279.67***(7.51)230.12***(9.58)222.55***(10.23)Permanent price impact (Δσs,t2u)146.83***(4.33)178.87***(3.21)189.04***(5.17) 6 ***Statistical significance at the.01 level.7 Notes: The t-statistics testing the null that the ratios equal 1 are presented in parentheses.  Δσs,t2i  and  Δσs,t2u  , which correspond to the temporary price impact and permanent price impact for stock s at time/interval t, are obtained from  σs,t2i  and  σs,t2u  (defined in Table 4). The ratios of  Δσs,t2u  and  Δσs,t2i  for each tweet-trade to the  Δσs,t2u  and  Δσs,t2i  for the other trades during an average trading day are then computed. The mean ratios are presented in Table 5 and their corresponding t-statistics are reported in parentheses.The estimates in Table 5 show that, on average, tweet-trades generate larger permanent and temporary intraday price impacts than non-tweet-trades. All the estimates are statistically significant at the.01 level, thus refuting the null hypothesis that there is no difference between the impact generated by tweet-trades and other trades. The price impact of a tweet-trade is 150–300 times larger than that of the average trade. Using the 60-second threshold,  Δσs,t2i  , which corresponds to the temporary price impact generated by the average tweet-trade, we find that the average tweet-trade has 279.67 times the impact of the average trade, suggesting that tweets result in large but momentary movements of price. This finding suggests that FGC generates temporary effects that can induce increases in the cost of trading a firm's stock and the firm's cost of capital ([14]; [17]).  Δσs,t2u  , the permanent impact of the average tweet-trade, is about 146.83 times larger than the average trade's permanent impact, suggesting that FGC can cause investors to update their expectations about a firm's future performance, which in turn leads to price movement. Overall, the analysis indicates that, on average, tweet-trades occurring in the wake of a potentially information-laden tweet substantially impact stock prices both permanently and temporarily relative to non-tweet trading activity.Estimating the effects of tweet-trades within subminute to minute windows addresses methodological issues associated with the occurrence of confounding events. Therefore, to a very high level of accuracy, we can attribute estimated temporary and permanent price impacts to the observed FGC. Given the fine-grained level of analysis, it is highly unlikely that any other relevant event could be driving the effects we capture. The sampling at high-frequency intervals also raises the question of whether investors and other trading agents could digest and act on the contents of tweets within the price impact windows we examine. Addressing this question requires an understanding of the nature of trading in financial markets today, especially in the case of highly traded stocks such as the S&P 500 stocks in our sample. Today's markets are dominated by algorithmic traders (commonly known as ""algos"") capable of digesting and acting on information in FGC (e.g., tweets) within the windows we examine in our analysis. The effects of this speed of activity are evidenced by the findings of [65], who, using S&P 500 stock data, show that information arriving in U.S. markets is exploited within seconds and that this activity is driven by algorithmic trading.Although all the ratios are statistically significant and suggest that FGC influences the permanent and temporary components of price, the obvious question is how economically meaningful tweet-trades are compared with other events that impact stock price. To answer this question, we conducted further analysis to examine the corresponding ratios of other large-impact non-tweet-trades in the same period by computing ratios similar to the ones presented in Table 5. This involved substituting a permanent price impact measure for each tweet-trade with that of other trades generating price impacts corresponding to one standard deviation or more above the daily mean in each stock. The obtained average ratios for the three thresholds are 7.9, 5.2, and 1.3 for the 30-, 45-, and 60-second windows, respectively. The inference drawn from this analysis is that the information content of tweet-trades is several times higher than that of the average non-tweet, high-impact trade. In comparing the temporary price impacts associated with the same trades with those of the tweet-trades, we find that tweet-trade ratios are again several times higher. This suggests that tweet-trades tend to be noisier than other trades associated with a more permanent price impact, and this provides a basis for demonstrating to marketers the significance of the relatively high levels of both permanent and temporary price impacts that can be generated in financial markets with the use of tweets. A robustness comparative analysis based on [22] is consistent with the presented findings (see Web Appendix G). Investigating the Temporary and Permanent Price Impacts of Tweet Valence and Subject MatterTo add intraday actionability, we used  Δσs,t2u  and  Δσs,t2i  , which encapsulate the permanent and temporary impacts of intraday tweets on firm value, as dependent variables to determine how tweet valence and subject matter (consumer and competitor orientation) influence the impact of tweets on stock price. To investigate whether tweet valence and subject matter drive the price impact of tweet-trades, we estimate Equation 7: PriceImpacts,t=αs+βt+γ1consumers,t+γ2competitors,t+γ3consumer*−ves,t+γ4competitor*−ves,t+γ5consumer*+ves,t+γ6competitor*+ves,t+γ7−ves,t+γ8+ves,t+∑k=17φkCk,s,t+ϵs,t, Graph( 7)where  PriceImpacts,t  corresponds to  Δσs,t2u  or  Δσs,t2i  , respectively, for a tweet-trade t in stock s.  αs  and  βt  are stock and time fixed effects. We used the VADER rule-based algorithm ([40]) to determine the valence of the tweets. VADER outperforms other commonly used benchmark methodologies such as Linguistic Inquiry and Word Count, Affective Norms for English Words, and the machine learning algorithm support vector machine in the literature as well as in our robustness tests. We also utilized [66] library and followed their method for measuring the competitor (  competitors,t  ) and consumer  (consumers,t)  orientation for each tweet, which is in line with [ 3] and [78]. Consumer and competitor subject matter are dummy variables that equal 1 when a tweet-trade's content is about consumer and/or competitors. We also studied the interaction effects of these attributes.  competitor*+ves,t  and  competitor*−ves,t  refer to positive-valence tweets related to competitors and negative-valence tweets related to competitors, respectively, for a tweet-trade t in stock s, and  consumer*+ves,t  and  consumer*−ves,t  refer to positive-valence tweets related to consumers and negative-valence tweets related to consumers, respectively, for a tweet-trade t in stock s.To avoid omitted variable bias and to ensure completeness, the model also includes  Ck,s,t  , which reflects a vector of known determinants of price impact based on past research in the market microstructure literature, as well as the natural logarithm of the number of an account's followers at the time of a tweet-trade t's tweet (  #followerss,t  ).  Ck,s,t  includes the natural logarithm of trading volume (  lnvolumes,t  ), the natural logarithm of average trade size (  lntradesizes,t  ), volatility (  volatilitys,t  ), effective spread (  Effectivespreads,t  ), the natural logarithm of a high-frequency trading proxy (  HFTs,t  ), and order imbalance (  OIBs,t  ). We measured trading volume as the dollar volume of transactions executed in stock s prior to a corresponding tweet-trade t. We computed average trade size as the trading volume prior to tweet-trade t divided by the number of transactions just prior to a corresponding tweet-trade t in stock s.  volatilitys,t  is the standard deviation of midpoint dollar price returns from the start of the trading day up to the trade just before the corresponding tweet-trade t in stock s.  Effectivespreads,t  (in basis points) was computed as twice the absolute value of the last trade price less the prevailing price midpoint prior to the corresponding tweet-trade t in stock s divided by the prevailing price midpoint. Price midpoint is the average of the prevailing best bid and ask prices.  HFTs,t  is the ratio of the number of messages (quotes, cancellations, and transactions) to actual transactions from the start of the trading day until prior to a corresponding tweet-trade t in stock s. Finally,  OIBs,t  is the ratio of the difference between the number of sell and buy orders and the average of both from the start of the trading day until prior to a corresponding tweet-trade t in stock s. To eliminate outliers in the data caused by the characteristic noisiness of high-frequency trading data, all variables are winsorized at.5 and 99.5 percentiles within each stock.We estimated Equation 7 using both panel least squares and 2SLS instrumental variable (IV) estimation approaches. Panel-corrected standard errors were computed to obtain heteroskedasticity and autocorrelation robust standard errors. We performed the IV estimation to account for the likelihood of endogeneity due to selection bias caused by a firm's decision regarding whether to use Twitter ([25]). The IV approach we employed was based on approaches adopted by an increasing number of studies in the marketing literature ([80]). For a given firm in our sample of S&P 500 IT firms, our approach involved first identifying the firms in the same two-digit Standard Industrial Classification that had sent a corresponding tweet on the previous or same day as the firm and then estimating the mean value of the potentially endogenous variables (consumer and competitor orientation) for these firms. The mean estimates were employed as an instrument for the firm in question. This variable met the requirements for an instrument because price impacts observed in the other firms' stocks were unlikely to be driven by the focal firm's tweets and, at the same time, tweeting activity has been shown to be correlated for firms in similar industries. In each of the first-stage regressions, we regressed each of the consumer and competitor variables separately on the corresponding IVs and the control variables defined previously for each firm/stock and obtained the F-statistics as tests of the null of weak instruments. The fitted values for each of the measures from the first-stage regressions were then employed as the variables in place of the consumer and competitor orientation variables in the second-stage regressions.The first-stage F-statistics, testing the null of weak instruments, show that our IV model does not suffer weak instrument issues. The test statistic is higher than the threshold of 10 needed for 2SLS inferences to be reliable when instrumenting for endogenous variables ([70]). We also conducted further tests to examine the instruments' relevance and the validity of the overidentifying restrictions in the IV regressions. The Cragg–Donald and Kleibergen–Paap Lagrange multiplier statistics we obtained reject the nulls of weak instruments and underidentification according to the [33] critical values, respectively. Essentially, these test the null hypothesis that the instruments we used have insufficient explanatory power to predict the endogenous variables in the model for identification of the parameters. All the p-values we obtained in the Sargan χ2 test also indicate that we cannot reject the null that the overidentifying restrictions are valid. All the 2SLS estimates for Equation 7 are presented in Table 6, and the results of the panel least squares estimations are presented in Web Appendix H.GraphTable 6. The Relationship Between Permanent and Temporary Price Impact and Tweet Valence and Orientation. VariablesPermanent Price Impact(Δσs,t2u)Temporary Price Impact(Δσs,t2i)Key Findingsconsumers,t−.007(1.06).009**(2.39)Consumer-related tweets are, on average, associated with a larger temporary price impact relative to other tweets.competitors,t−.034**(2.03).026**(2.46)Competitor-related tweets are, on average, associated with a larger temporary price impact and lower permanent price impact relative to other tweets.−ves,t−.063**(2.37).032**(2.46)Tweets with only negative or only positive valence are associated with increasing temporary price impact and decreasing permanent price impact.+ves,t−.108***(3.11).033**(2.20)consumers,t*−ves,t.047**(1.97).072**(2.50)Tweets reflecting both valence and subject matter are associated with increases in both permanent and temporary price impact. The increase in permanent price impact contrasts the decrease in permanent price impact that tweets with only valence or only subject matter are associated with.Except for tweets reflecting negative valence and consumer orientation (consumers,t*−ves,t), permanent price impact is more pronounced than temporary price impact.competitors,t*−ves,t.606***(3.69).011**(2.09)consumers,t*+ves,t.088**(2.10).019**(2.21)competitors,t*+ves,t.220***(4.88).077**(2.43)lnvolumes,t−.039***(-4.51)−.034***(-6.68)Increases in firm stock trading activity are linked with reductions in both permanent and temporary price impacts.lntradesizes,t.089***(6.64).061***(7.25)Larger firm stock trade sizes induce larger permanent and temporary price impacts.volatilitys,t−.123***(−3.62).015**(2.13)Firm stock volatility is linked with increases in temporary price impact and decreases in permanent price impact.Effectivespreads,t.241**(2.66).014**(2.06)Deterioration in firm stock liquidity is associated with increases in permanent and temporary price impacts.lnHFTs,t−.000(−.26)−.021***(−3.83)Algorithmic and high-frequency trading is linked with decreases in temporary price impact. Its effect on permanent price impact is benign.OIBs,t−.371***(−6.89).046**(2.39)Order imbalance is linked with reductions in permanent price impact and increases in temporary price impact.ln#followerss,t−.082**(−2.43).037**(2.43)The number of followers of a firm's twitter account amplifies the propensity for tweets to generate larger temporary price impact and reduce permanent price impact.R2¯.35.49Observations139,997139,997Kleibergen–Paap LM31.32***110.24***Tests the null hypothesis that the employed instruments have insufficient explanatory power to predict the endogenous variables in the model for identification of the parameter.Cragg–Donald79.08***88.66***Tests the same null hypothesis as the Kleibergen–Paap LM test.Sargan's χ2p-value.37.46Tests the null hypothesis that the overidentifying restrictions are valid. 8 **p <.05.9 ***p <.01.10 Notes: Table 6 reports 2SLS estimated coefficients for Equation 7. Standard errors are robust to heteroscedasticity and autocorrelation, coefficients are multiplied by 107, and t-statistics are reported in parentheses. LM = Lagrange multiplier.The results presented in Table 6 show the importance of tweet valence and subject matter in determining the permanent and temporary impacts of tweets on firm value. The existence of permanent and temporary price impacts associated with tweet attributes supports the signal theory perspective ([43]) and shows that investors pay attention to the tweet attributes of valence and subject matter. The estimates of permanent price impact for  γ7  and  γ8  are negative and statistically significant (−.063, p < .05, and −.108, p < .01, respectively). This suggests that tweets displaying only positive or only negative valence are linked to less permanent impacts in stock price. The positive and statistically significant  γ7  and  γ8  estimates of the temporary price impact estimation also indicate that such tweets are linked to increasing temporary price impact (.032, p < .05, and <.033, p < .05, respectively), and they suggest that tweet valence generally contributes more noise to stock price than stock-relevant information. The findings reinforce the role of positive and negative valence FGCs and their impact on firm value ([75]; [76]).With respect to tweet subject matter, only tweets conveying information about competitors generate statistically significant permanent price impacts (−.034, p < .05). Therefore, on average, tweets about a firm's competitors generate lower permanent price impact relative to other tweets. Conversely, the positive and statistically significant estimates for  γ2  for temporary price impact (.026, p < .05) show that these types of tweets are more likely to contribute to the noise component of price; in other words, they generate a larger temporary price impact than other tweets, on average. Thus, tweets conveying competitor orientation appear to result in a lower permanent price impact, suggesting that this form of subject matter is comparatively less impactful and relevant to investors' expectations about a firm's future performance ([48]). Notably, tweets about consumers do not yield any permanent price impact that is statistically different from that of other tweets and, thus, by themselves do not appear to offer a signal capable of causing investors to permanently update their firm performance expectations. Consumer-related tweets, similar to those about competitors, also generate more temporary price impact than other tweets on average, which suggests that their potential for inducing noise in stock price is higher than that of the average tweet in our sample. The  γ1  and  γ2  estimates of the temporary price impact are positive and statistically significant (.009, p < .05, and.026, p < .05, respectively). This finding implies that, as is the case with valence, tweets reflecting only competitor or only consumer orientation generate noise in the price discovery or trading processes and lower permanent price impact.Inferring from information-based market microstructure models ([24]; [47]), the more information about a firm investors observe, the more they become informed about the valuation of the firm. In line with this expectation, the interaction variables we include in Equation 7 should yield positive estimates for the  Δσs,t2u  estimations. As we expected, all the  γ3  ,  γ4  ,  γ5  , and  γ6  estimates of permanent price impact are positive and statistically significant (respectively,.047, p < .05;.606, p < .01;.088, p < .05; and.220, p < .01), even though, as already stated,  γ1  ,  γ2  ,  γ7  , and  γ8  are negative and statistically significant (except for  γ1  ). Thus, increases in both negative and positive valence, when viewed through the lens of subject matter, are linked with increased permanent price impact. These estimates show that tweet valence, when contextualized by subject matter or vice versa, is seen by investors/traders as firm-relevant information. In the context of these findings, the incorporation of valence and subject matter into FGC can yield increases in permanent price impact.Furthermore, the findings suggest that tweets about competitors with a negative valence are likely to have the highest permanent price impacts (.606, p < .001). This finding is crucial from the perspective of marketing practice and intraday social media marketing strategy design because valence and competitor subject matter as singular attributes of FGC are independently associated with decreasing permanent price impact. The findings underscore the view that investors seek additional information (i.e., information beyond what they already have) when making trading decisions ([ 6]; [73]) and that they operate according to classical market microstructure models. For example, [47] and [24] emphasize the crucial importance of information for price discovery in financial markets. This also confirms [51] findings that information from microblogging platforms, such as Twitter, impact investors' decisions.To illustrate the relevance of these findings, Figure 3 presents tweets A and B as examples of FGC characterized by negative and positive valence, respectively, but not containing any subject matter related to competitor or consumer orientation. Consistent with our findings, the permanent price impact estimates for the tweet trades corresponding to both tweets are more than three standard deviations lower than the average permanent price impact estimate and are thus below the tenth percentile of the estimates. The estimates for the negative and positive tweets' tweet-trades are.0017% and.0035%, respectively. In contrast with A and B, tweets C, D, and E reflect varying combinations of both valence and subject matter. Our findings suggest that these tweets should generate significant permanent price impact, and indeed, the permanent price impact estimates for the tweet-trades corresponding to tweets C, D, and E are above the 90th percentile in our sample of tweet-trades' permanent price impact estimates. The estimates are 3.74%, 2.84%, and 1.32% for tweets C, D, and E, respectively.Graph: Figure 3. Examples of tweets generating temporary and permanent price impacts.The effects of the tweet attributes we studied on temporary price impact,  Δσs,t2i  , also deserve attention. The results suggest that the relationship between valence and temporary price impact is generally magnified when combined with subject matter. For example, the  γ7  and  γ8  estimates, which capture the relationship between  Δσs,t2i  on the one hand and  −ves,t  and  +ves,t  on the other, are positive and statistically significant (.032, p < .05, and <.033, p < .05, respectively), while the estimates for  γ3  and  γ6  , which capture the relationship between  Δσs,t2i  on the one hand and  consumers,t*−ves,t  and  competitors,t*+ves,t  on the other, are also positive and statistically significant (.072, p < .05, and <.077, p < .05, respectively). The latter set of estimates is at least two times larger than the former. The overall implication of these positive and statistically significant coefficient estimates related to temporary price impacts is that, although tweets reflecting both valence and subject matter are likely to generate permanent price impact, these attributes may also be associated with increased temporary price impact. Thus, on average, tweets inject noise (uncertainty) into the prices of stocks traded in financial markets.In conclusion, the estimates presented in Table 6 highlight the relevance of tweet attributes for the price discovery process in financial markets and reinforce the importance of studying the multifaceted nature of FGC ([45]). We find that tweets, as with many events observed in relation to trading in financial markets, generate both permanent and temporary price impacts. However, whereas tweets containing singular attributes—either positive or negative valence or either consumer or competitor orientation—readily inject noise into the price discovery process and thus generate temporary price impact, those that include more than one attribute generate permanent price impact and thus generally enhance the efficiency of the price discovery process. DiscussionIn this research, we examine the real-time impact of FGC on the variance of firms' stock price. In the current fast-paced online communication landscape, marketers must understand the financial impact of firms' ""always on"" marketing ([64]). The assessment of FGC's financial impacts, however, is in an early stage ([ 8]; [15]; [64]). This research contributes to this emerging stream of marketing research and addresses multiple calls for new methods that are able to develop real-time insights from online data ([ 5]; [49]; [59]; [79]). By employing the market microstructure approach to study S&P 500 IT firms' Twitter activity, this study contributes to marketing literature and practice. Research ContributionsThis study offers several implications for marketing research. First, aligning with the work by [15], [ 8], and [64], it advances understanding of FGC's financial impact by providing an assessment of FGC's impact on the variance of stock price in real time (i.e., seconds). By employing a market microstructure approach, we show how to algorithmically link individual pieces of FGC to time-specific trading activity at a fine-grained level of analysis. In the process, we demonstrate the limitations of low-frequency methodologies such as daily event studies, which are subject to aggregation bias and may yield biased estimates of the impact of FGC on firms' financial outcomes, while offering an alternative and more robust method of analysis for studying intraday marketing activity. In our examination of the impact of FGC on variance, we fully utilized high-frequency transaction data characterized by unequal time intervals and demonstrate how to retain data that otherwise would have been eliminated in studies that use end-of-day stock price. By doing so, we provide marketing researchers with a new approach that allows them to harness the potential of online data.Second, we distinguish between FGC's temporary and permanent price impacts. Specifically, we show that FGC impacts investor expectations related to a firm's future performance, thus generating permanent price impact, and it also injects uncertainty about a firm's value into the firm's stock price, thus inducing temporary price impact. Our research, therefore, adds a new perspective to the marketing literature stream on the financial impact of FGC. This assessment of FGC's temporary and permanent price impacts adds richness to the examination of marketing's financial impact and enables the quantifying of long- and short-term financial impacts of marketing activity.Finally, this research has implications for the design of intraday marketing strategies. By examining FGC valence and subject matter (consumer and competitor orientation), we advance a growing body of research documenting the complex nature through which marketing signals impact financial markets and firm financial outcomes. We show that, by themselves, FGC valence and subject matter are more prone to injecting uncertainty about a firm's stock price into the market, and thus, they generate temporary price impacts rather than permanently changing investors' and traders' beliefs about firm value. Used together, FGC valence and subject matter both hold statistically significant and economically meaningful relevance for price discovery in financial markets. In other words, they can influence investors' expectations related to firms' future performance and thus result in permanent price impacts. Recent research by [ 6] provides evidence of interactions between marketing signals, and our research shows that the interaction between FGC valence and subject matter can also impact firm stock price. Managerial ImplicationsThus far, firms have struggled to demonstrate financial accountability regarding FGC's impact on firm value ([15]; [45]) or provide evidence of its immediate contribution to their financial outcomes ([53]; [58]). We provide marketing managers with evidence of FGC's impact on variance in firms' stock price. Specifically, we show that tweets can generate both permanent and temporary price impacts. By manipulating tweet attributes, such as valence and subject matter, marketing managers can design Twitter content to generate varying degrees of permanent or temporary impact. From a market quality perspective, firm managers should prefer tweets that generate a permanent price impact, and our research provides some useful indications about how to achieve this outcome. We show that tweets expressing degrees of positive or negative valence regarding either consumers or competitors generate a permanent price impact. We therefore encourage marketing managers to design information-rich tweets that both ( 1) focus on consumers or competitors and ( 2) communicate valence. Our results suggest that firms should utilize valence and subject matter in their tweets if they would like their stock to be more informative with respect to their value. Our analysis suggests that tweets about competitors with a negative valence are likely to have the highest permanent price impacts. Thus, by using permanent price impact as a metric to evaluate the longer-term impact of tweets, social media managers can design campaigns that have a sustainable impact on firm financial outcomes. The design recommendations from this study complement [41] work on social media content scheduling, as well as [64], which addresses real-time social media marketing and provides firms with information regarding which tweets to disseminate during the day for long-term effectiveness. We recognize that not all intraday tweets will, nor should they, have permanent impacts on firms' stock price. Some tweets are aimed at the creation of social media buzz, which is related to the temporary price impacts we examined in this study. Firms can indeed achieve social media buzz by tweeting, as our findings reveal that tweets, in aggregate, mostly generate temporary price impacts. We urge caution, however, because temporary price impacts are linked with larger transaction costs ([14]) and increases in firm cost of capital ([17]). This suggests that the benefits of designing tweets to generate buzz and incorporate information into stock price must be carefully managed. To support marketing managers in their intraday social media strategy design, Table 7 is designed as a set of insights based on our findings.GraphTable 7. Suggested Insights for Marketing Managers. Permanent Price ImpactTemporary Price ImpactResearch FindingsRecommendationExpected OutcomeInteraction EffectsPermanent Price ImpactTemporary Price ImpactResearch FindingsValencePositive valence Positive and negative valence-only FGC contributes to more noise in a firm's stock price.Add subject matter (e.g., competitor orientation such as ""competition,"" ""peer"")Increased permanent price impactPositive valence and competitor orientation Interaction of valence and subject matter increases/generates permanent price impact and amplifies temporary price impact. Permanent price impact is more pronounced than temporary price impact. The financial implication of these outcomes is a reduction in transaction and firm capital costs.Negative valenceAdd subject matter (e.g., consumer orientation such as ""customer,"" ""consumer,"" ""buyer"")Increased permanent price impactNegative valence and consumer orientationSubject MatterConsumer orientationSubject matter-only FGC contributes to the noise component in a firm's stock price.Add valence (e.g., positive valence such as ""help,"" ""solution,"" ""best"")Increased permanent price impactPositive valence and consumer orientationCompetitor orientationAdd valence (e.g., negative valence such as ""attack,"" ""stop,"" ""threat"")Increased permanent price impactNegative valence and competitor orientation 11 = no price impact; = negative impact on stock price component; = positive impact on stock price component; = increased positive impact on stock price component. LimitationsWe conclude by encouraging future research to address the limitations of our empirical study. One potential limitation of our analysis is its focus on firms in the IT sector. We recognize that these findings may not apply to other sectors. Future research could extend our analysis to other sectors to confirm whether similar price impacts hold. Second, the impact of tweets could depend on whether Twitter was the first source through which a firm released an important piece of news. For example, tweets could have been published in response to a competitor's tweet. In some cases, a firm's tweet could lead to a number of successive tweets, in which case the subsequent tweets might not be as impactful as the first. We do not discount the possibility that there could be some carryover or dampening effect in such situations. We note that, if this is the case, it would be highly unlikely for the magnitude of the effects we observe to occur, especially given the granular level of analysis that our market microstructure approach entails. Third, future work could explore high-frequency data generated by firms' use of FGC other than tweets, such as Facebook posts, where it has been reported firms post up to 80 times a day (Hutchinson 2018). It would be interesting to see whether the effect of FGC across social media platforms is consistent or if it varies. In addition to social media, it would be useful to examine firms' use of other online communication tools, such as webpages and blogging platforms. Researchers could also explore various types of FGC, including video content, as well as its characteristics, including emotions ([72]). As [36] show, there is an array of online marketing communication practices, and future research could therefore study the ""echoverse"" at a fine-grained level of analysis. Finally, we note that researchers can apply market microstructure to study user-generated content (UGC) in future research. We welcome future research that addresses the following questions: What is the real-time impact of UGC on firm value? What are the UGC attributes capable of generating permanent and temporary price impacts? Are these attributes the same as for FGC, or do they differ? Our research highlights the importance of interaction effects when examining the impact of FGC attributes on firm value; therefore, investigating the optimal mix of UGC attributes capable of generating temporary and permanent price impacts should be an interesting endeavor. Supplemental Materialsj-pdf-1-jmx-10.1177_00222429211042848 - Supplemental material for Measuring the Real-Time Stock Market Impact of Firm-Generated ContentSupplemental material, sj-pdf-1-jmx-10.1177_00222429211042848 for Measuring the Real-Time Stock Market Impact of Firm-Generated Content by Ewelina Lacka, D. Eric Boyd, Gbenga Ibikunle and P.K. Kannan in Journal of Marketing  "
16,"Regulating Product Recall Compliance in the Digital Age: Evidence from the ""Safe Cars Save Lives"" Campaign The unprecedented number of product recalls in recent years and subsequent low consumer recall compliance raise questions about the role of regulatory agencies in ensuring safety. In this study, the authors develop a conceptual framework to test the impact of a regulator-initiated digital marketing campaign (DMC) on consumer recall compliance. The empirical context is the launch of a nationwide DMC by the U.S. automobile industry's regulator. The analysis utilizes recall completion data from 296 product recalls active both before and after the DMC's launch. The results show that the DMC improves consumer recall compliance. In the first four quarters after it was introduced, the DMC increased the number of vehicles fixed, on average, by 20,712 per recall campaign over what would be expected without the DMC. Regarding boundary conditions, the study finds that the DMC is more effective for recall campaigns with greater media coverage and for those with older recalled products. However, the DMC's effect weakens as the time needed to repair a defective component increases. The findings should help regulators make compelling cases for greater resource allocation toward digital initiatives to improve recall compliance.Keywords: digital marketing campaign; product recalls; public policy; regulationProduct recalls and consumer safety are regulated by government agencies in several countries, including the United States, Canada, Germany, the United Kingdom, and Japan. While issuing a recall notice to inform consumers of a potential issue is critical, far too many defective products remain unremedied long after their recall notifications have been sent to consumers. For instance, J.D. Power reports that consumer recall compliance in the U.S. automobile industry is quite low, with approximately 30%–50% of vehicles on the road at any point in time having an unrepaired safety problem (J.D. [54]). Relatedly, according to a report by the Consumer Product Safety Commission (CPSC), only 10% of children's products recalled in 2012 were successfully corrected, replaced, or returned ([44]). For example, since the 2016 recall of millions of IKEA Malm dressers, only 1% of consumers, at best, have had the unstable furniture removed and been issued a refund (Consumers [19]).The continued use of defective products by consumers is a serious public health concern and raises the specter of injuries and fatalities. The sustained string of casualties in 2018 due to faulty Takata airbags, a safety problem for which a recall was issued in late 2014, underscores the perils of low consumer recall compliance. Unsurprisingly, regulatory agencies have been subjected to intense scrutiny and even rebuke for not taking adequate measures to improve consumer recall compliance. For example, an audit report released in 2018 by the U.S. Department of Transportation faults the National Highway Traffic Safety Administration (NHTSA) for a lack of proper oversight of the recall completion process ([52]).While numerous factors likely contribute to poor consumer recall compliance, low consumer awareness is often noted as a key issue responsible for inaction. In fact, the NHTSA's former Associate Administrator for Enforcement stated that a lack of public knowledge is ""the single greatest weakness"" in successfully addressing product recalls ([31], p. 232). Similarly, NHTSA focus group interviews conducted to understand the reasons for low recall compliance found that while over 70% of consumers preferred electronic recall notifications, only 7.4% reported receiving any. Notably, 90% of the respondents mentioned that recall notifications received electronically had a greater chance of being noticed ([28]). In the past, regulatory agencies such as the CPSC have conducted targeted national campaigns to raise public awareness, support industry compliance, and improve safety in specific consumer product categories ([20]). The issue of low consumer recall awareness is so pervasive that in 2014, the CPSC tried to crowdsource solutions, announcing a contest for application developers to create tools to inform the public of consumer product recalls ([26]).Although regulators introduce such initiatives to raise consumer awareness and reduce accidents, empirical evidence to support or refute these expectations is inconsistent. A potential concern is that campaigns by regulatory agencies might not be effective policy instruments if consumers view them as symbolic acts by government officials and are unresponsive to these efforts ([23]; [32]). In addition, prior research has found that consumers could develop reactance to regulation-related public awareness campaigns because they view such efforts as infringements on their personal freedoms ([15]; [47]). This contention is supported by popular press: a 2018 survey of over 1,500 U.S. consumers revealed that almost two-thirds of them did not believe that government-initiated product recall programs had much to do with increasing consumer safety. Instead, many consumers viewed the recall programs as government exercises in ""red tape"" ([62]).The study's objective is to investigate the impact of a regulator-initiated digital marketing campaign (DMC) on consumer recall compliance. The manuscript makes two contributions to marketing literature. First, this study is the only one that we are aware of that examines whether a DMC initiated by a regulator improves consumer recall compliance. The product recall literature has predominantly focused on the stock market and sales consequences of recalls (e.g., [13]; [29]), the ability to learn from and prevent future recalls ([33]; [37]), the drivers and consequences of recall timing decisions ([25]), and how recalls can impact marketing effectiveness ([16]; [65]). Little attention has been devoted to the consumer compliance process. A couple of studies have also investigated how recall attributes, including vehicle country of origin, vehicle age, and the publicity around recalls, influence compliance ([34]; [58]). However, the impact of a regulator's efforts on consumer compliance has, to our knowledge, received virtually no attention. Our study fills this research void and offers valuable insights to policy makers.Specifically, we examine the effectiveness of a DMC initiated by a regulatory agency. To tackle the problem of low consumer recall compliance, the NHTSA, the agency responsible for regulating consumer safety in the U.S. automobile industry, launched a full-coverage, nationwide DMC, ""Safe Cars Save Lives,"" in January 2016. The DMC featured paid search and online display advertisements that provided consumers with links to check for open recalls and access pertinent recall remedy information online. We exploit this setting to formally test the effectiveness of a regulator-initiated DMC to improve consumer recall compliance. The empirical analysis utilizes recall completion data pertaining to 296 product recalls active both before and after the DMC's launch. Our econometric analyses account for various potential confounds such as the DMC's possible endogenous nature, time trends in recall completion, recall campaign attributes, and unobserved vehicle make and temporal characteristics. The results show that in the first four quarters after it was introduced, the DMC increased the number of vehicles fixed, on average, by 20,712 per recall campaign above what was to be expected without the DMC. This improvement in consumer recall compliance should, in turn, lead to potentially fewer vehicle crashes, casualties, and lower economic costs.Second, we identify important boundary conditions for the DMC's effectiveness. We find that the DMC is more effective at increasing consumer compliance for recalls with greater media coverage. Although media coverage of recalls could be detrimental to the impacted brand's financial health, our finding implies that it plays a critical role in aiding the DMC to improve compliance. We also find that the DMC is more effective at increasing compliance for older (as opposed to newer) recalled products. This finding is critical for regulators who struggle to reach owners of older products using conventional communication methods. However, the DMC's impact on compliance is lower for recalls in which the defective component takes a longer time to repair. This suggests that the DMC may be unable to fully counteract the barrier of time-related inconvenience for consumers. Collectively, our findings enable regulators to make compelling cases to receive more resources for digital marketing initiatives in the future. Literature Review and Conceptual FrameworkRegulatory agencies introduce interventions with the goal of promoting and protecting consumer welfare. Government interventions supporting public welfare are comprised of pecuniary interventions (e.g., imposing higher taxes/penalties, offering financial incentives) as well as nonpecuniary interventions (e.g., focused on educating, building awareness) ([45]). Prior research has investigated the efficacy of interventions targeted at decreasing the consumption of unhealthy products, stimulating consumer spending, and promoting preventive health screenings ([18]; [59]; [66]). The effectiveness of interventions is often assessed by examining the extent to which consumers comply with or respond to the proposed initiatives.Prior research provides equivocal evidence regarding the effectiveness of pecuniary interventions. For example, although imposing higher taxes has been found to lower purchases of bottled water, reduce purchases of drinks with sugar additives, and decrease smoking prevalence, their associated unintended consequences may drive consumers toward more dangerous products ([ 6]; [18]; [67]). Relatedly, the ""Click It or Ticket"" campaign, intended to improve seat belt usage in vehicles and promote consumer safety by ticketing transgressions, has been shown to be successful ([61]). However, programs that offer consumers incentives to stimulate sales through initiatives such as ""Cash for Clunkers"" ([48]) or drive consumer spending through tax rebates ([59]) have been found to be generally ineffective.Likewise, evidence from prior research assessing the efficacy of nonpecuniary interventions is also inconsistent. For example, the impact of the Nutrition Labeling and Education Act (NLEA) in altering consumers' behaviors is somewhat nebulous. [49] found that the NLEA significantly increased consumers' nutrition information processing and comprehension, although information-sensitive consumers did not report increased use of nutrition information following its implementation. Relatedly, [ 3] did not find any change in consumers' consumption-related search and recall of nutrition information after the NLEA's enactment. [55] summarizes the effects of government agency-initiated public information campaigns across various contexts and even documents consumer responses that are opposite to those intended in some situations.The mixed evidence for the efficacy of regulator-initiated interventions (those pecuniary and nonpecuniary) documented in prior research is not satisfactory from a knowledge advancement perspective and raises important questions about the potential effectiveness of a regulator-initiated DMC. More specifically, is a regulator-initiated DMC effective in improving consumer recall compliance? Under what conditions is a DMC more or less effective?To answer these questions, we develop a conceptual framework (depicted in Figure 1) by drawing on insights from both the health belief model (HBM) and health warning streams of research.[ 6] With conceptual origins in the public health domain, the HBM contends that for individuals to take action to prevent a detrimental outcome, they need ( 1) a cue or trigger (internal and/or external) to overcome their avoidance tendencies, ( 2) to believe that they are susceptible to its risk, ( 3) to perceive that the occurrence of the outcome would have negative consequences for them, and ( 4) to believe that taking preventative action would lead to benefits outweighing the barriers of cost, convenience, pain, and embarrassment ([11]; [36]; [56]). Drawing on these insights, our framework examines the roles of ( 1) awareness cues, ( 2) the perceived susceptibility to risk, ( 3) the perceived threat of noncompliance, and ( 4) the perceived time inconvenience of compliance in eliciting consumer recall compliance following a regulator-initiated DMC.Graph: Figure 1. A conceptual model of the impact of a regulator-initiated digital marketing campaign on consumer recall compliance.Our framework also draws on health warning streams of research, specifically, prior work on consumer responses to persuasive public health appeals. This stream of research suggests that increasing the frequency of health warnings might desensitize consumers to the potential hazard, resulting in either inaction or actions that are opposite to the intended warning ([55]; [63]). In our context, government regulators, retailers, manufacturers, and consumer experts are concerned that product recalls have become so frequent in various industries, including food, consumer products, and automobiles, that consumers might suffer from ""recall fatigue"" ([39]). Therefore, our framework also considers the impact of multiple concurrent health warnings on a consumer's compliance with a regulator's request.Given that our goal is to investigate a DMC's effectiveness, we conceptualize a regulator-initiated DMC as the primary awareness cue intended to improve consumer recall compliance. The ability of the DMC to improve compliance depends on the extent to which consumer recall awareness is generated and the extent to which consumers are motivated to comply. Accordingly, we expect that the DMC's impact on compliance is contingent on four factors identified in the HBM and the presence of multiple concurrent health warnings for consumers. We propose that a DMC's effectiveness is moderated by ( 1) media coverage, ( 2) the age of the recalled product, ( 3) component hazard, ( 4) time for repair, and ( 5) concurrent recall campaigns.A DMC improves consumer recall compliance by providing relevant information to consumers impacted by recalls. These consumers could be unaware that their products have been recalled but are actively searching for recall-related information, benefiting from a DMC's paid search advertisements that make them aware of existing recalls and direct them to relevant information. Other consumers may be exposed to a DMC's online display advertisements and then cued to check if their products have been recalled on the regulator's webpage.In addition to a DMC providing the necessary information to aid consumers in complying with a recall request, media coverage of recalls could heighten a DMC's effectiveness by sensitizing consumers to the fact that their products may have defects and thereby influencing them to comply. Accordingly, we include media coverage (akin to another awareness cue, a factor in the HBM) as a moderator of a DMC's impact on consumer recall compliance.Because owners of older (vs. newer) products are less likely to receive information about recalls directly from their products' manufacturers, they are likely cognizant of their greater susceptibility to recall risk and more responsive to a DMC that provides relevant information. Accordingly, we include the age of the recalled product (akin to the perceived susceptibility to risk, a factor in the HBM) as a moderator of a DMC's impact on compliance.Following a DMC, a consumer learns about the hazard of a defective component, the time it will take to repair it, and the existence of concurrent recall campaigns for the product after being made aware that it has been recalled. As such, these factors are pertinent in explaining a consumer's motivation to comply with a recall request after being made aware of it through a DMC. Accordingly, we include component hazard (akin to the perceived threat of noncompliance, a factor in the HBM), time for repair (akin to the perceived time inconvenience of compliance, a factor in the HBM), and the presence of concurrent recall campaigns (akin to multiple concurrent health warnings) as moderators of the DMC's impact on consumer compliance ([36]; [55]; [63]). Research HypothesesPrior research suggests that marketing campaigns can be effective at improving individuals' compliance-related behaviors. For example, [66] showed that the introduction of a mass media campaign was associated with increased consumer information search, displayed by high click-through rates to a website and more referrals to medical centers. Relatedly, [69] found that patients were more likely to obtain a first prescription of a treatment following a direct-to-consumer advertising campaign designed to educate them and provide resources about health-related decisions. The positive impact of advertising initiatives by firms may even spill over to the category level, as [68] noted that advertising by a brand increased consumer drug therapy compliance of its rival brands' products. However, the positive effects of awareness campaigns documented in prior research are primarily the result of initiatives by firms, and not by regulators. One notable exception is work by [32], which found a positive effect from a government-initiated proenvironmental demarketing campaign to reduce water consumption. However, the campaign was more effective on the majority ethnic/religious group than on the minority groups.There are also reasons to believe that a marketing campaign may not be as effective if initiated by a regulator. For example, [53] found that fewer than half of the antismoking advertisements they showed to adolescents increased their nonsmoking intentions. Similarly, research examining the National Youth Anti-Drug Media Campaign in the United States reported no effects from the initiative ([35]). Although consumers may be skeptical about government-initiated awareness campaigns related to health or safety issues, the preponderance of evidence for these campaigns' positive effects leads us to our baseline hypothesis that a DMC will improve consumer recall compliance. H1:  A regulator-initiated DMC improves consumer recall compliance. The Moderating Role of Media CoverageThe effectiveness of a regulator-initiated DMC is likely to vary depending on whether a recall received media coverage. Mass media outlets are a unique voice in the marketplace that provide consumers with information that is different from that of firms or governmental agencies ([14]). Extant research argues that separate media channels could have a synergistic impact on the effectiveness of multichannel marketing campaigns ([12]; [50]). Importantly, in addition to the repetition of the message, the variation of the specificity of the message also likely affects consumers. Although both media coverage and a regulator-initiated DMC generate awareness about a recall, they also vary in the information they provide. In our context, a regulator-initiated DMC provides the specific information consumers need to determine whether they are personally affected by a recall and how to address it, whereas media coverage increases consumer awareness of a recall. We thus argue that consumers are more likely to comply with a recall notice if the information provided by a DMC is complemented by awareness generated through media coverage ([41]). This argument is also consistent with research suggesting that advertising is more effective when there is more associated publicity following a product-harm crisis ([16]; [21]). Drawing on these arguments, we hypothesize: H2:  Media coverage positively moderates the impact of a regulator-initiated DMC on consumer recall compliance, such that the impact is stronger for recall campaigns with greater media coverage. The Moderating Role of Age of Recalled ProductConsumers are, in general, more informed of product-related issues (e.g., defects, failures) when they interact with original equipment manufacturers (OEMs) during their products' warranty periods. As products age and their warranties expire, consumers' interactions with OEMs tend to be less frequent. For example, [58] propose that owners of older products are less likely to schedule routine product maintenance. In addition, the likelihood that product ownership could change hands increases as products age, reducing the probability that notifications sent from the OEM will reach the product's present owner ([ 2]; [27]). These decreased interactions with OEMs leave consumers who own older products at greater risk of being unaware that their products are the subject of active recalls. A regulator-initiated DMC can fill this communication gap for owners of older products, allowing them to receive recall-related information through paid search and online display ads. For these reasons, we expect consumers with older products, who likely perceive themselves as more susceptible to recall-related risk, to utilize a DMC's information to a greater extent than consumers owning newer products. Accordingly, we hypothesize that a DMC's impact on consumer recall compliance will be stronger for older (vs. newer) recalled products. H3:  The age of the recalled product positively moderates the impact of a regulator-initiated DMC on consumer recall compliance, such that the impact is stronger when the recalled product is older. The Moderating Role of Component HazardThe impact of a DMC on consumer recall compliance is also likely to vary depending on consumers' perceptions of the threat of not complying with a recall request. For example, [40] finds that a consumer's perception of threat is the most influential determinant of whether they seek medical care services. Similarly, consumers are more likely to comply with treatment regimens when they perceive that noncompliance would threaten their well-being ([11]). Relatedly, vehicle owners are more likely to repair products when the defective component's hazard is higher. [34] provide evidence that consumers remedy more severe defects at higher rates than less severe defects. In general, after a DMC makes consumers aware of a recall and the defective component involved, consumers should be more motivated to comply with a recall request if they perceive a high threat of noncompliance. If a defective component's perceived hazard is low, consumers are less likely to be motivated to comply with a recall request. Conversely, if the perceived hazard of a component is high, consumers are more likely to be motivated to comply. Therefore: H4:  Component hazard positively moderates the impact of a regulator-initiated DMC on consumer recall compliance, such that the impact is stronger for high hazard components. The Moderating Role of Time for RepairThe effectiveness of a DMC also depends on the extent to which consumers perceive recall compliance to be convenient. Time-related costs have been shown to be a significant barrier to achieving desired levels of compliance from individuals following a request ([10]; [36]). In addition to providing other relevant information, a DMC informs consumers of the time needed to repair their products. Prior research has found that consumers perceive compliance to be more convenient if the time it takes to complete a task is shorter. For example, [ 4] find that consumers who view recycling as more convenient are more motivated to recycle; consumers weigh the costs of sacrificing time to separate recyclable items and then dispose of them if they are under the threshold of inconvenience. Similarly, [11] find time costs to be better predictors of compliance than monetary costs. Likewise, we argue that while a DMC's value lies in creating awareness that a defective component needs repair, a consumer's motivation to comply with a recall following the DMC will be lower if the amount of time required to repair the product is greater. Therefore, we hypothesize the following: H5:  Time for repair negatively moderates the impact of a regulator-initiated DMC on consumer recall compliance, such that the impact is weaker for components requiring more time to be repaired. The Moderating Role of Concurrent Recall CampaignsThe impact of a DMC on consumer recall compliance is likely to vary depending on whether there are concurrent recall campaigns for the same product. The rationale for examining concurrent recalls stems from research suggesting that repeated exposure to product warnings might result in consumers getting habituated to or even developing psychological reactance toward warning messages ([30]; [63]). Relatedly, some consumers may be skeptical of multiple instances of health-related product warnings from government information campaigns ([55]). In such situations, consumers might ignore safety warnings as they begin to perceive messages by regulatory agencies seeking compliance as less credible or even coercive. The detrimental effect of warning messages is particularly damaging for frequent users of the products that receive them ([30]). Accordingly, after a DMC makes consumers aware of recalls that have affected them, its impact on recall compliance is likely weaker for consumers facing concurrent recalls for different product issues relative to consumers facing a single recall. If the DMC makes consumers aware that their products are involved in concurrent recalls, they could become desensitized to recalls, leading to recall requests being ignored. Therefore, the impact of a DMC on recall compliance will be weaker for recalls including products involved in concurrent recall campaigns. H6:  Concurrent recall campaigns negatively moderate the impact of a regulator-initiated DMC on consumer recall compliance, such that the impact is weaker for products involved in concurrent recall campaigns. Research MethodologyThe setting for this study is the automobile industry in the United States. The NHTSA regulates product recalls in the U.S. automobile industry, and several of the characteristics of this setting make it an attractive one to explore our research objectives. For instance, safety in this industry is highly regulated, and manufacturers are mandated to report progress on product recall campaign completion rates periodically. As a result, longitudinal data on recall completion is publicly available, thereby facilitating a systematic investigation of recall compliance over time. The ""Safe Cars Save Lives"" CampaignIn 2014, the U.S. automobile industry experienced the largest product recall in its history. Several automobile manufacturers notified the NHTSA that they were conducting recalls to address a possible safety defect involving Takata airbag inflators. The concern was that the defective airbags were at risk of rupturing violently following a collision, hurling fiery shrapnel into drivers and passengers. Reports indicated that in addition to fatalities, hundreds of drivers had been injured. The scandal's scope escalated sharply in the following months, leading to a record-breaking 63 million vehicles recalled in 2014 and 51 million in 2015.Against this backdrop, the NHTSA launched a nationwide DMC, ""Safe Cars Save Lives,"" in January 2016. Even though the DMC was motivated by the Takata airbag inflator issue, the campaign was a part of the agency's effort to improve consumer compliance amongst all recall campaigns. Lamenting low recall completion, the U.S. Transportation Secretary noted that informed consumers were ""allies"" in improving recall compliance ([24]). The DMC was a full-coverage initiative that sought to push consumers to use the NHTSA's recall lookup webpage to check for open recalls and then fix defective vehicles quickly.The DMC utilized paid search and online display advertisements to influence recall compliance. A key part of the campaign was the use of Google AdWords to target consumers searching for recall-related information online through keywords such as ""Vehicle Recalls,"" ""Check for Recalls,"" and ""Is My Vehicle Recalled,"" among others. A primary goal of sponsored advertisements is to guide consumers searching for information to the right location ([22]). In addition, the NHTSA used online display ads on media platforms such as Facebook that also directed consumers to the NHTSA's recall lookup webpage where they could receive recall-related information. A sample online display advertisement from the DMC is presented in Figure WA1 of the Web Appendix. Between January of 2016 and March of 2017, the NHTSA spent approximately $1 million on sponsored advertisements on Google, Facebook, and other platforms ([28]).To gain a preliminary understanding of the DMC's effectiveness, we collected data on the paid search traffic to the NHTSA's recall lookup webpage using SEMrush Analytics, a leading vendor providing a competitive research service on online marketing and advertising. Specifically, we collected data on the total number of paid search visits to the NHTSA's recall lookup webpage for every calendar quarter-year in our data period. Figure 2 depicts the paid search traffic to the recall lookup webpage before and after the DMC. The mean quarterly recall-related paid search traffic in the post-DMC period (quarter 1 [Q1] of 2016 onward) is 48,770 visits compared with 650 quarterly visits in the pre-DMC period.Graph: Figure 2. Model-free evidence of automobile recall–related paid search traffic to the regulator's recall lookup webpage over time. Data Sources and MeasuresThe unit of analysis for the study is the recall campaign-calendar quarter-year. The NHTSA maintains a database of every vehicle safety recall campaign issued from 1966 onward. A typical recall notice provides information on the vehicle make and models affected, the number of vehicles recalled, the nature of the defect, and the date of the recall's announcement.The data on the cumulative number of vehicles fixed, the measure for consumer recall compliance, was sourced from the NHTSA's database. The agency mandates that manufacturers provide quarterly updates on the number of vehicles affected by a recall and the number of affected vehicles fixed. These updates allow the NHTSA to monitor recall completion on an ongoing basis. Specifically, manufacturers report the number of vehicles fixed in each calendar quarter-year, and these reports are aggregated by unique recall identification numbers. We also collected data from the NHTSA on individual recall characteristics such as the ages of the recalled vehicle models, the defective component in the vehicles, the estimated time needed to complete the repairs, and if multiple recall notifications were sent to consumers. In addition, we gathered data from LexisNexis on the number of press articles about each recall campaign to capture the media coverage of a recall. Furthermore, we collected make-level advertising and sales data from Kantar's ad$pender database and Ward's Automotive Yearbook, respectively. Table 1 presents the variables and data sources for the study.GraphTable 1. Data Sources and Operationalization. Variable NameOperationalizationData Source(s)Role of VariableConsumer Recall ComplianceThe cumulative number of vehicles fixed in a recall campaign.NHTSADependent variableTime from RecallThe number of quarters elapsed from the time a recall campaign began until the calendar quarter directly preceding the start of the DMC.NHTSAExplanatory variableTransition to DMCCoded as 1 if the observation is during or after the first quarter of 2016 (during which the DMC was active), and 0 otherwise.NHTSAExplanatory variableTime Since DMC StartThe number of quarters elapsed since the NHTSA's DMC began.NHTSAExplanatory variableMedia CoverageThe number of media mentions for an individual recall campaign beginning from the day of the recall's announcement until three months afterward.LexisNexisModerator variableAge of Recalled ProductThe age, in years, of the oldest model recalled in a recall campaign at each respective recall completion report's calendar quarter-year.NHTSAModerator variableComponent HazardA binary measure of the hazard of the defective component identified in a recall campaign. Components range from 0 (low hazard) to 1 (high hazard).NHTSAModerator variableTime for RepairThe estimated time, in hours, needed for the dealer to complete the recommended repair.NHTSAModerator variableConcurrent Recall CampaignsA binary measure coded as 1 for recall campaigns including vehicles involved in concurrent recall campaigns, and 0 otherwise.NHTSAModerator variableAdvertising IntensityMake advertising expenses, in dollars, normalized by make sales, in units.Kantar ad$pender; Ward's Automotive YearbookControl variableMultiple Recall NotificationsA binary measure coded as 1 if the recalling firm sent an additional recall notification to consumers affected by a recall campaign above and beyond the mandatory notification required by the NHTSA, and 0 otherwise.NHTSAControl variableRecall SizeThe total number of vehicles affected by a recall.NHTSAControl variable Because the goal of the study is to examine the effectiveness of the DMC launched in January 2016, we only included recall campaigns in the sample if they were active both before and after the DMC began and had recall completion data available. Accordingly, we did not include recalls that ended before the DMC began or were announced after the DMC began. The final sample includes recall completion data from 296 recall campaigns in the U.S. automobile industry from 2015 to 2017, totaling 1,809 recall campaign-calendar quarter-year observations. Dependent measureWe operationalize Consumer Recall Compliance, the dependent variable, as the cumulative number of vehicles fixed for individual recall campaigns on a quarterly basis. For example, if a recall campaign had 10,000 vehicles fixed in its first calendar quarter-year, the dependent variable would equal 10,000. If an additional 4,000 vehicles were remedied in the subsequent quarter, the dependent variable would equal 14,000. Given that the DMC was an ongoing effort by the NHTSA, the cumulative number of vehicles fixed measure allows us to examine the DMC's effectiveness over time. Measures for moderator variablesWe operationalize Media Coverage as the count of the number of articles in leading news publications that covered a recall campaign during the three months following its announcement. For example, if a recall began on January 1, 2015, and four news articles were published covering the recall from the date of its announcement to March 31, 2015, the Media Coverage variable for that recall campaign would equal 4.We operationalize the Age of Recalled Product variable as the number of years elapsed between the manufacturing year of the oldest vehicle model in a recall campaign and the calendar quarter-year in which we observe its cumulative number of vehicles fixed. For example, consider a recall announced in January 2015 featuring two vehicle models manufactured in January 2013 and September 2014. Age of Recalled Product during the first quarter of the recall campaign would be coded as two years, since the January 2013 model is the oldest in the recall campaign. In April 2015 (the next quarter), the variable would equal 2.25 years.To operationalize Component Hazard, we used the NHTSA's database to identify the defective component in each recall campaign and past literature to determine the component's hazard rating. Consumers learn of the nature of their vehicle's defect in recall notification letters sent by manufacturers. Extant literature has classified recalls into two categories on the basis of the hazard or severity of the issue ([ 1], [ 2]; [34]; [57]; [58]). Consistent with previous research, we operationalize Component Hazard as a dummy variable by classifying defective components related to the driving functionality of a vehicle (i.e., those that could result in a loss of vehicle control due to acceleration, steering or braking, frame corrosion, fire, or repeated stalling) as ""high hazard"" (coded as 1). All other components not directly related to the drivability of a vehicle are classified as ""low hazard"" (coded as 0). A nonexhaustive list of examples of components and their hazard ratings is reported in Table WA1 of the Web Appendix.We operationalize the Time for Repair variable in terms of the estimated time (in hours) needed to repair the defective component provided in recall notification letters.To operationalize Concurrent Recall Campaigns, we examined active recalls to determine if vehicles of the same make-model-year whose dates of production overlapped were involved in multiple simultaneous recalls during our observation window. For example, we used the NHTSA's database to check if 2015 Toyota Camry vehicles experienced multiple active recalls. We operationalize Concurrent Recall Campaigns as a dummy variable by coding recall campaigns including vehicles involved in concurrent recall campaigns as 1 and all other recall campaigns that did not include vehicles involved in concurrent recalls as 0. Measures for control variablesIn addition to the DMC, make-level advertising by firms could impact how aware consumers are of active recalls and thereby improve their compliance. Accordingly, we include the make-level Advertising Intensity variable in our specification to control for firm efforts to improve make awareness. We use advertising expenses data from Kantar's ad$pender database at the make level and normalize it by make sales in units using Ward's Automotive Yearbook to account for automaker efforts at improving make awareness.Although all automakers are mandated to notify consumers about a product recall, in some cases, they issue more than one notification for the same recall. Given that multiple notifications could improve recall completion, we include the dummy variable Multiple Recall Notifications to control for this possibility. The variable takes a value of 1 if a manufacturer notifies consumers affected by a recall more than once and 0 otherwise. Furthermore, we control for the size of a recall, as the total number of vehicles impacted by a recall campaign could positively impact the cumulative number of vehicles fixed. We assemble data on the Recall Size variable from the NHTSA's quarterly recall completion reports. Finally, we control for unobserved automobile make and temporal (i.e., calendar quarter) characteristics with the Make and Quarter dummy variables, respectively. Model SpecificationWe ground our empirical specification in the interrupted time series analysis (ITSA) framework, a technique commonly used to evaluate the effectiveness of universally implemented policies. ITSA is a quasiexperimental design typically utilized to assess the longitudinal effects of interventions with observational data where full randomization is not possible. ITSA is implemented using a regression model that includes up to three types of time-related covariates: ( 1) Time, ( 2) Transition, and ( 3) Time Since Intervention. The interpretation of the covariates is sensitive to their coding and the presence or absence of the other covariates; their inclusion in the model is theory driven. We employ an absolute coding approach, which allows the impact of an intervention to be interpreted in absolute terms (i.e., relative to zero) ([ 8]). The key identifying assumption of ITSA is that in the absence of an intervention, the slope in the outcome will remain unchanged in the postintervention period ([ 8]; [38]; [60]).For a model that includes all three covariates and utilizes absolute coding, Time from Recall is operationalized as the number of calendar quarters elapsed from the time a recall began until the calendar quarter preceding the start of the DMC. The coefficient of Time from Recall captures the slope in the outcome before the DMC begins. Transition to DMC is coded as 1 for all observations during the DMC (Q1 of 2016 and onward) and 0 for observations before the DMC (Q1–Q4 of 2015). In the presence of Time from Recall, the coefficient for Transition to DMC captures the absolute change in the outcome relative to zero in the first time period the intervention is active (i.e., immediately after it is introduced), and its effect remains relevant in the remainder of the observation window thereafter; it is not limited to the first quarter after the DMC. Time Since DMC Start is operationalized as the number of quarters elapsed since the NHTSA's DMC began in Q1 of 2016; it is coded as 0 for all observations before Q2 of 2016. In the presence of Time from Recall and Transition to DMC, the coefficient of Time Since DMC Start captures the slope of the outcome following the DMC relative to zero. Table 2 illustrates the coding of the three ITSA time-related variables for a recall campaign beginning in Q1 of 2015 and ending in Q2 of 2017.GraphTable 2. Illustration of Absolute Coding of Time-Related Variables in the ITSA Framework. QuarterYearTime from RecallTransition to DMCTime Since DMC Start12015000220151003201520042015300120163102201631132016312420163131201731422017315 Consistent with model testing procedures outlined in previous research ([ 8]), we examine whether there are quadratic trends in the dependent variable. Prior research on prescription drug use compliance has found quadratic patterns in some instances ([11]). We compare the model fit for the specification in Equation 1 with and without quadratic pre- and post-DMC slopes. The inclusion of Time Squared and Time Since Intervention Squared decreases the Akaike information criterion for the model from 41,791 to 41,751, indicating better model fit. Accordingly, we augment the specification by including the Time Squared and Time Since Intervention Squared variables.In our context, we argue that Consumer Recall Compliance is a function of Time from Recall, Time from Recall Squared, Transition to DMC, Time Since DMC Start, and Time Since DMC Start Squared. The Time from Recall and Time from Recall Squared variables capture the quarterly pre-DMC slope in recall compliance. Transition to DMC captures the absolute change, relative to zero, in recall compliance immediately after the DMC was introduced in Q1 of 2016. In our context, the impact of Transition to DMC is reflected over the remainder of the observation window and is not just limited to the first quarter after the DMC. Time Since DMC Start and Time Since DMC Start Squared capture the post-DMC slope in recall compliance (i.e., the impact of the DMC following its introduction).In summary, the empirical model controls for various recall campaign-level and time-specific factors that could impact consumer recall compliance. We also account for automobile make- and calendar-quarter-specific unobserved heterogeneity using fixed effects. The empirical specification to test the main effect of the DMC on consumer recall compliance is as follows: ConsumerRecallComplianceit=β0+β1TimefromRecallit+β2TimefromRecallSquaredit+β3TransitiontoDMCt+β4TimeSinceDMCStartt+β5TimeSinceDMCStartSquaredt+β6AdvertisingIntensityit+β7MultipleRecallNotificationsi+β8RecallSizei+β9–63Makei+β64–66Quartert+εit, Graph( 1)where i refers to a recall campaign, t refers to time (by calendar quarter-year), and ɛ refers to the random error. Results DescriptivesTable 3 presents the descriptive statistics for the variables. The mean media coverage in our sample is.44 articles. The mean age of the oldest vehicle in each recall is 4.09 years, with vehicles ranging from less than a month old to 19.06 years old. The mean estimated time needed to repair a defective component is 1.88 hours, with a few requiring as many as 13 hours. The mean hazard rating of the components involved in recalls is.19. While there is a wide range for the advertising intensity of different vehicle makes, the mean is.98. That is, approximately $1 is spent on advertising for every unit sold. In the full interaction model sample, the mean number of calendar quarters for a recall campaign is 6.04 (min = 3, max = 10, SD = .70). The mean number of calendar quarters in the pre-DMC period for a recall campaign in the full interaction model sample is 2.47 (min = 1, max = 4, SD = 1.17), whereas in the post-DMC period it is 3.57 calendar quarters (min = 1, max = 8, SD = 1.32).GraphTable 3. Descriptive Statistics. Variable NameMeanSDMinMax# of ObservationsConsumer Recall Compliance38,503105,58601,397,6121,809Media Coverage.441.250101,809Age of Recalled Product (in years)4.093.51.0419.061,655Component Hazard.19.39011,809Time for Repair (in hours)1.882.110131,809Concurrent Recall Campaigns.35.48011,809Advertising Intensity (in $/unit sales).989.510208.861,785Multiple Recall Notifications.03.17011,809Recall Size77,366196,46811,814,2841,809  Results for the Main Effect of the DMC on Consumer Recall ComplianceThe results of the DMC's main effect on Consumer Recall Compliance are reported in Model 1 of Table 4. We note that the results reported in this analysis pertain to 296 recall campaigns involving issues unrelated to airbag inflators. We exclude airbag inflator–related recall campaigns from the sample to avoid a potential endogeneity issue arising from the NHTSA's unobserved efforts to improve the recall completion of airbag inflator–related recalls specifically. The discussion of endogeneity and the analyses appear in Table WA2 of the Web Appendix.GraphTable 4. Results for the Main Effect of the DMC on Consumer Recall Compliance. Model 1Main-Effects ModelDependent Variable: Consumer Recall ComplianceEstimate (SE)Hypothesis TestedIntercept−12,560.21(27,360.13)Time from Recall14,172.90***(3,674.81)Time from Recall Squared−2,453.12**(1,168.53)Transition to DMC11,112.42**H1 supported (+)(4,393.39)Time Since DMC Start−1,959.86H1 supported (+)(2,894.73)Time Since DMC Start Squared1,748.90***H1 supported (+)(606.74)Advertising Intensity−1.32(209.64)Multiple Recall Notifications9,011.68(20,151.71)Recall Size.43***(.02)Make Fixed EffectsYesQuarter Fixed EffectsYes N (sample size)1,785R-Squared Within.09R-Squared Between.74  1 *p < .10.2 **p < .05.3 ***p < .01.The results in Model 1 of Table 4 reveal that the coefficients for Advertising Intensity and Multiple Recall Notifications are not significant (p > .65). As we expected, the coefficient for Recall Size is positive (.43, p < .01). The coefficient of Time from Recall Squared is negative (2,453.12, p < .05). To understand the linear effect of Time from Recall, we estimate a model specification identical to Equation 1 except that the quadratic terms are dropped, as the average linear effect of a variable is not interpretable in the presence of its quadratic effect ([17], pp. 200–201). The results (not shown) indicate that the coefficient for Time from Recall is positive (8,725.57, p < .01). These results collectively suggest that consumer recall compliance increased at a decreasing rate prior to the start of the DMC.The coefficient for Transition to DMC is positive (11,112.42, p < .05), indicating that, immediately following the DMC's introduction, the cumulative number of vehicles fixed increased in absolute terms in Q1 of 2016. In our context, the impact of Transition to DMC associated with Q1 of 2016 remains relevant in the remainder of the observation window thereafter and is not just limited to the first quarter after the DMC. The interpretation of Transition to DMC is contingent on the presence of Time from Recall in the model.In the presence of Time from Recall, Time from Recall Squared, and Transition to DMC, Time Since DMC Start and Time Since DMC Start Squared represent the slope of consumer recall compliance in the post-DMC period. The results in Model 1 of Table 4 indicate that the coefficient for Time Since DMC Start Squared is positive (1,748.90, p < .01). As before, to understand the linear trend in Time Since DMC Start, we estimate Equation 1 without the quadratic terms in the model ([17], pp. 200–201). The results (not shown) indicate that the coefficient for Time Since DMC Start is positive ( 6,036.62, _I_p_i_ < .01). These results collectively suggest that following the DMC's introduction, consumer recall compliance increases at an increasing rate (i.e., a positive accelerating curve), in contrast to the pre-DMC trend. H1 is supported.The visual plots in Figure 3 provide a representation of the DMC's effectiveness for an average-sized recall using the estimates provided in Model 1 of Table 4. In Figure 3, the solid line (""Number of vehicles fixed with the DMC"") denotes the DMC's impact on recall compliance for a recall campaign that begins in Q1 of 2015 and ends in Q4 of 2016. The solid line shows that recall compliance increased at a decreasing rate before the start of the DMC. The DMC begins in Q1 of 2016 after four quarters have elapsed, represented by the vertical dotted line. Between four and five quarters elapsed, the solid line captures the impact of the DMC in Q1 of 2016. Following Q1 of 2016, the solid line captures both the positive linear and positive quadratic trends in consumer recall compliance. Thus, Figure 3 reveals that the DMC interrupted the negative accelerating curve in consumer recall compliance in the pre-DMC period and resulted in a positive acceleration in the cumulative number of vehicles fixed in the post-DMC period.Graph: Figure 3. A visual representation of the DMC's effect on consumer recall compliance.To estimate consumer recall compliance in the absence of the DMC, we set the post-DMC time-related variables Transition to DMC, Time Since DMC Start, and Time Since DMC Start Squared to zero in Equation 1. In the pre-DMC period, the dotted line with circular dots (""Number of vehicles fixed without the DMC"") is overlapped by the solid line, as both depict the pre-DMC slope. The dashed line (""Predicted number of vehicles fixed without the DMC"") depicts the predicted trend in recall compliance in the post-DMC period without the DMC. In line with the negative quadratic trend in compliance before the DMC, the model predicts that the cumulative number of vehicles fixed in Q1 of 2016 would be lower than in Q4 of 2015. However, because the cumulative number of vehicles fixed over time cannot decrease, using the model-based results to predict compliance without the DMC in the post-DMC period would overstate the DMC's positive effect and lead to erroneous conclusions. A reasonable prediction in our context is that compliance would level off from Q4 of 2015 onward without the DMC. Accordingly, the predicted trajectory of consumer recall compliance without the DMC in Figure 3 in the post-DMC period is depicted as a flat line.To quantify the DMC's positive impact, we compute the difference in the predicted cumulative number of vehicles fixed with and without the DMC for a recall campaign after eight calendar quarters have elapsed using the coefficient estimates in Model 1 of Table 4 (represented by the solid line) and the dashed line in Figure 3. We find that in the first four quarters after it was introduced, the DMC increased the number of vehicles fixed, on average, by 20,712 per recall campaign above what would be expected without the DMC. Testing for Moderators of the DMC's EffectivenessTo test the moderating effects (H2–H6), we estimate the following specification: ConsumerRecallComplianceit=β0+β1TimefromRecallit+β2TimefromRecallSquaredit+β3TransitiontoDMCt+β4TimeSinceDMCStartt+β5TimeSinceDMCStartSquaredt+β6AdvertisingIntensityit+β7MultipleRecallNotificationsi+β8RecallSizei+β9MediaCoveragei+β10AgeofRecalledProductit+β11ComponentHazardi+β12TimeforRepairi+β13ConcurrentRecallCampaignsi+β14TransitiontoDMCt×MediaCoveragei+β15TransitiontoDMCt×AgeofRecalledProductit+β16TransitiontoDMCt×ComponentHazardi+β17TransitiontoDMCt×TimeforRepairi+β18TransitiontoDMCt×ConcurrentRecallCampaignsi+β19TimeSinceDMCStartt×MediaCoveragei+β20TimeSinceDMCStartt×AgeofRecalledProductit+β21TimeSinceDMCStartt×ComponentHazardi+β22TimeSinceDMCStartt×TimeforRepairi+β23TimeSinceDMCStartt×ConcurrentRecallCampaignsi+β24–78Makei+β79–81Quartert+εit, Graph( 2)where i refers to a recall campaign, t refers to time (by calendar quarter-year), and ɛ refers to the random error. There are two points worth noting about this specification. First, Equation 2 is identical to the main-effect specification in Equation 1, aside from the addition of the moderators and interaction terms. Second, the interaction terms between Transition to DMC and the moderators test for boundary conditions of the DMC's effectiveness at the moment it is introduced in Q1 of 2016. The interaction terms between Time Since DMC Start and the moderators test for the DMC's boundary conditions following its introduction from Q2 of 2016 onward.[ 7]The results for the moderating effects are reported in Model 1 of Table 5. None of the interaction terms in Model 1 have variance inflation factors (VIFs) above 5.17. The VIFs for Model 1 are reported in Table WA3 of the Web Appendix. While the lower-order terms of the time-related variables in Model 1 of Table 5 have VIFs above 10, our interest is only in interpreting the coefficients of the interaction terms. Furthermore, the interpretation of an interaction term is not affected by multicollinearity, as this multicollinearity affects neither the interaction term's coefficient nor its standard error ([42], p. 399). The same is true for the coefficients and standard errors of higher-order polynomials, which are essentially interactions of the lower-order terms ([17]).GraphTable 5. Results for the Effect of the DMC on Consumer Recall Compliance: Interaction Models. Model 1Model 2Model 3Model 4Dependent Variable: Consumer Recall ComplianceInteraction ModelInteraction Model Without Make Fixed EffectsInteraction Model with Only Linear Slope TermsInteraction Model with Only Linear Slope Terms and Without Make Fixed EffectsEstimate (SE)Hypothesis TestedEstimate (SE)Estimate (SE)Estimate (SE)Intercept–6,809.36–3,814.06–5,232.021,761.74(20,975.05)(6,009.95)(20,958.88)(5,769.30)Time from Recall14,838.04*** 13,896.02***8,929.75***8,562.00***(3,697.30)(3,603.27)(1,545.46)(1,467.90)Time from Recall Squared–2,165.48* –1,988.51*(1,194.20)(1,178.13)Transition to DMC–2,315.27 –1,157.12–3,212.90–2,432.85(6,250.76)(6,123.64)(5,763.04)(5,668.35)Time Since DMC Start1,243.44 652.762,195.532,076.94(3,492.59)(3,445.95)(2,136.19)(2,113.41)Time Since DMC Start Squared201.56 314.61(677.47)(666.82)Advertising Intensity–25.31 48.54–49.5031.68(211.92)(164.90)(211.58)(164.66)Multiple Recall Notifications6,328.66 5,699.317,362.646,778.29(15,073.09)(13,263.56)(15,078.72)(13,250.10)Recall Size.51*** .51***.51***.51***(.01)(.01)(.01)(.01)Media Coverage–2,105.46 –2,162.93–2,263.14–2,291.67(1,614.43)(1,546.53)(1,612.49)(1,544.60)Age of Recalled Producta–5,434.18*** –5,373.96***–5,438.46***–5,400.57***(938.50)(809.64)(939.49)(809.57)Component Hazard3,789.08 2,573.144,034.832,797.70(7,013.30)(6,388.82)(7,018.76)(6,388.20)Time for Repairb–518.81 274.31–571.21262.58(1,700.56)(1,242.39)(1,702.20)(1,242.49)Concurrent Recall Campaigns4,380.81 4,753.414,539.604,777.33(6,334.25)(5,499.19)(6,340.05)(5,499.64)Transition to DMC × Media Coverage3,993.22**H2 supported (+)4,076.06**4,288.84**4,309.28**(1,770.60)(1,758.61)(1,748.75)(1,737.81)Transition to DMC × Age of Recalled Producta2,171.51***H3 supported (+)2,111.26***2,083.69***2,018.78***(684.75)(680.06)(681.70)(677.03)Transition to DMC × Component Hazard–1,538.73H4 not supported (+)–925.12–1,336.34–807.79(5,546.05)(5,506.80)(5,536.56)(5,497.76)Transition to DMC × Time for Repairb538.61H5 partially supportedc (−)468.71556.37490.69(1,077.57)(1,071.98)(1,077.58)(1,072.11)Transition to DMC × Concurrent Recall Campaigns3,498.99H6 not supported (−)3,363.493,602.483,495.35(4,782.83)(4,759.03)(4,780.47)(4,757.08)Time Since DMC Start × Media Coverage1,499.02***H2 supported (+)1,455.20***1,519.93***1,497.27***(516.48)(511.32)(501.47)(497.09)Time Since DMC Start × Age of Recalled Producta884.17***H3 supported (+)878.30***899.60***900.10***(246.13)(244.12)(241.40)(239.48)Time Since DMC Start × Component Hazard–3,080.78H4 not supported (+)–3,350.67–3,062.90–3,293.08(2,334.58)(2,308.70)(2,324.63)(2,298.18)Time Since DMC Start × Time for Repairb–830.94*H5 partially supportedc (−)–785.30*–841.29*–799.98*(458.47)(454.04)(458.16)(453.75)Time Since DMC Start × Concurrent Recall Campaigns2,156.78H6 not supported (−)2,478.382,112.022,390.51(1,986.83)(1,971.75)(1,976.25)(1,960.38)Make Fixed EffectsYesNoYesNoQuarter Fixed EffectsYesYesYesYesN (sample size)1,6371,6371,6371,637R-Squared Within.07.08.07.08R-Squared Between.87.87.87.87 4 *p <.10.5 **p <.05.6 ***p <.01.7 a In years.8 b In hours.9 c The interaction between Transition to DMC and Time for Repair is not significant. However, the interaction between Time Since DMC Start and Time for Repair is marginally significant (p =.07). Therefore, H5 is partially supported.10 Notes: The full set of results, with VIFs for the variables, is reported in Table WA3 of the Web Appendix.We find that the coefficient of the interaction between the Transition to DMC and Media Coverage variables is positive (3,993.22, p < .05). This finding suggests that at the moment the DMC was introduced in Q1 of 2016, its impact on consumer recall compliance was stronger for recall campaigns with greater media coverage. We also find that the coefficient of the interaction between Time Since DMC Start and Media Coverage is positive (1,499.02, p < .01), implying that Media Coverage continues to positively moderate the DMC–consumer recall compliance relationship from Q2 of 2016 onward. H2 is thus supported.We use visual plots in Figure 4, Panel A, to provide an intuitive understanding of interactions with Media Coverage. We set ""low"" (0) and ""high"" ( 1) values for Media Coverage to visually represent its impact on the DMC's effectiveness for a recall campaign of average size that spans from Q1 of 2015 until Q4 of 2016. As Panel A shows, the DMC's impact is stronger when media coverage for a recall is high relative to low.Graph: Figure 4. The moderating roles of media coverage, age of recalled product, and time for repair during the DMC.We also find that the coefficient of the interaction between Transition to DMC and Age of Recalled Product is positive (2,171.51, p < .01). That is, the DMC's impact on consumer recall compliance is stronger for older (vs. newer) recalled vehicles at the moment it is introduced in Q1 of 2016. We also find that the coefficient for the interaction between Time Since DMC Start and Age of Recalled Product is positive (884.17, p < .01), which suggests that the DMC also has a stronger impact on consumer recall compliance for older vehicles from Q2 of 2016 onward. H3 is supported. We set ""low"" (25th percentile) and ""high"" (75th percentile) values of the Age of Recalled Product variable in Figure 4, Panel B, which demonstrates that the DMC's effectiveness is stronger for older vehicles (i.e., for larger values of the variable) both at the moment of the DMC's introduction and thereafter.Inconsistent with H4, we find that neither the Transition to DMC × Component Hazard nor the Time Since DMC Start × Component Hazard interaction terms are significant (p > .78 and p > .18, respectively). Perhaps this insignificant finding implies that consumers are unable to actually discern the hazard of the defective components cited in recall notification letters.We find that the interaction between Transition to DMC and Time for Repair is not significant (p > .61). However, the coefficient for the interaction between Time Since DMC Start and Time for Repair is negative (830.94, p = .07) and marginally significant. Thus, following the DMC's introduction from Q2 of 2016 until Q4 of 2017, the DMC is less effective at improving consumer recall compliance as the time needed to repair a defective component increases. H5 is partially supported. Using ""low"" (25th percentile) and ""high"" (75th percentile) values for Time for Repair in Figure 4, Panel C, we demonstrate visually that following the DMC's introduction, the DMC's effectiveness is weaker when a defective component requires more versus less time to repair.Finally, we find that neither the Transition to DMC × Concurrent Recall Campaigns nor the Time Since DMC Start × Concurrent Recall Campaigns interaction variables are significant (p > .46 and p > .27, respectively). H6 is not supported. The coefficients for the Advertising Intensity and Multiple Recall Notifications variables are also not significant (p > .67), while the coefficient for the Recall Size variable is positive (.51, p < .01).The moderator results are robust to several alternate model specifications. We estimate three specifications similar to Model 1 in Table 5, removing automobile make–specific fixed effects and/or Time from Recall Squared and Time Since DMC Start Squared. The results of the alternate specifications are reported in Models 2, 3, and 4 of Table 5. Across each model, the estimates of the interaction terms are largely similar to Model 1, and the conclusions regarding the impact of the moderators on the DMC's effectiveness remain the same. DiscussionThis study addresses the important question of whether a regulator-initiated DMC can improve consumer recall compliance. Further, the study aims to understand the conditions under which a DMC is more or less effective. We were able to exploit a full-coverage national DMC launched in January of 2016 by the NHTSA, the regulator in the U.S. automobile industry, to answer these research questions. The study offers several implications for researchers and policy makers. Research ImplicationsThe question of whether awareness campaigns are successful in eliciting the required compliance-related behaviors from consumers has been investigated in various empirical settings (e.g., [32]; [66]). The fact that the awareness campaign in our study is introduced by a governmental agency and that it is its first major effort to improve recall compliance using digital media raises questions about its potential efficacy. We find that a regulator-initiated DMC is effective at improving consumer recall compliance, as reflected by the additional number of vehicles fixed above what was to be expected without it. This finding underscores the critical importance of regulators utilizing digital means to provide recall-related information to elicit greater compliance from consumers.The moderator analyses reveal that the DMC's effectiveness varies across recall campaigns. Specifically, the DMC's impact on consumer recall compliance is stronger for recall campaigns with greater media coverage. This finding is similar in spirit to research that has documented the moderating impact of negative publicity on postcrisis advertising and consumers' brand share and category purchases ([16]). Although brands aim to avoid negative publicity of their recalled products because of its adverse effects on sales, media outlets play an important functional role in reinforcing a regulator's efforts to improve consumer recall compliance. In our context, this effect is likely driven by the varying specificity of information provided by the regulator and the media.We also find that the DMC is more effective when the recalled products are older. Prior research has found that the utility of paid search campaigns is greater in contexts where consumers have low levels of familiarity with the firms' brands or products ([ 7]). Along similar lines, we find that the DMC is more effective when the recalled products are older. Consumers of older products may not be in contact with their products' OEMs as their warranties expire and thereby be less familiar with safety-related developments for their vehicles. A DMC can provide resources for these consumers who are more likely to seek out recall-related information digitally.Finally, we find that the DMC is less effective at improving consumer recall compliance for recall campaigns containing products with defects that require more (vs. less) time to repair. Previous work examining tax compliance similarly finds that time-related costs are significant barriers to compliance ([10]; [64]). Although our study finds that the DMC is effective at providing the necessary information to improve compliance, the inconvenience that higher repair times pose to consumers is still a significant impediment to compliance. Future research should examine whether regulators can improve compliance for recalls that require long repair times by using persuasive appeals. Public Policy ImplicationsOur study's findings also offer actionable insights for policy makers. In recent years, multiple audit reports submitted to U.S. Congress have questioned the adequacy of the NHTSA's oversight processes in managing consumer recall compliance ([52]). The overarching concern is that the agency has failed to carefully review safety issues, hold automakers accountable, collect safety data, or adequately train its staff, resulting in significant safety concerns being overlooked. Some industry experts go a step further and lament that product recalls may increase the number of crashes on the road, as the extra driving needed to remedy the issues heightens the probability of accidents ([43]). Our analysis reveals that in the first four quarters after it was introduced, the DMC increased the number of vehicles fixed, on average, by 20,712 per recall campaign over what would be expected without the DMC.The improvement in consumer compliance as a result of the DMC is economically meaningful because it could reduce the number of vehicle accidents and the economic costs associated with vehicle crashes. Research has shown that, on average, a 1% improvement in recall compliance in the automobile industry lowers the number of vehicle accidents in the next three years by.46% (Bae and Benítez-Silva [ 1]). The average economic costs (e.g., fatalities, nonfatal injuries, damaged vehicles) associated with a motor vehicle accident are about $14,000 ([ 9]). Therefore, by improving consumer recall compliance, the DMC reduces the number of automobile accidents on the road and lowers the total economic costs associated with accidents. The last few years have witnessed an average of 953 product recalls in the U.S. automobile industry, suggesting that the total annual economic impact of improved consumer recall compliance across all recall campaigns is not trivial.Further, our study implies that a lack of available relevant information is a significant contributing factor to low consumer recall compliance. For years, the NHTSA has tried to mandate electronic recall notices to counteract the issue of low consumer recall awareness ([46]). The regulatory agency's foray into the digital domain with ""Safe Cars Save Lives"" has proven successful, suggesting that improving consumer awareness of recall-related information is a crucial step in improving consumer recall compliance. Even though the automobile industry receives considerable media attention (e.g., GM's faulty ignition issue, Takata's airbag failures), getting consumers to pay attention to recall notifications is challenging.While extant work notes that manufacturers view greater media coverage of a recall as damaging to their brands' financial health ([ 5]), we find that media coverage helps improve safety outcomes by increasing the effectiveness of the DMC. Furthermore, we find that the DMC's impact on consumer recall compliance is stronger when recalled vehicles are older. Issues of low compliance are particularly prevalent among owners of these older products. According to J.D. [54], just 44% of vehicles manufactured between 2003 and 2007 had their defects remedied, drastically below recall completion percentages of 73% for vehicles of model years 2013–2017. Federal and industry leaders have cited improving compliance among owners of older vehicles as one of four key topic areas to address moving forward ([51]). The DMC's effectiveness on consumers owning older products further suggests that regulators' use of digital tools to facilitate consumer access to relevant information could improve compliance.Finally, our findings caution regulators to be mindful of perceived time inconvenience as a serious impediment to consumer recall compliance. While the DMC is effective at improving compliance, its impact is lower for recall campaigns with defective components that require longer to repair. Interestingly, consumers often do not cite the time needed to complete the repair as the most important factor in deciding whether to remedy safety defects ([28]). Yet, our findings suggest that perceived time inconvenience is a serious obstacle to achieving consumer compliance. The findings should enable regulatory agencies to make more compelling cases for financial resources to be devoted to DMCs aiming to improve compliance. Limitations and Future DirectionsThe findings of the study are subject to some limitations. First, the study is limited to the U.S. automobile industry and a single DMC, implying that caution is warranted in generalizing our findings to other settings. If systematic recall completion data were available in other contexts, the conceptual and methodological framework employed in this study could be useful in deriving empirical generalizations about the effectiveness of DMCs in other settings. Second, data on the NHTSA's total advertising expenditure on this campaign were not available. As such, the DMC's positive effect should not be interpreted as consumers' responsiveness to advertising dollars. Third, our study was unable to examine if a recall notification message's content impacts compliance because the mandated recall notification letters sent from manufacturers to consumers in our setting were standard, lacking variation. If regulators use assertive versus nonassertive language or fear- versus health- versus norm-based persuasion appeals in their advertisements or messaging, understanding which types improve compliance would be valuable. Supplemental Materialsj-pdf-1-jmx-10.1177_00222429211023016 - Supplemental material for Regulating Product Recall Compliance in the Digital Age: Evidence from the ""Safe Cars Save Lives"" CampaignSupplemental material, sj-pdf-1-jmx-10.1177_00222429211023016 for Regulating Product Recall Compliance in the Digital Age: Evidence from the ""Safe Cars Save Lives"" Campaign by Sotires Pagiavlas, Kartik Kalaignanam, Manpreet Gill, and Paul D. Bliese in Journal of Marketing  "
16,"Regulating Product Recall Compliance in the Digital Age: Evidence from the ""Safe Cars Save Lives"" Campaign The unprecedented number of product recalls in recent years and subsequent low consumer recall compliance raise questions about the role of regulatory agencies in ensuring safety. In this study, the authors develop a conceptual framework to test the impact of a regulator-initiated digital marketing campaign (DMC) on consumer recall compliance. The empirical context is the launch of a nationwide DMC by the U.S. automobile industry's regulator. The analysis utilizes recall completion data from 296 product recalls active both before and after the DMC's launch. The results show that the DMC improves consumer recall compliance. In the first four quarters after it was introduced, the DMC increased the number of vehicles fixed, on average, by 20,712 per recall campaign over what would be expected without the DMC. Regarding boundary conditions, the study finds that the DMC is more effective for recall campaigns with greater media coverage and for those with older recalled products. However, the DMC's effect weakens as the time needed to repair a defective component increases. The findings should help regulators make compelling cases for greater resource allocation toward digital initiatives to improve recall compliance.Keywords: digital marketing campaign; product recalls; public policy; regulationProduct recalls and consumer safety are regulated by government agencies in several countries, including the United States, Canada, Germany, the United Kingdom, and Japan. While issuing a recall notice to inform consumers of a potential issue is critical, far too many defective products remain unremedied long after their recall notifications have been sent to consumers. For instance, J.D. Power reports that consumer recall compliance in the U.S. automobile industry is quite low, with approximately 30%–50% of vehicles on the road at any point in time having an unrepaired safety problem (J.D. [54]). Relatedly, according to a report by the Consumer Product Safety Commission (CPSC), only 10% of children's products recalled in 2012 were successfully corrected, replaced, or returned ([44]). For example, since the 2016 recall of millions of IKEA Malm dressers, only 1% of consumers, at best, have had the unstable furniture removed and been issued a refund (Consumers [19]).The continued use of defective products by consumers is a serious public health concern and raises the specter of injuries and fatalities. The sustained string of casualties in 2018 due to faulty Takata airbags, a safety problem for which a recall was issued in late 2014, underscores the perils of low consumer recall compliance. Unsurprisingly, regulatory agencies have been subjected to intense scrutiny and even rebuke for not taking adequate measures to improve consumer recall compliance. For example, an audit report released in 2018 by the U.S. Department of Transportation faults the National Highway Traffic Safety Administration (NHTSA) for a lack of proper oversight of the recall completion process ([52]).While numerous factors likely contribute to poor consumer recall compliance, low consumer awareness is often noted as a key issue responsible for inaction. In fact, the NHTSA's former Associate Administrator for Enforcement stated that a lack of public knowledge is ""the single greatest weakness"" in successfully addressing product recalls ([31], p. 232). Similarly, NHTSA focus group interviews conducted to understand the reasons for low recall compliance found that while over 70% of consumers preferred electronic recall notifications, only 7.4% reported receiving any. Notably, 90% of the respondents mentioned that recall notifications received electronically had a greater chance of being noticed ([28]). In the past, regulatory agencies such as the CPSC have conducted targeted national campaigns to raise public awareness, support industry compliance, and improve safety in specific consumer product categories ([20]). The issue of low consumer recall awareness is so pervasive that in 2014, the CPSC tried to crowdsource solutions, announcing a contest for application developers to create tools to inform the public of consumer product recalls ([26]).Although regulators introduce such initiatives to raise consumer awareness and reduce accidents, empirical evidence to support or refute these expectations is inconsistent. A potential concern is that campaigns by regulatory agencies might not be effective policy instruments if consumers view them as symbolic acts by government officials and are unresponsive to these efforts ([23]; [32]). In addition, prior research has found that consumers could develop reactance to regulation-related public awareness campaigns because they view such efforts as infringements on their personal freedoms ([15]; [47]). This contention is supported by popular press: a 2018 survey of over 1,500 U.S. consumers revealed that almost two-thirds of them did not believe that government-initiated product recall programs had much to do with increasing consumer safety. Instead, many consumers viewed the recall programs as government exercises in ""red tape"" ([62]).The study's objective is to investigate the impact of a regulator-initiated digital marketing campaign (DMC) on consumer recall compliance. The manuscript makes two contributions to marketing literature. First, this study is the only one that we are aware of that examines whether a DMC initiated by a regulator improves consumer recall compliance. The product recall literature has predominantly focused on the stock market and sales consequences of recalls (e.g., [13]; [29]), the ability to learn from and prevent future recalls ([33]; [37]), the drivers and consequences of recall timing decisions ([25]), and how recalls can impact marketing effectiveness ([16]; [65]). Little attention has been devoted to the consumer compliance process. A couple of studies have also investigated how recall attributes, including vehicle country of origin, vehicle age, and the publicity around recalls, influence compliance ([34]; [58]). However, the impact of a regulator's efforts on consumer compliance has, to our knowledge, received virtually no attention. Our study fills this research void and offers valuable insights to policy makers.Specifically, we examine the effectiveness of a DMC initiated by a regulatory agency. To tackle the problem of low consumer recall compliance, the NHTSA, the agency responsible for regulating consumer safety in the U.S. automobile industry, launched a full-coverage, nationwide DMC, ""Safe Cars Save Lives,"" in January 2016. The DMC featured paid search and online display advertisements that provided consumers with links to check for open recalls and access pertinent recall remedy information online. We exploit this setting to formally test the effectiveness of a regulator-initiated DMC to improve consumer recall compliance. The empirical analysis utilizes recall completion data pertaining to 296 product recalls active both before and after the DMC's launch. Our econometric analyses account for various potential confounds such as the DMC's possible endogenous nature, time trends in recall completion, recall campaign attributes, and unobserved vehicle make and temporal characteristics. The results show that in the first four quarters after it was introduced, the DMC increased the number of vehicles fixed, on average, by 20,712 per recall campaign above what was to be expected without the DMC. This improvement in consumer recall compliance should, in turn, lead to potentially fewer vehicle crashes, casualties, and lower economic costs.Second, we identify important boundary conditions for the DMC's effectiveness. We find that the DMC is more effective at increasing consumer compliance for recalls with greater media coverage. Although media coverage of recalls could be detrimental to the impacted brand's financial health, our finding implies that it plays a critical role in aiding the DMC to improve compliance. We also find that the DMC is more effective at increasing compliance for older (as opposed to newer) recalled products. This finding is critical for regulators who struggle to reach owners of older products using conventional communication methods. However, the DMC's impact on compliance is lower for recalls in which the defective component takes a longer time to repair. This suggests that the DMC may be unable to fully counteract the barrier of time-related inconvenience for consumers. Collectively, our findings enable regulators to make compelling cases to receive more resources for digital marketing initiatives in the future. Literature Review and Conceptual FrameworkRegulatory agencies introduce interventions with the goal of promoting and protecting consumer welfare. Government interventions supporting public welfare are comprised of pecuniary interventions (e.g., imposing higher taxes/penalties, offering financial incentives) as well as nonpecuniary interventions (e.g., focused on educating, building awareness) ([45]). Prior research has investigated the efficacy of interventions targeted at decreasing the consumption of unhealthy products, stimulating consumer spending, and promoting preventive health screenings ([18]; [59]; [66]). The effectiveness of interventions is often assessed by examining the extent to which consumers comply with or respond to the proposed initiatives.Prior research provides equivocal evidence regarding the effectiveness of pecuniary interventions. For example, although imposing higher taxes has been found to lower purchases of bottled water, reduce purchases of drinks with sugar additives, and decrease smoking prevalence, their associated unintended consequences may drive consumers toward more dangerous products ([ 6]; [18]; [67]). Relatedly, the ""Click It or Ticket"" campaign, intended to improve seat belt usage in vehicles and promote consumer safety by ticketing transgressions, has been shown to be successful ([61]). However, programs that offer consumers incentives to stimulate sales through initiatives such as ""Cash for Clunkers"" ([48]) or drive consumer spending through tax rebates ([59]) have been found to be generally ineffective.Likewise, evidence from prior research assessing the efficacy of nonpecuniary interventions is also inconsistent. For example, the impact of the Nutrition Labeling and Education Act (NLEA) in altering consumers' behaviors is somewhat nebulous. [49] found that the NLEA significantly increased consumers' nutrition information processing and comprehension, although information-sensitive consumers did not report increased use of nutrition information following its implementation. Relatedly, [ 3] did not find any change in consumers' consumption-related search and recall of nutrition information after the NLEA's enactment. [55] summarizes the effects of government agency-initiated public information campaigns across various contexts and even documents consumer responses that are opposite to those intended in some situations.The mixed evidence for the efficacy of regulator-initiated interventions (those pecuniary and nonpecuniary) documented in prior research is not satisfactory from a knowledge advancement perspective and raises important questions about the potential effectiveness of a regulator-initiated DMC. More specifically, is a regulator-initiated DMC effective in improving consumer recall compliance? Under what conditions is a DMC more or less effective?To answer these questions, we develop a conceptual framework (depicted in Figure 1) by drawing on insights from both the health belief model (HBM) and health warning streams of research.[ 6] With conceptual origins in the public health domain, the HBM contends that for individuals to take action to prevent a detrimental outcome, they need ( 1) a cue or trigger (internal and/or external) to overcome their avoidance tendencies, ( 2) to believe that they are susceptible to its risk, ( 3) to perceive that the occurrence of the outcome would have negative consequences for them, and ( 4) to believe that taking preventative action would lead to benefits outweighing the barriers of cost, convenience, pain, and embarrassment ([11]; [36]; [56]). Drawing on these insights, our framework examines the roles of ( 1) awareness cues, ( 2) the perceived susceptibility to risk, ( 3) the perceived threat of noncompliance, and ( 4) the perceived time inconvenience of compliance in eliciting consumer recall compliance following a regulator-initiated DMC.Graph: Figure 1. A conceptual model of the impact of a regulator-initiated digital marketing campaign on consumer recall compliance.Our framework also draws on health warning streams of research, specifically, prior work on consumer responses to persuasive public health appeals. This stream of research suggests that increasing the frequency of health warnings might desensitize consumers to the potential hazard, resulting in either inaction or actions that are opposite to the intended warning ([55]; [63]). In our context, government regulators, retailers, manufacturers, and consumer experts are concerned that product recalls have become so frequent in various industries, including food, consumer products, and automobiles, that consumers might suffer from ""recall fatigue"" ([39]). Therefore, our framework also considers the impact of multiple concurrent health warnings on a consumer's compliance with a regulator's request.Given that our goal is to investigate a DMC's effectiveness, we conceptualize a regulator-initiated DMC as the primary awareness cue intended to improve consumer recall compliance. The ability of the DMC to improve compliance depends on the extent to which consumer recall awareness is generated and the extent to which consumers are motivated to comply. Accordingly, we expect that the DMC's impact on compliance is contingent on four factors identified in the HBM and the presence of multiple concurrent health warnings for consumers. We propose that a DMC's effectiveness is moderated by ( 1) media coverage, ( 2) the age of the recalled product, ( 3) component hazard, ( 4) time for repair, and ( 5) concurrent recall campaigns.A DMC improves consumer recall compliance by providing relevant information to consumers impacted by recalls. These consumers could be unaware that their products have been recalled but are actively searching for recall-related information, benefiting from a DMC's paid search advertisements that make them aware of existing recalls and direct them to relevant information. Other consumers may be exposed to a DMC's online display advertisements and then cued to check if their products have been recalled on the regulator's webpage.In addition to a DMC providing the necessary information to aid consumers in complying with a recall request, media coverage of recalls could heighten a DMC's effectiveness by sensitizing consumers to the fact that their products may have defects and thereby influencing them to comply. Accordingly, we include media coverage (akin to another awareness cue, a factor in the HBM) as a moderator of a DMC's impact on consumer recall compliance.Because owners of older (vs. newer) products are less likely to receive information about recalls directly from their products' manufacturers, they are likely cognizant of their greater susceptibility to recall risk and more responsive to a DMC that provides relevant information. Accordingly, we include the age of the recalled product (akin to the perceived susceptibility to risk, a factor in the HBM) as a moderator of a DMC's impact on compliance.Following a DMC, a consumer learns about the hazard of a defective component, the time it will take to repair it, and the existence of concurrent recall campaigns for the product after being made aware that it has been recalled. As such, these factors are pertinent in explaining a consumer's motivation to comply with a recall request after being made aware of it through a DMC. Accordingly, we include component hazard (akin to the perceived threat of noncompliance, a factor in the HBM), time for repair (akin to the perceived time inconvenience of compliance, a factor in the HBM), and the presence of concurrent recall campaigns (akin to multiple concurrent health warnings) as moderators of the DMC's impact on consumer compliance ([36]; [55]; [63]). Research HypothesesPrior research suggests that marketing campaigns can be effective at improving individuals' compliance-related behaviors. For example, [66] showed that the introduction of a mass media campaign was associated with increased consumer information search, displayed by high click-through rates to a website and more referrals to medical centers. Relatedly, [69] found that patients were more likely to obtain a first prescription of a treatment following a direct-to-consumer advertising campaign designed to educate them and provide resources about health-related decisions. The positive impact of advertising initiatives by firms may even spill over to the category level, as [68] noted that advertising by a brand increased consumer drug therapy compliance of its rival brands' products. However, the positive effects of awareness campaigns documented in prior research are primarily the result of initiatives by firms, and not by regulators. One notable exception is work by [32], which found a positive effect from a government-initiated proenvironmental demarketing campaign to reduce water consumption. However, the campaign was more effective on the majority ethnic/religious group than on the minority groups.There are also reasons to believe that a marketing campaign may not be as effective if initiated by a regulator. For example, [53] found that fewer than half of the antismoking advertisements they showed to adolescents increased their nonsmoking intentions. Similarly, research examining the National Youth Anti-Drug Media Campaign in the United States reported no effects from the initiative ([35]). Although consumers may be skeptical about government-initiated awareness campaigns related to health or safety issues, the preponderance of evidence for these campaigns' positive effects leads us to our baseline hypothesis that a DMC will improve consumer recall compliance. H1:  A regulator-initiated DMC improves consumer recall compliance. The Moderating Role of Media CoverageThe effectiveness of a regulator-initiated DMC is likely to vary depending on whether a recall received media coverage. Mass media outlets are a unique voice in the marketplace that provide consumers with information that is different from that of firms or governmental agencies ([14]). Extant research argues that separate media channels could have a synergistic impact on the effectiveness of multichannel marketing campaigns ([12]; [50]). Importantly, in addition to the repetition of the message, the variation of the specificity of the message also likely affects consumers. Although both media coverage and a regulator-initiated DMC generate awareness about a recall, they also vary in the information they provide. In our context, a regulator-initiated DMC provides the specific information consumers need to determine whether they are personally affected by a recall and how to address it, whereas media coverage increases consumer awareness of a recall. We thus argue that consumers are more likely to comply with a recall notice if the information provided by a DMC is complemented by awareness generated through media coverage ([41]). This argument is also consistent with research suggesting that advertising is more effective when there is more associated publicity following a product-harm crisis ([16]; [21]). Drawing on these arguments, we hypothesize: H2:  Media coverage positively moderates the impact of a regulator-initiated DMC on consumer recall compliance, such that the impact is stronger for recall campaigns with greater media coverage. The Moderating Role of Age of Recalled ProductConsumers are, in general, more informed of product-related issues (e.g., defects, failures) when they interact with original equipment manufacturers (OEMs) during their products' warranty periods. As products age and their warranties expire, consumers' interactions with OEMs tend to be less frequent. For example, [58] propose that owners of older products are less likely to schedule routine product maintenance. In addition, the likelihood that product ownership could change hands increases as products age, reducing the probability that notifications sent from the OEM will reach the product's present owner ([ 2]; [27]). These decreased interactions with OEMs leave consumers who own older products at greater risk of being unaware that their products are the subject of active recalls. A regulator-initiated DMC can fill this communication gap for owners of older products, allowing them to receive recall-related information through paid search and online display ads. For these reasons, we expect consumers with older products, who likely perceive themselves as more susceptible to recall-related risk, to utilize a DMC's information to a greater extent than consumers owning newer products. Accordingly, we hypothesize that a DMC's impact on consumer recall compliance will be stronger for older (vs. newer) recalled products. H3:  The age of the recalled product positively moderates the impact of a regulator-initiated DMC on consumer recall compliance, such that the impact is stronger when the recalled product is older. The Moderating Role of Component HazardThe impact of a DMC on consumer recall compliance is also likely to vary depending on consumers' perceptions of the threat of not complying with a recall request. For example, [40] finds that a consumer's perception of threat is the most influential determinant of whether they seek medical care services. Similarly, consumers are more likely to comply with treatment regimens when they perceive that noncompliance would threaten their well-being ([11]). Relatedly, vehicle owners are more likely to repair products when the defective component's hazard is higher. [34] provide evidence that consumers remedy more severe defects at higher rates than less severe defects. In general, after a DMC makes consumers aware of a recall and the defective component involved, consumers should be more motivated to comply with a recall request if they perceive a high threat of noncompliance. If a defective component's perceived hazard is low, consumers are less likely to be motivated to comply with a recall request. Conversely, if the perceived hazard of a component is high, consumers are more likely to be motivated to comply. Therefore: H4:  Component hazard positively moderates the impact of a regulator-initiated DMC on consumer recall compliance, such that the impact is stronger for high hazard components. The Moderating Role of Time for RepairThe effectiveness of a DMC also depends on the extent to which consumers perceive recall compliance to be convenient. Time-related costs have been shown to be a significant barrier to achieving desired levels of compliance from individuals following a request ([10]; [36]). In addition to providing other relevant information, a DMC informs consumers of the time needed to repair their products. Prior research has found that consumers perceive compliance to be more convenient if the time it takes to complete a task is shorter. For example, [ 4] find that consumers who view recycling as more convenient are more motivated to recycle; consumers weigh the costs of sacrificing time to separate recyclable items and then dispose of them if they are under the threshold of inconvenience. Similarly, [11] find time costs to be better predictors of compliance than monetary costs. Likewise, we argue that while a DMC's value lies in creating awareness that a defective component needs repair, a consumer's motivation to comply with a recall following the DMC will be lower if the amount of time required to repair the product is greater. Therefore, we hypothesize the following: H5:  Time for repair negatively moderates the impact of a regulator-initiated DMC on consumer recall compliance, such that the impact is weaker for components requiring more time to be repaired. The Moderating Role of Concurrent Recall CampaignsThe impact of a DMC on consumer recall compliance is likely to vary depending on whether there are concurrent recall campaigns for the same product. The rationale for examining concurrent recalls stems from research suggesting that repeated exposure to product warnings might result in consumers getting habituated to or even developing psychological reactance toward warning messages ([30]; [63]). Relatedly, some consumers may be skeptical of multiple instances of health-related product warnings from government information campaigns ([55]). In such situations, consumers might ignore safety warnings as they begin to perceive messages by regulatory agencies seeking compliance as less credible or even coercive. The detrimental effect of warning messages is particularly damaging for frequent users of the products that receive them ([30]). Accordingly, after a DMC makes consumers aware of recalls that have affected them, its impact on recall compliance is likely weaker for consumers facing concurrent recalls for different product issues relative to consumers facing a single recall. If the DMC makes consumers aware that their products are involved in concurrent recalls, they could become desensitized to recalls, leading to recall requests being ignored. Therefore, the impact of a DMC on recall compliance will be weaker for recalls including products involved in concurrent recall campaigns. H6:  Concurrent recall campaigns negatively moderate the impact of a regulator-initiated DMC on consumer recall compliance, such that the impact is weaker for products involved in concurrent recall campaigns. Research MethodologyThe setting for this study is the automobile industry in the United States. The NHTSA regulates product recalls in the U.S. automobile industry, and several of the characteristics of this setting make it an attractive one to explore our research objectives. For instance, safety in this industry is highly regulated, and manufacturers are mandated to report progress on product recall campaign completion rates periodically. As a result, longitudinal data on recall completion is publicly available, thereby facilitating a systematic investigation of recall compliance over time. The ""Safe Cars Save Lives"" CampaignIn 2014, the U.S. automobile industry experienced the largest product recall in its history. Several automobile manufacturers notified the NHTSA that they were conducting recalls to address a possible safety defect involving Takata airbag inflators. The concern was that the defective airbags were at risk of rupturing violently following a collision, hurling fiery shrapnel into drivers and passengers. Reports indicated that in addition to fatalities, hundreds of drivers had been injured. The scandal's scope escalated sharply in the following months, leading to a record-breaking 63 million vehicles recalled in 2014 and 51 million in 2015.Against this backdrop, the NHTSA launched a nationwide DMC, ""Safe Cars Save Lives,"" in January 2016. Even though the DMC was motivated by the Takata airbag inflator issue, the campaign was a part of the agency's effort to improve consumer compliance amongst all recall campaigns. Lamenting low recall completion, the U.S. Transportation Secretary noted that informed consumers were ""allies"" in improving recall compliance ([24]). The DMC was a full-coverage initiative that sought to push consumers to use the NHTSA's recall lookup webpage to check for open recalls and then fix defective vehicles quickly.The DMC utilized paid search and online display advertisements to influence recall compliance. A key part of the campaign was the use of Google AdWords to target consumers searching for recall-related information online through keywords such as ""Vehicle Recalls,"" ""Check for Recalls,"" and ""Is My Vehicle Recalled,"" among others. A primary goal of sponsored advertisements is to guide consumers searching for information to the right location ([22]). In addition, the NHTSA used online display ads on media platforms such as Facebook that also directed consumers to the NHTSA's recall lookup webpage where they could receive recall-related information. A sample online display advertisement from the DMC is presented in Figure WA1 of the Web Appendix. Between January of 2016 and March of 2017, the NHTSA spent approximately $1 million on sponsored advertisements on Google, Facebook, and other platforms ([28]).To gain a preliminary understanding of the DMC's effectiveness, we collected data on the paid search traffic to the NHTSA's recall lookup webpage using SEMrush Analytics, a leading vendor providing a competitive research service on online marketing and advertising. Specifically, we collected data on the total number of paid search visits to the NHTSA's recall lookup webpage for every calendar quarter-year in our data period. Figure 2 depicts the paid search traffic to the recall lookup webpage before and after the DMC. The mean quarterly recall-related paid search traffic in the post-DMC period (quarter 1 [Q1] of 2016 onward) is 48,770 visits compared with 650 quarterly visits in the pre-DMC period.Graph: Figure 2. Model-free evidence of automobile recall–related paid search traffic to the regulator's recall lookup webpage over time. Data Sources and MeasuresThe unit of analysis for the study is the recall campaign-calendar quarter-year. The NHTSA maintains a database of every vehicle safety recall campaign issued from 1966 onward. A typical recall notice provides information on the vehicle make and models affected, the number of vehicles recalled, the nature of the defect, and the date of the recall's announcement.The data on the cumulative number of vehicles fixed, the measure for consumer recall compliance, was sourced from the NHTSA's database. The agency mandates that manufacturers provide quarterly updates on the number of vehicles affected by a recall and the number of affected vehicles fixed. These updates allow the NHTSA to monitor recall completion on an ongoing basis. Specifically, manufacturers report the number of vehicles fixed in each calendar quarter-year, and these reports are aggregated by unique recall identification numbers. We also collected data from the NHTSA on individual recall characteristics such as the ages of the recalled vehicle models, the defective component in the vehicles, the estimated time needed to complete the repairs, and if multiple recall notifications were sent to consumers. In addition, we gathered data from LexisNexis on the number of press articles about each recall campaign to capture the media coverage of a recall. Furthermore, we collected make-level advertising and sales data from Kantar's ad$pender database and Ward's Automotive Yearbook, respectively. Table 1 presents the variables and data sources for the study.GraphTable 1. Data Sources and Operationalization. Variable NameOperationalizationData Source(s)Role of VariableConsumer Recall ComplianceThe cumulative number of vehicles fixed in a recall campaign.NHTSADependent variableTime from RecallThe number of quarters elapsed from the time a recall campaign began until the calendar quarter directly preceding the start of the DMC.NHTSAExplanatory variableTransition to DMCCoded as 1 if the observation is during or after the first quarter of 2016 (during which the DMC was active), and 0 otherwise.NHTSAExplanatory variableTime Since DMC StartThe number of quarters elapsed since the NHTSA's DMC began.NHTSAExplanatory variableMedia CoverageThe number of media mentions for an individual recall campaign beginning from the day of the recall's announcement until three months afterward.LexisNexisModerator variableAge of Recalled ProductThe age, in years, of the oldest model recalled in a recall campaign at each respective recall completion report's calendar quarter-year.NHTSAModerator variableComponent HazardA binary measure of the hazard of the defective component identified in a recall campaign. Components range from 0 (low hazard) to 1 (high hazard).NHTSAModerator variableTime for RepairThe estimated time, in hours, needed for the dealer to complete the recommended repair.NHTSAModerator variableConcurrent Recall CampaignsA binary measure coded as 1 for recall campaigns including vehicles involved in concurrent recall campaigns, and 0 otherwise.NHTSAModerator variableAdvertising IntensityMake advertising expenses, in dollars, normalized by make sales, in units.Kantar ad$pender; Ward's Automotive YearbookControl variableMultiple Recall NotificationsA binary measure coded as 1 if the recalling firm sent an additional recall notification to consumers affected by a recall campaign above and beyond the mandatory notification required by the NHTSA, and 0 otherwise.NHTSAControl variableRecall SizeThe total number of vehicles affected by a recall.NHTSAControl variable Because the goal of the study is to examine the effectiveness of the DMC launched in January 2016, we only included recall campaigns in the sample if they were active both before and after the DMC began and had recall completion data available. Accordingly, we did not include recalls that ended before the DMC began or were announced after the DMC began. The final sample includes recall completion data from 296 recall campaigns in the U.S. automobile industry from 2015 to 2017, totaling 1,809 recall campaign-calendar quarter-year observations. Dependent measureWe operationalize Consumer Recall Compliance, the dependent variable, as the cumulative number of vehicles fixed for individual recall campaigns on a quarterly basis. For example, if a recall campaign had 10,000 vehicles fixed in its first calendar quarter-year, the dependent variable would equal 10,000. If an additional 4,000 vehicles were remedied in the subsequent quarter, the dependent variable would equal 14,000. Given that the DMC was an ongoing effort by the NHTSA, the cumulative number of vehicles fixed measure allows us to examine the DMC's effectiveness over time. Measures for moderator variablesWe operationalize Media Coverage as the count of the number of articles in leading news publications that covered a recall campaign during the three months following its announcement. For example, if a recall began on January 1, 2015, and four news articles were published covering the recall from the date of its announcement to March 31, 2015, the Media Coverage variable for that recall campaign would equal 4.We operationalize the Age of Recalled Product variable as the number of years elapsed between the manufacturing year of the oldest vehicle model in a recall campaign and the calendar quarter-year in which we observe its cumulative number of vehicles fixed. For example, consider a recall announced in January 2015 featuring two vehicle models manufactured in January 2013 and September 2014. Age of Recalled Product during the first quarter of the recall campaign would be coded as two years, since the January 2013 model is the oldest in the recall campaign. In April 2015 (the next quarter), the variable would equal 2.25 years.To operationalize Component Hazard, we used the NHTSA's database to identify the defective component in each recall campaign and past literature to determine the component's hazard rating. Consumers learn of the nature of their vehicle's defect in recall notification letters sent by manufacturers. Extant literature has classified recalls into two categories on the basis of the hazard or severity of the issue ([ 1], [ 2]; [34]; [57]; [58]). Consistent with previous research, we operationalize Component Hazard as a dummy variable by classifying defective components related to the driving functionality of a vehicle (i.e., those that could result in a loss of vehicle control due to acceleration, steering or braking, frame corrosion, fire, or repeated stalling) as ""high hazard"" (coded as 1). All other components not directly related to the drivability of a vehicle are classified as ""low hazard"" (coded as 0). A nonexhaustive list of examples of components and their hazard ratings is reported in Table WA1 of the Web Appendix.We operationalize the Time for Repair variable in terms of the estimated time (in hours) needed to repair the defective component provided in recall notification letters.To operationalize Concurrent Recall Campaigns, we examined active recalls to determine if vehicles of the same make-model-year whose dates of production overlapped were involved in multiple simultaneous recalls during our observation window. For example, we used the NHTSA's database to check if 2015 Toyota Camry vehicles experienced multiple active recalls. We operationalize Concurrent Recall Campaigns as a dummy variable by coding recall campaigns including vehicles involved in concurrent recall campaigns as 1 and all other recall campaigns that did not include vehicles involved in concurrent recalls as 0. Measures for control variablesIn addition to the DMC, make-level advertising by firms could impact how aware consumers are of active recalls and thereby improve their compliance. Accordingly, we include the make-level Advertising Intensity variable in our specification to control for firm efforts to improve make awareness. We use advertising expenses data from Kantar's ad$pender database at the make level and normalize it by make sales in units using Ward's Automotive Yearbook to account for automaker efforts at improving make awareness.Although all automakers are mandated to notify consumers about a product recall, in some cases, they issue more than one notification for the same recall. Given that multiple notifications could improve recall completion, we include the dummy variable Multiple Recall Notifications to control for this possibility. The variable takes a value of 1 if a manufacturer notifies consumers affected by a recall more than once and 0 otherwise. Furthermore, we control for the size of a recall, as the total number of vehicles impacted by a recall campaign could positively impact the cumulative number of vehicles fixed. We assemble data on the Recall Size variable from the NHTSA's quarterly recall completion reports. Finally, we control for unobserved automobile make and temporal (i.e., calendar quarter) characteristics with the Make and Quarter dummy variables, respectively. Model SpecificationWe ground our empirical specification in the interrupted time series analysis (ITSA) framework, a technique commonly used to evaluate the effectiveness of universally implemented policies. ITSA is a quasiexperimental design typically utilized to assess the longitudinal effects of interventions with observational data where full randomization is not possible. ITSA is implemented using a regression model that includes up to three types of time-related covariates: ( 1) Time, ( 2) Transition, and ( 3) Time Since Intervention. The interpretation of the covariates is sensitive to their coding and the presence or absence of the other covariates; their inclusion in the model is theory driven. We employ an absolute coding approach, which allows the impact of an intervention to be interpreted in absolute terms (i.e., relative to zero) ([ 8]). The key identifying assumption of ITSA is that in the absence of an intervention, the slope in the outcome will remain unchanged in the postintervention period ([ 8]; [38]; [60]).For a model that includes all three covariates and utilizes absolute coding, Time from Recall is operationalized as the number of calendar quarters elapsed from the time a recall began until the calendar quarter preceding the start of the DMC. The coefficient of Time from Recall captures the slope in the outcome before the DMC begins. Transition to DMC is coded as 1 for all observations during the DMC (Q1 of 2016 and onward) and 0 for observations before the DMC (Q1–Q4 of 2015). In the presence of Time from Recall, the coefficient for Transition to DMC captures the absolute change in the outcome relative to zero in the first time period the intervention is active (i.e., immediately after it is introduced), and its effect remains relevant in the remainder of the observation window thereafter; it is not limited to the first quarter after the DMC. Time Since DMC Start is operationalized as the number of quarters elapsed since the NHTSA's DMC began in Q1 of 2016; it is coded as 0 for all observations before Q2 of 2016. In the presence of Time from Recall and Transition to DMC, the coefficient of Time Since DMC Start captures the slope of the outcome following the DMC relative to zero. Table 2 illustrates the coding of the three ITSA time-related variables for a recall campaign beginning in Q1 of 2015 and ending in Q2 of 2017.GraphTable 2. Illustration of Absolute Coding of Time-Related Variables in the ITSA Framework. QuarterYearTime from RecallTransition to DMCTime Since DMC Start12015000220151003201520042015300120163102201631132016312420163131201731422017315 Consistent with model testing procedures outlined in previous research ([ 8]), we examine whether there are quadratic trends in the dependent variable. Prior research on prescription drug use compliance has found quadratic patterns in some instances ([11]). We compare the model fit for the specification in Equation 1 with and without quadratic pre- and post-DMC slopes. The inclusion of Time Squared and Time Since Intervention Squared decreases the Akaike information criterion for the model from 41,791 to 41,751, indicating better model fit. Accordingly, we augment the specification by including the Time Squared and Time Since Intervention Squared variables.In our context, we argue that Consumer Recall Compliance is a function of Time from Recall, Time from Recall Squared, Transition to DMC, Time Since DMC Start, and Time Since DMC Start Squared. The Time from Recall and Time from Recall Squared variables capture the quarterly pre-DMC slope in recall compliance. Transition to DMC captures the absolute change, relative to zero, in recall compliance immediately after the DMC was introduced in Q1 of 2016. In our context, the impact of Transition to DMC is reflected over the remainder of the observation window and is not just limited to the first quarter after the DMC. Time Since DMC Start and Time Since DMC Start Squared capture the post-DMC slope in recall compliance (i.e., the impact of the DMC following its introduction).In summary, the empirical model controls for various recall campaign-level and time-specific factors that could impact consumer recall compliance. We also account for automobile make- and calendar-quarter-specific unobserved heterogeneity using fixed effects. The empirical specification to test the main effect of the DMC on consumer recall compliance is as follows: ConsumerRecallComplianceit=β0+β1TimefromRecallit+β2TimefromRecallSquaredit+β3TransitiontoDMCt+β4TimeSinceDMCStartt+β5TimeSinceDMCStartSquaredt+β6AdvertisingIntensityit+β7MultipleRecallNotificationsi+β8RecallSizei+β9–63Makei+β64–66Quartert+εit, Graph( 1)where i refers to a recall campaign, t refers to time (by calendar quarter-year), and ɛ refers to the random error. Results DescriptivesTable 3 presents the descriptive statistics for the variables. The mean media coverage in our sample is.44 articles. The mean age of the oldest vehicle in each recall is 4.09 years, with vehicles ranging from less than a month old to 19.06 years old. The mean estimated time needed to repair a defective component is 1.88 hours, with a few requiring as many as 13 hours. The mean hazard rating of the components involved in recalls is.19. While there is a wide range for the advertising intensity of different vehicle makes, the mean is.98. That is, approximately $1 is spent on advertising for every unit sold. In the full interaction model sample, the mean number of calendar quarters for a recall campaign is 6.04 (min = 3, max = 10, SD = .70). The mean number of calendar quarters in the pre-DMC period for a recall campaign in the full interaction model sample is 2.47 (min = 1, max = 4, SD = 1.17), whereas in the post-DMC period it is 3.57 calendar quarters (min = 1, max = 8, SD = 1.32).GraphTable 3. Descriptive Statistics. Variable NameMeanSDMinMax# of ObservationsConsumer Recall Compliance38,503105,58601,397,6121,809Media Coverage.441.250101,809Age of Recalled Product (in years)4.093.51.0419.061,655Component Hazard.19.39011,809Time for Repair (in hours)1.882.110131,809Concurrent Recall Campaigns.35.48011,809Advertising Intensity (in $/unit sales).989.510208.861,785Multiple Recall Notifications.03.17011,809Recall Size77,366196,46811,814,2841,809  Results for the Main Effect of the DMC on Consumer Recall ComplianceThe results of the DMC's main effect on Consumer Recall Compliance are reported in Model 1 of Table 4. We note that the results reported in this analysis pertain to 296 recall campaigns involving issues unrelated to airbag inflators. We exclude airbag inflator–related recall campaigns from the sample to avoid a potential endogeneity issue arising from the NHTSA's unobserved efforts to improve the recall completion of airbag inflator–related recalls specifically. The discussion of endogeneity and the analyses appear in Table WA2 of the Web Appendix.GraphTable 4. Results for the Main Effect of the DMC on Consumer Recall Compliance. Model 1Main-Effects ModelDependent Variable: Consumer Recall ComplianceEstimate (SE)Hypothesis TestedIntercept−12,560.21(27,360.13)Time from Recall14,172.90***(3,674.81)Time from Recall Squared−2,453.12**(1,168.53)Transition to DMC11,112.42**H1 supported (+)(4,393.39)Time Since DMC Start−1,959.86H1 supported (+)(2,894.73)Time Since DMC Start Squared1,748.90***H1 supported (+)(606.74)Advertising Intensity−1.32(209.64)Multiple Recall Notifications9,011.68(20,151.71)Recall Size.43***(.02)Make Fixed EffectsYesQuarter Fixed EffectsYes N (sample size)1,785R-Squared Within.09R-Squared Between.74  1 *p < .10.2 **p < .05.3 ***p < .01.The results in Model 1 of Table 4 reveal that the coefficients for Advertising Intensity and Multiple Recall Notifications are not significant (p > .65). As we expected, the coefficient for Recall Size is positive (.43, p < .01). The coefficient of Time from Recall Squared is negative (2,453.12, p < .05). To understand the linear effect of Time from Recall, we estimate a model specification identical to Equation 1 except that the quadratic terms are dropped, as the average linear effect of a variable is not interpretable in the presence of its quadratic effect ([17], pp. 200–201). The results (not shown) indicate that the coefficient for Time from Recall is positive (8,725.57, p < .01). These results collectively suggest that consumer recall compliance increased at a decreasing rate prior to the start of the DMC.The coefficient for Transition to DMC is positive (11,112.42, p < .05), indicating that, immediately following the DMC's introduction, the cumulative number of vehicles fixed increased in absolute terms in Q1 of 2016. In our context, the impact of Transition to DMC associated with Q1 of 2016 remains relevant in the remainder of the observation window thereafter and is not just limited to the first quarter after the DMC. The interpretation of Transition to DMC is contingent on the presence of Time from Recall in the model.In the presence of Time from Recall, Time from Recall Squared, and Transition to DMC, Time Since DMC Start and Time Since DMC Start Squared represent the slope of consumer recall compliance in the post-DMC period. The results in Model 1 of Table 4 indicate that the coefficient for Time Since DMC Start Squared is positive (1,748.90, p < .01). As before, to understand the linear trend in Time Since DMC Start, we estimate Equation 1 without the quadratic terms in the model ([17], pp. 200–201). The results (not shown) indicate that the coefficient for Time Since DMC Start is positive ( 6,036.62, _I_p_i_ < .01). These results collectively suggest that following the DMC's introduction, consumer recall compliance increases at an increasing rate (i.e., a positive accelerating curve), in contrast to the pre-DMC trend. H1 is supported.The visual plots in Figure 3 provide a representation of the DMC's effectiveness for an average-sized recall using the estimates provided in Model 1 of Table 4. In Figure 3, the solid line (""Number of vehicles fixed with the DMC"") denotes the DMC's impact on recall compliance for a recall campaign that begins in Q1 of 2015 and ends in Q4 of 2016. The solid line shows that recall compliance increased at a decreasing rate before the start of the DMC. The DMC begins in Q1 of 2016 after four quarters have elapsed, represented by the vertical dotted line. Between four and five quarters elapsed, the solid line captures the impact of the DMC in Q1 of 2016. Following Q1 of 2016, the solid line captures both the positive linear and positive quadratic trends in consumer recall compliance. Thus, Figure 3 reveals that the DMC interrupted the negative accelerating curve in consumer recall compliance in the pre-DMC period and resulted in a positive acceleration in the cumulative number of vehicles fixed in the post-DMC period.Graph: Figure 3. A visual representation of the DMC's effect on consumer recall compliance.To estimate consumer recall compliance in the absence of the DMC, we set the post-DMC time-related variables Transition to DMC, Time Since DMC Start, and Time Since DMC Start Squared to zero in Equation 1. In the pre-DMC period, the dotted line with circular dots (""Number of vehicles fixed without the DMC"") is overlapped by the solid line, as both depict the pre-DMC slope. The dashed line (""Predicted number of vehicles fixed without the DMC"") depicts the predicted trend in recall compliance in the post-DMC period without the DMC. In line with the negative quadratic trend in compliance before the DMC, the model predicts that the cumulative number of vehicles fixed in Q1 of 2016 would be lower than in Q4 of 2015. However, because the cumulative number of vehicles fixed over time cannot decrease, using the model-based results to predict compliance without the DMC in the post-DMC period would overstate the DMC's positive effect and lead to erroneous conclusions. A reasonable prediction in our context is that compliance would level off from Q4 of 2015 onward without the DMC. Accordingly, the predicted trajectory of consumer recall compliance without the DMC in Figure 3 in the post-DMC period is depicted as a flat line.To quantify the DMC's positive impact, we compute the difference in the predicted cumulative number of vehicles fixed with and without the DMC for a recall campaign after eight calendar quarters have elapsed using the coefficient estimates in Model 1 of Table 4 (represented by the solid line) and the dashed line in Figure 3. We find that in the first four quarters after it was introduced, the DMC increased the number of vehicles fixed, on average, by 20,712 per recall campaign above what would be expected without the DMC. Testing for Moderators of the DMC's EffectivenessTo test the moderating effects (H2–H6), we estimate the following specification: ConsumerRecallComplianceit=β0+β1TimefromRecallit+β2TimefromRecallSquaredit+β3TransitiontoDMCt+β4TimeSinceDMCStartt+β5TimeSinceDMCStartSquaredt+β6AdvertisingIntensityit+β7MultipleRecallNotificationsi+β8RecallSizei+β9MediaCoveragei+β10AgeofRecalledProductit+β11ComponentHazardi+β12TimeforRepairi+β13ConcurrentRecallCampaignsi+β14TransitiontoDMCt×MediaCoveragei+β15TransitiontoDMCt×AgeofRecalledProductit+β16TransitiontoDMCt×ComponentHazardi+β17TransitiontoDMCt×TimeforRepairi+β18TransitiontoDMCt×ConcurrentRecallCampaignsi+β19TimeSinceDMCStartt×MediaCoveragei+β20TimeSinceDMCStartt×AgeofRecalledProductit+β21TimeSinceDMCStartt×ComponentHazardi+β22TimeSinceDMCStartt×TimeforRepairi+β23TimeSinceDMCStartt×ConcurrentRecallCampaignsi+β24–78Makei+β79–81Quartert+εit, Graph( 2)where i refers to a recall campaign, t refers to time (by calendar quarter-year), and ɛ refers to the random error. There are two points worth noting about this specification. First, Equation 2 is identical to the main-effect specification in Equation 1, aside from the addition of the moderators and interaction terms. Second, the interaction terms between Transition to DMC and the moderators test for boundary conditions of the DMC's effectiveness at the moment it is introduced in Q1 of 2016. The interaction terms between Time Since DMC Start and the moderators test for the DMC's boundary conditions following its introduction from Q2 of 2016 onward.[ 7]The results for the moderating effects are reported in Model 1 of Table 5. None of the interaction terms in Model 1 have variance inflation factors (VIFs) above 5.17. The VIFs for Model 1 are reported in Table WA3 of the Web Appendix. While the lower-order terms of the time-related variables in Model 1 of Table 5 have VIFs above 10, our interest is only in interpreting the coefficients of the interaction terms. Furthermore, the interpretation of an interaction term is not affected by multicollinearity, as this multicollinearity affects neither the interaction term's coefficient nor its standard error ([42], p. 399). The same is true for the coefficients and standard errors of higher-order polynomials, which are essentially interactions of the lower-order terms ([17]).GraphTable 5. Results for the Effect of the DMC on Consumer Recall Compliance: Interaction Models. Model 1Model 2Model 3Model 4Dependent Variable: Consumer Recall ComplianceInteraction ModelInteraction Model Without Make Fixed EffectsInteraction Model with Only Linear Slope TermsInteraction Model with Only Linear Slope Terms and Without Make Fixed EffectsEstimate (SE)Hypothesis TestedEstimate (SE)Estimate (SE)Estimate (SE)Intercept–6,809.36–3,814.06–5,232.021,761.74(20,975.05)(6,009.95)(20,958.88)(5,769.30)Time from Recall14,838.04*** 13,896.02***8,929.75***8,562.00***(3,697.30)(3,603.27)(1,545.46)(1,467.90)Time from Recall Squared–2,165.48* –1,988.51*(1,194.20)(1,178.13)Transition to DMC–2,315.27 –1,157.12–3,212.90–2,432.85(6,250.76)(6,123.64)(5,763.04)(5,668.35)Time Since DMC Start1,243.44 652.762,195.532,076.94(3,492.59)(3,445.95)(2,136.19)(2,113.41)Time Since DMC Start Squared201.56 314.61(677.47)(666.82)Advertising Intensity–25.31 48.54–49.5031.68(211.92)(164.90)(211.58)(164.66)Multiple Recall Notifications6,328.66 5,699.317,362.646,778.29(15,073.09)(13,263.56)(15,078.72)(13,250.10)Recall Size.51*** .51***.51***.51***(.01)(.01)(.01)(.01)Media Coverage–2,105.46 –2,162.93–2,263.14–2,291.67(1,614.43)(1,546.53)(1,612.49)(1,544.60)Age of Recalled Producta–5,434.18*** –5,373.96***–5,438.46***–5,400.57***(938.50)(809.64)(939.49)(809.57)Component Hazard3,789.08 2,573.144,034.832,797.70(7,013.30)(6,388.82)(7,018.76)(6,388.20)Time for Repairb–518.81 274.31–571.21262.58(1,700.56)(1,242.39)(1,702.20)(1,242.49)Concurrent Recall Campaigns4,380.81 4,753.414,539.604,777.33(6,334.25)(5,499.19)(6,340.05)(5,499.64)Transition to DMC × Media Coverage3,993.22**H2 supported (+)4,076.06**4,288.84**4,309.28**(1,770.60)(1,758.61)(1,748.75)(1,737.81)Transition to DMC × Age of Recalled Producta2,171.51***H3 supported (+)2,111.26***2,083.69***2,018.78***(684.75)(680.06)(681.70)(677.03)Transition to DMC × Component Hazard–1,538.73H4 not supported (+)–925.12–1,336.34–807.79(5,546.05)(5,506.80)(5,536.56)(5,497.76)Transition to DMC × Time for Repairb538.61H5 partially supportedc (−)468.71556.37490.69(1,077.57)(1,071.98)(1,077.58)(1,072.11)Transition to DMC × Concurrent Recall Campaigns3,498.99H6 not supported (−)3,363.493,602.483,495.35(4,782.83)(4,759.03)(4,780.47)(4,757.08)Time Since DMC Start × Media Coverage1,499.02***H2 supported (+)1,455.20***1,519.93***1,497.27***(516.48)(511.32)(501.47)(497.09)Time Since DMC Start × Age of Recalled Producta884.17***H3 supported (+)878.30***899.60***900.10***(246.13)(244.12)(241.40)(239.48)Time Since DMC Start × Component Hazard–3,080.78H4 not supported (+)–3,350.67–3,062.90–3,293.08(2,334.58)(2,308.70)(2,324.63)(2,298.18)Time Since DMC Start × Time for Repairb–830.94*H5 partially supportedc (−)–785.30*–841.29*–799.98*(458.47)(454.04)(458.16)(453.75)Time Since DMC Start × Concurrent Recall Campaigns2,156.78H6 not supported (−)2,478.382,112.022,390.51(1,986.83)(1,971.75)(1,976.25)(1,960.38)Make Fixed EffectsYesNoYesNoQuarter Fixed EffectsYesYesYesYesN (sample size)1,6371,6371,6371,637R-Squared Within.07.08.07.08R-Squared Between.87.87.87.87 4 *p <.10.5 **p <.05.6 ***p <.01.7 a In years.8 b In hours.9 c The interaction between Transition to DMC and Time for Repair is not significant. However, the interaction between Time Since DMC Start and Time for Repair is marginally significant (p =.07). Therefore, H5 is partially supported.10 Notes: The full set of results, with VIFs for the variables, is reported in Table WA3 of the Web Appendix.We find that the coefficient of the interaction between the Transition to DMC and Media Coverage variables is positive (3,993.22, p < .05). This finding suggests that at the moment the DMC was introduced in Q1 of 2016, its impact on consumer recall compliance was stronger for recall campaigns with greater media coverage. We also find that the coefficient of the interaction between Time Since DMC Start and Media Coverage is positive (1,499.02, p < .01), implying that Media Coverage continues to positively moderate the DMC–consumer recall compliance relationship from Q2 of 2016 onward. H2 is thus supported.We use visual plots in Figure 4, Panel A, to provide an intuitive understanding of interactions with Media Coverage. We set ""low"" (0) and ""high"" ( 1) values for Media Coverage to visually represent its impact on the DMC's effectiveness for a recall campaign of average size that spans from Q1 of 2015 until Q4 of 2016. As Panel A shows, the DMC's impact is stronger when media coverage for a recall is high relative to low.Graph: Figure 4. The moderating roles of media coverage, age of recalled product, and time for repair during the DMC.We also find that the coefficient of the interaction between Transition to DMC and Age of Recalled Product is positive (2,171.51, p < .01). That is, the DMC's impact on consumer recall compliance is stronger for older (vs. newer) recalled vehicles at the moment it is introduced in Q1 of 2016. We also find that the coefficient for the interaction between Time Since DMC Start and Age of Recalled Product is positive (884.17, p < .01), which suggests that the DMC also has a stronger impact on consumer recall compliance for older vehicles from Q2 of 2016 onward. H3 is supported. We set ""low"" (25th percentile) and ""high"" (75th percentile) values of the Age of Recalled Product variable in Figure 4, Panel B, which demonstrates that the DMC's effectiveness is stronger for older vehicles (i.e., for larger values of the variable) both at the moment of the DMC's introduction and thereafter.Inconsistent with H4, we find that neither the Transition to DMC × Component Hazard nor the Time Since DMC Start × Component Hazard interaction terms are significant (p > .78 and p > .18, respectively). Perhaps this insignificant finding implies that consumers are unable to actually discern the hazard of the defective components cited in recall notification letters.We find that the interaction between Transition to DMC and Time for Repair is not significant (p > .61). However, the coefficient for the interaction between Time Since DMC Start and Time for Repair is negative (830.94, p = .07) and marginally significant. Thus, following the DMC's introduction from Q2 of 2016 until Q4 of 2017, the DMC is less effective at improving consumer recall compliance as the time needed to repair a defective component increases. H5 is partially supported. Using ""low"" (25th percentile) and ""high"" (75th percentile) values for Time for Repair in Figure 4, Panel C, we demonstrate visually that following the DMC's introduction, the DMC's effectiveness is weaker when a defective component requires more versus less time to repair.Finally, we find that neither the Transition to DMC × Concurrent Recall Campaigns nor the Time Since DMC Start × Concurrent Recall Campaigns interaction variables are significant (p > .46 and p > .27, respectively). H6 is not supported. The coefficients for the Advertising Intensity and Multiple Recall Notifications variables are also not significant (p > .67), while the coefficient for the Recall Size variable is positive (.51, p < .01).The moderator results are robust to several alternate model specifications. We estimate three specifications similar to Model 1 in Table 5, removing automobile make–specific fixed effects and/or Time from Recall Squared and Time Since DMC Start Squared. The results of the alternate specifications are reported in Models 2, 3, and 4 of Table 5. Across each model, the estimates of the interaction terms are largely similar to Model 1, and the conclusions regarding the impact of the moderators on the DMC's effectiveness remain the same. DiscussionThis study addresses the important question of whether a regulator-initiated DMC can improve consumer recall compliance. Further, the study aims to understand the conditions under which a DMC is more or less effective. We were able to exploit a full-coverage national DMC launched in January of 2016 by the NHTSA, the regulator in the U.S. automobile industry, to answer these research questions. The study offers several implications for researchers and policy makers. Research ImplicationsThe question of whether awareness campaigns are successful in eliciting the required compliance-related behaviors from consumers has been investigated in various empirical settings (e.g., [32]; [66]). The fact that the awareness campaign in our study is introduced by a governmental agency and that it is its first major effort to improve recall compliance using digital media raises questions about its potential efficacy. We find that a regulator-initiated DMC is effective at improving consumer recall compliance, as reflected by the additional number of vehicles fixed above what was to be expected without it. This finding underscores the critical importance of regulators utilizing digital means to provide recall-related information to elicit greater compliance from consumers.The moderator analyses reveal that the DMC's effectiveness varies across recall campaigns. Specifically, the DMC's impact on consumer recall compliance is stronger for recall campaigns with greater media coverage. This finding is similar in spirit to research that has documented the moderating impact of negative publicity on postcrisis advertising and consumers' brand share and category purchases ([16]). Although brands aim to avoid negative publicity of their recalled products because of its adverse effects on sales, media outlets play an important functional role in reinforcing a regulator's efforts to improve consumer recall compliance. In our context, this effect is likely driven by the varying specificity of information provided by the regulator and the media.We also find that the DMC is more effective when the recalled products are older. Prior research has found that the utility of paid search campaigns is greater in contexts where consumers have low levels of familiarity with the firms' brands or products ([ 7]). Along similar lines, we find that the DMC is more effective when the recalled products are older. Consumers of older products may not be in contact with their products' OEMs as their warranties expire and thereby be less familiar with safety-related developments for their vehicles. A DMC can provide resources for these consumers who are more likely to seek out recall-related information digitally.Finally, we find that the DMC is less effective at improving consumer recall compliance for recall campaigns containing products with defects that require more (vs. less) time to repair. Previous work examining tax compliance similarly finds that time-related costs are significant barriers to compliance ([10]; [64]). Although our study finds that the DMC is effective at providing the necessary information to improve compliance, the inconvenience that higher repair times pose to consumers is still a significant impediment to compliance. Future research should examine whether regulators can improve compliance for recalls that require long repair times by using persuasive appeals. Public Policy ImplicationsOur study's findings also offer actionable insights for policy makers. In recent years, multiple audit reports submitted to U.S. Congress have questioned the adequacy of the NHTSA's oversight processes in managing consumer recall compliance ([52]). The overarching concern is that the agency has failed to carefully review safety issues, hold automakers accountable, collect safety data, or adequately train its staff, resulting in significant safety concerns being overlooked. Some industry experts go a step further and lament that product recalls may increase the number of crashes on the road, as the extra driving needed to remedy the issues heightens the probability of accidents ([43]). Our analysis reveals that in the first four quarters after it was introduced, the DMC increased the number of vehicles fixed, on average, by 20,712 per recall campaign over what would be expected without the DMC.The improvement in consumer compliance as a result of the DMC is economically meaningful because it could reduce the number of vehicle accidents and the economic costs associated with vehicle crashes. Research has shown that, on average, a 1% improvement in recall compliance in the automobile industry lowers the number of vehicle accidents in the next three years by.46% (Bae and Benítez-Silva [ 1]). The average economic costs (e.g., fatalities, nonfatal injuries, damaged vehicles) associated with a motor vehicle accident are about $14,000 ([ 9]). Therefore, by improving consumer recall compliance, the DMC reduces the number of automobile accidents on the road and lowers the total economic costs associated with accidents. The last few years have witnessed an average of 953 product recalls in the U.S. automobile industry, suggesting that the total annual economic impact of improved consumer recall compliance across all recall campaigns is not trivial.Further, our study implies that a lack of available relevant information is a significant contributing factor to low consumer recall compliance. For years, the NHTSA has tried to mandate electronic recall notices to counteract the issue of low consumer recall awareness ([46]). The regulatory agency's foray into the digital domain with ""Safe Cars Save Lives"" has proven successful, suggesting that improving consumer awareness of recall-related information is a crucial step in improving consumer recall compliance. Even though the automobile industry receives considerable media attention (e.g., GM's faulty ignition issue, Takata's airbag failures), getting consumers to pay attention to recall notifications is challenging.While extant work notes that manufacturers view greater media coverage of a recall as damaging to their brands' financial health ([ 5]), we find that media coverage helps improve safety outcomes by increasing the effectiveness of the DMC. Furthermore, we find that the DMC's impact on consumer recall compliance is stronger when recalled vehicles are older. Issues of low compliance are particularly prevalent among owners of these older products. According to J.D. [54], just 44% of vehicles manufactured between 2003 and 2007 had their defects remedied, drastically below recall completion percentages of 73% for vehicles of model years 2013–2017. Federal and industry leaders have cited improving compliance among owners of older vehicles as one of four key topic areas to address moving forward ([51]). The DMC's effectiveness on consumers owning older products further suggests that regulators' use of digital tools to facilitate consumer access to relevant information could improve compliance.Finally, our findings caution regulators to be mindful of perceived time inconvenience as a serious impediment to consumer recall compliance. While the DMC is effective at improving compliance, its impact is lower for recall campaigns with defective components that require longer to repair. Interestingly, consumers often do not cite the time needed to complete the repair as the most important factor in deciding whether to remedy safety defects ([28]). Yet, our findings suggest that perceived time inconvenience is a serious obstacle to achieving consumer compliance. The findings should enable regulatory agencies to make more compelling cases for financial resources to be devoted to DMCs aiming to improve compliance. Limitations and Future DirectionsThe findings of the study are subject to some limitations. First, the study is limited to the U.S. automobile industry and a single DMC, implying that caution is warranted in generalizing our findings to other settings. If systematic recall completion data were available in other contexts, the conceptual and methodological framework employed in this study could be useful in deriving empirical generalizations about the effectiveness of DMCs in other settings. Second, data on the NHTSA's total advertising expenditure on this campaign were not available. As such, the DMC's positive effect should not be interpreted as consumers' responsiveness to advertising dollars. Third, our study was unable to examine if a recall notification message's content impacts compliance because the mandated recall notification letters sent from manufacturers to consumers in our setting were standard, lacking variation. If regulators use assertive versus nonassertive language or fear- versus health- versus norm-based persuasion appeals in their advertisements or messaging, understanding which types improve compliance would be valuable. Supplemental Materialsj-pdf-1-jmx-10.1177_00222429211023016 - Supplemental material for Regulating Product Recall Compliance in the Digital Age: Evidence from the ""Safe Cars Save Lives"" CampaignSupplemental material, sj-pdf-1-jmx-10.1177_00222429211023016 for Regulating Product Recall Compliance in the Digital Age: Evidence from the ""Safe Cars Save Lives"" Campaign by Sotires Pagiavlas, Kartik Kalaignanam, Manpreet Gill, and Paul D. Bliese in Journal of Marketing  "
17,"Sales and Self: The Noneconomic Value of Selling the Fruits of One's Labor A core assumption across many disciplines is that producers enter market exchange relationships for economic reasons. This research examines an overlooked factor; namely, the socioemotional benefits of selling the fruits of one's labor. Specifically, the authors find that individuals selling their products interpret sales as a signal from the market that serves as a source of self-validation, thus increasing their happiness above and beyond any monetary rewards from those sales. This effect highlights an information asymmetry that is opposite to what is found in traditional signaling theory. That is, the authors find that customers have information about product quality that they signal to the producer, validating the producer's skill level. Furthermore, the sales-as-signal effect is moderated by characteristics of the purchase transaction that determine the signal strength of sales: The effect is attenuated when product choice does not reflect a deliberate decision and is amplified when buyers incur higher monetary costs. In addition, sales have a stronger effect on happiness than alternative, nonmonetary forms of market signals such as likes. Finally, the sales-as-signal effect is more pronounced when individuals sell their self-made (vs. other-made) products and affects individuals' happiness beyond the happiness gained from producing.Keywords: self-production; signaling; selling; happiness; self-validationDigital platforms such as Etsy or Amazon Handmade have made it easy for individuals to sell their self-made products to other individuals. Commercial activities that were previously limited to economically marginal contexts such as flea markets have become big business. Although each individual producer's commercial activity might be small, the sheer number of such producers adds an important new source of competitive pressure for traditional firms in many industries, including clothing, food, and home furnishings. For example, in 2020, Etsy had around 4.4 million sellers and 82 million buyers, leading to a total transaction value of around $10 billion ([39]).Existing research has investigated the psychological and behavioral consequences of engaging in self-production. In short, people like the fruits of their labor and value products they produce themselves more than comparable products made by others ([17]; [31]). The higher valuation of self-made products stems from the sense of accomplishment, pride, and competence that individuals experience when they successfully self-design or assemble a product ([ 9]; [29]). Prior research has focused on individuals engaging in self-production with the objective of consuming the product themselves or giving it as a gift ([30]), but it provides little insight into the increasingly common situation in which individuals make products with the objective of selling them to ""the market""; that is, to unknown others.A common assumption in marketing, economics, and entrepreneurship is that producers participate in market exchanges for economic reasons ([18]; [36]). Despite this disciplinary emphasis on economic motives, it seems possible that individuals produce and subsequently sell products also for noneconomic reasons. Research in organizational behavior ([19]), economics ([ 2]), and entrepreneurship ([37]) has drawn attention to the socioemotional motivations of producing, but little attention has been paid specifically to the noneconomic benefits of selling the fruits of one's labor. Drawing on survey data from the field as well as a series of experiments, we document a sales-as-signal effect: Selling their self-made products increases individual producers' happiness above and beyond any monetary implications from these sales. This effect occurs because individual producers interpret the number of products sold as a market signal that validates their skills and competencies as producers.Our research makes several contributions. First, in demonstrating the socioemotional consequences of selling one's creations, we introduce a novel perspective on the value of sales. We propose that above and beyond their importance as a source of monetary income, sales can also have socioemotional value by providing a source of self-validation. Economic theory generally conceptualizes supply-side agents as profit-maximizers, such that ""managers of a firm make those choices that maximize the sum of current and future profits"" ([15], p. 769). Consequently, one would expect that the value individual producers derive from selling their products is a function of the money they make from these sales. However, we demonstrate that the value individual producers gain from sales cannot be solely defined in economic terms. Instead, individual producers also gain considerable happiness via feelings of self-validation from selling their products. We thus provide evidence for the importance of the noneconomic value of participating in market exchanges.Second, our findings contribute to the literature examining individuals' valuation of their self-made products ([17]; [30]; [31]). Whereas existing research has shown that individual producers feel competent and proud from successfully designing or assembling a product, our work examines the psychological consequences of selling self-made products and not of merely producing those products. We demonstrate that having actual buyers purchase one's self-made products functions as an external confirmation that the individual producer is competent and capable of creating a high-quality, marketable product, which fuels one's happiness beyond the happiness derived from production.Third, we offer a novel perspective on the role of signals in market exchanges by conceptualizing sales as a signal from the market. Research in economics, marketing, and management has typically conceptualized marketplace signals as actions taken by sellers to convey information about unobservable product qualities to buyers ([ 5]; [ 8]; [23]). Thus, signals are traditionally sent by sellers and interpreted by buyers. In contrast, we propose that sellers interpret the act of purchasing a product as a signal from the buyer that validates the seller's skills and competencies as producer. Furthermore, whereas signals are usually conceptualized as intentional actions meant to benefit the sender ([ 8]), we propose that sending this type of signal to the seller is often incidental to buyers' core motivation to buy, which lies in their consumption goals, and that the signal does not directly benefit the sender but rather the receiver of the signal. Moreover, traditional signaling research assumes that sellers know the quality of their product and that buyers are uncertain. Our work indicates that sellers are, to some extent, uncertain about their own product and thus about their competencies as a producer. Our work indicates that buyers have information about product quality that reduces sellers' uncertainty. Therefore, we propose that individual producers interpret sales as a signal from their buyers that serves as a source of self-validation.Fourth, our findings broaden the discussion on the societal role of market exchanges, a topic of intense interest among marketing scholars and practitioners ([ 7]). We add to this discussion on how marketing can help create a better world by drawing attention to the way selling might provide a positive source of meaning and happiness for individuals. Just like the social costs of marketing have often been underestimated, we argue that some important benefits have been neglected as well. Specifically, successfully marketing their products is a source of self-validation and happiness for producers. Theory Benefits of Selling Self-Made ProductsWhat benefits do individuals derive from selling their self-made products? Why do they continue to populate online marketplace platforms? The principal and most obvious benefit that people derive from selling their products is money. But can monetary incentives alone explain the increasing popularity of online marketplaces? We propose that learning a customer bought their products increases individual producers' happiness above and beyond the monetary implications of these sales.Specifically, we argue that sales validate one's skills and competencies as a producer. We found preliminary support for this notion in an exploratory qualitative study conducted as part of a master's thesis supervised by one of the authors ([42]). In ten in-depth interviews with Etsy sellers and nonobtrusive observations of Etsy's online discussion forums (see Web Appendix A [WA-A]), several informants highlighted that selling their products makes them happy; for example, ""creating something that I like and others like enough to spend their hard-earned money on, is bliss"" (Informant #7 from Etsy forum), and ""it's so flattering when people choose to buy your creations"" (#61, forum). The narratives suggest that the happiness derived from selling is not necessarily rooted in economic reasons but in one's perceived self-worth as a producer; for example, ""Etsy allows me to rediscover my worth"" (#31, forum). Describing a producer friend, one of our informants (#4, interview) stated, ""for Jeani, I think it is the fact that she is making something someone thinks is worthy of buying. You know, paying some money for and it's like an accolade of her creative talent."" Interestingly, the narratives suggest that the increased self-worth derived from selling motivates people to continue producing their own products; for example, ""a sale...usually motivates me to make more, since it makes me feel as though my items are appreciated."" In summary, our preliminary qualitative insights point to the possibility that sales make individual producers happy not only because of the monetary gains but also because sales more fundamentally validate their skills and competencies as a producer. The Sales-as-Signal EffectWe argue that sales function as a signal from the marketplace that boosts individual sellers' self-validation. Signaling theory was developed in information economics to study market interactions under conditions of information asymmetry between sellers and buyers ([38]). It generally assumes that sellers are aware of the quality of their goods but buyers are not. To distinguish low-quality sellers from high-quality sellers, buyers must detect and interpret the signals sent by sellers. Prices, advertising, brands, and different types of firm actions can constitute signals ([ 8]; [22]; [23]). Moreover, signaling theory and its applications in marketing usually presume that the signal originates from a seller and is received by a buyer.[ 7] In our research, we propose that sales constitute a signal that is sent by buyers (with or without buyers' intention to actually signal something) and that validates the seller's competencies as a producer. Thus, in this context, there is an information asymmetry in the opposite direction from traditional signaling theory: It is the buyer who has information that reduces the seller's uncertainty (about the seller's skills and competencies about the producer).Feelings of competence, which often result from others' validation of one's skills, are a central motivation for people to engage in creative tasks ([ 9]) and greatly affect how satisfied individuals are with their work ([33]). Crucially, feeling competent is a fundamental psychological need among humans, and its fulfillment strongly determines individuals' intrinsic motivation, life satisfaction, and mental health ([34]; [41]). [11], p. 231) even argue that the need to feel competent ""must be satisfied for long-term psychological health."" We thus propose that being validated as a competent producer through sales increases individual producers' happiness.In summary, we predict a sales-as-signal effect: Sales increase individual producers' happiness, even when controlling for the effect of monetary gains. This is because producers interpret sales as a positive signal from the market that validates them as competent producers. Formally: H1:  Sales increase individual producers' happiness above and beyond the monetary rewards from these sales (i.e., the sales-as-signal effect). H2:  The sales-as-signal effect is mediated by feelings of self-validation as a producer. Moderators of the Sales-as-Signal EffectThe strength of a signal is determined by the extent to which receivers interpret the signal as credible ([ 5]). Signals are perceived as more credible the more they are able to provide information about products' unobservable quality ([ 6]; [ 8]) and the higher the costs and associated risk in sending a signal ([ 3]; [23]). Accordingly, we predict that the strength of the sales signal will depend on at least two characteristics of the purchase transaction: ( 1) the extent to which the product choice reflects a deliberate decision and ( 2) the monetary cost involved in purchasing the product. We decided to focus on these two moderators because they provide an internally valid test of our proposed underlying process (self-validation) and provide actionable implications for the management of online marketplaces. Deliberateness of the product choiceSales should more credibly inform individual producers about their competencies the more the sales are a direct consequence of the quality of individual producers' products ([ 6]; [ 8]). Thus, the sales-as-signal effect should be stronger the more the buyer is seen as making a deliberate decision to acquire the product. That is, sales should be self-validating when the buyer intentionally chooses a producer's product, but much less so when the product was selected in a way that does not reflect the buyer's preference for a specific product. Examples of the latter include a chef's choice item on a menu, a surprise wine box subscription, the specific vegetables offered by a community-supported agriculture farm co-op, a ""mystery car"" car rental option, a sneak preview at a movie theatre, or a sweepstakes in which it is not clear in advance which participant will receive which prize. We hypothesize that, above and beyond the monetary rewards from sales, individual producers will feel greater self-validation, and thus happiness, the more the purchase appears to be the result of a buyer's deliberate decision. H3:  The sales-as-signal effect is stronger when the product purchase reflects a more (vs. less) deliberate decision. Cost of the purchaseIf signal credibility is a function of signal cost ([ 3]; [23]), varying the cost of buying a product should alter the strength of the sales-as-signal effect. The higher the costs involved in purchasing a product, the more credibly the sales signal should inform individual producers about their competencies as a producer—even if the higher costs do not translate into higher monetary rewards for the individual producer (such as when the buyer bears higher shipping costs). We hypothesize that individual producers will feel higher levels of self-validation, and thus happiness, when the buyer incurs higher monetary costs, even when the higher monetary costs do not lead to higher monetary income for the individual producer. H4:  The sales-as-signal effect is stronger when buyers incur higher (vs. lower) monetary costs. Sales Versus Other Forms of Market SignalsSales are not the only signals consumers might send. In addition to buying products, consumers may also signal quality through noneconomic signals, such as writing a review or liking a product or company on social media. One might argue that such noneconomic signals might have a stronger self-validating effect than sales because the noneconomic signals are sent intentionally (vs. being a usually unintentional by-product of the decision to buy). However, we propose that sales would be a more credible signal because they may be seen as more informative about the product's unobservable quality for several reasons.First, other forms of signals such as online reviews have been criticized for being unable to reveal a product's actual quality ([12]), and public displays of support for a cause on social media (e.g., Facebook likes) are often unreliable indicators of one's willingness to support the cause when doing so is costly ([26]). Thus, noneconomic signals such as likes may be seen as ""cheap talk."" In contrast, customers should decide to purchase a product only if they really deem the product to be of high, or at least sufficiently high, quality ([14]). Second, selling products may evoke the specific norms that are associated with an exchange domain rather than a relational domain. In an exchange domain, the normative signal of value may be sales rather than more relational signals such as likes.[ 8] In this domain, having buyers purchase one's product might be the ""ultimate"" form of appreciation of an individual's competencies as a producer. Thus, sales may more credibly inform individual producers about their skills and competencies compared with other common forms of market signals such as likes, even when the cost to the customer of sales and likes are kept the same. We hypothesize that individual producers will feel greater self-validation, and thus happiness, from sales than from noneconomic signals, even when sales do not lead to higher monetary rewards to the seller than noneconomic signals (and even when monetary cost to the customer is kept constant). H5:  Sales increase individual producers' happiness more than noneconomic signals above and beyond the monetary rewards from those sales to the producer (and above and beyond the monetary cost of sending those signals). Happiness from Selling Self-Made Versus Other-Made ProductsWe further propose that individuals derive greater happiness from sales when selling self-made products as opposed to selling products made by someone else. Our hypothesis is that the effect of sales of self-produced goods on happiness is, to an important extent, driven by validation of the producer's skills and competencies as a producer. Of course, selling self-made products might also validate individual producers' skills as a seller; that is, successful sales may be interpreted as a signal that the seller of self-produced products is a competent marketer rather than a competent producer. However, unlike selling one's self-made products, selling products made by others cannot validate one's skill as a producer. Thus, the effect of sales on happiness should be larger for self-produced than for other-produced products. H6:  The sales-as-signal effect is stronger among individuals selling self- (vs. other-) made products. Overview of StudiesWe tested our propositions in eight studies (N = 4,970). Study 1 and a supplementary study provide an initial exploration of our main hypothesis that sales increase individual producers' happiness above and beyond the monetary rewards from these sales (H1). In Study 1, we surveyed actual producers selling their self-made products (e.g., on Etsy). We find a positive relationship between sales and individual producers' happiness, controlling for the monetary implications of sales. In a supplementary study, we replicated this finding experimentally using a recall task among a sample of actual producers.In Studies 2–6, we provide further evidence for the core sales-as-signal effect, explore the underlying mechanism, examine boundary conditions, and investigate the incremental effects of sales among different samples of producers. Studies 2 and 3 show that the sales-as-signal effect is driven by feelings of self-validation (H2). These studies also show that the strength of the sales-as-signal effect depends on ( 1) the extent to which the product choice reflects a deliberate decision (H3) and ( 2) the monetary cost involved for the consumer in purchasing the product (H4). Study 4 and a supplementary study demonstrate that sales increase individual producers' happiness more than receiving likes (H5). Study 5 shows that the effect of sales on happiness is stronger when individuals sell self-made products than when they sell products made by others (H6). Finally, Study 6 shows that the sales-as-signal effect is different from the mere happiness individuals gain from producing. Study 6 also shows that trying to sell one's self-produced products can backfire when low sales become a signal of low competency. Study 1Study 1 provides an initial exploration of our core prediction that selling their products increases producers' happiness over and above any monetary rewards from those sales. We do so through a field survey of actual producers selling their self-made products online. MethodTo engage this special and difficult-to-recruit population (individual producers), we worked with the administrators of eight Facebook groups of producers of handmade goods to promote our survey. As an incentive to participate, each participant received a $5 Amazon voucher (see WA-B1 for detailed study materials). After agreeing to a data protection disclaimer, we told participants that the aim of the study was to gather knowledge about their life as producers of handmade goods. To increase the comparability of the responses, we asked participants to think about the last four weeks when answering the questions.The survey first captured our dependent variable, participants' satisfaction and happiness with their life as a producer of handmade products, which we measured with two items (r = .61; see survey scales in Table 1). We measured our key independent variable, one's current sales (i.e., the number of products sold), using two variables. First, we assessed the sales volume by asking participants to indicate the total number of items sold in the past four weeks. Second, we assessed sales growth by capturing how many items a given producer sold in the last four weeks compared to the average number of items sold per month in the last six months. The comparative nature of the measure is important because it is the within-person variance that most strongly predicts one's happiness at any given point in time ([35]).GraphTable 1. Survey Scales (Study 1). VariableMeasuresHappiness""If you think about the past four weeks, how satisfied are you with your life as a producer of handmade products?"" (1 = ""extremely dissatisfied,"" and 7 = ""extremely satisfied"") and ""if you think about the past four weeks, how happy are you with your life as a producer of handmade products?"" (1 = ""extremely unhappy,"" and 7 = ""extremely happy"")Sales volume""Over the past four weeks combined, how many handmade items did you sell?""Sales growth""Now, please compare the number of items that you sold in the past four weeks with the average number of items you sold per month in the past six months. Would you say that you sold more or fewer items in the past four weeks compared to the months before?"" (1 = ""much fewer,"" and 7 = ""much more"")Revenue""What was the total revenue on these items in the past four weeks? That is, how much money did you make by selling these items in the past four weeks?""Profit""How much profit did you make in the past four weeks by selling these items? That is, after subtracting all costs, how much money were you able to keep in your pockets?""Future profit expectations""If you think about the near future, how do you think your profits from selling your handmade products will develop?"" (1 = ""decrease a lot,"" and 7 = ""increase a lot"")Socioeconomic status""I don't think I'll have to worry about money too much in the future,"" ""I don't need to worry too much about paying my bills,"" and ""I have enough money to buy things I want"" (1 = ""totally disagree,"" and 7 = ""totally agree"") We included the following control variables to empirically isolate the sales-as-signal effect from a series of alternative explanations. We captured the direct monetary implications of sales by asking for the respective revenue and profit (in USD) in the said time period.[ 9] Although these measures are important for assessing our hypothesis (i.e., to control for any monetary implications of selling), they do not account for any future profit expectations. For example, one could argue that a positive sales trend in the current period might be (perceived to be) diagnostic of future sales and thus profit developments. Thus, a given happiness level at a given point in time might be due to future profit expectations based on the comparative number of items sold. To account for this alternative explanation, we asked participants how they expected their future profits to develop.GraphTable 2. Ordinary Least Squares Regressions on Happiness (Study 1). (1a)(1b)(2a)(2b)(3a)(3b)(4a)(4b)Ln(sales volume).06**(.03).05**(.03).07***(.02).06***(.02)Sales growth.34***(.02).34***(.02).19***(.03).19***(.03)Ln(revenue)−.01(.02).002(.02).01(.01).01(.02)Ln(profit)−.002(.02).01(.02).01(.01).02(.02)Future profit expectations.23***(.03).23***(.03).15***(.03).15***(.03)Ln(experience).10**(.05).10**(.05).13***(.05).13***(.05)Main job.07(.09).06(.09).10(.09).10(.09)Proportionate household income−.004**(.002)−.004**(.002)−.003*(.002)−.004**(.002)Socioeconomic status.28***(.03).28***(.03).21***(.03).21***(.03)Age.01(.004).005(.004).01(.004).01(.004)Gender−.07(.08)−.08(.08)−.02(.08)−.02(.08)Education controlsNoNoYesYesNoNoYesYesProduct category controlsYesYesYesYesYesYesYesYesObservations828828828828828828828828R2.07.07.32.32.25.25.35.35 1 Notes: Unstandardized regression coefficients. Standard errors in parentheses. Education controls: bachelor's degree is baseline. Product controls: jewelry is baseline. Main job: 0 = side job, 1 = main job. Gender: 0 = female, 1 = male.2 *p <.10.3 **p <.05.4 ***p <.01.In addition, we captured a series of control variables with regard to the business type and the producer. We captured the product domain(s) by asking what type(s) of products they sell (see WA-B2). We further asked participants about their experience as a producer (how long they have been selling their products [in months]), whether selling these products is their main or side job (0 = ""side job,"" and 1 = ""main job""), and how much the income from selling these products contributes to their total household income (in %). To assess each individual producer's socioeconomic status, we used a three-item scale (α = .82; [20]). Finally, participants indicated their gender (0 = ""female,"" and 1 = ""male""), age (in years), country of residence (0 = ""United States,"" and 1 = ""other""), relationship status (0 = ""single,"" 1 = ""committed relationship,"" 2 = ""married,"" and 3 = ""widowed""), and highest degree of education (see WA-B3). No further measures were taken. Sample DescriptionThe sample consisted of 828 individual producers (Mage = 35.22 years, SD = 8.43, 61.0% female; 90.6% U.S. residents). We successfully recruited a diverse sample of producers. Participants differed widely in their experience selling their products (M = 56.89 months, SD = 40.59, range: 0 to 360.00), revenue and profit (revenue: Mdn = US$4,100.00, M = US$44,904.99, SD = US$81,115.39, range: US$0 to US$520,000.00; profit: Mdn = US$2,000.00, M = US$14,545.44, SD = US$27,234.03, range: US$0 to US$350,000.00), percentage of household income from selling (M = 54.94%, SD = 34.51%, range: 0% to 100%), and types of products produced (see WA-B4). Because several of our open-ended measures were highly skewed (sales volume, revenue, profit, and experience), we log-transformed those variables (for descriptive statistics and interconstruct correlations, see WA-B5). ResultsWe tested the sales-as-signal effect by estimating a series of ordinary least squares (OLS) regression models accounting for different sets of control variables. Columns 1a and 1b in Table 2 report the results of regression models in which one's happiness as a producer, our dependent variable, is regressed on sales volume while controlling for product type(s) and for the economic implications of sales (in terms of either revenue or profit). We find that sales volume is indeed positively and significantly related to one's happiness, controlling for economic implications (Column 1a: b = .06, SE = .03, p = .02; Column 1b: b = .05, SE = .03, p = .04). This effect persists, and even gets a bit stronger, after adding all further control variables (see Column 2a: b = .07, SE = .02, p <.01; Column 2b: b = .06, SE = .02, p <.01).We ran the same models using sales growth as the independent variable, with consistent results. Columns 3a and 3b in Table 2 show that sales growth is positively and significantly related to producers' happiness when controlling for product domain(s) and producers' revenue (b = .34, SE = .02, p <.01) or profit (b = .34, SE = .02, p <.01). The effect remains positive and significant when adding all further control variables (see Column 4a: b = .19, SE = .03, p <.01; Column 4b: b = .19, SE = .03, p <.01). DiscussionUsing a survey of actual producers, we find real-world evidence for the sales-as-signal effect (H1): individual producers draw happiness from selling their self-made products above and beyond the money they make from these sales.In a supplementary study (see Study 1S in WA-C), we examined whether the observed relationship between sales and happiness, controlling for monetary considerations, holds in a more controlled setting with random assignment. We tested this using actual producers from the Australian marketplace madeit.com.au (n = 169) by making them recall a period of time in which they sold more (vs. fewer) products than normal and asking them about their happiness as a producer at that point in time. The results of this experiment replicate the main finding of Study 1. Consistent with our theorizing, we find that individual producers are happier at periods of time in which they sell more (vs. fewer) products even when controlling for the profit they made from these sales. In addition, the effect is robust to another potential happiness driver: future profit expectations. Study 2Study 2 aims to provide a behavioral test of causality for the effect of sales on happiness through feelings of self-validation (H2). We did this by conducting a two-stage behavioral experiment in which we first asked participants to actually produce their own products and then manipulated at a later stage whether their products were sold.Another aim of Study 2 was to investigate how the extent of deliberateness of product choice moderates the sales-as-signal effect. We theorize that sales credibly inform individuals about their skills and competencies as producers to the extent that they provide information about the quality of the producers' products (H3). When sales do not reflect a deliberate decision, for example when a product is chosen at random, they are less informative about the buyer's product quality perceptions and thus the sales-as-signal effect should be reduced. MethodWe invited 1,700 American workers from Amazon Mechanical Turk (MTurk) in two waves[10] to participate in a two-stage study. We recruited participants interested and skilled in drawing. The experiment employed a 2 (sales: product sold vs. product not sold) × 2 (choice: deliberate vs. random) between-participants design. We informed all participants that, to enrich life in times of crisis, a researcher was delegated by the university's program director to organize an exhibition of comics that symbolize the defeat of the coronavirus (SARS-CoV-2). We told participants that their task would be to draw a picture of a superhero conquering the coronavirus, that all drawings would be shown to potential customers (i.e., faculty and administrative staff at the university) who could purchase one drawing for $1.50, and that all purchased drawings would be exhibited. To ensure that participants only submitted their original work, we told participants to sign their hand-drawn picture with their MTurk ID. Finally, we told participants that they would receive a second survey about one week later that would inform them about whether their drawing was sold. Importantly, to keep the monetary rewards constant, we truthfully told participants that all participants would receive $1.50 as a compensation for their work—irrespective of whether their drawing was sold. We received 417 valid drawings (see Figure 1 for a selection of drawings; invalid submissions included blank submissions, pictures that were unrelated to the task, and unsigned submissions).Graph: Figure 1. Examples of drawings of superheroes defeating the coronavirus (Study 2).Approximately one week later, we invited workers who had submitted valid drawings to participate in the second part of this study, which included our manipulations. A total of 347 workers accepted this invitation (Mage = 33.39 years, SD = 11.15, 53.0% female). We first reminded all participants that the researcher showed all drawings to potential customers (i.e., faculty and administrative staff at the university), who could purchase one drawing for $1.50. In addition, participants were reminded that all artists would receive $1.50 as compensation for their work, irrespective of whether their drawing was sold. Then, participants either read that each customer who decided to purchase a drawing selected the one they wanted (deliberate choice) or that each customer who decided to purchase a drawing received one selected at random (random choice). In addition, participants read that their specific drawing either was or was not sold (see WA-D for study materials). Participants then indicated their level of happiness as a producer on two seven-point items (""extremely unhappy/extremely happy,"" ""extremely dissatisfied/extremely satisfied""; r = .87) and their feelings of self-validation on three seven-point items (""not at all skilled/extremely skilled,"" ""not at all competent/extremely competent,"" and ""not at all talented/extremely talented""; α = .95).[11] Finally, participants completed two attention checks (""was your drawing sold?""; ""yes, my drawing was sold/no, my drawing was not sold""; ""how did the purchase of drawings happen?""; based on customers' deliberate/random choice [the 'random selector' picked the drawing]/I cannot remember)[12] and indicated their age and gender. Results HappinessA 2 × 2 ANOVA on happiness reveals a significant main effect of sales (Msold = 6.03, SD = 1.47 vs. Mnot_sold = 3.67, SD = 1.27; F( 1, 343) = 263.27, p <.001) and a nonsignificant main effect of product choice (Mdeliberate = 4.81, SD = 1.91 vs. Mrandom = 4.93, SD = 1.70; F( 1, 343) = .65, p = .42). Importantly, we also obtained the expected significant interaction effect (F( 1, 343) = 11.20, p = .001; see Figure 2, Panel A). Planned contrasts reveal that when the product choice was deliberate, the effect of sales on happiness was significantly stronger (Msold = 6.21, SD = 1.17 vs. Mnot_sold = 3.37, SD = 1.38; F( 1, 343) = 194.37, p <.001) than when the product choice was random (Msold = 5.84, SD = 1.34 vs. Mnot_sold = 3.98, SD = 1.50; F( 1, 343) = 81.75, p <.001).Graph: Figure 2. Happiness and self-validation as a function of product choice and sales (Study 3).In addition, we find that participants whose products were sold were happier when the focal decision was deliberate versus random (Mdeliberate = 6.21, SD = 1.17 vs. Mrandom = 5.84, SD = 1.34; F( 1, 343) = 8.50, p = .004). Interestingly, for participants who learned that their products were not sold, happiness was marginally higher when the focal decision was random rather than deliberate (Mdeliberate = 3.37, SD = 1.38 vs. Mrandom = 3.98, SD = 1.50; F( 1, 343) = 3.28, p = .07). Self-validationA 2 × 2 analysis of variance (ANOVA) on self-validation reveals a significant main effect of sales (Msold = 4.97, SD = 1.32 vs. Mnot_sold = 3.44, SD = 1.55; F( 1, 343) = 99.45, p <.001) and a nonsignificant main effect of product choice (Mdeliberate = 4.17, SD = 1.71 vs. Mrandom = 4.26, SD = 1.53; F( 1, 343) = .34, p = .56). Importantly, we also obtained a significant interaction effect (F( 1, 343) = 12.23, p = .001; see Figure 2, Panel B). Planned contrasts reveal that when the product choice was deliberate, the effect of sales on self-validation was significantly stronger (Msold = 5.19, SD = 1.29 vs. Mnot_sold = 3.14, SD = 1.46; F( 1, 343) = 92.04, p <.001) than when the product choice was random (Msold = 4.74, SD = 1.31 vs. Mnot_sold = 3.76, SD = 1.60; F( 1, 343) = 20.67, p <.001).In addition, we find evidence that participants whose products were sold reported higher feelings of self-validation when the product choice was deliberate (Mdeliberate = 5.19, SD = 1.29 vs. Mrandom = 4.74, SD = 1.31; F( 1, 343) = 8.18, p = .004), whereas participants whose products were not sold reported higher feelings of self-validation when the product choice was random (Mdeliberate = 3.14, SD = 1.46 vs. Mrandom = 4.74, SD = 1.31; F( 1, 343) = 4.32, p = .04). Although not the focus of our theorizing, the latter difference further validates our signaling framework, as it indicates that a negative sales signal is more detrimental to feelings of self-validation when it is more easily interpreted as a reflection of one's competencies. Moderated mediationA moderated mediation analysis ([21], Model 7, n = 5,000) with sales (0 = not sold, 1 = sold) as the independent variable, product choice (0 = deliberate, 1 = random) as the moderator, self-validation as the mediator, and happiness as the dependent variable produces a significant index of moderated mediation (b = −.48, SE = .16, CI95% = [−.81, −.19]). Supporting our prediction, the effect of sales on happiness through feelings of self-validation was significantly stronger when the product choice was deliberate (b = .93, SE = .15, CI95% = [.66, 1.23]) versus random (b = .45, SE = .11, CI95% = [.23,.67]). DiscussionUsing a multiwave experimental paradigm involving actual production, Study 2 provides causal evidence in support of our primary prediction that, above and beyond the monetary reward, sales increase producers' happiness via elevated feelings of self-validation (H2). In addition, the results are consistent with our theorizing that sales have a stronger impact on individual producers' self-validation when product choice is more deliberate (H3).Although significantly smaller (as hypothesized), we also find a residual effect of sales on happiness and, to a lesser extent, self-validation when products were sold but selected at random. This finding is beyond the scope of our hypotheses, so we can only speculate about why even a random sale might make producers happy. One possibility is a process identified by Marx (1844/1993; see also [43]) in his Comments on James Mill. Marx's discussion of what it is like to produce as a human being (rather than being a cog in a machine), suggests that producing something that is used and enjoyed by another person provides important enjoyment of life and, to some extent, affirms the producer's unique competency as a person. Thus, the mere fact that another person has acquired one's product, even if the exact product choice was made at random, may provide some basic sense of self-validation and happiness in turn. Study 3Study 3 investigates another moderator of the sales-as-signal effect: the monetary cost involved in purchasing the product. We theorize that sales credibly inform individuals about their skills and competencies as a producer to the extent that the acquisition of the product is costly to the customer (H4). In this study, we manipulated the monetary cost of sales by varying the shipping cost that a buyer needed to pay to acquire the product. We predicted that, although the financial gain from the sale is constant (the buyer bears the shipping costs), individual producers would be happier when the buyer accepts paying higher (vs. lower) shipping costs. As in the previous study, we again tested whether the increase in happiness can be explained by feelings of self-validation (H2). MethodWith the help of hobbii.com, an online shop that sells knitting kits and supplies, we recruited 1,230 recreational knitters (Mage = 53.38 years, SD = 12.26, 99.4% female). The company promoted a link to our study in their weekly newsletter, which was received by German-speaking customers (and which yielded a response of N = 818) as well as customers from the United States (N = 412). As an incentive to participate, we raffled ten gift cards to the company's online shop worth $30 each.The experiment employed a between-participants design with three conditions (sales: baseline vs. higher cost vs. lower cost). We asked all participants to imagine marketing their self-made knitted accessories on an online platform. Specifically, as we ran this study in June and participants came from Europe and the United States, we asked participants to assume they currently produce and sell summer beanies (i.e., beanies that are made from thin, lightweight material). Next, participants read that they received an email from a customer in New Zealand asking about winter beanies. Participants then read that they were able to offer their self-made winter beanies for $30.00. Finally, participants either read that the customer from New Zealand did not respond to this offer (baseline), decided to purchase the beanie for $30.00 plus $20.90 shipping costs (higher cost), or decided to purchase the beanie for $30.00 plus $2.90 shipping costs (lower cost; for study materials, see WA-E1).Participants then indicated their level of happiness (r = .87) and feelings of self-validation (α = .93) on the same scales as in Study 2. To account for alternative mechanisms, participants indicated how much profit they made with the customer from New Zealand (1 = ""none,"" and 7 = ""a lot"") and how they thought their profit from selling products would develop in the near future (1 = ""decrease a lot,"" and 7 = ""increase a lot""). Participants further completed the following attention check: ""Did the customer from New Zealand buy your beanie?"" (yes/no). Participants in the lower cost and higher cost conditions additionally completed the following attention check: ""How much did it cost to ship the beanie to New Zealand?"" ($2.90/$20.90).[13] Finally, participants indicated their gender and age.[14] Results HappinessA one-way ANOVA with happiness as the dependent variable produces a significant effect (F( 2, 1,227) = 406.52, p <.001). Follow-up contrasts reveal that, compared to the baseline condition in which the customer from New Zealand did not purchase the product (M = 3.13, SD = 1.10), participants were happier when the customer from New Zealand decided to purchase the product (higher shipping cost: M = 5.90, SD = 1.38; t( 1,227) = 27.93, p <.001; lower shipping cost: M = 5.03, SD = 1.72; t( 1,227) = 19.03, p <.001). More importantly, participants reported significantly higher levels of happiness when the buyer paid higher versus lower shipping costs (t( 1,227) = 8.79, p <.001). Self-validationA one-way ANOVA with feelings of self-validation as the dependent variable produces a significant effect (F( 2, 1,227) = 160.52, p <.001). Follow-up contrasts reveal that, compared with the baseline condition (M = 4.09, SD = 1.27), participants reported greater self-validation when the customer from New Zealand decided to purchase the product (higher shipping cost: M = 5.46, SD = .98; t( 1,227) = 17.43, p <.001; lower shipping cost: M = 5.07, SD = 1.12; t( 1,227) = 12.38, p <.001). In support of our theorizing, we further find significantly higher feelings of self-validation in the case of higher (vs. lower) shipping costs (t( 1,227) = 4.98, p <.001). MediationWe conducted mediation analyses ([21], Model 4, n = 5,000) with our multicategorical independent variable, happiness as the dependent variable, and self-validation as the mediator. We find positive and significant indirect effects on happiness through self-validation when comparing ( 1) the higher shipping cost condition with the baseline condition (b = .78, SE = .07, CI95% = [.65,.91]), ( 2) the lower shipping cost condition with the baseline condition (b = .55, SE = .06, CI95% = [.45,.67]), and ( 3) the higher shipping cost condition with the lower shipping cost condition (b = .22, SE = .05, CI95% = [.14,.31]). These results are robust to the inclusion of current profits and future profit expectations as covariates (for detailed results, see WA-E2). DiscussionStudy 3 further corroborates our theorizing by showing that the extent to which sales provide self-validation, over and above monetary outcomes for the producer, depends on the monetary cost of the sales signal. In particular, participants reported greater self-validation and thus happiness when the buyer accepted to pay higher (vs. lower) shipping costs (H4). Study 4Study 4 compares the effect of sales with that of a noneconomic signal. We focus on comparing sales with likes, which are arguably the most common form of market signals on electronic platforms such as Etsy. Comparing the effects of sales and likes is also important from a practical point of view because the seller dashboards of prominent online platforms tend to display sales and likes (or related forms of noneconomic signals such as favorites) concurrently, raising the question of how these different forms of signals impact individual producers' happiness. We hypothesize that individual producers experience greater self-validation and thus more happiness from sales than from likes, even above and beyond the monetary rewards from these sales (H5). We tested this by conducting another two-stage behavioral experiment in which we first asked participants to actually produce products and then manipulated at a later stage whether their products were either acquired or liked by customers. To test whether the happiness advantage of sales over likes goes beyond their cost difference to the customer, Study 4 kept cost to the customer (along with the monetary outcomes for the producer) constant across signals. MethodWe invited 1,000 American workers on Prolific to participate in a two-stage study in which we asked them to demonstrate their writing skills. The experiment employed a 2 (signal: sales vs. likes) × 2 (number: high vs. low) between-participants design. All participants were told that their task would be to create a positive slogan for a ""post-COVID"" event that would be taking place at our university. We told participants that we would print each slogan on a poster and exhibit each poster at the event like a gallery exhibition. Furthermore, participants were told that all guests of the event would pay an entrance fee of $1.50 to cover costs and that, in return, each guest would receive a token. In the sales condition, we told participants that guests could use this token to purchase a poster of their choice and that each poster could only be purchased once. In the likes condition, we told participants that guests could use this token to like a poster of their choice by pinning the token on the poster and that each poster could only be liked once. Next, we asked all participants (those in both the sales and likes conditions) to create their slogan by finishing the following sentence: ""When Corona is over....""Two weeks later, we invited those who had submitted valid slogans (N = 1,000) to participate in the second part of the study, yielding 843 participants (Mage = 34.70 years, SD = 12.63, 48.2% female). A chi-squared test revealed that participation in the second part of the study did not depend on whether participants were assigned to the sales or the likes condition in the first part (χ2( 1) = .11, p = .75). First, we thanked all participants for submitting their slogan and reminded them that all slogans were printed on posters exhibited at the ""post-COVID"" event, at which guests could either purchase (sales condition) or like (likes condition) a poster of their choice. In addition, we reminded participants that each poster could only be purchased/liked once. Next, participants in the sales condition were either told that one (high sales) or no (low sales) customer(s) bought the poster with their slogan on it, while participants in the likes condition were either told that that one (high likes) or no (low likes) customer(s) liked the poster with their slogan on it. Next, all participants indicated their level of happiness (r = .91) and their feelings of self-validation (α = .96) on the same scales as in the previous studies. Finally, participants responded to an attention check (""how many guests [purchased/liked] the poster with your slogan on it?""; ""no (0) guest [bought/liked] the poster with my slogan on it/one ( 1) guest [bought/liked] the poster with my slogan on it"")[15] and indicated their gender and age (for study materials, see WA-F). Results HappinessA 2 × 2 ANOVA on happiness produces significant main effects of signal (Msales = 4.43, SD = 1.64 vs. Mlikes = 4.21, SD = 1.60; F( 1, 839) = 7.28, p = .007) and number (Mhigh = 5.37, SD = 1.27 vs. Mlow = 3.28, SD = 1.20; F( 1, 839) = 610.80, p <.001). More importantly, we obtained the predicted signal by number interaction (F( 1, 839) = 9.23, p = .002; see Figure 3, Panel A). Planned contrasts reveal that the effect of the sales signal was significantly stronger (Mhigh = 5.61, SD = 1.17 vs. Mlow = 3.27, SD = 1.11; F( 1, 839) = 386.45, p <.001) than the effect of the likes signal (Mhigh = 5.13, SD = 1.32 vs. Mlow = 3.30, SD = 1.29; F( 1, 839) = 234.11, p <.001). In addition, we find that participants whose poster was sold were happier than participants whose poster was liked (F( 1, 839) = 16.36, p <.001). We detected no such differences between participants whose poster was not sold versus not liked (F( 1, 839) = .06, p = .81).Graph: Figure 3. Happiness and self-validation as a function of signal and value (Study 4). Self-validationA similar 2 × 2 ANOVA on feelings of self-validation produces a significant main effect of signal (Msales = 3.62, SD = 1.48 vs. Mlikes = 3.41, SD = 1.50; F( 1, 839) = 5.44, p = .02) and a significant main effect of number (Mhigh = 4.05, SD = 1.40 vs. Mlow = 3.00, SD = 1.38; F( 1, 839) = 121.20, p <.001). This main effect was qualified by a significant interaction effect (F( 1, 839) = 7.36, p = .007; see Figure 3, Panel B), demonstrating that the effect of the sales signal was significantly stronger (Mhigh = 4.29, SD = 1.29 vs. Mlow = 2.98, SD = 1.37; F( 1, 839) = 94.48, p <.001) than the effect of the likes signal (Mhigh = 3.81, SD = 1.47 vs. Mlow = 3.02, SD = 1.39; F( 1, 839) = 34.29, p <.001). In addition, participants whose poster was sold felt more validated than participants whose poster was liked (F( 1, 839) = 12.66, p <.001). Feelings of self-validation did not differ between participants whose poster was not sold versus not liked (F( 1, 839) = .07, p = .79). Moderated mediationA moderated mediation analysis ([21], Model 7, n = 5,000 bootstraps) with number (0 = low, 1 = high) as the independent variable, signal (0 = likes, 1 = sales) as the moderator, self-validation as the mediator, and happiness as the dependent variable produces a significant index of moderated mediation (b = .25, SE = .09, CI95% = [.07,.44]). As expected, the mediating effect through self-validation on happiness was stronger when participants' posters were sold versus not sold (b = .63, SE = .08, CI95% = [.48,.78]) than when participants' posters were liked versus not liked (b = .38, SE = .07, CI95% = [.25,.51]). DiscussionStudy 4 compared the effects of sales and likes regarding their impact on individual producers' feelings of self-validation and happiness as a producer (H5). In support of our theorizing, we find that sales produce stronger effects on happiness and self-validation than likes, even when the associated monetary costs were kept constant between conditions. In a supplementary study (see Study 4S in WA-G), we tested the effects of sales and likes when presenting information about sales and likes simultaneously (mimicking the situation on online platforms such as Etsy) among a sample of recreational knitters (n = 161). Holding the monetary rewards to the producer (but not the costs to the customer) from sales and likes constant, we again find that sales make individual producers happier than likes and that this effect is driven by feelings of self-validation. Study 5Study 5 tests whether the magnitude of the sales-as-signal effect depends on whether individuals sell their self-made products or products that were made by someone else. We tested this by varying sales of products that were self-produced versus produced by someone else. Although selling more versus fewer products should increase individuals' happiness in both situations, we predict an incremental increase in happiness from selling self-made products (H6). MethodParticipants included 1,008 U.S. consumers recruited from MTurk (Mage = 39.81 years, SD = 12.41, 47.6% female). The experiment employed a 2 (sales: high vs. low) × 2 (product: self-made vs. other-made) between-participants design. All participants imagined selling muffins at a local food market. The muffins were either made by themselves (self-made condition) or by someone else (other-made condition). To keep the monetary rewards constant across conditions, we told participants that the organizers of the food market receive all sales revenue from people selling products at the market for the first time (which is how the food market finances itself). We further informed participants that there are no other costs involved in being able to sell products at the food market. Thus, across conditions, the monetary reward from potential sales was zero. Below this description, participants responded to an attention check verifying they understood that revenue from their first-time sales would go to the food market. Next, participants either read that they made (self-made condition) or received (other-made condition) a total of 50 muffins to sell and that they sold either 36 (high sales condition) or six (low sales condition) of them at the food market.After reading this information, participants indicated their level of happiness on the same scale as in the previous studies (r = .92). Next, participants completed two more attention checks (""which of the following statements is correct?""; ""at the food market, I sold muffins that were made by someone else/at the food market, I sold muffins that I made myself""; ""how many muffins were sold at the food market?""; 6/36).[16] Finally, participants indicated their gender and age (for complete study materials, see WA-H). ResultsA 2 × 2 ANOVA on happiness reveals the expected significant main effect of sales (Mhigh_sales = 4.53, SD = 1.54 vs. Mlow_sales = 2.59, SD = 1.49; F( 1, 1,004) = 418.73, p <.001) and a nonsignificant main effect of product (Mself-made = 3.58, SD = 1.83 vs. Mother-made = 3.55, SD = 1.78; F( 1, 1,004) = .18, p = .67). Importantly, we also obtained a significant interaction effect (F( 1, 1,004) = 19.65, p <.001; see Figure 4). As hypothesized, planned contrasts reveal that the effect of sales on happiness was significantly stronger when participants sold their self-made muffins (Mhigh_sales = 4.77, SD = 1.42 vs. Mlow_sales = 2.41, SD = 1.37; F( 1, 1,004) = 310.54, p <.001) than when participants sold muffins made by someone else (Mhigh_sales = 4.31, SD = 1.62 vs. Mlow_sales = 2.79, SD = 1.59; F( 1, 1,004) = 128.21, p <.001).Graph: Figure 4. Happiness as a function of product and sales (Study 5).Importantly, we find that participants who sold more muffins were happier when they made the muffins themselves versus someone else making the muffins (Mself-made = 4.77, SD = 1.42 vs. Mother-made = 4.31, SD = 1.62; F( 1, 1,004) = 11.85, p = .001). In contrast, participants who sold fewer muffins were happier when the muffins were made by someone else versus by themselves (Mself-made = 2.41, SD = 1.37 vs. Mother-made = 2.79, SD = 1.59; F( 1, 1,004) = 8.00, p = .005). DiscussionThe results of Study 5 complement the previous studies by demonstrating that sales have a stronger effect on individuals' happiness when individuals sell their self-made products than when they sell products that were made by someone else (H6). This finding provides additional evidence that the effect of selling self-produced products on happiness is driven by the effect of sales on self-validation as a competent producer, and it rules out the alternative explanation that the effect is driven solely by the effect of sales on self-validation as a competent seller or marketer. Study 6Study 6 further tests whether selling their self-made products has any incremental effects on producers' happiness compared with merely producing products. Because sales function as a credible signal regarding producers' skills and competencies, we expect that sales affect individual producers' self-validation and thus happiness beyond individual producers' self-validation and happiness derived from production. We tested this with another two-stage behavioral experiment in which we again asked participants to produce their own products and manipulated at a later stage whether their products were sold. In addition, we added a control condition in which participants' products were not offered for sale, and we assessed participants' feelings of self-validation and happiness derived from both production and selling. Doing so allowed us to assess any increase in self-validation and happiness as a result of a successful sale or any backfiring effect in the case of no sales, compared with the baseline of ""not going to the market"" to begin with. MethodWe invited 500 U.S. consumers on Prolific to participate in a two-stage study in which we asked them to demonstrate their writing skills. In the study description, all participants were informed that we would ask them to generate a slogan followed by a short survey and that they would receive a second survey in about two weeks. The experiment employed a 2 (stage: production vs. selling) × 3 (sales: control vs. product sold vs. product not sold) mixed design with stage as the within-subjects factor and sales as the between-subjects factor. In the production stage, we told all participants that their task would be to create a positive slogan one could print on a T-shirt that describes what they will do when the COVID crisis is over. Participants in the sales conditions were additionally told that all slogans from this study would be offered to our university community and that, if a given slogan finds a customer, that slogan will be printed on a T-shirt for that customer. We further informed participants in the sales conditions that each slogan will only be printed on a T-shirt once and that the price the customer will pay for the T-shirt will equal the cost incurred in having it produced. We also told participants in the sales conditions that the second survey would inform them about whether the T-shirt with their slogan had been purchased by a customer. Next, we asked all participants (those in the control and in the two sales conditions) to create their slogan by finishing the following sentence: ""When Corona is over...."" After creating their slogan, all participants indicated their level of happiness (r = .78) and their feelings of self-validation (α = .96) on the same scales used in the previous studies. Finally, we reminded all participants that they would receive a second survey in about two weeks.Two weeks later, we invited participants who had submitted valid slogans (N = 499) to participate in the second part of the study. A total of 384 participants accepted this invitation (Mage = 31.56, SD = 10.51, 45.6% female).[17] First, we thanked all participants for submitting their T-shirt slogan two weeks earlier. Participants in the sales conditions were additionally reminded that all slogans were offered to our university community and that the T-shirt with their slogan on it could only be purchased once. Next, we informed participants in the sales conditions that their T-shirt was either sold or not. Finally, all participants indicated their level of happiness (r = .85) and their feelings of self-validation (α = .97) on the same scales as in the production stage, as well as their gender and age (for study materials, see WA-I). Results HappinessA 2 × 3 mixed ANOVA on happiness with stage (production vs. selling) as the within-subject factor and sales (control vs. product sold vs. product not sold) as the between-subjects factor reveals the expected significant stage by sales interaction (F( 2, 381) = 127.47, p <.001; see Figure 5, Panel A). Participants' happiness did not differ across the sales conditions in the production stage (Mcontrol = 5.19, SD = 1.12 vs. Msold = 5.17, SD = 1.21 vs. Mnot_sold = 5.39, SD = 1.15; F( 2, 381) = 1.34, p = .26). In contrast, participants' happiness significantly differed across the sales conditions in the selling stage (F( 2, 381) = 157.63, p <.001). As expected, participants were happier when a T-shirt with their slogan on it was sold (M = 5.87, SD = 1.02) than when a T-shirt with their slogan on it was not sold (M = 3.37, SD = 1.17; t(381) = 17.47, p <.001). Compared with the control condition (M = 4.97, SD = 1.21), participants were happier when a T-shirt with their slogan on it was sold (t(381) = 6.26, p <.001) and less happy when a T-shirt with their slogan on it was not sold (t(381) = −11.40, p <.001).Graph: Figure 5. Happiness (left) and self-validation (right) as a function of stage and sales (Study 6).Comparing participants' happiness across the two stages reveals that, in the product sold condition, participants were happier in the selling stage (M = 5.87, SD = 1.02) than in the production stage (M = 5.17, SD = 1.21; F( 1, 381) = 30.93, p <.001). In the product not sold condition, participants were less happy in the selling stage (M = 3.37, SD = 1.17) than in the production stage (M = 5.39, SD = 1.15; F( 1, 381) = 279.11, p <.001). In the control condition, participants were marginally less happy in the selling stage (M = 4.97, SD = 1.21) than in the production stage (M = 5.19, SD = 1.12; F( 1, 381) = 3.38, p = .07). The main effect of stage was also significant (Mselling = 4.72, SD = 1.53 vs. Mproduction = 5.25, SD = 1.58; F( 1, 381) = 53.90, p <.001). Self-validationA similar 2 × 3 mixed ANOVA on self-validation reveals a significant stage by sales interaction (F( 2, 381) = 28.33, p <.001; see Figure 5, Panel B). In the production stage, participants' feelings of self-validation did not differ across the sales conditions (Mcontrol = 3.88, SD = 1.36 vs. Msold = 3.85, SD = 1.32 vs. Mnot_sold = 3.92, SD = 1.52; F( 2, 381) = .09, p = .91). In contrast, participants' feelings of self-validation significantly differed across the sales conditions in the selling stage (F( 2, 381) = 23.71, p <.001). As expected, participants felt more validated when a T-shirt with their slogan on it was sold (M = 4.35, SD = 1.27) than when a T-shirt with their slogan on it was not sold (M = 3.14, SD = 1.61; t(381) = 6.80, p <.001). Compared to the control condition (M = 3.89, SD = 1.35), participants felt more validated when a T-shirt with their slogan on it was sold (t(381) = 2.56, p = .01) and less validated when a T-shirt with their slogan on it was not sold (t(381) = −4.31, p <.001).In the product sold condition, participants felt more validated in the selling stage (M = 4.35, SD = 1.27) than in the production stage (M = 3.85, SD = 1.32; F( 1, 381) = 16.47, p <.001). In the product not sold condition, participants felt less validated in the selling stage (M = 3.14, SD = 1.61) than in the production stage (M = 3.92, SD = 1.52; F( 1, 381) = 42.19, p <.001). In the control condition, participants' feelings of self-validation did not differ between the selling stage (M = 3.89, SD = 1.35) and the production stage (M = 3.88, SD = 1.36; F( 1, 381) = .01, p = .92). The main effect of stage was not significant (F( 1, 381) = 1.61, p = .21). MediationWe conducted two mediation analyses ([21]; Model 4, n = 5,000 bootstraps) to test whether feelings of self-validation can explain the effect of our sales manipulation on happiness. We first looked at participants' ratings of happiness and self-validation obtained in the selling stage. We thus entered sales as the independent variable, self-validation in the selling stage as the mediator, and happiness in the selling stage as the dependent variable into the regression. This analysis produced positive indirect effects when comparing the product sold with the product not sold condition (b = .45, SE = .09, CI95% = [.29,.64]) and the control condition (b = .17, SE = .07, CI95% = [.05,.31]), and it produced a negative indirect effect when comparing the product not sold condition with the control condition (b = −.28, SE = .08, CI95% = [−.45, −.14]).We next examined whether relative differences in happiness between the selling stage and the production stage can be explained by relative differences in self-validation between the selling stage and the production stage. To do so, we calculated difference scores between happiness and self-validation ratings in the selling stage and the production stage. The respective regression analyses ([21]; Model 4, n = 5,000 bootstraps) produced positive indirect effects when comparing the product sold with the product not sold condition (b = .55, SE = .10, CI95% = [.36,.76]) and the control condition (b = .21, SE = .08, CI95% = [.07,.37]), and it produced a negative indirect effect when comparing the product not sold condition with the control condition (b = −.34, SE = .08, CI95% = [−.51, −.19]). DiscussionBy including a preselling baseline measure as well as a no-selling control condition, the results of Study 6 confirm there is a positive incremental effect of selling self-produced products on happiness. They also show, however, that marketing self-produced products can have a downside as well. Specifically, there is a happiness penalty to pay when products fail to sell, as the absence of sales leads to negative self-validation; that is, failing to sell products sends a negative signal about one's skills and competencies as a producer, reducing happiness. Thus, deciding to offer their self-made products for sale can enhance but also diminish individuals' happiness. General DiscussionThis research investigates the socioemotional benefits of selling self-made products. Eight studies provide evidence for a sales-as-signal effect: individual producers derive happiness from selling their products above and beyond the money they make from these sales (all studies). This is because sales validate their skills and competencies as a producer (Studies 2–4 and 6). In addition, Study 2 shows that the sales-as-signal effect is more pronounced when the choice mechanism that precedes the purchase is more versus less deliberate. Study 3 demonstrates that the increase in happiness from sales is higher when the buyer incurs higher (vs. lower) monetary costs in purchasing the product, even when this higher cost does not translate into higher financial return for the individual producer. Study 4 demonstrates that individual producers gain more happiness from sales than from receiving likes. Finally, Studies 5 and 6 show that individuals derive greater happiness from high sales of self-made products than from high sales of products that were made by someone else (Study 5) and that individuals show increases in happiness from pre- to post-selling but show decreases in happiness after (vs. before) failing to sell (Study 6). The studies (N = 4,970) span a variety of methodological approaches, designs, and procedures, and they feature different participant populations across three continents and producer communities. Contributions to TheoryOur research makes a number of theoretical contributions. First, previous research has extensively studied the psychological and behavioral consequences of engaging in self-production (e.g., [17]; [31]). This line of research has, however, focused on studying production for oneself or for gift giving. Our work goes a step further and examines a context in which individuals produce for the market; that is, with the aim of selling their creations to others. Our contribution thus lies in shedding light on the psychological consequences of participating in market exchanges. Our findings suggest that sales provide individual producers with a sense of self-validation and, in turn, happiness. We find that selling self-made products affects individuals' happiness beyond the happiness derived from producing the products.Second, we introduce a new perspective on the value of sales. Most models of producer behavior assume that producers are driven solely by a profit-maximization motive ([18]; [36]). Our findings caution against taking a reductionist view of sales by demonstrating that selling one's self-made products can also have important socioemotional value, as sales provide individual producers with self-validation regarding their skills and competencies as a producer. Thus, models of producer behavior may benefit from including the self-validation motive documented in this research.Third, in developing our theory, we conceptualized sales as a signal from buyers. This approach is different from the process investigated by existing research on signaling in marketing and management that conceptualizes signals as actions taken by sellers, who have low uncertainty about the product, to reduce uncertainty about product qualities for buyers ([ 5]; [ 8]; [23]). In our research, we find that sales constitute a signal that is sent by buyers and received by sellers. Moreover, our work suggests that sellers, even of self-produced products, actually do have uncertainty about their own products' qualities that is reduced by quality information from buyers. Thus, the traditional roles about who has information and who is uncertain are to some extent reversed. There may be less information asymmetry than is often assumed, and quality information may be exchanged in both directions. Furthermore, signals are traditionally conceptualized as intentional actions performed by the sender that in turn benefit the sender ([ 8]). Our account, in contrast, suggests that sending a signal can be a more or less incidental by-product (of a purchase) rather than an intentional signal and that the signal benefits the receiver of the signal (i.e., individual producer) by increasing self-validation. In addition, we demonstrate that the strength of a signal depends not only on the signal's costliness but also on its diagnosticity. For example, even when likes and sales are equally costly for the customer, sales may be seen as a more reliable indicator of product quality and producer skill. Likewise, a buyer's deliberate choice to purchase an item makes the sale of that item more diagnostic of producer skill than a sale that involves a buyer randomly picking a specific product. Thus, we believe that the present work offers a novel perspective on signaling.Finally, our research informs the ongoing debate within the marketing discipline regarding the implications of marketing for society ([ 7]). The backdrop of this debate involves widespread concern about the pernicious aspects of a consumer society ([24]), the negative environmental externalities of market exchanges ([25]), and the potentially exploitative nature of common marketing practices ([40]). We find that marketing one's products can actually have an important positive effect by providing individual producers with a significant happiness benefit through self-validation. We thus demonstrate how studying a classic marketing topic from a ""better world"" perspective can yield novel insights about marketing's potential to improve people's well-being. Moreover, marketing scholars studying the implications of marketing on society frequently focus on activities undertaken by marketers at large corporations, and more research is needed to uncover the societal impact of marketing beyond these contexts ([ 7]). Answering this call, we study the socioemotional benefits of individual producers participating in market exchanges. Our work also demonstrates the value of taking a behavioral approach to the study of supply-side behaviors. Despite calls for behavioral marketing research to broaden its focus beyond consumers ([27]; [44]), virtually all research by behavioral marketing scholars currently focuses on consumer behavior. We provide an example of how researchers can tackle important marketing phenomena on the supply side with a behavioral lens. Practical ImplicationsIn addition to these theoretical contributions, our work also provides several practical implications, especially for online marketplaces that focus on producers selling self-made products. First, the finding that sales can increase individual producers' happiness could be leveraged in, for example, the recruitment of prospective sellers by highlighting the socioemotional benefits of selling their products (e.g., ""Be a pro. Sell your tote bags on Etsy!""). Online marketplaces could also highlight socioemotional benefits to existing sellers to maintain motivation and retention. This could be done, for example, by stressing that customers recognize sellers' expertise by paying good money to buy their products (e.g., ""They voted with their wallets to tell you you're a pro"").In addition, our findings yield recommendations for the design of seller dashboards. Study 4 demonstrates that knowing how many people bought their products makes individual producers happier than knowing how many people liked their products. Study 3 suggests that showing how much customers paid might increase motivation, and thus production volume per seller and seller retention, beyond the amount of money the seller made. Therefore, we recommend designing seller dashboards so that the number of people who have made purchases and the average amount paid by customers (including shipping and other fees) are more prominent, rather than highlighting likes or aggregate revenue as is common in such dashboards.Finally, Study 2 shows that sales make individual producers happier when they know that buyers deliberately chose their products (vs. being chosen at random). This finding could be leveraged, for example, by encouraging buyers to leave a comment indicating why they chose that seller's product over other available product choices or by highlighting that customers decided to buy the focal producer's product despite having many other options. Future Research Opportunities Self-validation as a sellerWe hope that our findings will motivate other researchers to explore the under-researched topic of individual producers' motivations, beyond monetary considerations. Specifically, we identify several opportunities for future research. Our findings indicate that the sales-as-signal effect is strongly driven by individual producers' gain in self-validation as a competent producer (e.g., in Studies 2, 4, and 6 sales increased individual producers' happiness even when their self-made products were sold by someone else). This focus on self-validation as a producer was motivated by the fact that sellers on platforms such as Etsy typically spend more than half of their time designing and making their products and only about 10% of their time with marketing and promotion activities ([16]). However, to successfully sell their products, many individual producers not only engage in skilled production activities but also in skilled promotion- or selling-related activities, such as taking pictures of their offerings, pricing their products, maintaining their online appearance, and engaging in advertising on social media platforms. Therefore, sales might, at least for some people and in some contexts, also be an important source of happiness by validating individual producers' skills as a promoter or seller. Study 5 indeed shows that sales also increase the happiness of individuals selling products made by someone else above and beyond the monetary rewards from sales—though to a lesser extent compared to individuals selling their self-made products. Empirically examining the self-validation processes involved in selling would be a worthwhile future research direction. Likewise, while the entrepreneurship literature documents that financial success is related to entrepreneurs' satisfaction ([13]; [32]), future research could investigate whether entrepreneurial success (e.g., firm growth, increase in funding) has a causal effect on entrepreneurs' happiness beyond the related financial gains. In sum, our work provides a fruitful foundation for investigating the socioemotional benefits of participating in market exchanges across business contexts. Moderators of the sales-as-signal effect Characteristics of the purchase transactionOur studies investigated two characteristics of the purchase transaction—the deliberateness of the product choice and the monetary cost of buying—that moderate the strength of the sales signal. However, other important moderators of the sales-as-signal effect likely exist, and future research should expand the nomological network in which the sales-as-signal effect is situated. For example, the strength of the sales signal might depend on the expertise of the buyer. Compared to novices, experts are more capable of assessing products' quality ([ 1]). Having one's products bought by an expert in the respective product category should thus more credibly inform individual producers about their competencies as a producer than having one's products bought by a novice. Similarly, the number of buyers might affect how strongly individual producers perceive sales to correspond to the quality of their products. Would 40 buyers purchasing one product each provide more self-validation than four buyers purchasing ten items each?Besides the direct monetary cost associated with buying a product (Study 3), the sales-as-signal effect might also depend on the relative cost that buyers incur when purchasing a product. We would expect the effect of sales to be stronger when a buyer has a smaller (vs. larger) budget to spend. Likewise, relative price might also play a role, as the same cost might be a stronger signal of expertise when that cost is high compared to other products in the category than when that same cost is low compared to other products in the category. Finally, although customers accepting a high price should increase the strength of the sales signal, individual producers accepting higher prices might reach a point where they feel that customers are being treated unfairly (such as when customers have to pay extraordinarily high shipping costs) or that the prices might cause customers to refrain from making repeat purchases. Do concerns about fairness and relationship-building reduce the happiness individual producers gain from selling products at exorbitant prices? Characteristics of the individual producerOne could also argue that the strength of the sales-as-signal effect depends on certain characteristics of individual producers. We used the data obtained in Study 1 to test whether the effect of sales on happiness is moderated by any of the captured control variables. Only one statistically significant interaction emerged for both measures of our independent variable (i.e., sales volume and sales growth): a positive interaction effect between sales and producers' socioeconomic status (ps <.01; see WA-J1). This suggests that the effect of sales on happiness is stronger for producers that have a higher socioeconomic status—in other words, producers who feel financially secure. We also used the data obtained in Study 1S to examine whether the effect of sales on happiness is moderated by any of the captured control variables. Moderation analyses produce nonsignificant interaction effects between ( 1) sales and producers' experience and ( 2) sales and socioeconomic status (ps >.20; see WA-J2). Thus, although the moderation analyses of Study 1 suggest that the sales-as-signal effect is stronger for producers that have a relatively higher (vs. lower) socioeconomic status, we did not observe such an interaction effect in Study 1S. Therefore, future research might more closely look at the role socioeconomic status plays in the sales-as-signal effect.The strength of the sales-as-signal effect might also depend on individual producers' own evaluations of their products—that is, how competent individuals feel in producing their products might alter how happy they feel as a result of selling those products. We used the data of Study 6 to test this. A moderation analysis produces a nonsignificant interaction between sales (higher vs. lower) and self-validation from production on happiness from sales (p = .98; see WA-J3). This suggests that the positive effect of sales on happiness does not depend on producers' presales perception of their products. We encourage future researchers to look deeper into these, and possibly other, potential moderators of the sales-as-signal effect. ConclusionTraditional producer behavior models assume that producers' behavior is driven by the monetary rewards from selling their wares and that there is an information asymmetry that favors producers. In other words, these models assume producers have information about the quality of their products that (prospective) customers do not, leading producers to signal product quality to customers. This research shows that producers derive happiness from selling their wares that goes above and beyond any monetary rewards. It shows that there is also an information asymmetry in the other direction: Customers possess information about the skill level of the producer that they signal by purchasing the producer's products. This provides a feeling of self-validation to the producer, increasing their happiness. To understand the behavior of producers, it is critical to broaden our scope from purely monetary to self-validation benefits and from assuming a one-directional flow of quality information to a two-directional flow, with at least some quality signals being sent from customers to producers. Supplemental Materialsj-pdf-1-jmx-10.1177_00222429211064263 - Supplemental material for Sales and Self: The Noneconomic Value of Selling the Fruits of One's LaborSupplemental material, sj-pdf-1-jmx-10.1177_00222429211064263 for Sales and Self: The Noneconomic Value of Selling the Fruits of One's Labor by Benedikt Schnurr, Christoph Fuchs, Elisa Maira, Stefano Puntoni, Martin Schreier and Stijn M.J. van Osselaer in Journal of Marketing  "
17,"Sales and Self: The Noneconomic Value of Selling the Fruits of One's Labor A core assumption across many disciplines is that producers enter market exchange relationships for economic reasons. This research examines an overlooked factor; namely, the socioemotional benefits of selling the fruits of one's labor. Specifically, the authors find that individuals selling their products interpret sales as a signal from the market that serves as a source of self-validation, thus increasing their happiness above and beyond any monetary rewards from those sales. This effect highlights an information asymmetry that is opposite to what is found in traditional signaling theory. That is, the authors find that customers have information about product quality that they signal to the producer, validating the producer's skill level. Furthermore, the sales-as-signal effect is moderated by characteristics of the purchase transaction that determine the signal strength of sales: The effect is attenuated when product choice does not reflect a deliberate decision and is amplified when buyers incur higher monetary costs. In addition, sales have a stronger effect on happiness than alternative, nonmonetary forms of market signals such as likes. Finally, the sales-as-signal effect is more pronounced when individuals sell their self-made (vs. other-made) products and affects individuals' happiness beyond the happiness gained from producing.Keywords: self-production; signaling; selling; happiness; self-validationDigital platforms such as Etsy or Amazon Handmade have made it easy for individuals to sell their self-made products to other individuals. Commercial activities that were previously limited to economically marginal contexts such as flea markets have become big business. Although each individual producer's commercial activity might be small, the sheer number of such producers adds an important new source of competitive pressure for traditional firms in many industries, including clothing, food, and home furnishings. For example, in 2020, Etsy had around 4.4 million sellers and 82 million buyers, leading to a total transaction value of around $10 billion ([39]).Existing research has investigated the psychological and behavioral consequences of engaging in self-production. In short, people like the fruits of their labor and value products they produce themselves more than comparable products made by others ([17]; [31]). The higher valuation of self-made products stems from the sense of accomplishment, pride, and competence that individuals experience when they successfully self-design or assemble a product ([ 9]; [29]). Prior research has focused on individuals engaging in self-production with the objective of consuming the product themselves or giving it as a gift ([30]), but it provides little insight into the increasingly common situation in which individuals make products with the objective of selling them to ""the market""; that is, to unknown others.A common assumption in marketing, economics, and entrepreneurship is that producers participate in market exchanges for economic reasons ([18]; [36]). Despite this disciplinary emphasis on economic motives, it seems possible that individuals produce and subsequently sell products also for noneconomic reasons. Research in organizational behavior ([19]), economics ([ 2]), and entrepreneurship ([37]) has drawn attention to the socioemotional motivations of producing, but little attention has been paid specifically to the noneconomic benefits of selling the fruits of one's labor. Drawing on survey data from the field as well as a series of experiments, we document a sales-as-signal effect: Selling their self-made products increases individual producers' happiness above and beyond any monetary implications from these sales. This effect occurs because individual producers interpret the number of products sold as a market signal that validates their skills and competencies as producers.Our research makes several contributions. First, in demonstrating the socioemotional consequences of selling one's creations, we introduce a novel perspective on the value of sales. We propose that above and beyond their importance as a source of monetary income, sales can also have socioemotional value by providing a source of self-validation. Economic theory generally conceptualizes supply-side agents as profit-maximizers, such that ""managers of a firm make those choices that maximize the sum of current and future profits"" ([15], p. 769). Consequently, one would expect that the value individual producers derive from selling their products is a function of the money they make from these sales. However, we demonstrate that the value individual producers gain from sales cannot be solely defined in economic terms. Instead, individual producers also gain considerable happiness via feelings of self-validation from selling their products. We thus provide evidence for the importance of the noneconomic value of participating in market exchanges.Second, our findings contribute to the literature examining individuals' valuation of their self-made products ([17]; [30]; [31]). Whereas existing research has shown that individual producers feel competent and proud from successfully designing or assembling a product, our work examines the psychological consequences of selling self-made products and not of merely producing those products. We demonstrate that having actual buyers purchase one's self-made products functions as an external confirmation that the individual producer is competent and capable of creating a high-quality, marketable product, which fuels one's happiness beyond the happiness derived from production.Third, we offer a novel perspective on the role of signals in market exchanges by conceptualizing sales as a signal from the market. Research in economics, marketing, and management has typically conceptualized marketplace signals as actions taken by sellers to convey information about unobservable product qualities to buyers ([ 5]; [ 8]; [23]). Thus, signals are traditionally sent by sellers and interpreted by buyers. In contrast, we propose that sellers interpret the act of purchasing a product as a signal from the buyer that validates the seller's skills and competencies as producer. Furthermore, whereas signals are usually conceptualized as intentional actions meant to benefit the sender ([ 8]), we propose that sending this type of signal to the seller is often incidental to buyers' core motivation to buy, which lies in their consumption goals, and that the signal does not directly benefit the sender but rather the receiver of the signal. Moreover, traditional signaling research assumes that sellers know the quality of their product and that buyers are uncertain. Our work indicates that sellers are, to some extent, uncertain about their own product and thus about their competencies as a producer. Our work indicates that buyers have information about product quality that reduces sellers' uncertainty. Therefore, we propose that individual producers interpret sales as a signal from their buyers that serves as a source of self-validation.Fourth, our findings broaden the discussion on the societal role of market exchanges, a topic of intense interest among marketing scholars and practitioners ([ 7]). We add to this discussion on how marketing can help create a better world by drawing attention to the way selling might provide a positive source of meaning and happiness for individuals. Just like the social costs of marketing have often been underestimated, we argue that some important benefits have been neglected as well. Specifically, successfully marketing their products is a source of self-validation and happiness for producers. Theory Benefits of Selling Self-Made ProductsWhat benefits do individuals derive from selling their self-made products? Why do they continue to populate online marketplace platforms? The principal and most obvious benefit that people derive from selling their products is money. But can monetary incentives alone explain the increasing popularity of online marketplaces? We propose that learning a customer bought their products increases individual producers' happiness above and beyond the monetary implications of these sales.Specifically, we argue that sales validate one's skills and competencies as a producer. We found preliminary support for this notion in an exploratory qualitative study conducted as part of a master's thesis supervised by one of the authors ([42]). In ten in-depth interviews with Etsy sellers and nonobtrusive observations of Etsy's online discussion forums (see Web Appendix A [WA-A]), several informants highlighted that selling their products makes them happy; for example, ""creating something that I like and others like enough to spend their hard-earned money on, is bliss"" (Informant #7 from Etsy forum), and ""it's so flattering when people choose to buy your creations"" (#61, forum). The narratives suggest that the happiness derived from selling is not necessarily rooted in economic reasons but in one's perceived self-worth as a producer; for example, ""Etsy allows me to rediscover my worth"" (#31, forum). Describing a producer friend, one of our informants (#4, interview) stated, ""for Jeani, I think it is the fact that she is making something someone thinks is worthy of buying. You know, paying some money for and it's like an accolade of her creative talent."" Interestingly, the narratives suggest that the increased self-worth derived from selling motivates people to continue producing their own products; for example, ""a sale...usually motivates me to make more, since it makes me feel as though my items are appreciated."" In summary, our preliminary qualitative insights point to the possibility that sales make individual producers happy not only because of the monetary gains but also because sales more fundamentally validate their skills and competencies as a producer. The Sales-as-Signal EffectWe argue that sales function as a signal from the marketplace that boosts individual sellers' self-validation. Signaling theory was developed in information economics to study market interactions under conditions of information asymmetry between sellers and buyers ([38]). It generally assumes that sellers are aware of the quality of their goods but buyers are not. To distinguish low-quality sellers from high-quality sellers, buyers must detect and interpret the signals sent by sellers. Prices, advertising, brands, and different types of firm actions can constitute signals ([ 8]; [22]; [23]). Moreover, signaling theory and its applications in marketing usually presume that the signal originates from a seller and is received by a buyer.[ 7] In our research, we propose that sales constitute a signal that is sent by buyers (with or without buyers' intention to actually signal something) and that validates the seller's competencies as a producer. Thus, in this context, there is an information asymmetry in the opposite direction from traditional signaling theory: It is the buyer who has information that reduces the seller's uncertainty (about the seller's skills and competencies about the producer).Feelings of competence, which often result from others' validation of one's skills, are a central motivation for people to engage in creative tasks ([ 9]) and greatly affect how satisfied individuals are with their work ([33]). Crucially, feeling competent is a fundamental psychological need among humans, and its fulfillment strongly determines individuals' intrinsic motivation, life satisfaction, and mental health ([34]; [41]). [11], p. 231) even argue that the need to feel competent ""must be satisfied for long-term psychological health."" We thus propose that being validated as a competent producer through sales increases individual producers' happiness.In summary, we predict a sales-as-signal effect: Sales increase individual producers' happiness, even when controlling for the effect of monetary gains. This is because producers interpret sales as a positive signal from the market that validates them as competent producers. Formally: H1:  Sales increase individual producers' happiness above and beyond the monetary rewards from these sales (i.e., the sales-as-signal effect). H2:  The sales-as-signal effect is mediated by feelings of self-validation as a producer. Moderators of the Sales-as-Signal EffectThe strength of a signal is determined by the extent to which receivers interpret the signal as credible ([ 5]). Signals are perceived as more credible the more they are able to provide information about products' unobservable quality ([ 6]; [ 8]) and the higher the costs and associated risk in sending a signal ([ 3]; [23]). Accordingly, we predict that the strength of the sales signal will depend on at least two characteristics of the purchase transaction: ( 1) the extent to which the product choice reflects a deliberate decision and ( 2) the monetary cost involved in purchasing the product. We decided to focus on these two moderators because they provide an internally valid test of our proposed underlying process (self-validation) and provide actionable implications for the management of online marketplaces. Deliberateness of the product choiceSales should more credibly inform individual producers about their competencies the more the sales are a direct consequence of the quality of individual producers' products ([ 6]; [ 8]). Thus, the sales-as-signal effect should be stronger the more the buyer is seen as making a deliberate decision to acquire the product. That is, sales should be self-validating when the buyer intentionally chooses a producer's product, but much less so when the product was selected in a way that does not reflect the buyer's preference for a specific product. Examples of the latter include a chef's choice item on a menu, a surprise wine box subscription, the specific vegetables offered by a community-supported agriculture farm co-op, a ""mystery car"" car rental option, a sneak preview at a movie theatre, or a sweepstakes in which it is not clear in advance which participant will receive which prize. We hypothesize that, above and beyond the monetary rewards from sales, individual producers will feel greater self-validation, and thus happiness, the more the purchase appears to be the result of a buyer's deliberate decision. H3:  The sales-as-signal effect is stronger when the product purchase reflects a more (vs. less) deliberate decision. Cost of the purchaseIf signal credibility is a function of signal cost ([ 3]; [23]), varying the cost of buying a product should alter the strength of the sales-as-signal effect. The higher the costs involved in purchasing a product, the more credibly the sales signal should inform individual producers about their competencies as a producer—even if the higher costs do not translate into higher monetary rewards for the individual producer (such as when the buyer bears higher shipping costs). We hypothesize that individual producers will feel higher levels of self-validation, and thus happiness, when the buyer incurs higher monetary costs, even when the higher monetary costs do not lead to higher monetary income for the individual producer. H4:  The sales-as-signal effect is stronger when buyers incur higher (vs. lower) monetary costs. Sales Versus Other Forms of Market SignalsSales are not the only signals consumers might send. In addition to buying products, consumers may also signal quality through noneconomic signals, such as writing a review or liking a product or company on social media. One might argue that such noneconomic signals might have a stronger self-validating effect than sales because the noneconomic signals are sent intentionally (vs. being a usually unintentional by-product of the decision to buy). However, we propose that sales would be a more credible signal because they may be seen as more informative about the product's unobservable quality for several reasons.First, other forms of signals such as online reviews have been criticized for being unable to reveal a product's actual quality ([12]), and public displays of support for a cause on social media (e.g., Facebook likes) are often unreliable indicators of one's willingness to support the cause when doing so is costly ([26]). Thus, noneconomic signals such as likes may be seen as ""cheap talk."" In contrast, customers should decide to purchase a product only if they really deem the product to be of high, or at least sufficiently high, quality ([14]). Second, selling products may evoke the specific norms that are associated with an exchange domain rather than a relational domain. In an exchange domain, the normative signal of value may be sales rather than more relational signals such as likes.[ 8] In this domain, having buyers purchase one's product might be the ""ultimate"" form of appreciation of an individual's competencies as a producer. Thus, sales may more credibly inform individual producers about their skills and competencies compared with other common forms of market signals such as likes, even when the cost to the customer of sales and likes are kept the same. We hypothesize that individual producers will feel greater self-validation, and thus happiness, from sales than from noneconomic signals, even when sales do not lead to higher monetary rewards to the seller than noneconomic signals (and even when monetary cost to the customer is kept constant). H5:  Sales increase individual producers' happiness more than noneconomic signals above and beyond the monetary rewards from those sales to the producer (and above and beyond the monetary cost of sending those signals). Happiness from Selling Self-Made Versus Other-Made ProductsWe further propose that individuals derive greater happiness from sales when selling self-made products as opposed to selling products made by someone else. Our hypothesis is that the effect of sales of self-produced goods on happiness is, to an important extent, driven by validation of the producer's skills and competencies as a producer. Of course, selling self-made products might also validate individual producers' skills as a seller; that is, successful sales may be interpreted as a signal that the seller of self-produced products is a competent marketer rather than a competent producer. However, unlike selling one's self-made products, selling products made by others cannot validate one's skill as a producer. Thus, the effect of sales on happiness should be larger for self-produced than for other-produced products. H6:  The sales-as-signal effect is stronger among individuals selling self- (vs. other-) made products. Overview of StudiesWe tested our propositions in eight studies (N = 4,970). Study 1 and a supplementary study provide an initial exploration of our main hypothesis that sales increase individual producers' happiness above and beyond the monetary rewards from these sales (H1). In Study 1, we surveyed actual producers selling their self-made products (e.g., on Etsy). We find a positive relationship between sales and individual producers' happiness, controlling for the monetary implications of sales. In a supplementary study, we replicated this finding experimentally using a recall task among a sample of actual producers.In Studies 2–6, we provide further evidence for the core sales-as-signal effect, explore the underlying mechanism, examine boundary conditions, and investigate the incremental effects of sales among different samples of producers. Studies 2 and 3 show that the sales-as-signal effect is driven by feelings of self-validation (H2). These studies also show that the strength of the sales-as-signal effect depends on ( 1) the extent to which the product choice reflects a deliberate decision (H3) and ( 2) the monetary cost involved for the consumer in purchasing the product (H4). Study 4 and a supplementary study demonstrate that sales increase individual producers' happiness more than receiving likes (H5). Study 5 shows that the effect of sales on happiness is stronger when individuals sell self-made products than when they sell products made by others (H6). Finally, Study 6 shows that the sales-as-signal effect is different from the mere happiness individuals gain from producing. Study 6 also shows that trying to sell one's self-produced products can backfire when low sales become a signal of low competency. Study 1Study 1 provides an initial exploration of our core prediction that selling their products increases producers' happiness over and above any monetary rewards from those sales. We do so through a field survey of actual producers selling their self-made products online. MethodTo engage this special and difficult-to-recruit population (individual producers), we worked with the administrators of eight Facebook groups of producers of handmade goods to promote our survey. As an incentive to participate, each participant received a $5 Amazon voucher (see WA-B1 for detailed study materials). After agreeing to a data protection disclaimer, we told participants that the aim of the study was to gather knowledge about their life as producers of handmade goods. To increase the comparability of the responses, we asked participants to think about the last four weeks when answering the questions.The survey first captured our dependent variable, participants' satisfaction and happiness with their life as a producer of handmade products, which we measured with two items (r = .61; see survey scales in Table 1). We measured our key independent variable, one's current sales (i.e., the number of products sold), using two variables. First, we assessed the sales volume by asking participants to indicate the total number of items sold in the past four weeks. Second, we assessed sales growth by capturing how many items a given producer sold in the last four weeks compared to the average number of items sold per month in the last six months. The comparative nature of the measure is important because it is the within-person variance that most strongly predicts one's happiness at any given point in time ([35]).GraphTable 1. Survey Scales (Study 1). VariableMeasuresHappiness""If you think about the past four weeks, how satisfied are you with your life as a producer of handmade products?"" (1 = ""extremely dissatisfied,"" and 7 = ""extremely satisfied"") and ""if you think about the past four weeks, how happy are you with your life as a producer of handmade products?"" (1 = ""extremely unhappy,"" and 7 = ""extremely happy"")Sales volume""Over the past four weeks combined, how many handmade items did you sell?""Sales growth""Now, please compare the number of items that you sold in the past four weeks with the average number of items you sold per month in the past six months. Would you say that you sold more or fewer items in the past four weeks compared to the months before?"" (1 = ""much fewer,"" and 7 = ""much more"")Revenue""What was the total revenue on these items in the past four weeks? That is, how much money did you make by selling these items in the past four weeks?""Profit""How much profit did you make in the past four weeks by selling these items? That is, after subtracting all costs, how much money were you able to keep in your pockets?""Future profit expectations""If you think about the near future, how do you think your profits from selling your handmade products will develop?"" (1 = ""decrease a lot,"" and 7 = ""increase a lot"")Socioeconomic status""I don't think I'll have to worry about money too much in the future,"" ""I don't need to worry too much about paying my bills,"" and ""I have enough money to buy things I want"" (1 = ""totally disagree,"" and 7 = ""totally agree"") We included the following control variables to empirically isolate the sales-as-signal effect from a series of alternative explanations. We captured the direct monetary implications of sales by asking for the respective revenue and profit (in USD) in the said time period.[ 9] Although these measures are important for assessing our hypothesis (i.e., to control for any monetary implications of selling), they do not account for any future profit expectations. For example, one could argue that a positive sales trend in the current period might be (perceived to be) diagnostic of future sales and thus profit developments. Thus, a given happiness level at a given point in time might be due to future profit expectations based on the comparative number of items sold. To account for this alternative explanation, we asked participants how they expected their future profits to develop.GraphTable 2. Ordinary Least Squares Regressions on Happiness (Study 1). (1a)(1b)(2a)(2b)(3a)(3b)(4a)(4b)Ln(sales volume).06**(.03).05**(.03).07***(.02).06***(.02)Sales growth.34***(.02).34***(.02).19***(.03).19***(.03)Ln(revenue)−.01(.02).002(.02).01(.01).01(.02)Ln(profit)−.002(.02).01(.02).01(.01).02(.02)Future profit expectations.23***(.03).23***(.03).15***(.03).15***(.03)Ln(experience).10**(.05).10**(.05).13***(.05).13***(.05)Main job.07(.09).06(.09).10(.09).10(.09)Proportionate household income−.004**(.002)−.004**(.002)−.003*(.002)−.004**(.002)Socioeconomic status.28***(.03).28***(.03).21***(.03).21***(.03)Age.01(.004).005(.004).01(.004).01(.004)Gender−.07(.08)−.08(.08)−.02(.08)−.02(.08)Education controlsNoNoYesYesNoNoYesYesProduct category controlsYesYesYesYesYesYesYesYesObservations828828828828828828828828R2.07.07.32.32.25.25.35.35 1 Notes: Unstandardized regression coefficients. Standard errors in parentheses. Education controls: bachelor's degree is baseline. Product controls: jewelry is baseline. Main job: 0 = side job, 1 = main job. Gender: 0 = female, 1 = male.2 *p <.10.3 **p <.05.4 ***p <.01.In addition, we captured a series of control variables with regard to the business type and the producer. We captured the product domain(s) by asking what type(s) of products they sell (see WA-B2). We further asked participants about their experience as a producer (how long they have been selling their products [in months]), whether selling these products is their main or side job (0 = ""side job,"" and 1 = ""main job""), and how much the income from selling these products contributes to their total household income (in %). To assess each individual producer's socioeconomic status, we used a three-item scale (α = .82; [20]). Finally, participants indicated their gender (0 = ""female,"" and 1 = ""male""), age (in years), country of residence (0 = ""United States,"" and 1 = ""other""), relationship status (0 = ""single,"" 1 = ""committed relationship,"" 2 = ""married,"" and 3 = ""widowed""), and highest degree of education (see WA-B3). No further measures were taken. Sample DescriptionThe sample consisted of 828 individual producers (Mage = 35.22 years, SD = 8.43, 61.0% female; 90.6% U.S. residents). We successfully recruited a diverse sample of producers. Participants differed widely in their experience selling their products (M = 56.89 months, SD = 40.59, range: 0 to 360.00), revenue and profit (revenue: Mdn = US$4,100.00, M = US$44,904.99, SD = US$81,115.39, range: US$0 to US$520,000.00; profit: Mdn = US$2,000.00, M = US$14,545.44, SD = US$27,234.03, range: US$0 to US$350,000.00), percentage of household income from selling (M = 54.94%, SD = 34.51%, range: 0% to 100%), and types of products produced (see WA-B4). Because several of our open-ended measures were highly skewed (sales volume, revenue, profit, and experience), we log-transformed those variables (for descriptive statistics and interconstruct correlations, see WA-B5). ResultsWe tested the sales-as-signal effect by estimating a series of ordinary least squares (OLS) regression models accounting for different sets of control variables. Columns 1a and 1b in Table 2 report the results of regression models in which one's happiness as a producer, our dependent variable, is regressed on sales volume while controlling for product type(s) and for the economic implications of sales (in terms of either revenue or profit). We find that sales volume is indeed positively and significantly related to one's happiness, controlling for economic implications (Column 1a: b = .06, SE = .03, p = .02; Column 1b: b = .05, SE = .03, p = .04). This effect persists, and even gets a bit stronger, after adding all further control variables (see Column 2a: b = .07, SE = .02, p <.01; Column 2b: b = .06, SE = .02, p <.01).We ran the same models using sales growth as the independent variable, with consistent results. Columns 3a and 3b in Table 2 show that sales growth is positively and significantly related to producers' happiness when controlling for product domain(s) and producers' revenue (b = .34, SE = .02, p <.01) or profit (b = .34, SE = .02, p <.01). The effect remains positive and significant when adding all further control variables (see Column 4a: b = .19, SE = .03, p <.01; Column 4b: b = .19, SE = .03, p <.01). DiscussionUsing a survey of actual producers, we find real-world evidence for the sales-as-signal effect (H1): individual producers draw happiness from selling their self-made products above and beyond the money they make from these sales.In a supplementary study (see Study 1S in WA-C), we examined whether the observed relationship between sales and happiness, controlling for monetary considerations, holds in a more controlled setting with random assignment. We tested this using actual producers from the Australian marketplace madeit.com.au (n = 169) by making them recall a period of time in which they sold more (vs. fewer) products than normal and asking them about their happiness as a producer at that point in time. The results of this experiment replicate the main finding of Study 1. Consistent with our theorizing, we find that individual producers are happier at periods of time in which they sell more (vs. fewer) products even when controlling for the profit they made from these sales. In addition, the effect is robust to another potential happiness driver: future profit expectations. Study 2Study 2 aims to provide a behavioral test of causality for the effect of sales on happiness through feelings of self-validation (H2). We did this by conducting a two-stage behavioral experiment in which we first asked participants to actually produce their own products and then manipulated at a later stage whether their products were sold.Another aim of Study 2 was to investigate how the extent of deliberateness of product choice moderates the sales-as-signal effect. We theorize that sales credibly inform individuals about their skills and competencies as producers to the extent that they provide information about the quality of the producers' products (H3). When sales do not reflect a deliberate decision, for example when a product is chosen at random, they are less informative about the buyer's product quality perceptions and thus the sales-as-signal effect should be reduced. MethodWe invited 1,700 American workers from Amazon Mechanical Turk (MTurk) in two waves[10] to participate in a two-stage study. We recruited participants interested and skilled in drawing. The experiment employed a 2 (sales: product sold vs. product not sold) × 2 (choice: deliberate vs. random) between-participants design. We informed all participants that, to enrich life in times of crisis, a researcher was delegated by the university's program director to organize an exhibition of comics that symbolize the defeat of the coronavirus (SARS-CoV-2). We told participants that their task would be to draw a picture of a superhero conquering the coronavirus, that all drawings would be shown to potential customers (i.e., faculty and administrative staff at the university) who could purchase one drawing for $1.50, and that all purchased drawings would be exhibited. To ensure that participants only submitted their original work, we told participants to sign their hand-drawn picture with their MTurk ID. Finally, we told participants that they would receive a second survey about one week later that would inform them about whether their drawing was sold. Importantly, to keep the monetary rewards constant, we truthfully told participants that all participants would receive $1.50 as a compensation for their work—irrespective of whether their drawing was sold. We received 417 valid drawings (see Figure 1 for a selection of drawings; invalid submissions included blank submissions, pictures that were unrelated to the task, and unsigned submissions).Graph: Figure 1. Examples of drawings of superheroes defeating the coronavirus (Study 2).Approximately one week later, we invited workers who had submitted valid drawings to participate in the second part of this study, which included our manipulations. A total of 347 workers accepted this invitation (Mage = 33.39 years, SD = 11.15, 53.0% female). We first reminded all participants that the researcher showed all drawings to potential customers (i.e., faculty and administrative staff at the university), who could purchase one drawing for $1.50. In addition, participants were reminded that all artists would receive $1.50 as compensation for their work, irrespective of whether their drawing was sold. Then, participants either read that each customer who decided to purchase a drawing selected the one they wanted (deliberate choice) or that each customer who decided to purchase a drawing received one selected at random (random choice). In addition, participants read that their specific drawing either was or was not sold (see WA-D for study materials). Participants then indicated their level of happiness as a producer on two seven-point items (""extremely unhappy/extremely happy,"" ""extremely dissatisfied/extremely satisfied""; r = .87) and their feelings of self-validation on three seven-point items (""not at all skilled/extremely skilled,"" ""not at all competent/extremely competent,"" and ""not at all talented/extremely talented""; α = .95).[11] Finally, participants completed two attention checks (""was your drawing sold?""; ""yes, my drawing was sold/no, my drawing was not sold""; ""how did the purchase of drawings happen?""; based on customers' deliberate/random choice [the 'random selector' picked the drawing]/I cannot remember)[12] and indicated their age and gender. Results HappinessA 2 × 2 ANOVA on happiness reveals a significant main effect of sales (Msold = 6.03, SD = 1.47 vs. Mnot_sold = 3.67, SD = 1.27; F( 1, 343) = 263.27, p <.001) and a nonsignificant main effect of product choice (Mdeliberate = 4.81, SD = 1.91 vs. Mrandom = 4.93, SD = 1.70; F( 1, 343) = .65, p = .42). Importantly, we also obtained the expected significant interaction effect (F( 1, 343) = 11.20, p = .001; see Figure 2, Panel A). Planned contrasts reveal that when the product choice was deliberate, the effect of sales on happiness was significantly stronger (Msold = 6.21, SD = 1.17 vs. Mnot_sold = 3.37, SD = 1.38; F( 1, 343) = 194.37, p <.001) than when the product choice was random (Msold = 5.84, SD = 1.34 vs. Mnot_sold = 3.98, SD = 1.50; F( 1, 343) = 81.75, p <.001).Graph: Figure 2. Happiness and self-validation as a function of product choice and sales (Study 3).In addition, we find that participants whose products were sold were happier when the focal decision was deliberate versus random (Mdeliberate = 6.21, SD = 1.17 vs. Mrandom = 5.84, SD = 1.34; F( 1, 343) = 8.50, p = .004). Interestingly, for participants who learned that their products were not sold, happiness was marginally higher when the focal decision was random rather than deliberate (Mdeliberate = 3.37, SD = 1.38 vs. Mrandom = 3.98, SD = 1.50; F( 1, 343) = 3.28, p = .07). Self-validationA 2 × 2 analysis of variance (ANOVA) on self-validation reveals a significant main effect of sales (Msold = 4.97, SD = 1.32 vs. Mnot_sold = 3.44, SD = 1.55; F( 1, 343) = 99.45, p <.001) and a nonsignificant main effect of product choice (Mdeliberate = 4.17, SD = 1.71 vs. Mrandom = 4.26, SD = 1.53; F( 1, 343) = .34, p = .56). Importantly, we also obtained a significant interaction effect (F( 1, 343) = 12.23, p = .001; see Figure 2, Panel B). Planned contrasts reveal that when the product choice was deliberate, the effect of sales on self-validation was significantly stronger (Msold = 5.19, SD = 1.29 vs. Mnot_sold = 3.14, SD = 1.46; F( 1, 343) = 92.04, p <.001) than when the product choice was random (Msold = 4.74, SD = 1.31 vs. Mnot_sold = 3.76, SD = 1.60; F( 1, 343) = 20.67, p <.001).In addition, we find evidence that participants whose products were sold reported higher feelings of self-validation when the product choice was deliberate (Mdeliberate = 5.19, SD = 1.29 vs. Mrandom = 4.74, SD = 1.31; F( 1, 343) = 8.18, p = .004), whereas participants whose products were not sold reported higher feelings of self-validation when the product choice was random (Mdeliberate = 3.14, SD = 1.46 vs. Mrandom = 4.74, SD = 1.31; F( 1, 343) = 4.32, p = .04). Although not the focus of our theorizing, the latter difference further validates our signaling framework, as it indicates that a negative sales signal is more detrimental to feelings of self-validation when it is more easily interpreted as a reflection of one's competencies. Moderated mediationA moderated mediation analysis ([21], Model 7, n = 5,000) with sales (0 = not sold, 1 = sold) as the independent variable, product choice (0 = deliberate, 1 = random) as the moderator, self-validation as the mediator, and happiness as the dependent variable produces a significant index of moderated mediation (b = −.48, SE = .16, CI95% = [−.81, −.19]). Supporting our prediction, the effect of sales on happiness through feelings of self-validation was significantly stronger when the product choice was deliberate (b = .93, SE = .15, CI95% = [.66, 1.23]) versus random (b = .45, SE = .11, CI95% = [.23,.67]). DiscussionUsing a multiwave experimental paradigm involving actual production, Study 2 provides causal evidence in support of our primary prediction that, above and beyond the monetary reward, sales increase producers' happiness via elevated feelings of self-validation (H2). In addition, the results are consistent with our theorizing that sales have a stronger impact on individual producers' self-validation when product choice is more deliberate (H3).Although significantly smaller (as hypothesized), we also find a residual effect of sales on happiness and, to a lesser extent, self-validation when products were sold but selected at random. This finding is beyond the scope of our hypotheses, so we can only speculate about why even a random sale might make producers happy. One possibility is a process identified by Marx (1844/1993; see also [43]) in his Comments on James Mill. Marx's discussion of what it is like to produce as a human being (rather than being a cog in a machine), suggests that producing something that is used and enjoyed by another person provides important enjoyment of life and, to some extent, affirms the producer's unique competency as a person. Thus, the mere fact that another person has acquired one's product, even if the exact product choice was made at random, may provide some basic sense of self-validation and happiness in turn. Study 3Study 3 investigates another moderator of the sales-as-signal effect: the monetary cost involved in purchasing the product. We theorize that sales credibly inform individuals about their skills and competencies as a producer to the extent that the acquisition of the product is costly to the customer (H4). In this study, we manipulated the monetary cost of sales by varying the shipping cost that a buyer needed to pay to acquire the product. We predicted that, although the financial gain from the sale is constant (the buyer bears the shipping costs), individual producers would be happier when the buyer accepts paying higher (vs. lower) shipping costs. As in the previous study, we again tested whether the increase in happiness can be explained by feelings of self-validation (H2). MethodWith the help of hobbii.com, an online shop that sells knitting kits and supplies, we recruited 1,230 recreational knitters (Mage = 53.38 years, SD = 12.26, 99.4% female). The company promoted a link to our study in their weekly newsletter, which was received by German-speaking customers (and which yielded a response of N = 818) as well as customers from the United States (N = 412). As an incentive to participate, we raffled ten gift cards to the company's online shop worth $30 each.The experiment employed a between-participants design with three conditions (sales: baseline vs. higher cost vs. lower cost). We asked all participants to imagine marketing their self-made knitted accessories on an online platform. Specifically, as we ran this study in June and participants came from Europe and the United States, we asked participants to assume they currently produce and sell summer beanies (i.e., beanies that are made from thin, lightweight material). Next, participants read that they received an email from a customer in New Zealand asking about winter beanies. Participants then read that they were able to offer their self-made winter beanies for $30.00. Finally, participants either read that the customer from New Zealand did not respond to this offer (baseline), decided to purchase the beanie for $30.00 plus $20.90 shipping costs (higher cost), or decided to purchase the beanie for $30.00 plus $2.90 shipping costs (lower cost; for study materials, see WA-E1).Participants then indicated their level of happiness (r = .87) and feelings of self-validation (α = .93) on the same scales as in Study 2. To account for alternative mechanisms, participants indicated how much profit they made with the customer from New Zealand (1 = ""none,"" and 7 = ""a lot"") and how they thought their profit from selling products would develop in the near future (1 = ""decrease a lot,"" and 7 = ""increase a lot""). Participants further completed the following attention check: ""Did the customer from New Zealand buy your beanie?"" (yes/no). Participants in the lower cost and higher cost conditions additionally completed the following attention check: ""How much did it cost to ship the beanie to New Zealand?"" ($2.90/$20.90).[13] Finally, participants indicated their gender and age.[14] Results HappinessA one-way ANOVA with happiness as the dependent variable produces a significant effect (F( 2, 1,227) = 406.52, p <.001). Follow-up contrasts reveal that, compared to the baseline condition in which the customer from New Zealand did not purchase the product (M = 3.13, SD = 1.10), participants were happier when the customer from New Zealand decided to purchase the product (higher shipping cost: M = 5.90, SD = 1.38; t( 1,227) = 27.93, p <.001; lower shipping cost: M = 5.03, SD = 1.72; t( 1,227) = 19.03, p <.001). More importantly, participants reported significantly higher levels of happiness when the buyer paid higher versus lower shipping costs (t( 1,227) = 8.79, p <.001). Self-validationA one-way ANOVA with feelings of self-validation as the dependent variable produces a significant effect (F( 2, 1,227) = 160.52, p <.001). Follow-up contrasts reveal that, compared with the baseline condition (M = 4.09, SD = 1.27), participants reported greater self-validation when the customer from New Zealand decided to purchase the product (higher shipping cost: M = 5.46, SD = .98; t( 1,227) = 17.43, p <.001; lower shipping cost: M = 5.07, SD = 1.12; t( 1,227) = 12.38, p <.001). In support of our theorizing, we further find significantly higher feelings of self-validation in the case of higher (vs. lower) shipping costs (t( 1,227) = 4.98, p <.001). MediationWe conducted mediation analyses ([21], Model 4, n = 5,000) with our multicategorical independent variable, happiness as the dependent variable, and self-validation as the mediator. We find positive and significant indirect effects on happiness through self-validation when comparing ( 1) the higher shipping cost condition with the baseline condition (b = .78, SE = .07, CI95% = [.65,.91]), ( 2) the lower shipping cost condition with the baseline condition (b = .55, SE = .06, CI95% = [.45,.67]), and ( 3) the higher shipping cost condition with the lower shipping cost condition (b = .22, SE = .05, CI95% = [.14,.31]). These results are robust to the inclusion of current profits and future profit expectations as covariates (for detailed results, see WA-E2). DiscussionStudy 3 further corroborates our theorizing by showing that the extent to which sales provide self-validation, over and above monetary outcomes for the producer, depends on the monetary cost of the sales signal. In particular, participants reported greater self-validation and thus happiness when the buyer accepted to pay higher (vs. lower) shipping costs (H4). Study 4Study 4 compares the effect of sales with that of a noneconomic signal. We focus on comparing sales with likes, which are arguably the most common form of market signals on electronic platforms such as Etsy. Comparing the effects of sales and likes is also important from a practical point of view because the seller dashboards of prominent online platforms tend to display sales and likes (or related forms of noneconomic signals such as favorites) concurrently, raising the question of how these different forms of signals impact individual producers' happiness. We hypothesize that individual producers experience greater self-validation and thus more happiness from sales than from likes, even above and beyond the monetary rewards from these sales (H5). We tested this by conducting another two-stage behavioral experiment in which we first asked participants to actually produce products and then manipulated at a later stage whether their products were either acquired or liked by customers. To test whether the happiness advantage of sales over likes goes beyond their cost difference to the customer, Study 4 kept cost to the customer (along with the monetary outcomes for the producer) constant across signals. MethodWe invited 1,000 American workers on Prolific to participate in a two-stage study in which we asked them to demonstrate their writing skills. The experiment employed a 2 (signal: sales vs. likes) × 2 (number: high vs. low) between-participants design. All participants were told that their task would be to create a positive slogan for a ""post-COVID"" event that would be taking place at our university. We told participants that we would print each slogan on a poster and exhibit each poster at the event like a gallery exhibition. Furthermore, participants were told that all guests of the event would pay an entrance fee of $1.50 to cover costs and that, in return, each guest would receive a token. In the sales condition, we told participants that guests could use this token to purchase a poster of their choice and that each poster could only be purchased once. In the likes condition, we told participants that guests could use this token to like a poster of their choice by pinning the token on the poster and that each poster could only be liked once. Next, we asked all participants (those in both the sales and likes conditions) to create their slogan by finishing the following sentence: ""When Corona is over....""Two weeks later, we invited those who had submitted valid slogans (N = 1,000) to participate in the second part of the study, yielding 843 participants (Mage = 34.70 years, SD = 12.63, 48.2% female). A chi-squared test revealed that participation in the second part of the study did not depend on whether participants were assigned to the sales or the likes condition in the first part (χ2( 1) = .11, p = .75). First, we thanked all participants for submitting their slogan and reminded them that all slogans were printed on posters exhibited at the ""post-COVID"" event, at which guests could either purchase (sales condition) or like (likes condition) a poster of their choice. In addition, we reminded participants that each poster could only be purchased/liked once. Next, participants in the sales condition were either told that one (high sales) or no (low sales) customer(s) bought the poster with their slogan on it, while participants in the likes condition were either told that that one (high likes) or no (low likes) customer(s) liked the poster with their slogan on it. Next, all participants indicated their level of happiness (r = .91) and their feelings of self-validation (α = .96) on the same scales as in the previous studies. Finally, participants responded to an attention check (""how many guests [purchased/liked] the poster with your slogan on it?""; ""no (0) guest [bought/liked] the poster with my slogan on it/one ( 1) guest [bought/liked] the poster with my slogan on it"")[15] and indicated their gender and age (for study materials, see WA-F). Results HappinessA 2 × 2 ANOVA on happiness produces significant main effects of signal (Msales = 4.43, SD = 1.64 vs. Mlikes = 4.21, SD = 1.60; F( 1, 839) = 7.28, p = .007) and number (Mhigh = 5.37, SD = 1.27 vs. Mlow = 3.28, SD = 1.20; F( 1, 839) = 610.80, p <.001). More importantly, we obtained the predicted signal by number interaction (F( 1, 839) = 9.23, p = .002; see Figure 3, Panel A). Planned contrasts reveal that the effect of the sales signal was significantly stronger (Mhigh = 5.61, SD = 1.17 vs. Mlow = 3.27, SD = 1.11; F( 1, 839) = 386.45, p <.001) than the effect of the likes signal (Mhigh = 5.13, SD = 1.32 vs. Mlow = 3.30, SD = 1.29; F( 1, 839) = 234.11, p <.001). In addition, we find that participants whose poster was sold were happier than participants whose poster was liked (F( 1, 839) = 16.36, p <.001). We detected no such differences between participants whose poster was not sold versus not liked (F( 1, 839) = .06, p = .81).Graph: Figure 3. Happiness and self-validation as a function of signal and value (Study 4). Self-validationA similar 2 × 2 ANOVA on feelings of self-validation produces a significant main effect of signal (Msales = 3.62, SD = 1.48 vs. Mlikes = 3.41, SD = 1.50; F( 1, 839) = 5.44, p = .02) and a significant main effect of number (Mhigh = 4.05, SD = 1.40 vs. Mlow = 3.00, SD = 1.38; F( 1, 839) = 121.20, p <.001). This main effect was qualified by a significant interaction effect (F( 1, 839) = 7.36, p = .007; see Figure 3, Panel B), demonstrating that the effect of the sales signal was significantly stronger (Mhigh = 4.29, SD = 1.29 vs. Mlow = 2.98, SD = 1.37; F( 1, 839) = 94.48, p <.001) than the effect of the likes signal (Mhigh = 3.81, SD = 1.47 vs. Mlow = 3.02, SD = 1.39; F( 1, 839) = 34.29, p <.001). In addition, participants whose poster was sold felt more validated than participants whose poster was liked (F( 1, 839) = 12.66, p <.001). Feelings of self-validation did not differ between participants whose poster was not sold versus not liked (F( 1, 839) = .07, p = .79). Moderated mediationA moderated mediation analysis ([21], Model 7, n = 5,000 bootstraps) with number (0 = low, 1 = high) as the independent variable, signal (0 = likes, 1 = sales) as the moderator, self-validation as the mediator, and happiness as the dependent variable produces a significant index of moderated mediation (b = .25, SE = .09, CI95% = [.07,.44]). As expected, the mediating effect through self-validation on happiness was stronger when participants' posters were sold versus not sold (b = .63, SE = .08, CI95% = [.48,.78]) than when participants' posters were liked versus not liked (b = .38, SE = .07, CI95% = [.25,.51]). DiscussionStudy 4 compared the effects of sales and likes regarding their impact on individual producers' feelings of self-validation and happiness as a producer (H5). In support of our theorizing, we find that sales produce stronger effects on happiness and self-validation than likes, even when the associated monetary costs were kept constant between conditions. In a supplementary study (see Study 4S in WA-G), we tested the effects of sales and likes when presenting information about sales and likes simultaneously (mimicking the situation on online platforms such as Etsy) among a sample of recreational knitters (n = 161). Holding the monetary rewards to the producer (but not the costs to the customer) from sales and likes constant, we again find that sales make individual producers happier than likes and that this effect is driven by feelings of self-validation. Study 5Study 5 tests whether the magnitude of the sales-as-signal effect depends on whether individuals sell their self-made products or products that were made by someone else. We tested this by varying sales of products that were self-produced versus produced by someone else. Although selling more versus fewer products should increase individuals' happiness in both situations, we predict an incremental increase in happiness from selling self-made products (H6). MethodParticipants included 1,008 U.S. consumers recruited from MTurk (Mage = 39.81 years, SD = 12.41, 47.6% female). The experiment employed a 2 (sales: high vs. low) × 2 (product: self-made vs. other-made) between-participants design. All participants imagined selling muffins at a local food market. The muffins were either made by themselves (self-made condition) or by someone else (other-made condition). To keep the monetary rewards constant across conditions, we told participants that the organizers of the food market receive all sales revenue from people selling products at the market for the first time (which is how the food market finances itself). We further informed participants that there are no other costs involved in being able to sell products at the food market. Thus, across conditions, the monetary reward from potential sales was zero. Below this description, participants responded to an attention check verifying they understood that revenue from their first-time sales would go to the food market. Next, participants either read that they made (self-made condition) or received (other-made condition) a total of 50 muffins to sell and that they sold either 36 (high sales condition) or six (low sales condition) of them at the food market.After reading this information, participants indicated their level of happiness on the same scale as in the previous studies (r = .92). Next, participants completed two more attention checks (""which of the following statements is correct?""; ""at the food market, I sold muffins that were made by someone else/at the food market, I sold muffins that I made myself""; ""how many muffins were sold at the food market?""; 6/36).[16] Finally, participants indicated their gender and age (for complete study materials, see WA-H). ResultsA 2 × 2 ANOVA on happiness reveals the expected significant main effect of sales (Mhigh_sales = 4.53, SD = 1.54 vs. Mlow_sales = 2.59, SD = 1.49; F( 1, 1,004) = 418.73, p <.001) and a nonsignificant main effect of product (Mself-made = 3.58, SD = 1.83 vs. Mother-made = 3.55, SD = 1.78; F( 1, 1,004) = .18, p = .67). Importantly, we also obtained a significant interaction effect (F( 1, 1,004) = 19.65, p <.001; see Figure 4). As hypothesized, planned contrasts reveal that the effect of sales on happiness was significantly stronger when participants sold their self-made muffins (Mhigh_sales = 4.77, SD = 1.42 vs. Mlow_sales = 2.41, SD = 1.37; F( 1, 1,004) = 310.54, p <.001) than when participants sold muffins made by someone else (Mhigh_sales = 4.31, SD = 1.62 vs. Mlow_sales = 2.79, SD = 1.59; F( 1, 1,004) = 128.21, p <.001).Graph: Figure 4. Happiness as a function of product and sales (Study 5).Importantly, we find that participants who sold more muffins were happier when they made the muffins themselves versus someone else making the muffins (Mself-made = 4.77, SD = 1.42 vs. Mother-made = 4.31, SD = 1.62; F( 1, 1,004) = 11.85, p = .001). In contrast, participants who sold fewer muffins were happier when the muffins were made by someone else versus by themselves (Mself-made = 2.41, SD = 1.37 vs. Mother-made = 2.79, SD = 1.59; F( 1, 1,004) = 8.00, p = .005). DiscussionThe results of Study 5 complement the previous studies by demonstrating that sales have a stronger effect on individuals' happiness when individuals sell their self-made products than when they sell products that were made by someone else (H6). This finding provides additional evidence that the effect of selling self-produced products on happiness is driven by the effect of sales on self-validation as a competent producer, and it rules out the alternative explanation that the effect is driven solely by the effect of sales on self-validation as a competent seller or marketer. Study 6Study 6 further tests whether selling their self-made products has any incremental effects on producers' happiness compared with merely producing products. Because sales function as a credible signal regarding producers' skills and competencies, we expect that sales affect individual producers' self-validation and thus happiness beyond individual producers' self-validation and happiness derived from production. We tested this with another two-stage behavioral experiment in which we again asked participants to produce their own products and manipulated at a later stage whether their products were sold. In addition, we added a control condition in which participants' products were not offered for sale, and we assessed participants' feelings of self-validation and happiness derived from both production and selling. Doing so allowed us to assess any increase in self-validation and happiness as a result of a successful sale or any backfiring effect in the case of no sales, compared with the baseline of ""not going to the market"" to begin with. MethodWe invited 500 U.S. consumers on Prolific to participate in a two-stage study in which we asked them to demonstrate their writing skills. In the study description, all participants were informed that we would ask them to generate a slogan followed by a short survey and that they would receive a second survey in about two weeks. The experiment employed a 2 (stage: production vs. selling) × 3 (sales: control vs. product sold vs. product not sold) mixed design with stage as the within-subjects factor and sales as the between-subjects factor. In the production stage, we told all participants that their task would be to create a positive slogan one could print on a T-shirt that describes what they will do when the COVID crisis is over. Participants in the sales conditions were additionally told that all slogans from this study would be offered to our university community and that, if a given slogan finds a customer, that slogan will be printed on a T-shirt for that customer. We further informed participants in the sales conditions that each slogan will only be printed on a T-shirt once and that the price the customer will pay for the T-shirt will equal the cost incurred in having it produced. We also told participants in the sales conditions that the second survey would inform them about whether the T-shirt with their slogan had been purchased by a customer. Next, we asked all participants (those in the control and in the two sales conditions) to create their slogan by finishing the following sentence: ""When Corona is over...."" After creating their slogan, all participants indicated their level of happiness (r = .78) and their feelings of self-validation (α = .96) on the same scales used in the previous studies. Finally, we reminded all participants that they would receive a second survey in about two weeks.Two weeks later, we invited participants who had submitted valid slogans (N = 499) to participate in the second part of the study. A total of 384 participants accepted this invitation (Mage = 31.56, SD = 10.51, 45.6% female).[17] First, we thanked all participants for submitting their T-shirt slogan two weeks earlier. Participants in the sales conditions were additionally reminded that all slogans were offered to our university community and that the T-shirt with their slogan on it could only be purchased once. Next, we informed participants in the sales conditions that their T-shirt was either sold or not. Finally, all participants indicated their level of happiness (r = .85) and their feelings of self-validation (α = .97) on the same scales as in the production stage, as well as their gender and age (for study materials, see WA-I). Results HappinessA 2 × 3 mixed ANOVA on happiness with stage (production vs. selling) as the within-subject factor and sales (control vs. product sold vs. product not sold) as the between-subjects factor reveals the expected significant stage by sales interaction (F( 2, 381) = 127.47, p <.001; see Figure 5, Panel A). Participants' happiness did not differ across the sales conditions in the production stage (Mcontrol = 5.19, SD = 1.12 vs. Msold = 5.17, SD = 1.21 vs. Mnot_sold = 5.39, SD = 1.15; F( 2, 381) = 1.34, p = .26). In contrast, participants' happiness significantly differed across the sales conditions in the selling stage (F( 2, 381) = 157.63, p <.001). As expected, participants were happier when a T-shirt with their slogan on it was sold (M = 5.87, SD = 1.02) than when a T-shirt with their slogan on it was not sold (M = 3.37, SD = 1.17; t(381) = 17.47, p <.001). Compared with the control condition (M = 4.97, SD = 1.21), participants were happier when a T-shirt with their slogan on it was sold (t(381) = 6.26, p <.001) and less happy when a T-shirt with their slogan on it was not sold (t(381) = −11.40, p <.001).Graph: Figure 5. Happiness (left) and self-validation (right) as a function of stage and sales (Study 6).Comparing participants' happiness across the two stages reveals that, in the product sold condition, participants were happier in the selling stage (M = 5.87, SD = 1.02) than in the production stage (M = 5.17, SD = 1.21; F( 1, 381) = 30.93, p <.001). In the product not sold condition, participants were less happy in the selling stage (M = 3.37, SD = 1.17) than in the production stage (M = 5.39, SD = 1.15; F( 1, 381) = 279.11, p <.001). In the control condition, participants were marginally less happy in the selling stage (M = 4.97, SD = 1.21) than in the production stage (M = 5.19, SD = 1.12; F( 1, 381) = 3.38, p = .07). The main effect of stage was also significant (Mselling = 4.72, SD = 1.53 vs. Mproduction = 5.25, SD = 1.58; F( 1, 381) = 53.90, p <.001). Self-validationA similar 2 × 3 mixed ANOVA on self-validation reveals a significant stage by sales interaction (F( 2, 381) = 28.33, p <.001; see Figure 5, Panel B). In the production stage, participants' feelings of self-validation did not differ across the sales conditions (Mcontrol = 3.88, SD = 1.36 vs. Msold = 3.85, SD = 1.32 vs. Mnot_sold = 3.92, SD = 1.52; F( 2, 381) = .09, p = .91). In contrast, participants' feelings of self-validation significantly differed across the sales conditions in the selling stage (F( 2, 381) = 23.71, p <.001). As expected, participants felt more validated when a T-shirt with their slogan on it was sold (M = 4.35, SD = 1.27) than when a T-shirt with their slogan on it was not sold (M = 3.14, SD = 1.61; t(381) = 6.80, p <.001). Compared to the control condition (M = 3.89, SD = 1.35), participants felt more validated when a T-shirt with their slogan on it was sold (t(381) = 2.56, p = .01) and less validated when a T-shirt with their slogan on it was not sold (t(381) = −4.31, p <.001).In the product sold condition, participants felt more validated in the selling stage (M = 4.35, SD = 1.27) than in the production stage (M = 3.85, SD = 1.32; F( 1, 381) = 16.47, p <.001). In the product not sold condition, participants felt less validated in the selling stage (M = 3.14, SD = 1.61) than in the production stage (M = 3.92, SD = 1.52; F( 1, 381) = 42.19, p <.001). In the control condition, participants' feelings of self-validation did not differ between the selling stage (M = 3.89, SD = 1.35) and the production stage (M = 3.88, SD = 1.36; F( 1, 381) = .01, p = .92). The main effect of stage was not significant (F( 1, 381) = 1.61, p = .21). MediationWe conducted two mediation analyses ([21]; Model 4, n = 5,000 bootstraps) to test whether feelings of self-validation can explain the effect of our sales manipulation on happiness. We first looked at participants' ratings of happiness and self-validation obtained in the selling stage. We thus entered sales as the independent variable, self-validation in the selling stage as the mediator, and happiness in the selling stage as the dependent variable into the regression. This analysis produced positive indirect effects when comparing the product sold with the product not sold condition (b = .45, SE = .09, CI95% = [.29,.64]) and the control condition (b = .17, SE = .07, CI95% = [.05,.31]), and it produced a negative indirect effect when comparing the product not sold condition with the control condition (b = −.28, SE = .08, CI95% = [−.45, −.14]).We next examined whether relative differences in happiness between the selling stage and the production stage can be explained by relative differences in self-validation between the selling stage and the production stage. To do so, we calculated difference scores between happiness and self-validation ratings in the selling stage and the production stage. The respective regression analyses ([21]; Model 4, n = 5,000 bootstraps) produced positive indirect effects when comparing the product sold with the product not sold condition (b = .55, SE = .10, CI95% = [.36,.76]) and the control condition (b = .21, SE = .08, CI95% = [.07,.37]), and it produced a negative indirect effect when comparing the product not sold condition with the control condition (b = −.34, SE = .08, CI95% = [−.51, −.19]). DiscussionBy including a preselling baseline measure as well as a no-selling control condition, the results of Study 6 confirm there is a positive incremental effect of selling self-produced products on happiness. They also show, however, that marketing self-produced products can have a downside as well. Specifically, there is a happiness penalty to pay when products fail to sell, as the absence of sales leads to negative self-validation; that is, failing to sell products sends a negative signal about one's skills and competencies as a producer, reducing happiness. Thus, deciding to offer their self-made products for sale can enhance but also diminish individuals' happiness. General DiscussionThis research investigates the socioemotional benefits of selling self-made products. Eight studies provide evidence for a sales-as-signal effect: individual producers derive happiness from selling their products above and beyond the money they make from these sales (all studies). This is because sales validate their skills and competencies as a producer (Studies 2–4 and 6). In addition, Study 2 shows that the sales-as-signal effect is more pronounced when the choice mechanism that precedes the purchase is more versus less deliberate. Study 3 demonstrates that the increase in happiness from sales is higher when the buyer incurs higher (vs. lower) monetary costs in purchasing the product, even when this higher cost does not translate into higher financial return for the individual producer. Study 4 demonstrates that individual producers gain more happiness from sales than from receiving likes. Finally, Studies 5 and 6 show that individuals derive greater happiness from high sales of self-made products than from high sales of products that were made by someone else (Study 5) and that individuals show increases in happiness from pre- to post-selling but show decreases in happiness after (vs. before) failing to sell (Study 6). The studies (N = 4,970) span a variety of methodological approaches, designs, and procedures, and they feature different participant populations across three continents and producer communities. Contributions to TheoryOur research makes a number of theoretical contributions. First, previous research has extensively studied the psychological and behavioral consequences of engaging in self-production (e.g., [17]; [31]). This line of research has, however, focused on studying production for oneself or for gift giving. Our work goes a step further and examines a context in which individuals produce for the market; that is, with the aim of selling their creations to others. Our contribution thus lies in shedding light on the psychological consequences of participating in market exchanges. Our findings suggest that sales provide individual producers with a sense of self-validation and, in turn, happiness. We find that selling self-made products affects individuals' happiness beyond the happiness derived from producing the products.Second, we introduce a new perspective on the value of sales. Most models of producer behavior assume that producers are driven solely by a profit-maximization motive ([18]; [36]). Our findings caution against taking a reductionist view of sales by demonstrating that selling one's self-made products can also have important socioemotional value, as sales provide individual producers with self-validation regarding their skills and competencies as a producer. Thus, models of producer behavior may benefit from including the self-validation motive documented in this research.Third, in developing our theory, we conceptualized sales as a signal from buyers. This approach is different from the process investigated by existing research on signaling in marketing and management that conceptualizes signals as actions taken by sellers, who have low uncertainty about the product, to reduce uncertainty about product qualities for buyers ([ 5]; [ 8]; [23]). In our research, we find that sales constitute a signal that is sent by buyers and received by sellers. Moreover, our work suggests that sellers, even of self-produced products, actually do have uncertainty about their own products' qualities that is reduced by quality information from buyers. Thus, the traditional roles about who has information and who is uncertain are to some extent reversed. There may be less information asymmetry than is often assumed, and quality information may be exchanged in both directions. Furthermore, signals are traditionally conceptualized as intentional actions performed by the sender that in turn benefit the sender ([ 8]). Our account, in contrast, suggests that sending a signal can be a more or less incidental by-product (of a purchase) rather than an intentional signal and that the signal benefits the receiver of the signal (i.e., individual producer) by increasing self-validation. In addition, we demonstrate that the strength of a signal depends not only on the signal's costliness but also on its diagnosticity. For example, even when likes and sales are equally costly for the customer, sales may be seen as a more reliable indicator of product quality and producer skill. Likewise, a buyer's deliberate choice to purchase an item makes the sale of that item more diagnostic of producer skill than a sale that involves a buyer randomly picking a specific product. Thus, we believe that the present work offers a novel perspective on signaling.Finally, our research informs the ongoing debate within the marketing discipline regarding the implications of marketing for society ([ 7]). The backdrop of this debate involves widespread concern about the pernicious aspects of a consumer society ([24]), the negative environmental externalities of market exchanges ([25]), and the potentially exploitative nature of common marketing practices ([40]). We find that marketing one's products can actually have an important positive effect by providing individual producers with a significant happiness benefit through self-validation. We thus demonstrate how studying a classic marketing topic from a ""better world"" perspective can yield novel insights about marketing's potential to improve people's well-being. Moreover, marketing scholars studying the implications of marketing on society frequently focus on activities undertaken by marketers at large corporations, and more research is needed to uncover the societal impact of marketing beyond these contexts ([ 7]). Answering this call, we study the socioemotional benefits of individual producers participating in market exchanges. Our work also demonstrates the value of taking a behavioral approach to the study of supply-side behaviors. Despite calls for behavioral marketing research to broaden its focus beyond consumers ([27]; [44]), virtually all research by behavioral marketing scholars currently focuses on consumer behavior. We provide an example of how researchers can tackle important marketing phenomena on the supply side with a behavioral lens. Practical ImplicationsIn addition to these theoretical contributions, our work also provides several practical implications, especially for online marketplaces that focus on producers selling self-made products. First, the finding that sales can increase individual producers' happiness could be leveraged in, for example, the recruitment of prospective sellers by highlighting the socioemotional benefits of selling their products (e.g., ""Be a pro. Sell your tote bags on Etsy!""). Online marketplaces could also highlight socioemotional benefits to existing sellers to maintain motivation and retention. This could be done, for example, by stressing that customers recognize sellers' expertise by paying good money to buy their products (e.g., ""They voted with their wallets to tell you you're a pro"").In addition, our findings yield recommendations for the design of seller dashboards. Study 4 demonstrates that knowing how many people bought their products makes individual producers happier than knowing how many people liked their products. Study 3 suggests that showing how much customers paid might increase motivation, and thus production volume per seller and seller retention, beyond the amount of money the seller made. Therefore, we recommend designing seller dashboards so that the number of people who have made purchases and the average amount paid by customers (including shipping and other fees) are more prominent, rather than highlighting likes or aggregate revenue as is common in such dashboards.Finally, Study 2 shows that sales make individual producers happier when they know that buyers deliberately chose their products (vs. being chosen at random). This finding could be leveraged, for example, by encouraging buyers to leave a comment indicating why they chose that seller's product over other available product choices or by highlighting that customers decided to buy the focal producer's product despite having many other options. Future Research Opportunities Self-validation as a sellerWe hope that our findings will motivate other researchers to explore the under-researched topic of individual producers' motivations, beyond monetary considerations. Specifically, we identify several opportunities for future research. Our findings indicate that the sales-as-signal effect is strongly driven by individual producers' gain in self-validation as a competent producer (e.g., in Studies 2, 4, and 6 sales increased individual producers' happiness even when their self-made products were sold by someone else). This focus on self-validation as a producer was motivated by the fact that sellers on platforms such as Etsy typically spend more than half of their time designing and making their products and only about 10% of their time with marketing and promotion activities ([16]). However, to successfully sell their products, many individual producers not only engage in skilled production activities but also in skilled promotion- or selling-related activities, such as taking pictures of their offerings, pricing their products, maintaining their online appearance, and engaging in advertising on social media platforms. Therefore, sales might, at least for some people and in some contexts, also be an important source of happiness by validating individual producers' skills as a promoter or seller. Study 5 indeed shows that sales also increase the happiness of individuals selling products made by someone else above and beyond the monetary rewards from sales—though to a lesser extent compared to individuals selling their self-made products. Empirically examining the self-validation processes involved in selling would be a worthwhile future research direction. Likewise, while the entrepreneurship literature documents that financial success is related to entrepreneurs' satisfaction ([13]; [32]), future research could investigate whether entrepreneurial success (e.g., firm growth, increase in funding) has a causal effect on entrepreneurs' happiness beyond the related financial gains. In sum, our work provides a fruitful foundation for investigating the socioemotional benefits of participating in market exchanges across business contexts. Moderators of the sales-as-signal effect Characteristics of the purchase transactionOur studies investigated two characteristics of the purchase transaction—the deliberateness of the product choice and the monetary cost of buying—that moderate the strength of the sales signal. However, other important moderators of the sales-as-signal effect likely exist, and future research should expand the nomological network in which the sales-as-signal effect is situated. For example, the strength of the sales signal might depend on the expertise of the buyer. Compared to novices, experts are more capable of assessing products' quality ([ 1]). Having one's products bought by an expert in the respective product category should thus more credibly inform individual producers about their competencies as a producer than having one's products bought by a novice. Similarly, the number of buyers might affect how strongly individual producers perceive sales to correspond to the quality of their products. Would 40 buyers purchasing one product each provide more self-validation than four buyers purchasing ten items each?Besides the direct monetary cost associated with buying a product (Study 3), the sales-as-signal effect might also depend on the relative cost that buyers incur when purchasing a product. We would expect the effect of sales to be stronger when a buyer has a smaller (vs. larger) budget to spend. Likewise, relative price might also play a role, as the same cost might be a stronger signal of expertise when that cost is high compared to other products in the category than when that same cost is low compared to other products in the category. Finally, although customers accepting a high price should increase the strength of the sales signal, individual producers accepting higher prices might reach a point where they feel that customers are being treated unfairly (such as when customers have to pay extraordinarily high shipping costs) or that the prices might cause customers to refrain from making repeat purchases. Do concerns about fairness and relationship-building reduce the happiness individual producers gain from selling products at exorbitant prices? Characteristics of the individual producerOne could also argue that the strength of the sales-as-signal effect depends on certain characteristics of individual producers. We used the data obtained in Study 1 to test whether the effect of sales on happiness is moderated by any of the captured control variables. Only one statistically significant interaction emerged for both measures of our independent variable (i.e., sales volume and sales growth): a positive interaction effect between sales and producers' socioeconomic status (ps <.01; see WA-J1). This suggests that the effect of sales on happiness is stronger for producers that have a higher socioeconomic status—in other words, producers who feel financially secure. We also used the data obtained in Study 1S to examine whether the effect of sales on happiness is moderated by any of the captured control variables. Moderation analyses produce nonsignificant interaction effects between ( 1) sales and producers' experience and ( 2) sales and socioeconomic status (ps >.20; see WA-J2). Thus, although the moderation analyses of Study 1 suggest that the sales-as-signal effect is stronger for producers that have a relatively higher (vs. lower) socioeconomic status, we did not observe such an interaction effect in Study 1S. Therefore, future research might more closely look at the role socioeconomic status plays in the sales-as-signal effect.The strength of the sales-as-signal effect might also depend on individual producers' own evaluations of their products—that is, how competent individuals feel in producing their products might alter how happy they feel as a result of selling those products. We used the data of Study 6 to test this. A moderation analysis produces a nonsignificant interaction between sales (higher vs. lower) and self-validation from production on happiness from sales (p = .98; see WA-J3). This suggests that the positive effect of sales on happiness does not depend on producers' presales perception of their products. We encourage future researchers to look deeper into these, and possibly other, potential moderators of the sales-as-signal effect. ConclusionTraditional producer behavior models assume that producers' behavior is driven by the monetary rewards from selling their wares and that there is an information asymmetry that favors producers. In other words, these models assume producers have information about the quality of their products that (prospective) customers do not, leading producers to signal product quality to customers. This research shows that producers derive happiness from selling their wares that goes above and beyond any monetary rewards. It shows that there is also an information asymmetry in the other direction: Customers possess information about the skill level of the producer that they signal by purchasing the producer's products. This provides a feeling of self-validation to the producer, increasing their happiness. To understand the behavior of producers, it is critical to broaden our scope from purely monetary to self-validation benefits and from assuming a one-directional flow of quality information to a two-directional flow, with at least some quality signals being sent from customers to producers. Supplemental Materialsj-pdf-1-jmx-10.1177_00222429211064263 - Supplemental material for Sales and Self: The Noneconomic Value of Selling the Fruits of One's LaborSupplemental material, sj-pdf-1-jmx-10.1177_00222429211064263 for Sales and Self: The Noneconomic Value of Selling the Fruits of One's Labor by Benedikt Schnurr, Christoph Fuchs, Elisa Maira, Stefano Puntoni, Martin Schreier and Stijn M.J. van Osselaer in Journal of Marketing  "
18,"The Influence of Social Norms on Consumer Behavior: A Meta-Analysis Social norms shape consumer behavior. However, it is not clear under what circumstances social norms are more versus less effective in doing so. This gap is addressed through an interdisciplinary meta-analysis examining the impact of social norms on consumer behavior across a wide array of contexts involving the purchase, consumption, use, and disposal of products and services, including socially approved (e.g., fruit consumption, donations) and disapproved (e.g., smoking, gambling) behaviors. Drawing from reactance theory and based on a cross-disciplinary data set of 250 effect sizes from research spanning 1978–2019 representing 112,478 respondents from 22 countries, the authors examine the effects of five categories of moderators of the effectiveness of social norms on consumer behavior: ( 1) target behavior characteristics, ( 2) communication factors, ( 3) consumer costs, ( 4) environmental factors, and ( 5) methodological characteristics. The findings suggest that while the effect of social norms on approved behavior is stable across time and cultures, their effect on disapproved behavior has grown over time and is stronger in survival and traditional cultures. Communications identifying specific organizations or close group members enhance compliance with social norms, as does the presence of monetary costs. The authors leverage their findings to offer managerial implications and a future research agenda for the field.Keywords: cultural influence; meta-analysis; reactance; social approval; social influence; social marketing; social norms marketing; social normSocial norms shape consumer behavior. Defined as ""rules and standards that are understood by members of a group, and that guide and/or constrain social behavior without the force of laws"" ([26], p. 152), social norms influence various forms of everyday consumption, including food choices ([87]), responses to new products ([51]), and loyalty ([63]). For example, signs in a hotel stating that other hotel guests reuse their towels increase towel reuse ([38]). Social norms are often leveraged by marketers and policy makers to encourage various socially approved behaviors, such as conserving energy ([95], [96]), complying with product recalls ([85]), and making tax payments (Cabinet Office UK [22]). They are also used to discourage socially disapproved behaviors, such as polluting the environment ([109]) and smoking or excessive alcohol or drug use ([108]).The academic literature examining social norms has produced conflicting findings ([61]; [95], [96]). Some studies report large-scale favorable results for using social norms to curb socially disapproved behaviors ([21]). [90], for example, report a significant reduction (13%) in the prevalence of impaired driving among students. However, some campaigns encouraging socially approved behaviors have backfired. For example, [95], [96]) find that social norms for energy preservation can increase energy consumption. These mixed findings suggest contingent effects of social norms on behavior. A second reason for mixed findings is that some research studies actual behavior, while other research examines behavioral intentions. A final reason for mixed findings may lie in the fact that the country context introduces cultural factors into the study of norms that are important to their impact.Our article looks across a wide range of research on social norms across behaviors, time, and cultures to resolve these conflicting findings and to synthesize the extant literature on social norms. Specifically, we investigate the effects of social norms on actual consumer behavior and identify moderators of these effects, using reactance theory as a theoretical lens ([18]; [92]). We contend that the effectiveness of social norms varies with the level of consumer reactance they trigger ([18]); norms that are less likely to trigger reactance are more likely to be effective.We conduct a meta-analysis that examines the effects of five categories of moderators of the effectiveness of social norms on consumer behavior, matching central factors that may induce reactance. Specifically, we examine how the relationship between social norms and behaviors depends on ( 1) social approval or disapproval of behavior and other target behavior characteristics, ( 2) communication factors, ( 3) consumer costs, ( 4) environmental factors (e.g., culture, time), and ( 5) methodological characteristics (e.g., type of sample, study).We collected 250 effect sizes from 136 articles published between 1978 and 2019 across different fields (e.g., marketing, psychology, health, environmental studies), representing 112,478 respondents from 22 countries. In conducting this research, we encountered several meta-analyses related to social norms. However, most prior meta-analyses focus on a single behavior, such as condom usage ([98]), or else investigate limited set of communication factors, such as whether the norm is descriptive or injunctive ([76]; [91]). Moreover, most include consideration of behavioral intentions rather than actual behavior, which is our focus. Finally, some prior meta-analysis focus on studies that use a specific theoretical framework, such as the theory of planned behavior ([ 2]; [70]), which limits their generalizability.We aim to go beyond these insights by investigating critical moderators that have not been addressed in prior research. We not only study the new moderator of socially approved versus disapproved behavior but also examine new and managerially actionable moderators, such as target behaviors, communication factors, and consumer costs. Importantly, this study investigates behaviors (observed or reported) rather than intentions and covers consumer behaviors across domains, regardless of the theoretical framework used in primary studies. With this comprehensive approach, we establish that social norms have significant impacts on behavior, but the effect varies systematically according to the influence of a wide range of moderators.This research makes several contributions across domains. First, we go beyond previous meta-analyses and contribute to theories of reactance and social influence by uncovering previously overlooked moderators and establishing several new empirical generalizations. Second, for social norms marketing ([38]; [110]), we specify the effects of social norms for a broad spectrum of consumer behaviors and detail how practitioners and government officials can utilize actionable moderators, such as using appropriate communication elements for certain behaviors, countries, and consumers. This should improve their success rate which has been mixed to date.Third, we contribute to the literature on cross-cultural marketing ([88]; [93]; [106]) by establishing how cultural differences can determine the effects of social norms on both socially approved and disapproved behaviors. Finally, we develop a comprehensive research agenda, based on insights from our meta-analysis. Theoretical Background Social NormsSocial norms are a shared understanding among members of a society about which behaviors are permitted, forbidden, or obligatory ([29]). They result from exposure to and observations of others' behavior and act as a ""social proof,"" whereby consumers follow the actions or opinions of others, in the belief that ""If everyone is doing it, it must be a sensible thing to do"" ([25], p. 1015). Social norms serve as decision shortcuts for choosing how to behave in a given situation ([24]).One of their distinctive features is that social norms are shared, which implies the existence of some group through which they spread ([26]). Humans maintain social harmony by complying with the social order and developing coping strategies to ""fit in"" ([66]) or ""copy the successful"" ([46]). Consequently, humans have an almost automatic propensity to learn social norms ([81]). Yet, this propensity does not necessarily result in compliance with them ([84]).The reason is that unlike laws, social norms are informal—they regulate behaviors without formal enforcement ([43]), so consumers have the freedom to follow or violate social norms ([26]). Accordingly, their impact on behavior stems from two evolutionary desires: ( 1) for social acceptance or affiliation and ( 2) for avoiding negative social outcomes such as social exclusion ([ 8]; [66]). As people are free to comply, and are inclined to do so, understanding why they do not comply is key for identifying systematic differences in reactions to social norms. Social Norms and ReactanceDespite consumers' natural inclination to comply with social norms, research consistently shows that attempts at influence that cite social norms can evoke psychological reactance ([18]; [92]). Reactance stems from individuals cherishing their autonomy and freedom of choice. As [19], p. 420) explain, ""For a given individual at a given time, there is a set of behaviors in which he believes he is free to engage. Any reduction or threat of reduction in that set of free behaviors arouses a motivational state, 'reactance,' which is directed toward reestablishment of the lost or threatened freedom."" If consumers believe their freedom to engage in a specific behavior is threatened, this evokes reactance, which enhances the attractiveness of the threatened behavior.Reactance theory is useful for approaching the vast, heterogeneous literature on social norms because it provides a broad theoretical lens for investigating the influence of diverse factors, including behavioral, communication, consumer, and environmental factors ([92]). Thus, we build on the key antecedents of reactance: consumer expectations of freedom and the extent of the freedom threat ([92]) to understand the drivers of systematic differences in the effects of social norms on behavior. Expectations of freedomConsumers do not perceive all their behaviors as freedoms ([20]), so reactance is contingent on an expectation that the person can freely choose among different behavioral alternatives ([27]). Thus, consumers likely exhibit reactance to social norms that appear to undermine their freedom ([19]). If they lack expectations of freedom in the first place, social norms should trigger less reactance ([92]). Several studies affirm that consumer reactance to attempts to influence decreases if an experimental manipulation lowers their perceptions of choice freedom ([36]; [58]). Freedom threatA social influence attempt that implies that someone is trying to reduce freedom represents a threat ([18]). This threat of social norms is exacerbated if the norms exert greater pressure for change ([20]). The threat level tends to reflect the way a social norm is communicated ([92]), so more forceful messages prompt more reactance ([59]; [111]). For example, research suggests reduced compliance with messages that advocate teetotalling rather than limited drinking ([ 9]). Yet, freedom threats may also stem from barriers to performing a behavior, such as consumer costs. When costs are a barrier to free choice, the aroused reactance is directed at maintaining the threatened behavior and therefore increasing its desirability ([27]). Conceptual FrameworkBuilding on reactance theory, we identify different groups of moderators driving behavioral compliance with social norms, as shown in Figure 1. These moderators include target behavior characteristics, communication factors, consumer costs, and environmental factors.Graph: Figure 1. Conceptual framework: Factors that influence the effect of social norms on behavior. Target Behavior CharacteristicsThe effects of social norms may vary across behaviors because characteristics inherent to the behavior influence perceptions of the freedom to perform it and threat to that freedom. Social approval/disapproval of behaviorsSocieties have developed social reinforcement mechanisms that encourage some behaviors and discourage others ([48]; [72]). We define socially approved behaviors as being explicitly encouraged by society (e.g., recycling, volunteering), socially acceptable (e.g., carpooling), and/or perceived as appropriate by society (e.g., using condoms). We define socially disapproved behaviors as being explicitly discouraged (e.g., smoking), socially unacceptable (e.g., littering), and/or perceived as inappropriate (e.g., binge drinking). A socially approved behavior evokes positive reinforcement via social outcomes, such as inclusion, acceptance, and affiliation ([24]). A socially disapproved behavior instead induces negative reinforcement via social consequences, such as social exclusion, alienation, or ridicule ([66]). Social approval versus disapproval of behaviors is thus a crucial factor that has implications for consumers' expectations of their freedoms to perform them.Performing a socially disapproved behavior is potentially more damaging to society as a whole than failing to perform an approved behavior ([47]). Thus, to maintain social order, societies tend to be more punitive of disapproved behaviors ([33]; [46]). In contrast, not adopting an approved behavior is less harshly punished and sometimes can even bring positive benefits, such as elevation in inferred social status ([ 8]). Thus, consumers are less likely to perceive social norms regulating socially disapproved behaviors as limitations to their freedoms, which diminishes reactance and increases compliance with social norms discouraging these behaviors. Yet, social norms targeting socially approved behaviors are seen as freedom limitations, causing more reactance and reduced compliance. Thus, we expect social norms pertaining to socially disapproved behaviors to be more effective than those pertaining to socially approved behaviors (H1). Existing versus new behaviorsExisting behaviors are already performed by consumers, at least sometimes, in contrast to entirely new behaviors. Consumers already have exercised their freedom to perform the existing behaviors, so they may feel less threatened when encouraging existing behaviors and their reactance to social norms that target existing behaviors may be relatively low. In contrast, targeting a new behavior may represent a stronger threat to freedom and, thus, induce reactance and decrease compliance. Consistently, for example, compliance with hand-washing advice has been higher than compliance with mask-wearing advice during COVID-19 and other infection outbreaks ([32]; [103]). Hand washing is an existing behavior and engrained into daily routines, whereas mask wearing was new for most consumers and generated more reactance. Therefore, we expect social norms pertaining to existing behaviors to be more effective than those pertaining to new behaviors (H2). Hedonic versus utilitarian behaviorsHedonic behaviors are those driven by pleasure-related goals and are evaluated primarily on the benefits related to enjoyment, taste, aesthetics, and symbolic meaning. Utilitarian behaviors, instead, are driven by functionality goals and are performed and evaluated primarily on the basis of functional, instrumental, and practical benefits ([23]). Social norms can pertain to both types, including utilitarian behaviors such as banking (e.g., ""Most millennials use online banking"") and hedonic ones such as buying cosmetics (e.g., ""12 makeup bag must-haves""). But their effectiveness is not clear a priori. On the one hand, reactance to social norms might be higher for hedonic behaviors because consumers have a stronger desire to perform those behaviors as part of their sense of freedom ([79]). Consumers can leverage social norms to justify a desirable behavior for themselves and enhance their perceptions of freedom to perform it. For example, the justification that ""everyone's doing it"" is common for hedonic behaviors ([39]) and can increase perceived freedom for engaging in these behaviors. On the other hand, indulging in a hedonic behavior often prompts a sense of guilt, making it harder to justify ([75]; [83]), which may reduce consumers' perception of freedom. With these opposing predictions, we treat the effects of social norms on hedonic versus utilitarian behaviors as an empirical question. Behaviors benefiting other peopleSome behaviors benefit other people directly (e.g., donating to charity), whereas others have indirect benefits (e.g., recycling). Social norms stem from group considerations, so consumers' willingness to enact their freedom may decrease if they realize that others will be negatively affected by their social norm violations ([101]). Correspondingly, behaviors that have negative implications for others yield lower reactance levels ([34]). We therefore expect that when others benefit from the behavior, this will enhance the effect of social norms on that behavior (H3). Public versus private behaviorsWe define public behaviors as those that are performed in public or can be observed by others (e.g., using public transport), in contrast to private behaviors (e.g., reducing energy consumption at home). Private behaviors are not subject to others' scrutiny, so consumers' perception of freedom threat to perform them may be relatively low, which should decrease reactance ([92]). This argument would imply that compliance with social norms regulating private (vs. public) behaviors should be higher. Yet, for public behaviors, reactance may also be reduced but for a different reason. Specifically, consumers are often concerned with how others perceive them ([65]), which reduces their willingness to enact their freedom and, in turn, reduces reactance. This would suggest higher compliance with social norms regulating public (vs. private) behaviors. Given these two opposing predictions, we refrain from making a directional hypothesis about this variable. Communication FactorsThe use of social norms can trigger reactance because communication factors influence the perceived threat to behavioral freedom ([ 9]; [92]). We consider several communication factors, such as how the norm is formulated as well as whether it benefits an organization, references specific groups, and includes explicit sanctions or rewards. Norm formulationSocial norms can be formulated as descriptive or injunctive. Descriptive norms describe typical behaviors of some relevant group and signal which behaviors are most popular ([25]; [95]). Injunctive norms instead prescribe certain behaviors and indicate what the target consumer should or should not do. For example, a list of ""bestsellers"" represents descriptive norms, but ""ten must-read books"" lists communicate injunctive norms. As injunctive norms convey explicit demands, which consumers likely perceive as forceful threats to their freedom, they should generate more reactance than descriptive norms ([69]; [109]). Consumers exposed to descriptive norms instead may come up with reasons for the behavior of the majority and adjust their own behavior accordingly, without much reactance ([95]). Therefore, we expect a stronger impact of descriptive (vs. injunctive) social norms on behaviors (H4). Organization-benefiting normsWe define social norm communications that reveal a specific entity, such as a firm or government body, which would benefit from compliance with the social norm, as organization-benefiting social norms. For example, ""my friends subscribed to the university's gym program"" would benefit the gym if the target consumer complied with this behavior. While specific entity matters, overall, social norms that refer to organizations tend to be more concrete and specific because they activate situational factors (i.e., where and when the norm applies) ([ 1]). Such specificity and concreteness diminish the general threat to freedom for consumers by limiting it to the particular situation, which lowers their reactance ([38]). We thus expect organization-benefiting social norms to be more effective than those that do not mention organizations (H5). Close group membersCommunications about social norms often specify close group members—that is, people who are genetically related (e.g., family) or similar (e.g., close friends)—rather than refer to an abstract group (e.g., fellow citizens, people). Evolutionary predictions of social cooperation highlight kinship mechanisms. Namely, a request that activates a kin care motive reduces reactance and promotes compliance without expectations of reciprocation ([41]; [46]). The closer the relationship is, the less reactance consumers are likely to experience, enhancing norm compliance ([79]; [99]). Thus, we expect social norms referring to a close group member to be more effective than social norms referring to abstract or distant groups (H6). Authority figuresCommunications around social norms often refer to authority figures, or individuals who can exercise power over others, formally or informally (e.g., superiors, experts, government officials, teachers), to enhance compliance. [78] famous studies show that formal orders from an authority figure (real or perceived) increase obedience. Yet, because social norms are informal, being required to do something by an authoritative source may make the threat to freedom more salient and trigger reactance ([ 4]). For example, expert recommendations may lead to reactance and diminish compliance ([36]). For these reasons, we expect social norms referring to authority figures to be less effective than those that do not refer to authority figures (H7). Explicit sanctions and rewardsThe sanctions and rewards associated with noncompliance and compliance with social norms might be either implicit, meaning they are indirect and left for consumers to infer, or explicit, meaning they are clearly stated. If sanctions and rewards are explicit, they might diminish behavioral compliance because they make the persuasive nature of the social norm message salient ([86]) and threaten freedom expressly ([58]). Both aspects increase the perceived threat to freedom to perform the behaviors and reactance ([ 4]). Thus, we expect social norms that specify potential sanctions (for failing to comply) (H8) or potential rewards (for complying) (H9) to be less effective than social norms that do not make those consequences explicit. Consumer CostsThe costs incurred to perform a behavior can create barriers. For example, social norms may direct consumers to buy an electric car, which is considerably more expensive than a regular car. We believe such costs will increase consumer reactance ([92]). However, the direction of the cost effect is not clear a priori (see [27]). On the one hand, a high cost may signal the desirability or status of the behavior, thereby motivating compliance. On the other hand, a high cost may dissuade consumers from attempting the behavior. Given these two opposing forces, we refrain from making directional hypotheses about costs, including costs associated with effort, money, and time. EffortWe define ""effort costs"" as the amount of physical or mental resources consumers must invest to comply with social norms. Some behaviors require more effort (e.g., exercising), others less (e.g., not littering). Social norms that require more effort demand greater behavioral change. They may either increase compliance by increasing the attractiveness of the effortful behavior or decrease compliance by decreasing the attractiveness of the effortful behavior (by derogating it because of reduced attainability) ([27]). Which of these two forces is stronger is an empirical question. Monetary costsConsumers may incur additional monetary costs to comply with social norms. For example, buying organic rather than conventional food requires more monetary resources. However, reusing a hotel towel does not result in monetary costs. Monetary costs constitute a direct barrier to free choice because consumers must sacrifice extra resources to comply. When social norms regulate costly behaviors, the monetary costs may imply a greater threat to the freedom to engage in this behavior, enhancing the attractiveness of this option and increasing compliance with such social norms ([27]). Yet, monetary costs may also emphasize the unattainability of the option, which would reduce compliance. Thus, we treat the effect of monetary costs on compliance with social norms as an empirical question. Temporal costsCompliance with social norms may require long-term (e.g., adhering to a healthy eating program) or temporary (e.g., reusing hotel towels) commitment. The temporal costs barrier is greater for behaviors with long-term commitments because these social norms impose more behavioral constraints than those that require only temporary commitments. Thus, on the one hand, consumers may also have stronger resistance to losing an option with potential longer-term consequences ([58]), which would increase compliance with social norms regulating longer-term behaviors. On the other hand, perceived unattainability of behaviors is also greater if they persist, now and into the future, rather than if they involve a single instance, which could decrease compliance with social norms involving longer-term commitment. Thus, we treat the effect of temporal costs on compliance with social norms as an empirical question. Environmental FactorsConsumers form freedom expectations through socialization in a specific cultural environment at a particular time ([80]). Culture shapes expectations by providing a logic for acting both housed in members' knowledge and beliefs and observed in members' behaviors ([102]). In some cultures, the range of approved behaviors is wide, and behavioral transgressions of social norms are tolerated. Other cultures allow a narrower range of behaviors and exhibit lower tolerance for deviations from social norms ([74]; [106]). Consumer reactance and compliance to social norms should thus differ systematically across cultures ([94]).To account for cultural differences, we adopt Inglehart's cultural framework ([52]) with two bipolar dimensions: traditional versus secular-rational and survival versus self-expression values. These dimensions have clear implications for reactance to social norms because they influence tolerance for transgressions (traditional–secular-relational) and the range of approved behaviors (survival–self-expression). Moreover, these dimensions are measured regularly, which enables us to account for cultural dynamics ([104]).[ 6] Traditional versus secular-rationalThis dimension contrasts traditional societies, in which religion is very important, and secular-rational societies, in which it is not ([52]). Traditional societies also emphasize deference to authority, absolute standards, cultural protectionism, and national pride, and they generally exhibit less tolerance for transgressions of social norms. Secular-rational societies reflect opposing values. We expect the effect of social norms on behavior to be stronger in cultures closer to the traditional (vs. the secular-rational) pole (H10) because they effectively restrict consumers' awareness and expectations of freedom, which should decrease reactance. Survival versus self-expressionThis dimension reflects transitions from industrial to postindustrial societies ([52]). Survival societies emphasize economic and physical security and familiar norms to maximize the predictability of others' behaviors, which results in a relatively narrow range of behaviors that may be perceived as freedoms. Consumers in survival societies have low expectations of personal freedoms and identify less freedom to be threatened ([54]). In contrast, self-expression values emphasize variety, imagination, and tolerance of outgroups. As societies move toward self-expression, people generally become freer to make choices for themselves ([106]), which enhances their reactance to social norms and decreases compliance. In cultures that value self-expression, noncompliance with social norms may even signal the person's freedom to be unique, which is valued by consumers of these societies ([40]; [106]). In contrast, in survival cultures, violation of social norms is more likely to jeopardize economic or physical security ([49]), diminishing perception of these behaviors as freedoms. Thus, we expect the effect of social norms on behavior to be stronger in cultures close to the survival (vs. the self-expression) pole (H11). TimeThe human propensity to comply with social norms has resulted from evolutionary processes ([41]). Therefore, the effect of social norms on behaviors should be stable throughout the short time (in evolutionary terms) marketers have been using them as a persuasion strategy. Yet research into conformity to social pressures also indicates some changes over time, including studies that document that conformity in the United States has declined ([16]), increased ([60]), or fluctuated ([62]) due to changes in social media and the cohesiveness of society, among other things. Thus, the effectiveness of social norms over time is an empirical question. Moral freedomCultures also vary in moral freedom, which reflects the extent to which people make their own moral choices rather than being influenced by state intervention ([ 3]). We expect the effect of social norms on behavior to be stronger in countries lacking moral freedom (H12), because of the lower expectations of freedom in those countries. Interaction Effects The interaction of social approval/disapproval and the environmental factorsThus far, our discussion has focused on main effects. However, consumers do not learn social norms in isolation; instead, they become aware of freedoms to perform certain behaviors through socialization in a particular culture and by observing different behaviors over time ([80]). To the extent that different societies shape consumer awareness of social norms, we expect the effects of the behavior being socially approved versus disapproved to be moderated by environmental characteristics (i.e., culture and time).Specifically, with respect to the traditional versus secular-rational cultural dimension, we note that participation in a world religion makes punishments for socially disapproved behaviors more salient to people ([47]). In Christianity, seven of the Ten Commandments start with the phrase ""you shall not."" In Judaism, of 613 mitzvot in the Torah, 365 (60%) forbid bad behaviors. Islam explicitly specifies an extended list of behaviors that are haram, or forbidden ([71]). Thus, in traditional cultures, where religion is more important, reactance to social norms that target socially disapproved (vs. approved) behaviors might be lower, because many of these behaviors already have been forbidden by religions. Thus, we expect the stronger effect of social norms on behaviors in traditional cultures to be especially pertinent for socially disapproved (vs. approved) behaviors (H13).With respect to the survival versus self-expression cultural dimension, engaging in socially disapproved behaviors in survival cultures is more likely to jeopardize economic or physical security ([49]), diminishing perception of these behaviors as freedoms. In contrast, in societies leaning toward the self-expression pole, engaging in disapproved behaviors would be more tolerated ([52]). Thus, we expect that the effect of social norms on behaviors in survival cultures is especially strong for socially disapproved (vs. approved) behaviors (H14).As to the effect of time, we expect that social media might enhance the effects of social norms by exposing consumers to more regular reinforcements pertaining to a wider range of socially approved and disapproved behaviors ([10]). Social media enables consumers to share content and feedback in real time, much of which remains available indefinitely and can be tracked by other parties ([45]). Exposure to norm violations in such settings triggers exhibitions of moral outrage, as manifested in the notion of a ""cancel culture,"" whereby social media users shame and punish perpetrators of bad behaviors, signaling that such behaviors are not tolerated ([30]). These developments imply that, over time, engaging in socially disapproved behaviors is stigmatized more severely than not engaging in socially approved behaviors ([33]), which reduces expected freedom to perform socially disapproved behaviors. Thus, we expect a stronger, more positive impact of social norms on socially disapproved (vs. approved) behaviors over time (H15). Other interactionsIn addition to the aforementioned hypothesized interaction effects, given the importance of the fundamental distinction between social norms regulating socially approved versus disapproved behaviors, we also explore additional interactions. Specifically, we investigate the interaction between social approval/disapproval and the target behavior characteristics, communication factors, and consumer costs. When these interactions are significant, we return to them in the discussion and highlight theoretical and managerial insights. Methodological CharacteristicsSystematic differences in the methodologies used by studies may cause variation in the reported effects ([14]). We control for ( 1) type of data ([quasi]experiment vs. other), ( 2) whether a sample involves students or regular consumers, ( 3) whether participants were exposed to (vs. indicated their perceptions of) social norms, and ( 4) whether participants' behavior was self-reported (vs. observed). Finally, to account for publication bias, which arises when the effect sizes in published studies are not representative of the entire population of effect sizes ([17]), we control for ( 5) the association between the strength and the precision of the effect sizes ([100]). More details follow. Methodology SampleTo identify relevant studies of the impact of social norms on consumer behaviors, we retrieved references from Google Scholar, Online Contents National, PsycINFO, and the Web of Science up to March 2019. We searched for keywords such as ""norm,"" ""social norms,"" and ""social pressure"" (for the full list of keywords, see Web Appendix A). We also checked the websites of the Social Science Research Network, the National Social Norms Resource Center, and Higher Education Center for Alcohol and Other Drug Abuse and Violence Prevention for relevant studies. We posted requests for unpublished manuscripts and working papers on the online academic platform ELMAR. Finally, we examined all cross-references from applicable documents. The procedure resulted in articles from five research domains: psychology (35.2%), health (34.4%), marketing (10.4%), food and nutrition (10.8%), and the environment (9.2%).Our dependent variable is the strength of the relationship between social norms and consumer behavior in the studies, which constitutes their observed effect sizes. We selected Pearson's product-moment correlation coefficient to measure effect sizes, because most studies operationalize both social norms and the target behavior as continuous variables. The consumer behaviors investigated in these eligible studies refer to the purchase, consumption, use, or disposal of products, services, material objects, or consumption experiences (e.g., buying organic products, subscribing to a gym, adopting mobile banking, donating). We exclude studies that focus on ( 1) aggregate entities (e.g., countries, societies) rather than individual consumers; ( 2) behaviors unrelated to consumption, such as social perceptions or interpersonal relations (e.g., stereotypes); ( 3) criminal behaviors, because the influence of the law would be confounded with the influence of social norms; and ( 4) consumers with impaired autonomy, such as workers making job-related decisions who must follow organizational policy, patients who rely extensively on others to make medical decisions ([77]), or people whose addictions limit their decision-making ability ([64]). Furthermore, to be included an eligible study must ( 1) examine actual behaviors, reported or observed (rather than intentions); ( 2) contain enough information to calculate the correlations between social norms and behaviors; and ( 3) support computations of the unconfounded effects of social norms. To illustrate ( 3), we excluded studies that collapsed the impacts of social norms and marketing promotion (e.g., [112]) or injunctive and descriptive social norms (e.g., [56]).The final sample thus consists of 252 effect sizes extracted from 137 articles, comprising 177 studies over the period 1978–2019. Web Appendix B lists the articles, effect sizes, and moderator values. The sample sizes of the primary studies range from 28 to 44,108 (median = 269), so that they produce a total of 112,929 unique respondents from 22 countries. Three of the 252 effect sizes have studentized residuals that are greater than 2.57 ([107]); two of them (r = −.19, n = 353; r = .71, n = 451) are influential, in that they lie outside the prediction interval (i.e., range of plausible values for any individual effect size) and cannot be explained by small sample sizes ([17]). We remove them from the subsequent analyses, leaving 250 effect sizes from 136 articles, based on 112,478 unique respondents. (As we detail in Web Appendix C, the primary studies provide an explanation for the extreme values of these outliers; the results are robust for including them in the analysis.) Variable CodingTwo independent coders (blind to the hypotheses) coded the moderators and cataloged the technical information (e.g., sample size). The intercoder agreement was 94.8%, and any disagreements were resolved through discussion. Effect sizesWe retrieved zero-order correlations, measuring the association between social norms and the target behavior, from the studies' correlation matrices or else converted statistics (e.g., F-value, t-value, p-value, χ2) into r (see [17]; [67]). If partial correlations were available, we also retrieved them from the studies. (The results are robust whether we use partial or zero-order correlations as measures of effect sizes.) We transformed the correlations into Fisher's z-scores ([17]) to satisfy the assumptions of normal distributions and known sampling variance of the effect sizes to estimate the model (for details, see the ""Model"" section).[ 7] In turn, we estimate the meta-analytic regression model with Fisher's z-scores as the dependent variable. We obtain the mean effect sizes, confidence intervals, predicted values, and plots by back-transforming Fisher's z-scores into correlation coefficients to facilitate interpretation (for details, see Web Appendix D). For robustness, we perform the analyses also by using the correlation coefficients. The effect sizes are coded such that a positive sign indicates a positive change in behavior (i.e., increase in socially approved or decrease in disapproved behaviors).[ 8] Moderators Table 1 shows the coding scheme for all the moderators. We mean-centered all continuous moderators and all dummy variables involved in interactions ([89]). We retrieve scores for the cultural dimensions from the World Values Survey ([53]) for each effect size, using the country and year of publication of each study. For the time variable, the code reflects the year of publication.[ 9] The precision of the effect size estimate is measured as the inverse of its standard deviation ([100]). If a publication bias is present, retrieved small sample studies are more likely to yield stronger effect sizes than those that are not retrieved, which implies a negative relationship between precision and effect size. By controlling for precision, the effects can be estimated more accurately ([35]).GraphTable 1. Coding Scheme for the Moderators of Social Norms–Behavior Effects. VariableCodeTarget Behavior CharacteristicsSocial approvalDummy = 1 if the behavior is socially approved (i.e., discussed positively by the authors) and 0 if the behavior is socially disapproved (i.e., noted as problematic by the authors). Mean-centered.ExistingDummy = 1 if the behavior exists (i.e., consumers already engage in it at least sometimes) and 0 if the behavior is new (i.e., consumers have not adopted the behavior yet).HedonicDummy = 1 if the behavior is hedonic (i.e., driven by pleasure-related goals) and 0 if the behavior is utilitarian (i.e., driven by functionality-related goals).Benefits to other peopleDummy = 1 if the behavior brings about social benefits and 0 otherwise.Public behaviorDummy = 1 if the behavior is public (i.e., is visible to others) and 0 if the behavior is private (i.e., is invisible to others).Communication FactorsSocial norm formulationDummy = 1 if the social norm is descriptive (i.e., describes behaviors of others) and 0 if the social norm is injunctive (i.e., suggests what should be done).Organization-benefitingDummy = 1 if the social norm benefits a specific organization and 0 otherwise.Close groupDummy = 1 if the norm refers to a person close to the individual and 0 otherwise.Authority figureDummy = 1 if the norm refers to a person in a position of authority and 0 otherwise.Explicit sanctionsDummy = 1 if the negative consequences of not abiding by the norms are made explicit and 0 otherwise.Explicit rewardsDummy = 1 if the positive consequences of abiding by the norm are made explicit and 0 otherwise.Consumer CostsEffortDummy = 1 if complying with the social norm entails much physical or mental effort and 0 if compliance entails little effort.Monetary costsDummy = 1 if complying with the social norm entails additional monetary costs and 0 otherwise.Temporal costsDummy = 1 if complying with the social norm entails a long-term investment and 0 if it entails a temporary investment.Environmental FactorsTraditional–Secular-rationalContinuous: scores for the Inglehart dimension in the year of publication minus 2 and country of data collection. Mean-centered.Survival–Self-expressionContinuous: scores for the Inglehart dimension in the year of publication minus 2 and country of data collection. Mean-centered.TimeContinuous: year of publication of the paper from which the effect sizes are extracted minus 2. Mean-centered.Moral freedomContinuous: World Index of Moral Freedom (i.e., extent to which individuals make their own moral choices rather than being influenced by state intervention; Álvarez, Kotera, and Pina 2020). Mean-centered.Methodological ControlsType of dataDummy = 1 if the study is an experiment or a quasiexperiment and 0 otherwise.SampleDummy = 1 when a student sample was used and 0 otherwise.Effect size precisionContinuous: inverse of the standard error of the effect sizes (Fisher's z-transformed). Mean-centered.Behavior operationalizationDummy = 1 when participants self-report the behavior and 0 when the behavior is observed.Social norm operationalizationDummy = 1 when participants are exposed to the social norms and 0 when the social norms are perceived. 1 Notes: We control for the operationalizations of behaviors and social norms in a robustness check (see Web Appendix G). Meta-Analytic Model and EstimationTo test the conceptual framework in Figure 1, the model should account for the structure of the data, because the effect sizes are nested within samples that are nested within articles, which could lead to correlated errors. We specify a mixed-effects meta-regression model using a multilevel parameterization ([105]) in which ( 1) observed effect sizes are assumed to be a normally distributed random sample from the population of true effect sizes; and ( 2) the variance distribution of true effect sizes can be explained by random effects at the effect size, sample, and article levels, to account for data nesting, and by the fixed effects of the moderators. Thus, the effect size i extracted from sample j in article u is modeled as follows:Effect Sizeiju = β0 + β1 Social approvaliju + β2 Existing behavioriju + β3 Hedoniciju + β4 Benefit peopleiju + β5 Public behavioriju + β6 Norm formulationiju + β7 Organization-benefitingiju + β8 Close groupiju + β9 Authority figureiju + β10 Explicit sanctionsju + β11 Explicit rewardsiju + β12 Effortiju + β13 Monetary costsiju + b14 Temporal costsiju + β15 Traditional–secular-rationalu + β16 Survival–self-expressionu + β17 Timeu + β18 Moral freedomu + β19 Social approvaliju × Traditional–secular-rationalu + β20 Social approvaliju × Survival–self-expressionu + β21 Social approvaliju × Timeu + β22 Type datau + β23 Sampleju + β24 Effect size precisioniju + δu + εi + γju + φiju, where δu ∼  N(0,  σδ2  ) is a random effect that reflects the variance among articles, εi ∼ N(0,  σϵ2  ) is the sampling variance of the observed effect sizes, γju ∼  N(0,  σγ2  ) is a random effect estimating the variance across samples nested within articles, φiju ∼  N(0,  σφ2  ) is a random effect that indicates the variance among effect sizes nested within samples and within articles, β0 is the intercept, and β1–24 are the parameter estimates for the moderators defined in Table 1. We perform all the analyses with the Metafor package for R ([107]). ResultsWe first present the grand mean effect size and the distribution of individual effect sizes. Next, we present the results of the meta-regressions testing for the moderators. To provide evidence without the multilevel specification, Model 1 does not account for the nested data structure. The hypothesized moderators (H1–H15) are then tested with Model 2, which accounts for the nested structure. We note that the findings of Model 1 versus Model 2 are very similar (see Table 3), despite their different approaches to nesting. Drawing on Model 2, Table 3 also presents predicted effect sizes for each level of the categorical moderators with all the other moderators set at their sample average ([15]) as well as simple mean correlations. The predicted values and simple mean correlations do not differ substantially, suggesting a good balance of moderator conditions across studies ([57]). Model 3 adopts an iterative approach to identify additional significant interactions between socially approved versus disapproved behaviors and target behavior characteristics, consumer costs, and communication factors, and we discuss its results where relevant. Grand Mean Effect Size and Distribution of Effect SizesOverall, social norms have a positive, small to medium impact on behaviors (i.e., the grand mean effect size is positive and significant, with  r¯   = .254 and a 95% confidence interval [CI95%] ranging from.232 to.277; [28]). The Q-statistic, which represents the total weighted deviation of each individual effect size from the mean, is significant (Q = 4,360, p < .001). Most observed effect size variance thus is systematic rather than due to sampling error and can be explained by moderators ([17]). Other heterogeneity indicators (Tau2 and I2) lead to the same conclusion (see Web Appendix E).The distribution of individual effect sizes, shown in Figure 2, reveals that they range from r = −.22 to r = .63 (M = .249, Mdn = .240, SD = .152), and 67% of them fall within a.10–.40 interval. Multicollinearity is not a concern. The largest bivariate correlation, r = .54, is between monetary costs and socially approved behaviors. The variance inflation factors are 3.73 or less for all variables. Table 2 provides the correlation matrix and descriptive statistics.Graph: Figure 2. Social norm–behavior effect size frequency distribution (k = 250).GraphTable 2. Bivariate Correlations and Descriptive Statistics for the Social Norm–Behavior Effect and Moderators. MeanSD12345678910111213141516171819202122 1. Effect size (r).25.1521 2. Social approval.79−.1211 3. Existing.26.004.0321 4. Hedonic.32.141−.525.0681 5. Benefits to people.32.095−.237−.074.0131 6. Public behavior.52.066.050.023.127−.1051 7. Norm formulation.30.287−.152−.056.150.118.0351 8. Org. benefiting.52.319−.028−.087.161−.208.038.1401 9. Close group.66.182−.207.005.112.125−.030.009−.014110. Authority figure.10−.120.048−.040−.121.078−.092−.137−.145−.004111. Explicit sanctions.15.014−.038.155.078−.127.029−.057.084.141−.018112. Explicit rewards.41−.078.106.064−.093−.162−.005−.076.085.109.116.101113. Effort.30−.215−.019−.150−.155−.169−.026−.129−.218.034.003.000.092114. Monetary costs.57−.005.540−.007−.429−.490−.006−.175.107−.058−.050.080.201.185115. Temporal costs.69−.038−.159−.039.073−.434.027.008.182.027−.138.005.135.389.150116. Trad.–Sec-rational.12.406.090.156−.193−.110−.074.007.104.067−.062.022.135−.111.129.109.049117. Survival–Self-exp..80.330−.054.202.034−.200−.035.034−.082.001−.106−.058−.016−.066.160.051−.040.134118. Time (year)20049.219.164.026.087.009−.101.266.324.049−.231.071−.052−.073.025.106.133.207−.018119. Moral freedom74.918.71−.096.108.130−.072−.016−.041.007.030−.085.036.024.018.110−.012−.004.154.436.059120. Type of data.16−.065−.194−.249−.022.333.122−.133−.060.094−.126−.165−.009−.270−.211−.086−.240−.027−.195.006121. Student sample.41.034−.346.170.285−.057.167.048.098−.016.046−.048.053−.137.173.081−.253−.070.084−.041−.044122. Effect size precision20.4719.77.144.029−.183.117−.023.155.168−.033−.103−.031−.011−.063−.091−.101.055.130−.009.169.076−.166.2291 2 Notes: Boldfaced correlations are significant at α = .05. For the dummy variables, only the mean is provided as a descriptive statistic. Target Behaviors Characteristics Social approval/disapprovalContrary to H1, social norms are not more effective for disapproved (vs. approved) behavior (b = −.002, p = .957). Although the insignificant main effect suggests that social norms are equally effective for approved and disapproved behaviors overall, this variable is involved in several interactions with other moderators, as we discuss next. Existing versus newThe effect of existing (vs. new) behavior is not significant (b = .017, p = .514); thus, H2 is not supported. However, according to Model 3, this variable interacts with the social approval of behavior at marginal significance (b = .138, p = .072). Figure 3, Panel A, shows that when targeting socially approved behaviors, social norms tend to be more effective in encouraging existing (predicted  r¯   = .253) versus new (predicted  r¯   = .217) behaviors. For socially disapproved behaviors, the opposite pattern emerges, as social norms tend to be more effective for discouraging new (predicted  r¯   = .298) versus existing (predicted  r¯   = .207) behaviors.Graph: Figure 3. Social norm–behavior effect sizes for socially approved and disapproved behaviors as a function of focal moderators. Hedonic versus utilitarianThe impact of social norms does not differ across hedonic and utilitarian behaviors (b = .016, p = .599). This finding is not surprising given the two opposing forces (desire and guilt) driving hedonic behaviors. Benefits to peopleSocial norms are more effective when behaviors benefit others (predicted  r¯   = .289) than when they do not (predicted  r¯   = .229, b = .064, p = .035), in support of H3. Public behaviorThe effect of public behavior is not significant (b = .013, p = .500), suggesting that social norms are equally effective for public and private behaviors. Communication Factors Norm formulationIn support of H4, descriptive norms have stronger effects on behavior (predicted  r¯   = .305) than injunctive norms (predicted  r¯   = .223, b = .088, p < .001). Further, the interaction between norm formulation and socially approved behaviors in Model 3 is also significant (b = −.110, p = .018). Figure 3, Panel B, shows that descriptive (vs. injunctive) norms are more effective when targeting disapproved behaviors (predicted  r¯ descriptive = .337 vs. predicted  r¯ injunctive = .183, Δ = .154) than when targeting approved behaviors (predicted  r¯ descriptive = .281 vs. predicted  r¯ injunctive = .228, Δ = .053). Thus, descriptive social norms that describe how the majority behaves are especially effective in curbing disapproved behaviors. Organization-benefitingIn support of H5, social norms are more effective when they benefit an organization (predicted  r¯   = .279) than otherwise (predicted  r¯   = .214, b = .069, p = .004). Close groupIn support of H6, social norms are more effective if they refer to a close group member (predicted  r¯   = .266) versus an abstract group (predicted  r¯   = .213, b = .057, p = .008). Authority figureThe effectiveness of social norms is not impeded by references to an authority figure (b = −.012, p = .729), in contrast to H7. Sanctions and rewardsThe effect of social norms does not depend on the presence of explicit sanctions (b = −.010, p = .767), disconfirming H8. However, social norms with explicit mentions of rewards (predicted  r¯   = .208) are marginally less effective than those where rewards are not mentioned (predicted  r¯   = .255, b = −.050, p = .078), in line with H9. Consumer Costs EffortThe amount of effort required to comply with social norms does not have a significant effect on compliance (b = −.031, p = .255). This finding is consistent with the idea that there are two opposing forces behind this effect that counteract each other. Monetary costsSocial norms exert stronger effects on behavior when compliance entails monetary costs (predicted  r¯   = .276) than when it does not (predicted  r¯  =.211, b = .069, p = .017). This finding is consistent with the idea that barriers such as monetary costs can make the behavior more desirable for consumers (e.g., via status signaling; [27]). Temporal costsSocial norms seem to be equally effective for behaviors requiring a long-term or a temporary investment, as temporal costs are nonsignificant (b = −.011, p = .706). Environmental Factors Traditional–secular-rationalThe traditional–secular-rational cultural dimension does not have a main effect on the effectiveness of social norm (b = −.017, p = .643), which fails to support H10.[10] However, we find some support for H13 because the interaction with the social approval of behaviors is positive and marginally significant (b = .211, p = .104). Figure 3, Panel C, shows that the impact of social norms on socially disapproved behaviors is somewhat weaker in more secular-rational cultures, whereas their impact on socially approved behaviors remains stable across the traditional–secular-rational dimension. Survival–self-expressionThe effect of this cultural dimension is negative, as we expected, but it is not significant (b = −.027, p = .448), which does not support H11. However, in support of H14, its interaction with the social approval of behaviors is positive and significant (b = .180, p = .033). Thus, consistent with our expectations, social norms are less effective for socially disapproved behaviors in cultures closer to the self-expression pole, whereas their effectiveness for socially approved behaviors is stable across the survival–self-expression dimension (Figure 3, Panel D). TimeThe effect of time is not significant (b = .001, p = .956), but its interaction with the social approval of behaviors is negative and significant (b = −.006, p = .036), in support of H15. Social norms have become more effective over time at curbing socially disapproved behaviors, while their influence on socially approved behaviors remains stable (Figure 3, Panel E). Moral freedomThe national level of moral freedom lowers behavioral compliance at a marginal level of significance (b = −.002, p = .094), in support of H12. Methodological CharacteristicsThe studies in our meta-analysis yield the same results regardless of the type of data (b = −.045, p = .189) and whether they rely on student samples (b = −.001, p = .986). We obtain similar results if we control for whether the studies operationalize social norms using exposure or perception (note that we exclude this control variable in the main model because of multicollinearity but address it in supplementary analyses). Precision has a positive, marginally significant effect (b = .001, p = .082); therefore, effect sizes greater than the mean might be missing, which suggests that publication bias is not an issue. Robustness ChecksTo confirm that our results are robust, we first perform a series of diagnostic tests (Web Appendices E and F) to rule out publication bias; they reconfirm the positive influence of effect size precision in the meta-regression model. Next, we performed analyses based on 16 alternative model specifications. In Tables A1 and A2 in Web Appendix G, we present the results when we adopt alternative methodological choices: ( 1) including theoretical moderators only; ( 2) adding two demographic controls—the primary study participants' mean age and the percentage of men; ( 3) accounting for effect sizes coming from marketing journals; ( 4) estimating the meta-regression parameters with t-tests instead of z-values; ( 5) estimating the model including the two outliers (k = 252); ( 6) using the raw effect sizes—r and the variance of r—rather than Fisher's z transforms; ( 7) relying on partial instead of the zero-order correlation, if both measures could be retrieved from the primary study; and ( 8) controlling for the operationalization of the behavior (self-reported vs. observed) and for the operationalization of social norms (exposed vs. perceived) instead of controlling for the type of data. Across these methodological choices, the results remain consistent with the findings of the main models (Table 3).GraphTable 3. Meta-Regression Results. Model 1:Model 2:Model 3:Predicted ValuesSimple Mean CorrelationsPredictorsb (SE)b (SE)b (SE)r¯[CI95%]r¯[CI95%]Intercept.097 (.047)*.104 (.049)*.127 (.048)**——Social disapproval (k = 53)———.250 [.181,.317].299 [.248,.348]Social approval (k = 197).009 (.040)−.002 (.042).015 (.044).248 [.225,.271].250 [.230,.269]New behavior (k = 84)———.236 [.196,.276].263 [.221,.303]Existing behavior (k = 186).016 (.025).017 (.026).010 (.027).253 [.228,.277].259 [.237,.280]Utilitarian behavior (k = 170)———.244 [.216,.270].244 [.222,.265]Hedonic behavior (k = 80)−.012 (.028).016 (.030).001 (.031).258 [.216,.300].295 [.257,.331]Benefits people: no (k = 171)———.229 [.200,.258].251 [.228,.274]Benefits people: yes (k = 79).064 (.029)*.064 (.030)*.066 (.032)*.289 [.249,.328].279 [.244,.314]Private behavior (k = 120)———.242 [.214,.270].250 [.224,.275]Public behavior (k = 130).016 (.019).013 (.019).015 (.019).254 [.228,.280].269 [.241,.297]Norm formulation: injunctive (k = 175)———.223 [.200,.247].229 [.208,.251]Norm formulation: descriptive (k = 75).086 (.022)***.088 (.021)***.080 (.021)***.305 [.272,.338].329 [.293,.363]Organization-benefiting: no (k = 120)———.214 [.183,.245].208 [.181,.235]Organization-benefiting: yes (k = 130).077 (.022)***.069 (.024)**.078 (.025)**.279 [.250,.308].304 [.280.329]Close group: no (k = 85)———.213 [.179,.246].215 [.185,.244]Close group: yes (k = 165).066 (.021)**.057 (.022)**.051 (.022)*.266 [.242,.290].283 [.259,.306]Authority figure: no (k = 224)———.249 [.228,.271].266 [.246,.286]Authority figure: yes (k = 26)−.001 (.033)−.012 (.034)−.004 (.034).238 [.179,.297].202 [.149,.253]Explicit sanctions: no (k = 227)———.249 [.228,.270].259 [.239,.279]Explicit sanctions: yes (k = 23)−.019 (.032)−.010 (.035).001 (.035).239 [.177,.300].266 [.204,.325]Explicit rewards: no (k = 213)———.255 [.234,.277].265 [.244,.286]Explicit rewards: yes (k = 37)−.056 (.027)*−.050 (.028)†−.041 (.029).208 [.158,.257].229 [.184,.272]Effort: small (k = 174)———.257 [.231,.282].281 [.259,.303]Effort: large (k = 76)−.034 (.026)−.031 (.027)−.044 (.028).228 [.187,.268].210 [.174,.246]Monetary costs: no (k = 107)———.211 [.173,.249].262 [.230,.294]Monetary costs: yes (k = 143).071 (.027)**.069 (.029)*.072 (.029)*.276 [.246,.305].259 [.235,.282]Temporal costs: temporary (k = 78)———.256 [.211,.299].267 [.237,.297]Temporal costs: long-term (k = 172)−.012 (.028)−.011 (.029)−.010 (.031).245 [.220,.270].256 [.232,.281]Traditional–Secular-rational−.014 (.035)−.017 (.038)−.031 (.039)——Survival–Self-expression−.026 (.033)−.027 (.036)−.021 (.037)——Time−.001 (.001).001 (.001).001 (.001)——Moral freedom−.002 (.001)†−.002 (.001)†−.003 (.001)*——Social approval × Traditional–Secular-rational.175 (.121).211 (.130)†.298 (.145)*——Social approval × Survival–Self-expression.159 (.079)*.180 (.084)*.206 (.087)*——Social approval × Time−.005 (.003)*−.006 (.003)*−.008 (.004)*——Social approval × Existing behavior——.138 (.077)†——Social approval × Norm formulation——−.110 (.046)*——Type of data: other (k = 210)———.255 [.233,.277].265 [.244,.286]Type of data: (quasi)experiment (k = 40)−.048 (.033)−.045 (.034)−.014 (.036).213 [.155,.270].229 [.182,.274]Sample: nonstudent (k = 148)———.249 [.222,.275].254 [.230,.278]Sample: student (k = 102)−.001 (.023)−.001 (.024).011 (.025).248 [.214,.282].269 [.237,.301]Effect size precision.001 (.0005)†.001 (.0005)†.001 (.0006)†——Pseudo R230%29%30%Variances componentsTau2 = .016;σϵ2 = .001σδ2 = .0001;σϵ2 = .001;σγ2 = .004;σφ2 = .012σδ2 = .0006;σϵ2 = .001;σγ2 = .005;σφ2 = .011 3 *p < .05.4 **p < .01.5 ***p < .001.6 †p < .10.7 Notes: There is no regression coefficient for the reference category of the moderators. The pseudo R2 is the amount of between-effect size variance explained by the moderators. In Model 1, the nested structure of the data is not modeled and Tau2 represents the residual between-effect size variance. In Models 2 and 3, the nested structure of the data is accounted for and the variance components are the following:  σδ2   = variance across articles,  σϵ2  = sampling error variance,  σγ2   = variance between samples within articles, and  σφ2   = variance between effect sizes within samples within articles. Predicted values are based on Model 2 parameter estimates, with the levels of all the other moderators set at the sample average. The slight discrepancy between the predicted values for the categories of the moderators (e.g., monetary costs: no = .211; yes = .276, Δ = .065) and the corresponding regression coefficient (b = .069 > .065) arises because the parameter estimates are based on Fisher's z-scores, whereas the predicted values are expressed as correlation coefficients. The simple mean correlations are obtained using a random-effect model with restricted maximum likelihood as a between-effect size variance estimator. Predicted values and simple mean correlations are not provided for the continuous moderators, because their effect is linear (see Figure 3).In Table A3 (Web Appendix G), we also rule out the effects of four alternative moderators: ( 9) whether the target behavior entails environmental benefits, (10) whether the target behavior has social and physical consequences for the individual, and (11) whether the norm features a promotion or prevention frame. None of these alternative variables is significant.Finally, Table A4 (Web Appendix G) presents the results when we include additional country controls based on (12) country-level gross domestic product and population density, as well as (13) Hofstede's (vs. Inglehart's) cultural dimensions. The results again remain similar to those from the main models. Further, to account for a potential cultural invariance bias, we control for (14) the language spoken in the country of the data collection and (15) whether the study was conducted in an Asian country; (16) we also model the nesting of articles within countries. The stable regression coefficients suggest that cultural invariance does not affect the results. Across all 16 alternative models, the magnitudes, signs, and significance of the parameter estimates are consistent and aligned with our main findings, which strengthens our conclusions and affirms the robustness of the effects obtained in the main model. SummaryBy meta-analyzing the extant empirical evidence, our research provides new evidence for the effects of social norms on consumer behavior. On average, social norms significantly influence behavior and their effect (  r¯   = .254) is small to medium in size ([28]). Importantly, several moderators can explain substantial variation among these effects. Table 4 summarizes our findings. We next detail the theoretical and practical implications of our study.GraphTable 4. Hypotheses and Empirical Questions. Moderator and Predictions (Hypotheses) or Competing Explanations (Empirical Questions)Reactance ExplanationEvidenceTarget Behavior CharacteristicsSocial approval (vs. disapproval) (H1): Social norms for socially disapproved behaviors are more effective than those pertaining to approved behaviors.Expectations of freedom are lower for socially disapproved vs. socially approved behaviors×Existing (vs. new) (H2): Social norms are more effective when targeting existing (vs. new) behaviors.Freedom threat is lower for existing than for new behaviors×Hedonic (vs. utilitarian) (Empirical question): Greater desire or more guilt for hedonic benefits may either increase or decrease freedom threat from social normsN.A.n.s.Benefits to other people (H3): The presence of benefits to other people of a behavior enhances the effect of social norms on that behavior.Expectations of freedom are lower for behaviors that are beneficial to others✓Public behavior (Empirical question): Freedom threat from social norms may be lower for private behaviors due to a lack of scrutiny from others, or it may be lower for public behaviors due to concerns about perceptions from others.N.A.n.s.Communication FactorsNorm formulation (H4): Descriptive social norms have a stronger impact on behaviors than injunctive norms.Descriptive norms imply less freedom threat than injunctive norms✓Organization-benefiting (H5): Organization-benefiting social norms are more effective than social norms that do not mention organizations.Organization-benefiting social norms imply less freedom threat than social norms that do not benefit organizations✓Close group members (H6): Social norms referring to a close group member are more effective than social norms referring to abstract or distant groups.Close group members imply less freedom threat than abstract or distant groups✓Authority figures (H7): Social norms referring to authority figures are less effective than social norms that do not refer to them.Authority figures imply more freedom threat×Explicit sanctions (H8): Social norms that specify potential sanctions are less effective than social norms that do not make these consequences explicit.Explicit sanctions and rewards imply more freedom threat×Explicit rewards (H9): Social norms that specify potential rewards are less effective than social norms that do not make these consequences explicit.✓Consumer Costs Effort, monetary costs, and temporal costs (Empirical questions): The presence of costs barriers arouses reactance, which either makes the behaviors more attractive, or dissuades consumers from attempting the behaviors.Costs create barriers to free choice and increase reactanceMonetary costs lift social norm effect; n.s. for effort and temporal costsEnvironmental FactorsTraditional–Secular-rational (H10): The effect of social norms on behavior is stronger in cultures closer to the traditional (vs. secular-rational) pole.Expectations of freedom are lower in cultures toward the traditional pole and toward the survival pole×Survival–Self-expression (H11): The effect of social norms on behavior is stronger in cultures near the survival pole versus the self-expression pole.×Time (Empirical question): Evolutionary forces driving social norms effectiveness should be stable over time but fluctuations of conformism over time is observed.N.A.n.s.Moral freedom (H12): The effect of social norms on behavior is stronger in countries lacking moral freedom.Expectations of freedom are lower in low moral freedom countries✓InteractionsTraditional–Secular-rational × Approved behavior (H13): The stronger effect of social norms on behaviors in traditional cultures is driven by socially disapproved (vs. socially approved) behaviors.Expectations of freedom for socially disapproved behaviors are especially low in traditional and survival cultures✓Survival–Self-expression × Approved behavior (H14): The effect of social norms on behaviors in survival cultures is especially strong for socially disapproved (vs. approved) behaviors.✓Time × Approved behavior (H15): There is a more positive impact of social norms for socially disapproved than for socially approved behaviors over time.Expectations of freedom for socially disapproved behaviors decrease over time✓Existing (vs. new) × Approved behavior (Exploratory interaction)N.A.Social norms are effective for existing, approved behaviorNorm formulation × Approved behavior (Exploratory interaction)N.A.Descriptive norms are effective for disapproved behavior 8 Notes: N.A. = not applicable; n.s. = not significant; x = not supported. Theoretical ImplicationsOur results go beyond previous meta-analyses by uncovering previously overlooked boundary conditions of the effects of social norms on consumer behavior using reactance theory as our theoretical lens. While our meta-analysis provides important insights about whether and how social norms can influence behavior, any meta-analysis is limited to the factors included in available primary studies. Those gaps also open avenues for future research, as discussed next. The Role of Social Approval/Disapproval on Social Norms Key insightsWe reveal that the effects of social norms on behavior differ systematically for behaviors that are socially approved versus disapproved in the presence of certain environmental factors. We find that the effects of social norms on socially disapproved behaviors have increased over time and are particularly strong in survival (vs. secular-rational) and traditional (vs. self-expression) cultures. In contrast, the effects of social norms on socially approved behaviors are more stable across cultures and time. Future researchThe critical difference in social norm effectiveness for regulating socially approved versus disapproved behaviors establishes the need to investigate drivers of this difference. In the process of adopting socially approved behaviors, consumers might glean benefits beyond the direct consequences of complying with norms. For example, does compliance promote positive emotions, due to an enhanced sense of belonging, acceptance, or well-being, independent of the benefits of adopting the behavior? And do consumers suffer negative emotions (e.g., guilt) when they engage in socially disapproved behaviors, and do social norms reinforce these emotions? Another issue is how legal mandates affect behaviors, both when they contradict social norms of disapproved behaviors (e.g., using cell phones while driving) and when they reinforce approved behaviors (e.g., driving below the speed limit). Implications for Social Influence Literature Key insightsWe shed new light on several unexplored moderators of the effects of social norms on consumer behavior and thereby contribute to the literature on social influence ([24]; [109]; [110]). For example, social norms have stronger effects on behaviors that benefit others, but they are weaker for already existing socially disapproved behaviors. Social norms are equally effective for private and public behaviors, for hedonic and utilitarian behaviors, for behaviors requiring much versus little effort, and for behaviors requiring long-term versus temporary commitment. Importantly, monetary costs do not deter consumers; on the contrary, they make them more likely to comply with social norms regulating costly behaviors, in line with the reactance theory explanation that barriers enhance behaviors' desirability. Future researchThe intriguing finding that private behaviors are just as likely to be influenced by social norms as public ones suggests new research avenues. Perhaps consumers overestimate the extent to which they are monitored by others because of the spotlight effect ([37]). If so, social norms could be effective in nonsocial settings, where they traditionally have been perceived as less influential. Further, finding that social norms are equally effective for hedonic and utilitarian behaviors highlights the need to clarify the roles of desire and guilt as underlying processes. Teasing out these effects could also help enhance communication strategies (e.g., downplaying or emphasizing desire and guilt). Yet, the lack of effect of some behavioral characteristics might also be due to selection; for example, researchers may be inclined study situations in which they expect social norms to matter. Thus, testing the effects of social norms in situations where they a priori seem less relevant is insightful. Studying the effectiveness of social norms when behavioral autonomy is impaired (e.g., employees making decisions on behalf of firms) is also intriguing. Finally, research could address behaviors consumers adopt to distinguish themselves from the group (e.g., to enhance authenticity; [82]) and the leadership behaviors they display to encourage others to go against current social norms (e.g., boycotting, brand sabotaging; [55]). Implications for Marketing Communications Literature Key insightsWe contribute to marketing communication research by identifying communication strategies that enhance the effectiveness of social norms in several ways. First, several commonly used social norm communication factors appear to be ineffective, such as referring to authority figures or specifying explicit sanctions; the same is true for rewards, which even seem to hinder social norms' effectiveness. Second, formulating norms as descriptive (vs. injunctive), organization-benefiting, and/or referring to close others enhance the effect of social norms on behaviors. Further, our exploratory results suggest that injunctive norms are especially weak when targeting disapproved behaviors, which is consistent with reactance theory. To discourage socially disapproved behaviors, injunctive norms tend to be proscriptive (""you should not"") rather than prescriptive (""you should""). Proscription represents a greater threat to freedom than prescription, so it prompts more reactance to injunctive norms ([11]). Instead, socially disapproved behaviors can be better curbed by descriptive norms.The finding that organization-benefiting social norms enhance compliance contributes to the emerging stream of research that examines how situational factors influence the effectiveness of social norms ([ 1]; [38]). While which type of organization benefits should matter as well, on a broader level, our results suggest that organization-benefiting social norms, because they are situation-specific, enhance compliance. Future researchMore research is needed for boundary conditions for the effectiveness of different communication strategies. For example, there are multiple avenues for future research on the impacts of sanctions and rewards. First, reward size and reward type (e.g., material vs. nonmaterial; [73]) might be important. Second, rewards or sanctions may not exert effects past a ceiling level of the impact of social norms when many consumers are ""uninfluenceable."" Third, research should identify ways to account for consumer needs and boost perceptions of the benefits of following and the costs of not following social norms without triggering reactance.Boundary conditions under which injunctive norms are more effective than descriptive ones (e.g., the level of ambiguity, the source of the norm: ""Why fight city hall?"") should be addressed. Further, because norms are group-specific, research could explore how group exclusivity should be communicated. Social norms linked to homogeneous exclusive groups (""Harvard students recycle"") might evoke less reactance because they distinguish group members from outsiders while also satisfying the need to belong ([13]). The effect of group exclusivity on behavior might be stronger if the communication contrasts ingroup versus outgroup norms (""Harvard students, unlike [vs. similar to] MIT students, recycle""). Relatedly, communicating the size of the group that shares social norms could also enhance social norms' influence ([46]).Finally, most of the primary studies in our meta-analysis predated social media, but our results suggest that social media may have disrupted the influence of social norms on behavior. Online interactions enable consumers to develop friendships with people they have never met in real life, so further research might investigate how social norms evolve on social media. For example, how does the immediate, often permanent feedback available through social media shape the effects of social norms on online behaviors? Consumers might not just comply with social norms but also try to become agents of influence by spreading the norm further on social media. Future research should investigate which factors facilitate such efforts ([68]), and the role of social norms in interpersonal relationships in general. Finally, future research should address the interaction between social norms and marketing-mix instruments, particularly promotion. Implications for Social Reactance Literature Key insightsOur results inform discussions on reactance ([18]; [92]) and the role of intrinsic versus extrinsic behavioral motivations ([31]). The nonsignificant effect of public behavior implies that consumers follow social norms even if their behavior is not observable to others. Further, while effort may impede social norms' effectiveness, the monetary costs of behavior enhance it, suggesting that consumer effort and temporal costs are a greater barrier to compliance than monetary costs. These results, together with the finding that explicit sanctions and rewards do not help, suggest that complying with social norms may be a more intrinsically motivated activity than previously believed. Future researchFollow-up research should investigate other barriers that may incite reactance and ways to circumvent them. Noting our finding that monetary costs enhance social norm compliance, we call for research that specifies the boundary conditions of this effect. More research is needed to clarify what makes costly behavior attractive to consumers. For example, costly behaviors may help signal status ([42]).Research should address the relative explanatory power of alternative mechanisms parallel to reactance that might influence the effect of social norms on behaviors. For example, the literature identifies other potential mechanisms, such as self-efficacy ([ 5]), sense of belonging ([ 6]), internalization ([97]), and justification of behaviors ([39]). Contributions to PracticeThe findings offer insights for marketers and public policy makers by identifying effective (and some commonly used but ineffective) strategies for enhancing the impact of social norms on consumer behavior. In contrast to conventional wisdom ([ 7]), our results suggest that the influence of social norms can prompt private acceptance. Thus, marketers and policy makers can leverage social norms to encourage both private and public behaviors. Communication StrategiesThe content of the communication should feature descriptive rather than injunctive forms of social norms (i.e., describe what [most] people actually do rather than what they should do[11]). Further, we recommend that marketers should avoid specifying explicit sanctions and rewards associated with social norms. Instead, strategies that highlight benefits to others or consumer freedom (e.g., a communication with a postscript ""The choice is yours""; [12]) may mitigate reactance and thus be more effective for inducing the target behavior.Practitioners might worry about highlighting a specific organization when communicating about social norms, but our results suggest that referring to a specific firm, governmental body, or nongovernmental organization can make communications about social norms more influential. Social norms are also more powerful when they cite people who are perceived as close to the target consumers. Thus, practitioners should target social norm communication toward nano-influencers on social media, with their smaller, more engaged audiences. In contrast, our results indicate that references to authority figures when using social norms do not affect consumer behavior.In communicating norms, marketers can acknowledge the monetary costs associated with the targeted behaviors. Although monetary costs are a financial barrier, they seem to also increase the desirability of the behavior, so social norms can be particularly effective for promoting costly behaviors like donations or buying (more expensive) organic food. Further, social norms are equally effective irrespective of required effort, and the time investment in complying. Cultural Differences Between Countries Socially approved versus disapproved behaviorsThe impact of social norms on socially disapproved behaviors varies significantly depending on the country of implementation, but it is more stable for socially approved behaviors. Social norms have stronger influences on socially approved than disapproved behaviors in secular-rational and self-expression cultures. These findings have important public health implications when group behavior is essential. To encourage mask wearing in most Western countries, for example, public officials should communicate that wearing a mask is a socially approved behavior that close others adopt. In most survival countries, the communications should highlight that not wearing a mask is socially disapproved. Cultural profilesTo specify the net effect of culture at the country level, we estimate the impact of social norms in countries with different cultural profiles. We calculate the predicted effect sizes for socially disapproved and approved behaviors in Figure 4 for eight countries that represent different regions of the world. We use descriptive norms as a base category and country scores on Inglehart dimensions from the latest wave of the World Values Survey.Graph: Figure 4. Predicted current effect sizes (Pearson's product moment correlations) of descriptive norms for socially approved and disapproved behaviors, by global regions.In Scandinavian countries such as Denmark (eight effect sizes), which score high on secular-rational values (85th percentile) and high on self-expression values (98th percentile), the mean effect size for socially approved behaviors is  r¯   = .365 (CI95% = [.282,.442]), whereas it is not significant for disapproved behaviors (  r¯   = .176, CI95% = [−.077,.407]). Campaigns using social norms thus may be effective for encouraging healthy eating, for example, but are likely not the best choice to curb excess drinking in such countries.In Western European (e.g., France) and Commonwealth (e.g., Canada, United Kingdom) countries with medium to high scores on both dimensions (but lower than Scandinavia), social norms are equally effective regardless of the social approval of behavior. For example, in Australia (20 effect sizes), a country with average secular-rational values (55th percentile) and high self-expression values (96th percentile), the impact of social norms on approved behaviors (  r¯   = .344, CI95% = [.273,.411]) is the same as the impact on disapproved behaviors (  r¯   = .355, CI95% = [.249,.452], Wald-type test = −.175, p = .431).In contrast, in the United States (104 effect sizes), which is more traditional (43rd percentile for secular-rational) and has high self-expression values (85th percentile), the effect of social norms on socially disapproved behaviors (  r¯   = .460, CI95% = [.374,.538]) is stronger than their impact on approved behaviors (  r¯   = .330, CI95% = [.262,.395], Wald-type test = −2.406, p = .008).Interestingly, we find the same pattern in Southern Europe (e.g., Italy) and China, even though these areas represent relatively high secular-rational (65th percentile) and low self-expression values (28th percentile). In these regions, social norms' effectiveness is greater for disapproved behaviors (  r¯   = .530, CI95% = [.397,.642]) than for approved behaviors (  r¯   = .319, CI95% = [.213,.417], Wald-type test = −2.611, p = .005).Finally, in countries with strong traditional and survival values, such as most African and Muslim-majority countries, social norms' impact on disapproved behaviors is much stronger than on approved behaviors. Consider Ethiopia (25th and 30th percentiles), where the mean effect size for disapproved behaviors (  r¯   = .660, CI95% = [.455,.799]) is much greater than that for approved behaviors (  r¯   = .297, CI95% = [.188,.398]). In these countries, social norms are especially effective for discouraging disapproved behaviors. ConclusionThis extensive meta-analysis shows that social norms significantly impact behavior and uncovers novel contingencies of this effect. We hope the proposed research agenda, which reflects our comprehensive investigation of the extant literature, sparks additional research in the fascinating ways in which social norms shape (or do not shape) consumer behavior.  "
18,"The Influence of Social Norms on Consumer Behavior: A Meta-Analysis Social norms shape consumer behavior. However, it is not clear under what circumstances social norms are more versus less effective in doing so. This gap is addressed through an interdisciplinary meta-analysis examining the impact of social norms on consumer behavior across a wide array of contexts involving the purchase, consumption, use, and disposal of products and services, including socially approved (e.g., fruit consumption, donations) and disapproved (e.g., smoking, gambling) behaviors. Drawing from reactance theory and based on a cross-disciplinary data set of 250 effect sizes from research spanning 1978–2019 representing 112,478 respondents from 22 countries, the authors examine the effects of five categories of moderators of the effectiveness of social norms on consumer behavior: ( 1) target behavior characteristics, ( 2) communication factors, ( 3) consumer costs, ( 4) environmental factors, and ( 5) methodological characteristics. The findings suggest that while the effect of social norms on approved behavior is stable across time and cultures, their effect on disapproved behavior has grown over time and is stronger in survival and traditional cultures. Communications identifying specific organizations or close group members enhance compliance with social norms, as does the presence of monetary costs. The authors leverage their findings to offer managerial implications and a future research agenda for the field.Keywords: cultural influence; meta-analysis; reactance; social approval; social influence; social marketing; social norms marketing; social normSocial norms shape consumer behavior. Defined as ""rules and standards that are understood by members of a group, and that guide and/or constrain social behavior without the force of laws"" ([26], p. 152), social norms influence various forms of everyday consumption, including food choices ([87]), responses to new products ([51]), and loyalty ([63]). For example, signs in a hotel stating that other hotel guests reuse their towels increase towel reuse ([38]). Social norms are often leveraged by marketers and policy makers to encourage various socially approved behaviors, such as conserving energy ([95], [96]), complying with product recalls ([85]), and making tax payments (Cabinet Office UK [22]). They are also used to discourage socially disapproved behaviors, such as polluting the environment ([109]) and smoking or excessive alcohol or drug use ([108]).The academic literature examining social norms has produced conflicting findings ([61]; [95], [96]). Some studies report large-scale favorable results for using social norms to curb socially disapproved behaviors ([21]). [90], for example, report a significant reduction (13%) in the prevalence of impaired driving among students. However, some campaigns encouraging socially approved behaviors have backfired. For example, [95], [96]) find that social norms for energy preservation can increase energy consumption. These mixed findings suggest contingent effects of social norms on behavior. A second reason for mixed findings is that some research studies actual behavior, while other research examines behavioral intentions. A final reason for mixed findings may lie in the fact that the country context introduces cultural factors into the study of norms that are important to their impact.Our article looks across a wide range of research on social norms across behaviors, time, and cultures to resolve these conflicting findings and to synthesize the extant literature on social norms. Specifically, we investigate the effects of social norms on actual consumer behavior and identify moderators of these effects, using reactance theory as a theoretical lens ([18]; [92]). We contend that the effectiveness of social norms varies with the level of consumer reactance they trigger ([18]); norms that are less likely to trigger reactance are more likely to be effective.We conduct a meta-analysis that examines the effects of five categories of moderators of the effectiveness of social norms on consumer behavior, matching central factors that may induce reactance. Specifically, we examine how the relationship between social norms and behaviors depends on ( 1) social approval or disapproval of behavior and other target behavior characteristics, ( 2) communication factors, ( 3) consumer costs, ( 4) environmental factors (e.g., culture, time), and ( 5) methodological characteristics (e.g., type of sample, study).We collected 250 effect sizes from 136 articles published between 1978 and 2019 across different fields (e.g., marketing, psychology, health, environmental studies), representing 112,478 respondents from 22 countries. In conducting this research, we encountered several meta-analyses related to social norms. However, most prior meta-analyses focus on a single behavior, such as condom usage ([98]), or else investigate limited set of communication factors, such as whether the norm is descriptive or injunctive ([76]; [91]). Moreover, most include consideration of behavioral intentions rather than actual behavior, which is our focus. Finally, some prior meta-analysis focus on studies that use a specific theoretical framework, such as the theory of planned behavior ([ 2]; [70]), which limits their generalizability.We aim to go beyond these insights by investigating critical moderators that have not been addressed in prior research. We not only study the new moderator of socially approved versus disapproved behavior but also examine new and managerially actionable moderators, such as target behaviors, communication factors, and consumer costs. Importantly, this study investigates behaviors (observed or reported) rather than intentions and covers consumer behaviors across domains, regardless of the theoretical framework used in primary studies. With this comprehensive approach, we establish that social norms have significant impacts on behavior, but the effect varies systematically according to the influence of a wide range of moderators.This research makes several contributions across domains. First, we go beyond previous meta-analyses and contribute to theories of reactance and social influence by uncovering previously overlooked moderators and establishing several new empirical generalizations. Second, for social norms marketing ([38]; [110]), we specify the effects of social norms for a broad spectrum of consumer behaviors and detail how practitioners and government officials can utilize actionable moderators, such as using appropriate communication elements for certain behaviors, countries, and consumers. This should improve their success rate which has been mixed to date.Third, we contribute to the literature on cross-cultural marketing ([88]; [93]; [106]) by establishing how cultural differences can determine the effects of social norms on both socially approved and disapproved behaviors. Finally, we develop a comprehensive research agenda, based on insights from our meta-analysis. Theoretical Background Social NormsSocial norms are a shared understanding among members of a society about which behaviors are permitted, forbidden, or obligatory ([29]). They result from exposure to and observations of others' behavior and act as a ""social proof,"" whereby consumers follow the actions or opinions of others, in the belief that ""If everyone is doing it, it must be a sensible thing to do"" ([25], p. 1015). Social norms serve as decision shortcuts for choosing how to behave in a given situation ([24]).One of their distinctive features is that social norms are shared, which implies the existence of some group through which they spread ([26]). Humans maintain social harmony by complying with the social order and developing coping strategies to ""fit in"" ([66]) or ""copy the successful"" ([46]). Consequently, humans have an almost automatic propensity to learn social norms ([81]). Yet, this propensity does not necessarily result in compliance with them ([84]).The reason is that unlike laws, social norms are informal—they regulate behaviors without formal enforcement ([43]), so consumers have the freedom to follow or violate social norms ([26]). Accordingly, their impact on behavior stems from two evolutionary desires: ( 1) for social acceptance or affiliation and ( 2) for avoiding negative social outcomes such as social exclusion ([ 8]; [66]). As people are free to comply, and are inclined to do so, understanding why they do not comply is key for identifying systematic differences in reactions to social norms. Social Norms and ReactanceDespite consumers' natural inclination to comply with social norms, research consistently shows that attempts at influence that cite social norms can evoke psychological reactance ([18]; [92]). Reactance stems from individuals cherishing their autonomy and freedom of choice. As [19], p. 420) explain, ""For a given individual at a given time, there is a set of behaviors in which he believes he is free to engage. Any reduction or threat of reduction in that set of free behaviors arouses a motivational state, 'reactance,' which is directed toward reestablishment of the lost or threatened freedom."" If consumers believe their freedom to engage in a specific behavior is threatened, this evokes reactance, which enhances the attractiveness of the threatened behavior.Reactance theory is useful for approaching the vast, heterogeneous literature on social norms because it provides a broad theoretical lens for investigating the influence of diverse factors, including behavioral, communication, consumer, and environmental factors ([92]). Thus, we build on the key antecedents of reactance: consumer expectations of freedom and the extent of the freedom threat ([92]) to understand the drivers of systematic differences in the effects of social norms on behavior. Expectations of freedomConsumers do not perceive all their behaviors as freedoms ([20]), so reactance is contingent on an expectation that the person can freely choose among different behavioral alternatives ([27]). Thus, consumers likely exhibit reactance to social norms that appear to undermine their freedom ([19]). If they lack expectations of freedom in the first place, social norms should trigger less reactance ([92]). Several studies affirm that consumer reactance to attempts to influence decreases if an experimental manipulation lowers their perceptions of choice freedom ([36]; [58]). Freedom threatA social influence attempt that implies that someone is trying to reduce freedom represents a threat ([18]). This threat of social norms is exacerbated if the norms exert greater pressure for change ([20]). The threat level tends to reflect the way a social norm is communicated ([92]), so more forceful messages prompt more reactance ([59]; [111]). For example, research suggests reduced compliance with messages that advocate teetotalling rather than limited drinking ([ 9]). Yet, freedom threats may also stem from barriers to performing a behavior, such as consumer costs. When costs are a barrier to free choice, the aroused reactance is directed at maintaining the threatened behavior and therefore increasing its desirability ([27]). Conceptual FrameworkBuilding on reactance theory, we identify different groups of moderators driving behavioral compliance with social norms, as shown in Figure 1. These moderators include target behavior characteristics, communication factors, consumer costs, and environmental factors.Graph: Figure 1. Conceptual framework: Factors that influence the effect of social norms on behavior. Target Behavior CharacteristicsThe effects of social norms may vary across behaviors because characteristics inherent to the behavior influence perceptions of the freedom to perform it and threat to that freedom. Social approval/disapproval of behaviorsSocieties have developed social reinforcement mechanisms that encourage some behaviors and discourage others ([48]; [72]). We define socially approved behaviors as being explicitly encouraged by society (e.g., recycling, volunteering), socially acceptable (e.g., carpooling), and/or perceived as appropriate by society (e.g., using condoms). We define socially disapproved behaviors as being explicitly discouraged (e.g., smoking), socially unacceptable (e.g., littering), and/or perceived as inappropriate (e.g., binge drinking). A socially approved behavior evokes positive reinforcement via social outcomes, such as inclusion, acceptance, and affiliation ([24]). A socially disapproved behavior instead induces negative reinforcement via social consequences, such as social exclusion, alienation, or ridicule ([66]). Social approval versus disapproval of behaviors is thus a crucial factor that has implications for consumers' expectations of their freedoms to perform them.Performing a socially disapproved behavior is potentially more damaging to society as a whole than failing to perform an approved behavior ([47]). Thus, to maintain social order, societies tend to be more punitive of disapproved behaviors ([33]; [46]). In contrast, not adopting an approved behavior is less harshly punished and sometimes can even bring positive benefits, such as elevation in inferred social status ([ 8]). Thus, consumers are less likely to perceive social norms regulating socially disapproved behaviors as limitations to their freedoms, which diminishes reactance and increases compliance with social norms discouraging these behaviors. Yet, social norms targeting socially approved behaviors are seen as freedom limitations, causing more reactance and reduced compliance. Thus, we expect social norms pertaining to socially disapproved behaviors to be more effective than those pertaining to socially approved behaviors (H1). Existing versus new behaviorsExisting behaviors are already performed by consumers, at least sometimes, in contrast to entirely new behaviors. Consumers already have exercised their freedom to perform the existing behaviors, so they may feel less threatened when encouraging existing behaviors and their reactance to social norms that target existing behaviors may be relatively low. In contrast, targeting a new behavior may represent a stronger threat to freedom and, thus, induce reactance and decrease compliance. Consistently, for example, compliance with hand-washing advice has been higher than compliance with mask-wearing advice during COVID-19 and other infection outbreaks ([32]; [103]). Hand washing is an existing behavior and engrained into daily routines, whereas mask wearing was new for most consumers and generated more reactance. Therefore, we expect social norms pertaining to existing behaviors to be more effective than those pertaining to new behaviors (H2). Hedonic versus utilitarian behaviorsHedonic behaviors are those driven by pleasure-related goals and are evaluated primarily on the benefits related to enjoyment, taste, aesthetics, and symbolic meaning. Utilitarian behaviors, instead, are driven by functionality goals and are performed and evaluated primarily on the basis of functional, instrumental, and practical benefits ([23]). Social norms can pertain to both types, including utilitarian behaviors such as banking (e.g., ""Most millennials use online banking"") and hedonic ones such as buying cosmetics (e.g., ""12 makeup bag must-haves""). But their effectiveness is not clear a priori. On the one hand, reactance to social norms might be higher for hedonic behaviors because consumers have a stronger desire to perform those behaviors as part of their sense of freedom ([79]). Consumers can leverage social norms to justify a desirable behavior for themselves and enhance their perceptions of freedom to perform it. For example, the justification that ""everyone's doing it"" is common for hedonic behaviors ([39]) and can increase perceived freedom for engaging in these behaviors. On the other hand, indulging in a hedonic behavior often prompts a sense of guilt, making it harder to justify ([75]; [83]), which may reduce consumers' perception of freedom. With these opposing predictions, we treat the effects of social norms on hedonic versus utilitarian behaviors as an empirical question. Behaviors benefiting other peopleSome behaviors benefit other people directly (e.g., donating to charity), whereas others have indirect benefits (e.g., recycling). Social norms stem from group considerations, so consumers' willingness to enact their freedom may decrease if they realize that others will be negatively affected by their social norm violations ([101]). Correspondingly, behaviors that have negative implications for others yield lower reactance levels ([34]). We therefore expect that when others benefit from the behavior, this will enhance the effect of social norms on that behavior (H3). Public versus private behaviorsWe define public behaviors as those that are performed in public or can be observed by others (e.g., using public transport), in contrast to private behaviors (e.g., reducing energy consumption at home). Private behaviors are not subject to others' scrutiny, so consumers' perception of freedom threat to perform them may be relatively low, which should decrease reactance ([92]). This argument would imply that compliance with social norms regulating private (vs. public) behaviors should be higher. Yet, for public behaviors, reactance may also be reduced but for a different reason. Specifically, consumers are often concerned with how others perceive them ([65]), which reduces their willingness to enact their freedom and, in turn, reduces reactance. This would suggest higher compliance with social norms regulating public (vs. private) behaviors. Given these two opposing predictions, we refrain from making a directional hypothesis about this variable. Communication FactorsThe use of social norms can trigger reactance because communication factors influence the perceived threat to behavioral freedom ([ 9]; [92]). We consider several communication factors, such as how the norm is formulated as well as whether it benefits an organization, references specific groups, and includes explicit sanctions or rewards. Norm formulationSocial norms can be formulated as descriptive or injunctive. Descriptive norms describe typical behaviors of some relevant group and signal which behaviors are most popular ([25]; [95]). Injunctive norms instead prescribe certain behaviors and indicate what the target consumer should or should not do. For example, a list of ""bestsellers"" represents descriptive norms, but ""ten must-read books"" lists communicate injunctive norms. As injunctive norms convey explicit demands, which consumers likely perceive as forceful threats to their freedom, they should generate more reactance than descriptive norms ([69]; [109]). Consumers exposed to descriptive norms instead may come up with reasons for the behavior of the majority and adjust their own behavior accordingly, without much reactance ([95]). Therefore, we expect a stronger impact of descriptive (vs. injunctive) social norms on behaviors (H4). Organization-benefiting normsWe define social norm communications that reveal a specific entity, such as a firm or government body, which would benefit from compliance with the social norm, as organization-benefiting social norms. For example, ""my friends subscribed to the university's gym program"" would benefit the gym if the target consumer complied with this behavior. While specific entity matters, overall, social norms that refer to organizations tend to be more concrete and specific because they activate situational factors (i.e., where and when the norm applies) ([ 1]). Such specificity and concreteness diminish the general threat to freedom for consumers by limiting it to the particular situation, which lowers their reactance ([38]). We thus expect organization-benefiting social norms to be more effective than those that do not mention organizations (H5). Close group membersCommunications about social norms often specify close group members—that is, people who are genetically related (e.g., family) or similar (e.g., close friends)—rather than refer to an abstract group (e.g., fellow citizens, people). Evolutionary predictions of social cooperation highlight kinship mechanisms. Namely, a request that activates a kin care motive reduces reactance and promotes compliance without expectations of reciprocation ([41]; [46]). The closer the relationship is, the less reactance consumers are likely to experience, enhancing norm compliance ([79]; [99]). Thus, we expect social norms referring to a close group member to be more effective than social norms referring to abstract or distant groups (H6). Authority figuresCommunications around social norms often refer to authority figures, or individuals who can exercise power over others, formally or informally (e.g., superiors, experts, government officials, teachers), to enhance compliance. [78] famous studies show that formal orders from an authority figure (real or perceived) increase obedience. Yet, because social norms are informal, being required to do something by an authoritative source may make the threat to freedom more salient and trigger reactance ([ 4]). For example, expert recommendations may lead to reactance and diminish compliance ([36]). For these reasons, we expect social norms referring to authority figures to be less effective than those that do not refer to authority figures (H7). Explicit sanctions and rewardsThe sanctions and rewards associated with noncompliance and compliance with social norms might be either implicit, meaning they are indirect and left for consumers to infer, or explicit, meaning they are clearly stated. If sanctions and rewards are explicit, they might diminish behavioral compliance because they make the persuasive nature of the social norm message salient ([86]) and threaten freedom expressly ([58]). Both aspects increase the perceived threat to freedom to perform the behaviors and reactance ([ 4]). Thus, we expect social norms that specify potential sanctions (for failing to comply) (H8) or potential rewards (for complying) (H9) to be less effective than social norms that do not make those consequences explicit. Consumer CostsThe costs incurred to perform a behavior can create barriers. For example, social norms may direct consumers to buy an electric car, which is considerably more expensive than a regular car. We believe such costs will increase consumer reactance ([92]). However, the direction of the cost effect is not clear a priori (see [27]). On the one hand, a high cost may signal the desirability or status of the behavior, thereby motivating compliance. On the other hand, a high cost may dissuade consumers from attempting the behavior. Given these two opposing forces, we refrain from making directional hypotheses about costs, including costs associated with effort, money, and time. EffortWe define ""effort costs"" as the amount of physical or mental resources consumers must invest to comply with social norms. Some behaviors require more effort (e.g., exercising), others less (e.g., not littering). Social norms that require more effort demand greater behavioral change. They may either increase compliance by increasing the attractiveness of the effortful behavior or decrease compliance by decreasing the attractiveness of the effortful behavior (by derogating it because of reduced attainability) ([27]). Which of these two forces is stronger is an empirical question. Monetary costsConsumers may incur additional monetary costs to comply with social norms. For example, buying organic rather than conventional food requires more monetary resources. However, reusing a hotel towel does not result in monetary costs. Monetary costs constitute a direct barrier to free choice because consumers must sacrifice extra resources to comply. When social norms regulate costly behaviors, the monetary costs may imply a greater threat to the freedom to engage in this behavior, enhancing the attractiveness of this option and increasing compliance with such social norms ([27]). Yet, monetary costs may also emphasize the unattainability of the option, which would reduce compliance. Thus, we treat the effect of monetary costs on compliance with social norms as an empirical question. Temporal costsCompliance with social norms may require long-term (e.g., adhering to a healthy eating program) or temporary (e.g., reusing hotel towels) commitment. The temporal costs barrier is greater for behaviors with long-term commitments because these social norms impose more behavioral constraints than those that require only temporary commitments. Thus, on the one hand, consumers may also have stronger resistance to losing an option with potential longer-term consequences ([58]), which would increase compliance with social norms regulating longer-term behaviors. On the other hand, perceived unattainability of behaviors is also greater if they persist, now and into the future, rather than if they involve a single instance, which could decrease compliance with social norms involving longer-term commitment. Thus, we treat the effect of temporal costs on compliance with social norms as an empirical question. Environmental FactorsConsumers form freedom expectations through socialization in a specific cultural environment at a particular time ([80]). Culture shapes expectations by providing a logic for acting both housed in members' knowledge and beliefs and observed in members' behaviors ([102]). In some cultures, the range of approved behaviors is wide, and behavioral transgressions of social norms are tolerated. Other cultures allow a narrower range of behaviors and exhibit lower tolerance for deviations from social norms ([74]; [106]). Consumer reactance and compliance to social norms should thus differ systematically across cultures ([94]).To account for cultural differences, we adopt Inglehart's cultural framework ([52]) with two bipolar dimensions: traditional versus secular-rational and survival versus self-expression values. These dimensions have clear implications for reactance to social norms because they influence tolerance for transgressions (traditional–secular-relational) and the range of approved behaviors (survival–self-expression). Moreover, these dimensions are measured regularly, which enables us to account for cultural dynamics ([104]).[ 6] Traditional versus secular-rationalThis dimension contrasts traditional societies, in which religion is very important, and secular-rational societies, in which it is not ([52]). Traditional societies also emphasize deference to authority, absolute standards, cultural protectionism, and national pride, and they generally exhibit less tolerance for transgressions of social norms. Secular-rational societies reflect opposing values. We expect the effect of social norms on behavior to be stronger in cultures closer to the traditional (vs. the secular-rational) pole (H10) because they effectively restrict consumers' awareness and expectations of freedom, which should decrease reactance. Survival versus self-expressionThis dimension reflects transitions from industrial to postindustrial societies ([52]). Survival societies emphasize economic and physical security and familiar norms to maximize the predictability of others' behaviors, which results in a relatively narrow range of behaviors that may be perceived as freedoms. Consumers in survival societies have low expectations of personal freedoms and identify less freedom to be threatened ([54]). In contrast, self-expression values emphasize variety, imagination, and tolerance of outgroups. As societies move toward self-expression, people generally become freer to make choices for themselves ([106]), which enhances their reactance to social norms and decreases compliance. In cultures that value self-expression, noncompliance with social norms may even signal the person's freedom to be unique, which is valued by consumers of these societies ([40]; [106]). In contrast, in survival cultures, violation of social norms is more likely to jeopardize economic or physical security ([49]), diminishing perception of these behaviors as freedoms. Thus, we expect the effect of social norms on behavior to be stronger in cultures close to the survival (vs. the self-expression) pole (H11). TimeThe human propensity to comply with social norms has resulted from evolutionary processes ([41]). Therefore, the effect of social norms on behaviors should be stable throughout the short time (in evolutionary terms) marketers have been using them as a persuasion strategy. Yet research into conformity to social pressures also indicates some changes over time, including studies that document that conformity in the United States has declined ([16]), increased ([60]), or fluctuated ([62]) due to changes in social media and the cohesiveness of society, among other things. Thus, the effectiveness of social norms over time is an empirical question. Moral freedomCultures also vary in moral freedom, which reflects the extent to which people make their own moral choices rather than being influenced by state intervention ([ 3]). We expect the effect of social norms on behavior to be stronger in countries lacking moral freedom (H12), because of the lower expectations of freedom in those countries. Interaction Effects The interaction of social approval/disapproval and the environmental factorsThus far, our discussion has focused on main effects. However, consumers do not learn social norms in isolation; instead, they become aware of freedoms to perform certain behaviors through socialization in a particular culture and by observing different behaviors over time ([80]). To the extent that different societies shape consumer awareness of social norms, we expect the effects of the behavior being socially approved versus disapproved to be moderated by environmental characteristics (i.e., culture and time).Specifically, with respect to the traditional versus secular-rational cultural dimension, we note that participation in a world religion makes punishments for socially disapproved behaviors more salient to people ([47]). In Christianity, seven of the Ten Commandments start with the phrase ""you shall not."" In Judaism, of 613 mitzvot in the Torah, 365 (60%) forbid bad behaviors. Islam explicitly specifies an extended list of behaviors that are haram, or forbidden ([71]). Thus, in traditional cultures, where religion is more important, reactance to social norms that target socially disapproved (vs. approved) behaviors might be lower, because many of these behaviors already have been forbidden by religions. Thus, we expect the stronger effect of social norms on behaviors in traditional cultures to be especially pertinent for socially disapproved (vs. approved) behaviors (H13).With respect to the survival versus self-expression cultural dimension, engaging in socially disapproved behaviors in survival cultures is more likely to jeopardize economic or physical security ([49]), diminishing perception of these behaviors as freedoms. In contrast, in societies leaning toward the self-expression pole, engaging in disapproved behaviors would be more tolerated ([52]). Thus, we expect that the effect of social norms on behaviors in survival cultures is especially strong for socially disapproved (vs. approved) behaviors (H14).As to the effect of time, we expect that social media might enhance the effects of social norms by exposing consumers to more regular reinforcements pertaining to a wider range of socially approved and disapproved behaviors ([10]). Social media enables consumers to share content and feedback in real time, much of which remains available indefinitely and can be tracked by other parties ([45]). Exposure to norm violations in such settings triggers exhibitions of moral outrage, as manifested in the notion of a ""cancel culture,"" whereby social media users shame and punish perpetrators of bad behaviors, signaling that such behaviors are not tolerated ([30]). These developments imply that, over time, engaging in socially disapproved behaviors is stigmatized more severely than not engaging in socially approved behaviors ([33]), which reduces expected freedom to perform socially disapproved behaviors. Thus, we expect a stronger, more positive impact of social norms on socially disapproved (vs. approved) behaviors over time (H15). Other interactionsIn addition to the aforementioned hypothesized interaction effects, given the importance of the fundamental distinction between social norms regulating socially approved versus disapproved behaviors, we also explore additional interactions. Specifically, we investigate the interaction between social approval/disapproval and the target behavior characteristics, communication factors, and consumer costs. When these interactions are significant, we return to them in the discussion and highlight theoretical and managerial insights. Methodological CharacteristicsSystematic differences in the methodologies used by studies may cause variation in the reported effects ([14]). We control for ( 1) type of data ([quasi]experiment vs. other), ( 2) whether a sample involves students or regular consumers, ( 3) whether participants were exposed to (vs. indicated their perceptions of) social norms, and ( 4) whether participants' behavior was self-reported (vs. observed). Finally, to account for publication bias, which arises when the effect sizes in published studies are not representative of the entire population of effect sizes ([17]), we control for ( 5) the association between the strength and the precision of the effect sizes ([100]). More details follow. Methodology SampleTo identify relevant studies of the impact of social norms on consumer behaviors, we retrieved references from Google Scholar, Online Contents National, PsycINFO, and the Web of Science up to March 2019. We searched for keywords such as ""norm,"" ""social norms,"" and ""social pressure"" (for the full list of keywords, see Web Appendix A). We also checked the websites of the Social Science Research Network, the National Social Norms Resource Center, and Higher Education Center for Alcohol and Other Drug Abuse and Violence Prevention for relevant studies. We posted requests for unpublished manuscripts and working papers on the online academic platform ELMAR. Finally, we examined all cross-references from applicable documents. The procedure resulted in articles from five research domains: psychology (35.2%), health (34.4%), marketing (10.4%), food and nutrition (10.8%), and the environment (9.2%).Our dependent variable is the strength of the relationship between social norms and consumer behavior in the studies, which constitutes their observed effect sizes. We selected Pearson's product-moment correlation coefficient to measure effect sizes, because most studies operationalize both social norms and the target behavior as continuous variables. The consumer behaviors investigated in these eligible studies refer to the purchase, consumption, use, or disposal of products, services, material objects, or consumption experiences (e.g., buying organic products, subscribing to a gym, adopting mobile banking, donating). We exclude studies that focus on ( 1) aggregate entities (e.g., countries, societies) rather than individual consumers; ( 2) behaviors unrelated to consumption, such as social perceptions or interpersonal relations (e.g., stereotypes); ( 3) criminal behaviors, because the influence of the law would be confounded with the influence of social norms; and ( 4) consumers with impaired autonomy, such as workers making job-related decisions who must follow organizational policy, patients who rely extensively on others to make medical decisions ([77]), or people whose addictions limit their decision-making ability ([64]). Furthermore, to be included an eligible study must ( 1) examine actual behaviors, reported or observed (rather than intentions); ( 2) contain enough information to calculate the correlations between social norms and behaviors; and ( 3) support computations of the unconfounded effects of social norms. To illustrate ( 3), we excluded studies that collapsed the impacts of social norms and marketing promotion (e.g., [112]) or injunctive and descriptive social norms (e.g., [56]).The final sample thus consists of 252 effect sizes extracted from 137 articles, comprising 177 studies over the period 1978–2019. Web Appendix B lists the articles, effect sizes, and moderator values. The sample sizes of the primary studies range from 28 to 44,108 (median = 269), so that they produce a total of 112,929 unique respondents from 22 countries. Three of the 252 effect sizes have studentized residuals that are greater than 2.57 ([107]); two of them (r = −.19, n = 353; r = .71, n = 451) are influential, in that they lie outside the prediction interval (i.e., range of plausible values for any individual effect size) and cannot be explained by small sample sizes ([17]). We remove them from the subsequent analyses, leaving 250 effect sizes from 136 articles, based on 112,478 unique respondents. (As we detail in Web Appendix C, the primary studies provide an explanation for the extreme values of these outliers; the results are robust for including them in the analysis.) Variable CodingTwo independent coders (blind to the hypotheses) coded the moderators and cataloged the technical information (e.g., sample size). The intercoder agreement was 94.8%, and any disagreements were resolved through discussion. Effect sizesWe retrieved zero-order correlations, measuring the association between social norms and the target behavior, from the studies' correlation matrices or else converted statistics (e.g., F-value, t-value, p-value, χ2) into r (see [17]; [67]). If partial correlations were available, we also retrieved them from the studies. (The results are robust whether we use partial or zero-order correlations as measures of effect sizes.) We transformed the correlations into Fisher's z-scores ([17]) to satisfy the assumptions of normal distributions and known sampling variance of the effect sizes to estimate the model (for details, see the ""Model"" section).[ 7] In turn, we estimate the meta-analytic regression model with Fisher's z-scores as the dependent variable. We obtain the mean effect sizes, confidence intervals, predicted values, and plots by back-transforming Fisher's z-scores into correlation coefficients to facilitate interpretation (for details, see Web Appendix D). For robustness, we perform the analyses also by using the correlation coefficients. The effect sizes are coded such that a positive sign indicates a positive change in behavior (i.e., increase in socially approved or decrease in disapproved behaviors).[ 8] Moderators Table 1 shows the coding scheme for all the moderators. We mean-centered all continuous moderators and all dummy variables involved in interactions ([89]). We retrieve scores for the cultural dimensions from the World Values Survey ([53]) for each effect size, using the country and year of publication of each study. For the time variable, the code reflects the year of publication.[ 9] The precision of the effect size estimate is measured as the inverse of its standard deviation ([100]). If a publication bias is present, retrieved small sample studies are more likely to yield stronger effect sizes than those that are not retrieved, which implies a negative relationship between precision and effect size. By controlling for precision, the effects can be estimated more accurately ([35]).GraphTable 1. Coding Scheme for the Moderators of Social Norms–Behavior Effects. VariableCodeTarget Behavior CharacteristicsSocial approvalDummy = 1 if the behavior is socially approved (i.e., discussed positively by the authors) and 0 if the behavior is socially disapproved (i.e., noted as problematic by the authors). Mean-centered.ExistingDummy = 1 if the behavior exists (i.e., consumers already engage in it at least sometimes) and 0 if the behavior is new (i.e., consumers have not adopted the behavior yet).HedonicDummy = 1 if the behavior is hedonic (i.e., driven by pleasure-related goals) and 0 if the behavior is utilitarian (i.e., driven by functionality-related goals).Benefits to other peopleDummy = 1 if the behavior brings about social benefits and 0 otherwise.Public behaviorDummy = 1 if the behavior is public (i.e., is visible to others) and 0 if the behavior is private (i.e., is invisible to others).Communication FactorsSocial norm formulationDummy = 1 if the social norm is descriptive (i.e., describes behaviors of others) and 0 if the social norm is injunctive (i.e., suggests what should be done).Organization-benefitingDummy = 1 if the social norm benefits a specific organization and 0 otherwise.Close groupDummy = 1 if the norm refers to a person close to the individual and 0 otherwise.Authority figureDummy = 1 if the norm refers to a person in a position of authority and 0 otherwise.Explicit sanctionsDummy = 1 if the negative consequences of not abiding by the norms are made explicit and 0 otherwise.Explicit rewardsDummy = 1 if the positive consequences of abiding by the norm are made explicit and 0 otherwise.Consumer CostsEffortDummy = 1 if complying with the social norm entails much physical or mental effort and 0 if compliance entails little effort.Monetary costsDummy = 1 if complying with the social norm entails additional monetary costs and 0 otherwise.Temporal costsDummy = 1 if complying with the social norm entails a long-term investment and 0 if it entails a temporary investment.Environmental FactorsTraditional–Secular-rationalContinuous: scores for the Inglehart dimension in the year of publication minus 2 and country of data collection. Mean-centered.Survival–Self-expressionContinuous: scores for the Inglehart dimension in the year of publication minus 2 and country of data collection. Mean-centered.TimeContinuous: year of publication of the paper from which the effect sizes are extracted minus 2. Mean-centered.Moral freedomContinuous: World Index of Moral Freedom (i.e., extent to which individuals make their own moral choices rather than being influenced by state intervention; Álvarez, Kotera, and Pina 2020). Mean-centered.Methodological ControlsType of dataDummy = 1 if the study is an experiment or a quasiexperiment and 0 otherwise.SampleDummy = 1 when a student sample was used and 0 otherwise.Effect size precisionContinuous: inverse of the standard error of the effect sizes (Fisher's z-transformed). Mean-centered.Behavior operationalizationDummy = 1 when participants self-report the behavior and 0 when the behavior is observed.Social norm operationalizationDummy = 1 when participants are exposed to the social norms and 0 when the social norms are perceived. 1 Notes: We control for the operationalizations of behaviors and social norms in a robustness check (see Web Appendix G). Meta-Analytic Model and EstimationTo test the conceptual framework in Figure 1, the model should account for the structure of the data, because the effect sizes are nested within samples that are nested within articles, which could lead to correlated errors. We specify a mixed-effects meta-regression model using a multilevel parameterization ([105]) in which ( 1) observed effect sizes are assumed to be a normally distributed random sample from the population of true effect sizes; and ( 2) the variance distribution of true effect sizes can be explained by random effects at the effect size, sample, and article levels, to account for data nesting, and by the fixed effects of the moderators. Thus, the effect size i extracted from sample j in article u is modeled as follows:Effect Sizeiju = β0 + β1 Social approvaliju + β2 Existing behavioriju + β3 Hedoniciju + β4 Benefit peopleiju + β5 Public behavioriju + β6 Norm formulationiju + β7 Organization-benefitingiju + β8 Close groupiju + β9 Authority figureiju + β10 Explicit sanctionsju + β11 Explicit rewardsiju + β12 Effortiju + β13 Monetary costsiju + b14 Temporal costsiju + β15 Traditional–secular-rationalu + β16 Survival–self-expressionu + β17 Timeu + β18 Moral freedomu + β19 Social approvaliju × Traditional–secular-rationalu + β20 Social approvaliju × Survival–self-expressionu + β21 Social approvaliju × Timeu + β22 Type datau + β23 Sampleju + β24 Effect size precisioniju + δu + εi + γju + φiju, where δu ∼  N(0,  σδ2  ) is a random effect that reflects the variance among articles, εi ∼ N(0,  σϵ2  ) is the sampling variance of the observed effect sizes, γju ∼  N(0,  σγ2  ) is a random effect estimating the variance across samples nested within articles, φiju ∼  N(0,  σφ2  ) is a random effect that indicates the variance among effect sizes nested within samples and within articles, β0 is the intercept, and β1–24 are the parameter estimates for the moderators defined in Table 1. We perform all the analyses with the Metafor package for R ([107]). ResultsWe first present the grand mean effect size and the distribution of individual effect sizes. Next, we present the results of the meta-regressions testing for the moderators. To provide evidence without the multilevel specification, Model 1 does not account for the nested data structure. The hypothesized moderators (H1–H15) are then tested with Model 2, which accounts for the nested structure. We note that the findings of Model 1 versus Model 2 are very similar (see Table 3), despite their different approaches to nesting. Drawing on Model 2, Table 3 also presents predicted effect sizes for each level of the categorical moderators with all the other moderators set at their sample average ([15]) as well as simple mean correlations. The predicted values and simple mean correlations do not differ substantially, suggesting a good balance of moderator conditions across studies ([57]). Model 3 adopts an iterative approach to identify additional significant interactions between socially approved versus disapproved behaviors and target behavior characteristics, consumer costs, and communication factors, and we discuss its results where relevant. Grand Mean Effect Size and Distribution of Effect SizesOverall, social norms have a positive, small to medium impact on behaviors (i.e., the grand mean effect size is positive and significant, with  r¯   = .254 and a 95% confidence interval [CI95%] ranging from.232 to.277; [28]). The Q-statistic, which represents the total weighted deviation of each individual effect size from the mean, is significant (Q = 4,360, p < .001). Most observed effect size variance thus is systematic rather than due to sampling error and can be explained by moderators ([17]). Other heterogeneity indicators (Tau2 and I2) lead to the same conclusion (see Web Appendix E).The distribution of individual effect sizes, shown in Figure 2, reveals that they range from r = −.22 to r = .63 (M = .249, Mdn = .240, SD = .152), and 67% of them fall within a.10–.40 interval. Multicollinearity is not a concern. The largest bivariate correlation, r = .54, is between monetary costs and socially approved behaviors. The variance inflation factors are 3.73 or less for all variables. Table 2 provides the correlation matrix and descriptive statistics.Graph: Figure 2. Social norm–behavior effect size frequency distribution (k = 250).GraphTable 2. Bivariate Correlations and Descriptive Statistics for the Social Norm–Behavior Effect and Moderators. MeanSD12345678910111213141516171819202122 1. Effect size (r).25.1521 2. Social approval.79−.1211 3. Existing.26.004.0321 4. Hedonic.32.141−.525.0681 5. Benefits to people.32.095−.237−.074.0131 6. Public behavior.52.066.050.023.127−.1051 7. Norm formulation.30.287−.152−.056.150.118.0351 8. Org. benefiting.52.319−.028−.087.161−.208.038.1401 9. Close group.66.182−.207.005.112.125−.030.009−.014110. Authority figure.10−.120.048−.040−.121.078−.092−.137−.145−.004111. Explicit sanctions.15.014−.038.155.078−.127.029−.057.084.141−.018112. Explicit rewards.41−.078.106.064−.093−.162−.005−.076.085.109.116.101113. Effort.30−.215−.019−.150−.155−.169−.026−.129−.218.034.003.000.092114. Monetary costs.57−.005.540−.007−.429−.490−.006−.175.107−.058−.050.080.201.185115. Temporal costs.69−.038−.159−.039.073−.434.027.008.182.027−.138.005.135.389.150116. Trad.–Sec-rational.12.406.090.156−.193−.110−.074.007.104.067−.062.022.135−.111.129.109.049117. Survival–Self-exp..80.330−.054.202.034−.200−.035.034−.082.001−.106−.058−.016−.066.160.051−.040.134118. Time (year)20049.219.164.026.087.009−.101.266.324.049−.231.071−.052−.073.025.106.133.207−.018119. Moral freedom74.918.71−.096.108.130−.072−.016−.041.007.030−.085.036.024.018.110−.012−.004.154.436.059120. Type of data.16−.065−.194−.249−.022.333.122−.133−.060.094−.126−.165−.009−.270−.211−.086−.240−.027−.195.006121. Student sample.41.034−.346.170.285−.057.167.048.098−.016.046−.048.053−.137.173.081−.253−.070.084−.041−.044122. Effect size precision20.4719.77.144.029−.183.117−.023.155.168−.033−.103−.031−.011−.063−.091−.101.055.130−.009.169.076−.166.2291 2 Notes: Boldfaced correlations are significant at α = .05. For the dummy variables, only the mean is provided as a descriptive statistic. Target Behaviors Characteristics Social approval/disapprovalContrary to H1, social norms are not more effective for disapproved (vs. approved) behavior (b = −.002, p = .957). Although the insignificant main effect suggests that social norms are equally effective for approved and disapproved behaviors overall, this variable is involved in several interactions with other moderators, as we discuss next. Existing versus newThe effect of existing (vs. new) behavior is not significant (b = .017, p = .514); thus, H2 is not supported. However, according to Model 3, this variable interacts with the social approval of behavior at marginal significance (b = .138, p = .072). Figure 3, Panel A, shows that when targeting socially approved behaviors, social norms tend to be more effective in encouraging existing (predicted  r¯   = .253) versus new (predicted  r¯   = .217) behaviors. For socially disapproved behaviors, the opposite pattern emerges, as social norms tend to be more effective for discouraging new (predicted  r¯   = .298) versus existing (predicted  r¯   = .207) behaviors.Graph: Figure 3. Social norm–behavior effect sizes for socially approved and disapproved behaviors as a function of focal moderators. Hedonic versus utilitarianThe impact of social norms does not differ across hedonic and utilitarian behaviors (b = .016, p = .599). This finding is not surprising given the two opposing forces (desire and guilt) driving hedonic behaviors. Benefits to peopleSocial norms are more effective when behaviors benefit others (predicted  r¯   = .289) than when they do not (predicted  r¯   = .229, b = .064, p = .035), in support of H3. Public behaviorThe effect of public behavior is not significant (b = .013, p = .500), suggesting that social norms are equally effective for public and private behaviors. Communication Factors Norm formulationIn support of H4, descriptive norms have stronger effects on behavior (predicted  r¯   = .305) than injunctive norms (predicted  r¯   = .223, b = .088, p < .001). Further, the interaction between norm formulation and socially approved behaviors in Model 3 is also significant (b = −.110, p = .018). Figure 3, Panel B, shows that descriptive (vs. injunctive) norms are more effective when targeting disapproved behaviors (predicted  r¯ descriptive = .337 vs. predicted  r¯ injunctive = .183, Δ = .154) than when targeting approved behaviors (predicted  r¯ descriptive = .281 vs. predicted  r¯ injunctive = .228, Δ = .053). Thus, descriptive social norms that describe how the majority behaves are especially effective in curbing disapproved behaviors. Organization-benefitingIn support of H5, social norms are more effective when they benefit an organization (predicted  r¯   = .279) than otherwise (predicted  r¯   = .214, b = .069, p = .004). Close groupIn support of H6, social norms are more effective if they refer to a close group member (predicted  r¯   = .266) versus an abstract group (predicted  r¯   = .213, b = .057, p = .008). Authority figureThe effectiveness of social norms is not impeded by references to an authority figure (b = −.012, p = .729), in contrast to H7. Sanctions and rewardsThe effect of social norms does not depend on the presence of explicit sanctions (b = −.010, p = .767), disconfirming H8. However, social norms with explicit mentions of rewards (predicted  r¯   = .208) are marginally less effective than those where rewards are not mentioned (predicted  r¯   = .255, b = −.050, p = .078), in line with H9. Consumer Costs EffortThe amount of effort required to comply with social norms does not have a significant effect on compliance (b = −.031, p = .255). This finding is consistent with the idea that there are two opposing forces behind this effect that counteract each other. Monetary costsSocial norms exert stronger effects on behavior when compliance entails monetary costs (predicted  r¯   = .276) than when it does not (predicted  r¯  =.211, b = .069, p = .017). This finding is consistent with the idea that barriers such as monetary costs can make the behavior more desirable for consumers (e.g., via status signaling; [27]). Temporal costsSocial norms seem to be equally effective for behaviors requiring a long-term or a temporary investment, as temporal costs are nonsignificant (b = −.011, p = .706). Environmental Factors Traditional–secular-rationalThe traditional–secular-rational cultural dimension does not have a main effect on the effectiveness of social norm (b = −.017, p = .643), which fails to support H10.[10] However, we find some support for H13 because the interaction with the social approval of behaviors is positive and marginally significant (b = .211, p = .104). Figure 3, Panel C, shows that the impact of social norms on socially disapproved behaviors is somewhat weaker in more secular-rational cultures, whereas their impact on socially approved behaviors remains stable across the traditional–secular-rational dimension. Survival–self-expressionThe effect of this cultural dimension is negative, as we expected, but it is not significant (b = −.027, p = .448), which does not support H11. However, in support of H14, its interaction with the social approval of behaviors is positive and significant (b = .180, p = .033). Thus, consistent with our expectations, social norms are less effective for socially disapproved behaviors in cultures closer to the self-expression pole, whereas their effectiveness for socially approved behaviors is stable across the survival–self-expression dimension (Figure 3, Panel D). TimeThe effect of time is not significant (b = .001, p = .956), but its interaction with the social approval of behaviors is negative and significant (b = −.006, p = .036), in support of H15. Social norms have become more effective over time at curbing socially disapproved behaviors, while their influence on socially approved behaviors remains stable (Figure 3, Panel E). Moral freedomThe national level of moral freedom lowers behavioral compliance at a marginal level of significance (b = −.002, p = .094), in support of H12. Methodological CharacteristicsThe studies in our meta-analysis yield the same results regardless of the type of data (b = −.045, p = .189) and whether they rely on student samples (b = −.001, p = .986). We obtain similar results if we control for whether the studies operationalize social norms using exposure or perception (note that we exclude this control variable in the main model because of multicollinearity but address it in supplementary analyses). Precision has a positive, marginally significant effect (b = .001, p = .082); therefore, effect sizes greater than the mean might be missing, which suggests that publication bias is not an issue. Robustness ChecksTo confirm that our results are robust, we first perform a series of diagnostic tests (Web Appendices E and F) to rule out publication bias; they reconfirm the positive influence of effect size precision in the meta-regression model. Next, we performed analyses based on 16 alternative model specifications. In Tables A1 and A2 in Web Appendix G, we present the results when we adopt alternative methodological choices: ( 1) including theoretical moderators only; ( 2) adding two demographic controls—the primary study participants' mean age and the percentage of men; ( 3) accounting for effect sizes coming from marketing journals; ( 4) estimating the meta-regression parameters with t-tests instead of z-values; ( 5) estimating the model including the two outliers (k = 252); ( 6) using the raw effect sizes—r and the variance of r—rather than Fisher's z transforms; ( 7) relying on partial instead of the zero-order correlation, if both measures could be retrieved from the primary study; and ( 8) controlling for the operationalization of the behavior (self-reported vs. observed) and for the operationalization of social norms (exposed vs. perceived) instead of controlling for the type of data. Across these methodological choices, the results remain consistent with the findings of the main models (Table 3).GraphTable 3. Meta-Regression Results. Model 1:Model 2:Model 3:Predicted ValuesSimple Mean CorrelationsPredictorsb (SE)b (SE)b (SE)r¯[CI95%]r¯[CI95%]Intercept.097 (.047)*.104 (.049)*.127 (.048)**——Social disapproval (k = 53)———.250 [.181,.317].299 [.248,.348]Social approval (k = 197).009 (.040)−.002 (.042).015 (.044).248 [.225,.271].250 [.230,.269]New behavior (k = 84)———.236 [.196,.276].263 [.221,.303]Existing behavior (k = 186).016 (.025).017 (.026).010 (.027).253 [.228,.277].259 [.237,.280]Utilitarian behavior (k = 170)———.244 [.216,.270].244 [.222,.265]Hedonic behavior (k = 80)−.012 (.028).016 (.030).001 (.031).258 [.216,.300].295 [.257,.331]Benefits people: no (k = 171)———.229 [.200,.258].251 [.228,.274]Benefits people: yes (k = 79).064 (.029)*.064 (.030)*.066 (.032)*.289 [.249,.328].279 [.244,.314]Private behavior (k = 120)———.242 [.214,.270].250 [.224,.275]Public behavior (k = 130).016 (.019).013 (.019).015 (.019).254 [.228,.280].269 [.241,.297]Norm formulation: injunctive (k = 175)———.223 [.200,.247].229 [.208,.251]Norm formulation: descriptive (k = 75).086 (.022)***.088 (.021)***.080 (.021)***.305 [.272,.338].329 [.293,.363]Organization-benefiting: no (k = 120)———.214 [.183,.245].208 [.181,.235]Organization-benefiting: yes (k = 130).077 (.022)***.069 (.024)**.078 (.025)**.279 [.250,.308].304 [.280.329]Close group: no (k = 85)———.213 [.179,.246].215 [.185,.244]Close group: yes (k = 165).066 (.021)**.057 (.022)**.051 (.022)*.266 [.242,.290].283 [.259,.306]Authority figure: no (k = 224)———.249 [.228,.271].266 [.246,.286]Authority figure: yes (k = 26)−.001 (.033)−.012 (.034)−.004 (.034).238 [.179,.297].202 [.149,.253]Explicit sanctions: no (k = 227)———.249 [.228,.270].259 [.239,.279]Explicit sanctions: yes (k = 23)−.019 (.032)−.010 (.035).001 (.035).239 [.177,.300].266 [.204,.325]Explicit rewards: no (k = 213)———.255 [.234,.277].265 [.244,.286]Explicit rewards: yes (k = 37)−.056 (.027)*−.050 (.028)†−.041 (.029).208 [.158,.257].229 [.184,.272]Effort: small (k = 174)———.257 [.231,.282].281 [.259,.303]Effort: large (k = 76)−.034 (.026)−.031 (.027)−.044 (.028).228 [.187,.268].210 [.174,.246]Monetary costs: no (k = 107)———.211 [.173,.249].262 [.230,.294]Monetary costs: yes (k = 143).071 (.027)**.069 (.029)*.072 (.029)*.276 [.246,.305].259 [.235,.282]Temporal costs: temporary (k = 78)———.256 [.211,.299].267 [.237,.297]Temporal costs: long-term (k = 172)−.012 (.028)−.011 (.029)−.010 (.031).245 [.220,.270].256 [.232,.281]Traditional–Secular-rational−.014 (.035)−.017 (.038)−.031 (.039)——Survival–Self-expression−.026 (.033)−.027 (.036)−.021 (.037)——Time−.001 (.001).001 (.001).001 (.001)——Moral freedom−.002 (.001)†−.002 (.001)†−.003 (.001)*——Social approval × Traditional–Secular-rational.175 (.121).211 (.130)†.298 (.145)*——Social approval × Survival–Self-expression.159 (.079)*.180 (.084)*.206 (.087)*——Social approval × Time−.005 (.003)*−.006 (.003)*−.008 (.004)*——Social approval × Existing behavior——.138 (.077)†——Social approval × Norm formulation——−.110 (.046)*——Type of data: other (k = 210)———.255 [.233,.277].265 [.244,.286]Type of data: (quasi)experiment (k = 40)−.048 (.033)−.045 (.034)−.014 (.036).213 [.155,.270].229 [.182,.274]Sample: nonstudent (k = 148)———.249 [.222,.275].254 [.230,.278]Sample: student (k = 102)−.001 (.023)−.001 (.024).011 (.025).248 [.214,.282].269 [.237,.301]Effect size precision.001 (.0005)†.001 (.0005)†.001 (.0006)†——Pseudo R230%29%30%Variances componentsTau2 = .016;σϵ2 = .001σδ2 = .0001;σϵ2 = .001;σγ2 = .004;σφ2 = .012σδ2 = .0006;σϵ2 = .001;σγ2 = .005;σφ2 = .011 3 *p < .05.4 **p < .01.5 ***p < .001.6 †p < .10.7 Notes: There is no regression coefficient for the reference category of the moderators. The pseudo R2 is the amount of between-effect size variance explained by the moderators. In Model 1, the nested structure of the data is not modeled and Tau2 represents the residual between-effect size variance. In Models 2 and 3, the nested structure of the data is accounted for and the variance components are the following:  σδ2   = variance across articles,  σϵ2  = sampling error variance,  σγ2   = variance between samples within articles, and  σφ2   = variance between effect sizes within samples within articles. Predicted values are based on Model 2 parameter estimates, with the levels of all the other moderators set at the sample average. The slight discrepancy between the predicted values for the categories of the moderators (e.g., monetary costs: no = .211; yes = .276, Δ = .065) and the corresponding regression coefficient (b = .069 > .065) arises because the parameter estimates are based on Fisher's z-scores, whereas the predicted values are expressed as correlation coefficients. The simple mean correlations are obtained using a random-effect model with restricted maximum likelihood as a between-effect size variance estimator. Predicted values and simple mean correlations are not provided for the continuous moderators, because their effect is linear (see Figure 3).In Table A3 (Web Appendix G), we also rule out the effects of four alternative moderators: ( 9) whether the target behavior entails environmental benefits, (10) whether the target behavior has social and physical consequences for the individual, and (11) whether the norm features a promotion or prevention frame. None of these alternative variables is significant.Finally, Table A4 (Web Appendix G) presents the results when we include additional country controls based on (12) country-level gross domestic product and population density, as well as (13) Hofstede's (vs. Inglehart's) cultural dimensions. The results again remain similar to those from the main models. Further, to account for a potential cultural invariance bias, we control for (14) the language spoken in the country of the data collection and (15) whether the study was conducted in an Asian country; (16) we also model the nesting of articles within countries. The stable regression coefficients suggest that cultural invariance does not affect the results. Across all 16 alternative models, the magnitudes, signs, and significance of the parameter estimates are consistent and aligned with our main findings, which strengthens our conclusions and affirms the robustness of the effects obtained in the main model. SummaryBy meta-analyzing the extant empirical evidence, our research provides new evidence for the effects of social norms on consumer behavior. On average, social norms significantly influence behavior and their effect (  r¯   = .254) is small to medium in size ([28]). Importantly, several moderators can explain substantial variation among these effects. Table 4 summarizes our findings. We next detail the theoretical and practical implications of our study.GraphTable 4. Hypotheses and Empirical Questions. Moderator and Predictions (Hypotheses) or Competing Explanations (Empirical Questions)Reactance ExplanationEvidenceTarget Behavior CharacteristicsSocial approval (vs. disapproval) (H1): Social norms for socially disapproved behaviors are more effective than those pertaining to approved behaviors.Expectations of freedom are lower for socially disapproved vs. socially approved behaviors×Existing (vs. new) (H2): Social norms are more effective when targeting existing (vs. new) behaviors.Freedom threat is lower for existing than for new behaviors×Hedonic (vs. utilitarian) (Empirical question): Greater desire or more guilt for hedonic benefits may either increase or decrease freedom threat from social normsN.A.n.s.Benefits to other people (H3): The presence of benefits to other people of a behavior enhances the effect of social norms on that behavior.Expectations of freedom are lower for behaviors that are beneficial to others✓Public behavior (Empirical question): Freedom threat from social norms may be lower for private behaviors due to a lack of scrutiny from others, or it may be lower for public behaviors due to concerns about perceptions from others.N.A.n.s.Communication FactorsNorm formulation (H4): Descriptive social norms have a stronger impact on behaviors than injunctive norms.Descriptive norms imply less freedom threat than injunctive norms✓Organization-benefiting (H5): Organization-benefiting social norms are more effective than social norms that do not mention organizations.Organization-benefiting social norms imply less freedom threat than social norms that do not benefit organizations✓Close group members (H6): Social norms referring to a close group member are more effective than social norms referring to abstract or distant groups.Close group members imply less freedom threat than abstract or distant groups✓Authority figures (H7): Social norms referring to authority figures are less effective than social norms that do not refer to them.Authority figures imply more freedom threat×Explicit sanctions (H8): Social norms that specify potential sanctions are less effective than social norms that do not make these consequences explicit.Explicit sanctions and rewards imply more freedom threat×Explicit rewards (H9): Social norms that specify potential rewards are less effective than social norms that do not make these consequences explicit.✓Consumer Costs Effort, monetary costs, and temporal costs (Empirical questions): The presence of costs barriers arouses reactance, which either makes the behaviors more attractive, or dissuades consumers from attempting the behaviors.Costs create barriers to free choice and increase reactanceMonetary costs lift social norm effect; n.s. for effort and temporal costsEnvironmental FactorsTraditional–Secular-rational (H10): The effect of social norms on behavior is stronger in cultures closer to the traditional (vs. secular-rational) pole.Expectations of freedom are lower in cultures toward the traditional pole and toward the survival pole×Survival–Self-expression (H11): The effect of social norms on behavior is stronger in cultures near the survival pole versus the self-expression pole.×Time (Empirical question): Evolutionary forces driving social norms effectiveness should be stable over time but fluctuations of conformism over time is observed.N.A.n.s.Moral freedom (H12): The effect of social norms on behavior is stronger in countries lacking moral freedom.Expectations of freedom are lower in low moral freedom countries✓InteractionsTraditional–Secular-rational × Approved behavior (H13): The stronger effect of social norms on behaviors in traditional cultures is driven by socially disapproved (vs. socially approved) behaviors.Expectations of freedom for socially disapproved behaviors are especially low in traditional and survival cultures✓Survival–Self-expression × Approved behavior (H14): The effect of social norms on behaviors in survival cultures is especially strong for socially disapproved (vs. approved) behaviors.✓Time × Approved behavior (H15): There is a more positive impact of social norms for socially disapproved than for socially approved behaviors over time.Expectations of freedom for socially disapproved behaviors decrease over time✓Existing (vs. new) × Approved behavior (Exploratory interaction)N.A.Social norms are effective for existing, approved behaviorNorm formulation × Approved behavior (Exploratory interaction)N.A.Descriptive norms are effective for disapproved behavior 8 Notes: N.A. = not applicable; n.s. = not significant; x = not supported. Theoretical ImplicationsOur results go beyond previous meta-analyses by uncovering previously overlooked boundary conditions of the effects of social norms on consumer behavior using reactance theory as our theoretical lens. While our meta-analysis provides important insights about whether and how social norms can influence behavior, any meta-analysis is limited to the factors included in available primary studies. Those gaps also open avenues for future research, as discussed next. The Role of Social Approval/Disapproval on Social Norms Key insightsWe reveal that the effects of social norms on behavior differ systematically for behaviors that are socially approved versus disapproved in the presence of certain environmental factors. We find that the effects of social norms on socially disapproved behaviors have increased over time and are particularly strong in survival (vs. secular-rational) and traditional (vs. self-expression) cultures. In contrast, the effects of social norms on socially approved behaviors are more stable across cultures and time. Future researchThe critical difference in social norm effectiveness for regulating socially approved versus disapproved behaviors establishes the need to investigate drivers of this difference. In the process of adopting socially approved behaviors, consumers might glean benefits beyond the direct consequences of complying with norms. For example, does compliance promote positive emotions, due to an enhanced sense of belonging, acceptance, or well-being, independent of the benefits of adopting the behavior? And do consumers suffer negative emotions (e.g., guilt) when they engage in socially disapproved behaviors, and do social norms reinforce these emotions? Another issue is how legal mandates affect behaviors, both when they contradict social norms of disapproved behaviors (e.g., using cell phones while driving) and when they reinforce approved behaviors (e.g., driving below the speed limit). Implications for Social Influence Literature Key insightsWe shed new light on several unexplored moderators of the effects of social norms on consumer behavior and thereby contribute to the literature on social influence ([24]; [109]; [110]). For example, social norms have stronger effects on behaviors that benefit others, but they are weaker for already existing socially disapproved behaviors. Social norms are equally effective for private and public behaviors, for hedonic and utilitarian behaviors, for behaviors requiring much versus little effort, and for behaviors requiring long-term versus temporary commitment. Importantly, monetary costs do not deter consumers; on the contrary, they make them more likely to comply with social norms regulating costly behaviors, in line with the reactance theory explanation that barriers enhance behaviors' desirability. Future researchThe intriguing finding that private behaviors are just as likely to be influenced by social norms as public ones suggests new research avenues. Perhaps consumers overestimate the extent to which they are monitored by others because of the spotlight effect ([37]). If so, social norms could be effective in nonsocial settings, where they traditionally have been perceived as less influential. Further, finding that social norms are equally effective for hedonic and utilitarian behaviors highlights the need to clarify the roles of desire and guilt as underlying processes. Teasing out these effects could also help enhance communication strategies (e.g., downplaying or emphasizing desire and guilt). Yet, the lack of effect of some behavioral characteristics might also be due to selection; for example, researchers may be inclined study situations in which they expect social norms to matter. Thus, testing the effects of social norms in situations where they a priori seem less relevant is insightful. Studying the effectiveness of social norms when behavioral autonomy is impaired (e.g., employees making decisions on behalf of firms) is also intriguing. Finally, research could address behaviors consumers adopt to distinguish themselves from the group (e.g., to enhance authenticity; [82]) and the leadership behaviors they display to encourage others to go against current social norms (e.g., boycotting, brand sabotaging; [55]). Implications for Marketing Communications Literature Key insightsWe contribute to marketing communication research by identifying communication strategies that enhance the effectiveness of social norms in several ways. First, several commonly used social norm communication factors appear to be ineffective, such as referring to authority figures or specifying explicit sanctions; the same is true for rewards, which even seem to hinder social norms' effectiveness. Second, formulating norms as descriptive (vs. injunctive), organization-benefiting, and/or referring to close others enhance the effect of social norms on behaviors. Further, our exploratory results suggest that injunctive norms are especially weak when targeting disapproved behaviors, which is consistent with reactance theory. To discourage socially disapproved behaviors, injunctive norms tend to be proscriptive (""you should not"") rather than prescriptive (""you should""). Proscription represents a greater threat to freedom than prescription, so it prompts more reactance to injunctive norms ([11]). Instead, socially disapproved behaviors can be better curbed by descriptive norms.The finding that organization-benefiting social norms enhance compliance contributes to the emerging stream of research that examines how situational factors influence the effectiveness of social norms ([ 1]; [38]). While which type of organization benefits should matter as well, on a broader level, our results suggest that organization-benefiting social norms, because they are situation-specific, enhance compliance. Future researchMore research is needed for boundary conditions for the effectiveness of different communication strategies. For example, there are multiple avenues for future research on the impacts of sanctions and rewards. First, reward size and reward type (e.g., material vs. nonmaterial; [73]) might be important. Second, rewards or sanctions may not exert effects past a ceiling level of the impact of social norms when many consumers are ""uninfluenceable."" Third, research should identify ways to account for consumer needs and boost perceptions of the benefits of following and the costs of not following social norms without triggering reactance.Boundary conditions under which injunctive norms are more effective than descriptive ones (e.g., the level of ambiguity, the source of the norm: ""Why fight city hall?"") should be addressed. Further, because norms are group-specific, research could explore how group exclusivity should be communicated. Social norms linked to homogeneous exclusive groups (""Harvard students recycle"") might evoke less reactance because they distinguish group members from outsiders while also satisfying the need to belong ([13]). The effect of group exclusivity on behavior might be stronger if the communication contrasts ingroup versus outgroup norms (""Harvard students, unlike [vs. similar to] MIT students, recycle""). Relatedly, communicating the size of the group that shares social norms could also enhance social norms' influence ([46]).Finally, most of the primary studies in our meta-analysis predated social media, but our results suggest that social media may have disrupted the influence of social norms on behavior. Online interactions enable consumers to develop friendships with people they have never met in real life, so further research might investigate how social norms evolve on social media. For example, how does the immediate, often permanent feedback available through social media shape the effects of social norms on online behaviors? Consumers might not just comply with social norms but also try to become agents of influence by spreading the norm further on social media. Future research should investigate which factors facilitate such efforts ([68]), and the role of social norms in interpersonal relationships in general. Finally, future research should address the interaction between social norms and marketing-mix instruments, particularly promotion. Implications for Social Reactance Literature Key insightsOur results inform discussions on reactance ([18]; [92]) and the role of intrinsic versus extrinsic behavioral motivations ([31]). The nonsignificant effect of public behavior implies that consumers follow social norms even if their behavior is not observable to others. Further, while effort may impede social norms' effectiveness, the monetary costs of behavior enhance it, suggesting that consumer effort and temporal costs are a greater barrier to compliance than monetary costs. These results, together with the finding that explicit sanctions and rewards do not help, suggest that complying with social norms may be a more intrinsically motivated activity than previously believed. Future researchFollow-up research should investigate other barriers that may incite reactance and ways to circumvent them. Noting our finding that monetary costs enhance social norm compliance, we call for research that specifies the boundary conditions of this effect. More research is needed to clarify what makes costly behavior attractive to consumers. For example, costly behaviors may help signal status ([42]).Research should address the relative explanatory power of alternative mechanisms parallel to reactance that might influence the effect of social norms on behaviors. For example, the literature identifies other potential mechanisms, such as self-efficacy ([ 5]), sense of belonging ([ 6]), internalization ([97]), and justification of behaviors ([39]). Contributions to PracticeThe findings offer insights for marketers and public policy makers by identifying effective (and some commonly used but ineffective) strategies for enhancing the impact of social norms on consumer behavior. In contrast to conventional wisdom ([ 7]), our results suggest that the influence of social norms can prompt private acceptance. Thus, marketers and policy makers can leverage social norms to encourage both private and public behaviors. Communication StrategiesThe content of the communication should feature descriptive rather than injunctive forms of social norms (i.e., describe what [most] people actually do rather than what they should do[11]). Further, we recommend that marketers should avoid specifying explicit sanctions and rewards associated with social norms. Instead, strategies that highlight benefits to others or consumer freedom (e.g., a communication with a postscript ""The choice is yours""; [12]) may mitigate reactance and thus be more effective for inducing the target behavior.Practitioners might worry about highlighting a specific organization when communicating about social norms, but our results suggest that referring to a specific firm, governmental body, or nongovernmental organization can make communications about social norms more influential. Social norms are also more powerful when they cite people who are perceived as close to the target consumers. Thus, practitioners should target social norm communication toward nano-influencers on social media, with their smaller, more engaged audiences. In contrast, our results indicate that references to authority figures when using social norms do not affect consumer behavior.In communicating norms, marketers can acknowledge the monetary costs associated with the targeted behaviors. Although monetary costs are a financial barrier, they seem to also increase the desirability of the behavior, so social norms can be particularly effective for promoting costly behaviors like donations or buying (more expensive) organic food. Further, social norms are equally effective irrespective of required effort, and the time investment in complying. Cultural Differences Between Countries Socially approved versus disapproved behaviorsThe impact of social norms on socially disapproved behaviors varies significantly depending on the country of implementation, but it is more stable for socially approved behaviors. Social norms have stronger influences on socially approved than disapproved behaviors in secular-rational and self-expression cultures. These findings have important public health implications when group behavior is essential. To encourage mask wearing in most Western countries, for example, public officials should communicate that wearing a mask is a socially approved behavior that close others adopt. In most survival countries, the communications should highlight that not wearing a mask is socially disapproved. Cultural profilesTo specify the net effect of culture at the country level, we estimate the impact of social norms in countries with different cultural profiles. We calculate the predicted effect sizes for socially disapproved and approved behaviors in Figure 4 for eight countries that represent different regions of the world. We use descriptive norms as a base category and country scores on Inglehart dimensions from the latest wave of the World Values Survey.Graph: Figure 4. Predicted current effect sizes (Pearson's product moment correlations) of descriptive norms for socially approved and disapproved behaviors, by global regions.In Scandinavian countries such as Denmark (eight effect sizes), which score high on secular-rational values (85th percentile) and high on self-expression values (98th percentile), the mean effect size for socially approved behaviors is  r¯   = .365 (CI95% = [.282,.442]), whereas it is not significant for disapproved behaviors (  r¯   = .176, CI95% = [−.077,.407]). Campaigns using social norms thus may be effective for encouraging healthy eating, for example, but are likely not the best choice to curb excess drinking in such countries.In Western European (e.g., France) and Commonwealth (e.g., Canada, United Kingdom) countries with medium to high scores on both dimensions (but lower than Scandinavia), social norms are equally effective regardless of the social approval of behavior. For example, in Australia (20 effect sizes), a country with average secular-rational values (55th percentile) and high self-expression values (96th percentile), the impact of social norms on approved behaviors (  r¯   = .344, CI95% = [.273,.411]) is the same as the impact on disapproved behaviors (  r¯   = .355, CI95% = [.249,.452], Wald-type test = −.175, p = .431).In contrast, in the United States (104 effect sizes), which is more traditional (43rd percentile for secular-rational) and has high self-expression values (85th percentile), the effect of social norms on socially disapproved behaviors (  r¯   = .460, CI95% = [.374,.538]) is stronger than their impact on approved behaviors (  r¯   = .330, CI95% = [.262,.395], Wald-type test = −2.406, p = .008).Interestingly, we find the same pattern in Southern Europe (e.g., Italy) and China, even though these areas represent relatively high secular-rational (65th percentile) and low self-expression values (28th percentile). In these regions, social norms' effectiveness is greater for disapproved behaviors (  r¯   = .530, CI95% = [.397,.642]) than for approved behaviors (  r¯   = .319, CI95% = [.213,.417], Wald-type test = −2.611, p = .005).Finally, in countries with strong traditional and survival values, such as most African and Muslim-majority countries, social norms' impact on disapproved behaviors is much stronger than on approved behaviors. Consider Ethiopia (25th and 30th percentiles), where the mean effect size for disapproved behaviors (  r¯   = .660, CI95% = [.455,.799]) is much greater than that for approved behaviors (  r¯   = .297, CI95% = [.188,.398]). In these countries, social norms are especially effective for discouraging disapproved behaviors. ConclusionThis extensive meta-analysis shows that social norms significantly impact behavior and uncovers novel contingencies of this effect. We hope the proposed research agenda, which reflects our comprehensive investigation of the extant literature, sparks additional research in the fascinating ways in which social norms shape (or do not shape) consumer behavior.  "
19,"The Pet Exposure Effect: Exploring the Differential Impact of Dogs Versus Cats on Consumer Mindsets Despite the ubiquity of pets in consumers' lives, scant research has examined how exposure to them (e.g., recalling past interactions with dogs and cats, viewing ads featuring a dog or a cat) influences consumer behavior. The authors demonstrate that exposure to dogs (cats) reminds consumers of the stereotypical temperaments and behaviors of the pet species, which activates a promotion- (prevention-) focused motivational mindset among consumers. Using secondary data, Study 1 shows that people in states with a higher percentage of dog (cat) owners Google more promotion- (prevention-) focused words and report a higher COVID-19 transmission rate. Using multiple products, Studies 2 and 3 demonstrate that these regulatory mindsets, when activated by pet exposure, carry over to influence downstream consumer judgments, purchase intentions, and behaviors, even in pet-unrelated consumption contexts. Study 4 shows that pet stereotypicality moderates the proposed effect such that the relationship between pet exposure and regulatory orientations persists to the extent consumers are reminded of the stereotypical temperaments and behaviors of the pet species. Studies 5–7 examine the role of regulatory fit and evince that exposure to dogs (cats) leads to more favorable responses toward advertising messages featuring promotion- (prevention-) focused appeals.Keywords: pets; regulatory orientation; advertising; COVID-19Pets are prevalent and play important roles in consumers' daily lives ([ 2]; Cavanaugh, Leonard, and Scammon 2008; Hirschman 1994; Holbrook and Woodside 2008; Serpell and Paul 2011). According to the survey of the American Pet Products Association ([ 4]), 68% of U.S. households, or 84.6 million homes, own a pet. Dogs and cats are the most popular pets, with 48% of U.S. households (60 million homes) owning at least one dog and 37% of U.S. households (47 million homes) owning at least one cat. Pet adoption rates have climbed significantly, with about one in five households having acquired a dog or cat since the outbreak of the COVID-19 pandemic (American Society for the Prevention of Cruelty to Animals [ASPCA] 2021). Pets also frequently appear in popular culture, mass media, and marketing communications. For example, Target uses a dog as its brand mascot, Microsoft features dogs in its 2020 holiday commercial to inspire people to find joy, and Wells Fargo uses a cat in its commercial to advertise its suspicious card activity alert services.Despite the significance of pets in people's lives and in mass media, popular culture, and marketing communications, scant research has examined how pets may influence consumers' judgments, decisions, and behaviors. Existing research on pet–human relationships largely revolves around examining how owning a pet influences the owner's pet-related judgments and behaviors. For example, attesting to a strong tie between owners and their pets ([ 2]; Cavanaugh, Leonard, and Scammon 2008), this stream of literature suggests that pets provide not only companionship but also a sense of safety and belongingness for their owners ([79]). Pet owners have significantly greater physical and psychological well-being than non-pet owners ([ 2]) and are more likely to endorse causes protecting animal rights (Kidd, Kidd, and Zasloff 1995).Research that examines how pets may influence consumer behavior beyond the immediate context of pet ownership is lacking, however. Such knowledge would provide novel and important insights to marketers and allow them to develop marketing strategies based on pet exposure situations. For example, marketers might choose to recommend more fitting products or services or craft appropriate communication messages to effectively target consumers depending on the type of pets to which they are exposed. Consider the following scenario: A newly opened massage center is pondering the language to use in direct mail to potential customers and whether to focus on how its therapies help reduce fatigue and stress or how its therapies promote metabolism and energy levels. Which strategy might be more effective if the company features a cat (dog) figure in its advertisement? This article provides a theory-based answer to this question.In our research, we focus on the effects of exposure to pets (e.g., recalling consumers' past interactions with dogs or cats, viewing ads featuring a dog or a cat) and examine how such experience influences consumers' judgments and decision making through the lens of regulatory focus theory. Drawing from literature on human–animal relationships, research on regulatory orientation, and work on mindset, we suggest that exposure to pets will remind consumers of the stereotypical personality traits, temperaments, and behaviors of the pets and thus will evoke different regulatory mindsets among consumers. Specifically, we predict and find that exposure to dogs fosters a more promotion-focused motivational mindset whereas exposure to cats activates a more prevention-focused motivational mindset. We further identify pet stereotypicality as a moderator for our findings, such that our results on the relationship between pet exposure and regulatory orientations persist only when consumers are reminded of the stereotypical temperaments and behaviors of the pet species, and that our main proposed effects dissipate when consumers are reminded of information inconsistent with the pet stereotypes. Moreover, we show that these regulatory mindsets, when activated by pet exposure, carry over to influence downstream consumer judgments, purchase intentions, and behaviors, even in pet-unrelated consumption contexts. Theoretical Framework Regulatory Orientation and Its Social OriginRegulatory focus theory suggests that consumer judgments, decisions, and behaviors are motivated by two regulatory orientations: promotion and prevention focus (Higgins 1997; Lee, Aaker, and Gardner 2000; Pham and Avnet 2004; Wang and Lee 2006). A promotion focus in self-regulation reflects consumers' motivations to attain growth and nurturance in an effort to align their actual selves with their ideal selves (achieving accomplishments and fulfilling aspirations; Higgins 1987, [28]). Promotion-focused consumers are characterized by an eagerness regulatory system during behavioral regulation (Crowe and Higgins 1997; Lee, Keller, and Sternthal 2010; Pham and Chang 2010; Wang and Lee 2006). For example, they are sensitive to the presence or absence of positive outcomes (gains and successes; Higgins 1997), concerned about reducing errors of omission (Croe and Higgins 1997), and more risk seeking when processing information and rendering decisions (Zhou and Pham 2004). By contrast, self-regulation with a prevention focus reflects consumers' motivations to attain safety and security in an attempt to bring their actual selves into alignment with their ""ought"" selves (fulfilling duties and obligations; Higgins 1987, [28]). Thus, prevention-focused consumers are more vigilant and cautious during behavior regulation (Crowe and Higgins 1997; Lee, Keller, and Sternthal 2010; Pham and Chang 2010; Wang and Lee 2006). In this system, consumers are sensitive to the presence or absence of negative outcomes (losses and failures; Higgins 1997), concerned about reducing errors of commission (Crowe and Higgins 1997), and more risk averse (Zhou and Pham 2004) when processing information and making decisions.More germane to our research is the finding that social influences play a pivotal role in shaping people's regulatory orientations. For example, interactions with childhood caretakers and parents' parenting style can influence the formulation of consumers' regulatory orientations during the socialization process (Crowe and Higgins 1997; Higgins 1996). Social exclusion causes a shift toward prevention motivation (Park and Baumeister 2015), whereas making choices for other people instigates a promotion focus ([59]). In addition, distinct social relationships can activate alternative regulatory orientations, such that reminders of friends activate a promotion focus while reminders of family members engender a prevention focus ([22]). Similarly, positive role models induce a promotion focus, whereas negative role models instigate a prevention focus (Lockwood, Jordan, and Kunda 2002).Given pets' prevalence in consumers' daily lives, we posit that consumers' interactions with pets are also an important part of socialization that can influence their regulatory orientations. These socialization activities can involve direct or indirect interactions with pets (e.g., observing pets' interactions with other people). Indeed, research on human–animal relationships evinces that pets play an important part in people's socialization process, influencing the development of various cognitive and social abilities (e.g., worldviews, empathy; [ 2]; Myers 1999; Purewal et al. 2017). Exposure to Pets and Regulatory OrientationsAs we have mentioned, dogs and cats are two primary types of pets ([ 4]). Despite within-species breed differences, research on animal behavior has identified systematic cross-species differences between domesticated dogs and cats ([11], [12]; Jardim-Messeder et al. 2017). This stream of research suggests that a promotion-oriented eagerness system better captures dogs' temperaments and behavioral characteristics, whereas a prevention-focused cautious system better describes cats' temperaments and behavioral characteristics. For example, on a temperament level, dogs tend to be open and expressive, while cats are elusive and cautious ([11], [12]; Potter and Mills 2015). Consistent with the promotion orientation's receptivity to change (Boldero and Higgins 2011; [43]), dogs (vs. cats) cope better with and adapt quicker to changes in the environment, such as moving into a new house or having a new person in the household ([11], [12]; Langenfeld 2020). In line with the prevention orientation's preference for the status quo ([10]; Chernev 2004), cats (vs. dogs) appear more concerned with the protection their owners provide and the consistency and stability of their social and physical surroundings ([11], [12]). Similarly, consistent with the eagerness prediction of a promotion regulatory system ([15]), dogs are more responsive to rewards (e.g., food, praise, petting) than cats and thus are easier to train ([49]).When interacting with human beings and other pets, dogs are more eager to please their owners and socialize with other dogs, whereas cats are more cautious, suspicious, boundary setting, and anxious when surrounded by unfamiliar people or other cats ([11], [12]; Potter and Mills 2015). Indeed, research has shown that dogs are more attentive and responsive to human's social cues (e.g., gestures) than cats ([49]; Wynne, Udell, and Lord 2008). Dogs' eagerness can be exemplified by the spike in oxytocin (a hormone mammals release when they feel love or affection for someone) when their owners are around ([53]). A study conducted by scientists at BBC shows that dogs produce five times more oxytocin than cats upon seeing their owners ([21]).We further predict that through repeated socialization episodes with pets (through either direct or indirect interaction with pets), the traits and motivational characteristics of dogs (cats) are gradually associated with a promotion-focused (prevention-focused) eagerness (cautiousness) system. These learned associations are brought to mind and thus accessible when consumers interact with pets or encounter stimuli featuring pets (e.g., ads) in their daily lives. To confirm the associations of dogs and cats with promotion and prevention orientations, we conducted a pilot study, which found that participants indeed associated promotion-focused words with dogs and prevention-focused words with cats (for details of the pilot study, see Web Appendix A). HypothesesDrawing from research on motivational mindset, which we review next, we further predict that exposure to dogs (cats) or stimuli featuring them (e.g., ads) will remind consumers of the temperaments and behaviors of the dogs (cats), which will in turn activate a promotion-focused (prevention-focused) mindset among consumers and guide their subsequent judgment and decision making. Mindset reflects ""the activation and use of a procedure that is stored in memory as part of declarative knowledge"" ([76], p. 110). That is, engaging in a particular operation when pursuing a goal in a prior task may give rise to a mindset (e.g., a promotion-focused mindset) that remains accessible in consumers' memory and, in turn, guides their pursuit of a different goal in a subsequent, unrelated context.A growing body of literature has found considerable evidence of the role of mindset across a wide range of information-processing activities, from comprehension, to judgment, to decision making (Ma and Roese 2014; Wyer and Xu 2010; Xu et al. 2020). In some situations, mindsets involve cognitive procedures induced by engaging in a prior task that spills over to influence a subsequent, unrelated context. For example, Xu et al. (2020) show that managers during election years are more likely to adopt a comparative mindset due to the omnipresence of comparative political advertisements. Accordingly, they spend more money on their managerial decisions because the comparative mindset accentuates ""which option to spend money on"" and forgoes the ""whether or not to spend"" consideration. More germane to our theorizing, mindsets may also be based on motivation ([76]), such that the motivational mindset induced by pursuing a goal in a prior task will guide consumers' subsequent behavior in an unrelated context (e.g., pursuit of a different goal). For example, Wyer and Xu (2010) assert that the promotion (prevention) regulatory mindset can be induced procedurally by, for example, making salient participants' desire to achieve their ideal (ought) self. When activated, the promotion (prevention) regulatory mindset produces a cross-domain effect, making consumers, for example, more likely to approach positive (avoid negative) consequences in their decision making.Drawing on this stream of literature, we posit that exposure to pets (e.g., recalling an interaction with a pet, viewing ads featuring a pet as the spokescharacter) in a prior task may render different regulatory mindsets salient. Specifically, because the stereotypical personality traits, temperaments, and behaviors of dogs (cats) brought to mind by the pet exposure are associated with eagerness (cautiousness) strategies commonly employed by a promotion (prevention) orientation, consumers' different regulatory orientations (promotion vs. prevention) will be activated. When evoked, these motivational regulatory mindsets will carry over to influence consumers' subsequent, unrelated judgments and decision making, rendering them more eager (cautious) during behavioral regulation, leading them to pursue promotion- (prevention-) focused goals such as growth and advancement (safety and stability), and making them more risk seeking (more risk averse) in decision making. Thus,H1: Pet exposure activates different regulatory motivational mindsets among consumers, such that (a) exposure to dogs or dog-featuring stimuli (vs. cats or cat-featuring stimuli) activates a more promotion-focused mindset and (b) exposure to cats or cat-featuring (vs. dogs or dog-featuring) stimuli activates a more prevention-focused mindset.A key premise of our theorizing that exposure to pets activates different regulatory mindsets is that such exposure will remind consumers of the stereotypical temperaments and behavioral characteristics of dogs (cats), giving rise to a promotion-focused (prevention-focused) mindset. In other words, through repeated socialization, consumers have developed preestablished mental connections between dogs' (cats') typical temperaments and behaviors and the promotion (prevention) focus, and exposure to pets or pet-featuring stimuli can render these stereotypical associations accessible, thus activating the corresponding regulatory-focus mindset among consumers. Prior research has shown that established mental associations are likely to be temporarily weakened, nullified, or even reversed when presented with information inconsistent with the original associations. For example, Gorn, Jiang, and Johar (2008) reversed the association between baby-faceness and unintentionality by presenting counterassociation information about a baby-faced person intentionally harming others. Thus, if our reasoning that mental associations between dogs (cats) and promotion- (prevention-) focused mindsets is right, our proposed effects should persist to the extent consumers are reminded of the stereotypical behaviors and temperaments of the pet species; when consumers are reminded of pet information inconsistent with the stereotypes of the species (e.g., dogs [cats] unlike a stereotypical dog [cat]), we are likely to show that our results are attenuated. More formally,H2: Pet stereotypicality moderates the effect of pet exposure on regulatory mindsets, such that the effect dissipates when consumers are exposed to pets that are inconsistent with the stereotypes of the species.We expect that the impact of pet exposure on consumers' motivational mindsets will carry over to influence downstream variables, including ad evaluation, purchase intention, and real purchasing behavior, even in pet-unrelated consumption contexts. We anticipate that the effects of pet exposure on these variables will stem from the activation of a regulatory mindset and regulatory fit. Regulatory fit occurs when the regulatory strategies individuals employ during goal pursuit are compatible with their regulatory orientations (Higgins 2000; Hong and Lee 2007); it usually results in favorable effects on downstream consumer responses, such as enhanced value of the product ([ 7]), brand attitudes ([37]), self-regulation (Hong and Lee 2007), and decision making (Zhou and Pham 2004).Therefore, in accordance with this literature, we anticipate that consumers who are exposed to dogs or dog-featuring stimuli will experience higher regulatory fit and develop more favorable product evaluations when presented with ad messages featuring promotion-focused claims. By contrast, consumers who are exposed to cats or cat-featuring stimuli will experience higher regulatory fit and develop more favorable product evaluations when presented with ad messages featuring prevention-focused claims. Thus,H3: There is an interaction between pet exposure and the regulatory focus of an ad on consumers' evaluations of the advertised product, such that (a) when exposed to ads with promotion-focused claims, exposure to dogs or dog-featuring stimuli (vs. cats or cat-featuring stimuli) leads consumers to form more favorable product evaluations, and (b) when exposed to ads with prevention-focused claims, exposure to cats or cat-featuring stimuli (vs. dogs or dog-featuring stimuli) leads consumers to form more favorable product evaluations.H4: Regulatory fit mediates the interaction between pet exposure and ads' regulatory focus proposed in H3 on product evaluations. Overview of StudiesStudies 1 and 2 provide initial evidence for our prediction by showing that long-term exposure to dogs (cats) is associated with a promotion (prevention) focus. Specifically, using secondary data gathered from the American Veterinary Medical Association, Google Trends, and Centers for Disease Control and Prevention (CDC), Study 1 finds that people in states with a higher percentage of dog (cat) owners search more promotion- (prevention-) focused words (Study 1a) and show a higher COVID-19 transmission rate (Study 1b). Study 2 shows that dog (vs. cat) owners are more likely to invest in stocks and are less likely to invest in mutual funds in financial decision making. Studies 3a–3d establish the basic effect that exposure to dogs (cats) activates a promotion- (prevention-) focused motivational mindset by employing multiple experimental manipulations of pet exposure and different measures of regulatory orientation in both pet-related and pet-unrelated contexts, including incentive-compatible choices. Study 4 explores a moderating effect for our findings, showing that our hypothesized effects will dissipate when consumers are exposed to pet information inconsistent with the stereotypes of the pet species. Study 5 examines the downstream effects of pet exposure on consumers' incentive-compatible behaviors, showing that consumers exposed to dogs (cats) bid higher for products framed with a promotion (prevention) focus. Studies 6 and 7 provide additional support for our theorizing by examining the mediation effect of regulatory fit. Table 1 in Web Appendix B provides a summary of all studies.GraphTable 1. Study 1a: Results from the Multiple Regression Model. btpPet ownership indexa.362.62.012Median household income−.15−.63.534Per capita GDP−.25−1.26.214Political orientationb−.05−.29.774 1 a Higher scores indicate more dog (vs. cat) owners.2 b 1 = Democratic, 2 = Republican.3 Notes: Dependent variable = Regulatory-focus index: high scores indicate more promotion- (vs. prevention-) focused. Study 1: Pet Ownership and Regulatory Orientations—Evidence from Google Trends and COVID-19 C...Relying on secondary data and operationalizing pet exposure as pet ownership, Study 1 aims to provide preliminary evidence for our prediction that exposure to dogs and cats is associated with different regulatory mindsets. We collected aggregated state-level data on pet owner statistics, public interest in promotion- versus prevention-oriented behaviors, and per capita COVID-19 cases during the pandemic. We expect that at the state level, having a relatively higher dog-owning (cat-owning) population will be related to more search interests in promotion-oriented (prevention-oriented) behaviors in general (Study 1a) and more per capita COVID-19 cases during the pandemic (Study 1b). Study 1a: Pet Ownership and Search Interests in Regulatory BehaviorsFor pet ownership, we obtained the latest (2016) state-level pet ownership data set (n = 49) from the U.S. Pet Ownership and Demographics Sourcebook released by the [ 1]. This data set provides the most complete data on pet population demographics, covering the 48 U.S. continental states and the District of Columbia (excluding Alaska and Hawaii). For each state, we divided the percentage of dog-owning households by the percentage of cat-owning households to obtain a pet-owning index, with a higher score indicating more dog-owning (vs. cat-owning) households in the state.To obtain a proxy for citizens' public interest in regulatory-oriented behaviors in each state, we examined the search interest scores data from Google Trends (Du, Hu, and Damangir 2015; Kozinets, Patterson, and Ashman 2017). Google is the most often-used internet search engine in the United States (accounting for 88% of the market share; Schultheiß and Lewandowski 2021), and Google Trends counts how often a particular search term is entered relative to the total search volume across various geographic regions. After a search term, period, and interested geographic area are entered, Google Trends displays how often that search term appears on Google in that geographic area and in that period relative to the total search volume on a standardized scale ranging from 0 (lowest search volume) to 100 (highest search volume). Given its viable role in monitoring public interests, Google Trends has become an increasingly used data source for research in psychology ([46]), political sciences (Mellon 2013; Weeks and Southwell 2010), and marketing (Du, Hu, and Damangir 2015; Kozinets, Patterson, and Ashman 2017).To build the state-level regulatory orientation index, we first selected ten representative promotion-focused words (i.e., ""growth,"" ""gain,"" ""achievement,"" ""aspiration,"" ""pleasure,"" ""proud,"" ""hope,"" ""earn,"" ""win,"" and ""spontaneous"") and ten representative prevention-focused words (i.e., ""privacy,"" ""safety,"" ""loss,"" ""prevention,"" ""pain,"" ""stable,"" ""saving,"" ""frugal,"" ""rules,"" and ""risky""), in line with literature on regulatory focus (Higgins 1997, [29], [30]; Scholer, Cornwell, and Higgins 2019). We then obtained search interest scores of these words on Google Trends from January 1, 2016, to December 31, 2020, across the 48 continental states plus the District of Columbia. For each state, we calculated the average search interest score for the ten promotion-focused words (α = .85) and the ten prevention-focused words (α = .74). Finally, we built a regulatory orientation index for each state by dividing the promotion search interest score by the prevention search interest score (i.e., higher numbers indicate a higher promotion focus).To demonstrate the ecological validity of our findings, we also controlled for state-level microeconomic influence (income), macroeconomic influence (gross domestic product [GDP]), and political orientations. Specifically, we included the (state-level) covariates median household income in 2016 (U.S. Bureau of the Census 2017), per capita GDP in 2016 (U.S. Bureau of Economic Analysis 2019), and political orientation based on the 2016 presidential election results (The New York Times 2017).We conducted a multiple linear regression with the pet ownership index as the independent variable, regulatory-orientation index as the dependent variable, and median household income, per capita GDP, and political orientation as covariates. Table 1 shows the results. The results reveal that our regression model was significant (F( 4, 44) = 4.86, p = .002), suggesting that the independent variables significantly explained the variance in regulatory orientation. More importantly, after controlling for the covariates, the pet ownership index (b = .36, t = 2.62, p = .012) significantly predicted the regulatory-orientation index, showing that at the state level, a relatively higher dog-owning (cat-owning) population is associated with more search interests in promotion-oriented (prevention-oriented) behaviors in general (for additional analyses, see Web Appendix C). Study 1b: Pet Ownership and Per Capita COVID-19 Cases During the PandemicStudy 1b uses the same pet-ownership data from Study 1a but focuses on per capita COVID-19 cases (CDC 2020) as a proxy for regulation-related behaviors. Considering the findings that promotion- (prevention-) focused people are more risk seeking (risk averse; Zhou and Pham 2004), we expect that dog owners will have an increased probability to engage in promotion-focused, relatively risky behaviors that may result in COVID-19 transmission (e.g., more willing to dine in restaurants, letting their guard down when following social distancing); by contrast, cat owners will have an increased probability to be more cautious and engage in less risky, prevention behaviors (e.g., behaving extra cautiously, practicing social distancing, wearing face masks). Accordingly, we predict that states with more dog (cat) owners will report a higher (lower) number of per capita COVID-19 cases.To obtain a state-level proxy for regulatory-oriented behavior, we examined each state's COVID-19 cases per 100,000 people reported to the CDC from January 21, 2020 (the earliest available date) to November 1, 2020 (the date Study 1b was conducted). As of November 1, 2020, the 48 continental states and the District of Columbia had reported 2,819 COVID-19 cases per 100,000 people, on average, with Vermont being the lowest (348 per 100,000) and North Dakota the highest ( 6,054 per 100,000).We performed a linear regression on COVID-19 cases (per 100,000), with the pet ownership index as the independent variable, and controlled for the same covariates as in Study 1a. The results reveal that our regression model was significant (F( 4, 44) = 8.93, p <.001), suggesting that the independent variables significantly explained the variance in COVID-19 cases. As Table 2 shows, the pet ownership index was significantly related to an increase in COVID-19 cases per 100,000 people (b = .38, t = 3.15, p = .003), suggesting that, at the state level, a relatively higher dog-owning (cat-owning) population was associated with more reported per capita COVID-19 cases. Consistent with this finding, our ancillary analyses (for detailed analyses and results, see Web Appendix D) also suggest that the pet ownership index (higher scores indicating more dog [vs. cat] owners) significantly increased search interests (per Google Trends during the same period as the COVID-19 data) in promotion-focused behaviors, such as dining in, but significantly reduced search interest in prevention-focused behaviors, such as face mask and social distancing.GraphTable 2. Study 1b: Results from the Multiple Regression Model. btpPet ownership indexa.383.15.003Median household income.09.56.579Per capita GDP.13.97.336Political orientationb.533.71.001 4 a Higher scores indicate more dog (vs. cat) owners.5 b 1 = Democratic, 2 = Republican.6 Notes: Dependent variable = number of COVID-19 cases per 100,000 people. DiscussionUsing aggregated state-level data across different data sources, Study 1 provides support for our prediction of a significant association between long-term pet exposure and people's regulatory orientations, such that dog (cat) exposure is associated with a promotion (prevention) focus. Specifically, we find that, at the state level, a relatively higher dog-owning (cat-owning) population is associated with more search interests in promotion- (prevention-) focused behaviors in general (Study 1a) and more reported per capita COVID-19 cases (Study 1b). In subsequent studies, we use individual-level data to provide additional support for our prediction. Study 2: Pet Ownership and the Choice Between Stocks and Mutual FundsStudy 2 also operationalizes pet exposure as pet ownership and examines whether consumers' pet-owning situations are associated with different regulatory mindsets. Unlike Study 1, which used aggregate, state-level data, Study 2 relies on individual-level pet ownership data. In addition, we used an established measure of regulatory orientation ([65]), which involved participants in a financial decision-making task choosing between two investment options: stock (a proxy for promotion focus) and mutual fund (a proxy for prevention focus).We recruited 145 pet owners from Amazon Mechanical Turk (MTurk) (Mage = 35.3 years; 53.1% female; 53% dog owners and 47% cat owners). We recruited only participants who own dogs only or cats only; owners of both dogs and cats were excluded (for the screening criteria, see Web Appendix E). We asked participants to partake in a financial decision-making task, which served as our measure of regulatory orientation (Zhou and Pham 2004). We first gave them basic definitions of stocks and mutual funds and told them that stock investments were typically associated with a higher level of risk, whereas mutual fund investments were typically associated with a lower level of risk and therefore more conservative. Next, we asked participants to imagine that they had $2,000 and were considering investing in two assets: a stock and a mutual fund. Afterward, we asked them to indicate which asset they would invest in if they could choose only one asset and then to indicate the amount of money they would invest. Given that prior research has shown that the activation of a promotion- (vs. prevention-) focused mindset entails greater risk taking (vs. risk aversion; Zhou and Pham 2004), we expected that dog (vs. cat) owners would be more willing to take risks in their financial investments and choose the stock option. After the financial decision-making task, participants completed measures of their mood using PANAS (Positive Affect Negative Affect Schedule; Watson, Clark, and Tellegen 1988) and a few demographic measures, including their age, gender, ethnicity, and income level (Web Appendix F presents the measures used). ResultsA logistic regression showed a significant effect of pet ownership (cat owners = 0, dog owners = 1) on investment choice, such that dog owners (36.4%) were more likely to choose to invest in the riskier stock option than cat owners (20.6%; b = .79, SE = .38, χ2 = 4.73, p = .039, Exp (B) = 2.20). Similarly, a one-way analysis of variance (ANOVA) revealed a significant effect of pet ownership on money allocations. As we expected, dog owners allocated more money to the stock option (Mdog = $796.10, SD = $524.45) than cat owners (Mcat = $603.69, SD = $474.90; F( 1, 143) = 5.31, p = .023,  ηp2   = .04).To rule out possible alternative explanations that participants' mood, gender, age, ethnicity, or income level accounted for our findings, we controlled for these variables simultaneously. Our results for both choice (χ2 = 5.47, p = .019) and money allocations (F = 5.74, p = .018) remained significant even after we controlled for these covariates. DiscussionStudy 2's findings show that dog (vs. cat) owners were more likely to take risks in their financial decisions, showing more preference for stock investment. Importantly, incorporating the demographic variables age, ethnicity, gender, and income as covariates did not change the results. Taken together, using pet ownership as an operationalization, Studies 1 and 2 provide initial support for our prediction that exposure to pets is associated with different regulatory mindsets. However, despite the extra steps taken, such as controlling for demographic variables (e.g., income) to rule out alternative explanations, Studies 1 and 2 were correlational in nature. To provide stronger causal evidence for our prediction, in the subsequent studies, we manipulate exposure to pets in various ways. Study 3: Effects of Pet Exposure on Consumers' Regulatory OrientationsThe purpose of Study 3 is twofold. First, the study aims to establish causality between pet exposure and the formation of regulatory motivation mindsets by using multiple manipulations of pet exposure. Second, the study operationalizes regulatory orientations in various ways and across different (pet-related and pet-unrelated) contexts.In a pet-related domain, Study 3a shows that participants exposed to dogs (vs. cats) will be more likely to prefer a pet toothpaste ad with promotion- (vs. prevention-) focused benefits. Studies 3b–3d test the effect in pet-unrelated domains. Consistent with prior research showing that the activation of a promotion- (prevention) focused mindset entails greater risk taking (risk aversion) (Zhou and Pham 2004), participants who are exposed to dogs (vs. cats) will be more willing to take risks to participate in a lottery (an incentive-compatible behavior; Study 3b) and in their financial investment decisions (Study 3c). In a health product context, Study 3d demonstrates that participants who are exposed to dogs (vs. cats) will be more likely to prefer a vitamin product with promotion- (vs. prevention-) focused benefits.In this and subsequent studies, participants completed mood measures and demographic measures, including pet ownership, gender, age, income level, and ethnicity, at the end of study. Incorporating these variables as covariates did not influence our results (for the exact measures used, see Web Appendix F; for results pertaining to the impact of pet ownership across studies, see Web Appendix G), and thus we do not discuss them further. Study 3aIn Study 3a, we examine our prediction in the context of pet-related decisions: pet toothpaste choice. One hundred eighty-three participants recruited from MTurk completed the study, which featured a two-cell (pet exposure: dog vs. cat) between-subjects design, for a small financial compensation (Mage = 37.4 years; 54.6% female). To manipulate pet exposure (dog vs. cat), under the cover story that we wanted to examine consumers' day-to-day experiences, participants were asked to recall and write down a past experience in which they interacted with a dog or cat (for details of the recall instructions, see Web Appendix H).Afterward, participants were told that a pet toothpaste brand was testing advertisements for its new product and needed their opinions on two ad versions (adapted from Wang and Lee [2006]). Corresponding to their assigned pet exposure condition, participants in the dog (cat) condition viewed dog (cat) toothpaste ads. Ad A, which featured a promotion-focused claim, read, ""Our product helps your dog [cat] freshen breath and strengthen tooth enamel!"" Ad B, which emphasized the prevention-focused benefits of the product, read, ""Our product helps your dog [cat] prevent gingivitis and fight plaque buildup!"" A separate pretest confirmed that Ad A (B) was indeed perceived as more promotion (prevention) focused (Web Appendix H).After viewing the two ads, participants indicated their preference for one of the two ads on three seven-point scales (1 = ""definitely/for sure/certainly Ad A,"" and 7 = ""definitely/for sure/certainly Ad B""). We created a preference index by averaging participants' responses to the three items (α = .99), with higher scores indicating a preference for Ad B, the prevention-focused version.As we expected, a one-way ANOVA revealed a significant effect of exposure to pets on ad preference. Specifically, participants in the dog condition indicated a stronger preference for the promotion-focused ad (Mdog = 4.10, SD = 2.06) than those in the cat condition (Mcat = 4.85, SD = 1.83; F( 1, 181) = 6.78, p = .010,  ηp2   = .04). Study 3bStudy 3b aims to examine our prediction using an incentive-compatible behavior in a pet-unrelated domain. One hundred eighty MTurk workers completed the study, which featured a two-cell (pet exposure: dog vs. cat) between-subjects design, in exchange for a small financial compensation (Mage = 39.5 years; 60% female). We told participants that the study was about people's general knowledge about pets and their past experiences with pets. We randomly assigned them to one of the two conditions (dog vs. cat). Participants first answered five quiz questions about dogs (cats; see Web Appendix I) and then recalled a past experience interacting with a dog (cat) and wrote it down (following the same instructions as in Study 3a).We next told participants that they could participate in a lottery and explained the options they had as follows. If they chose not to participate in the lottery, they would still get paid the initial amount ($.40) as described in the study, so there was nothing to lose. If they chose to participate in the lottery, they had a 50% chance to receive a bonus ($.20) in addition to the base pay; however, they also had a 50% chance to lose half the base pay ($.20). A separate pretest confirmed that the lottery participation (nonparticipation) option was indeed perceived as more promotion- (prevention-) focused (Web Appendix I).Because the promotion (prevention) focus prompts people to focus more on gains (losses) and thus be more risk seeking and open to change (risk averse and status quo oriented) (Liberman et al. 1999; Zhou and Pham 2004), we expected participants who were exposed to dogs to be more likely to participate in the lottery than their counterparts who were exposed to cats. A logistic regression showed a significant effect of pet exposure (cat = 0, dog = 1) on lottery participation, such that participants in the dog condition showed a higher likelihood to take part in the lottery (63.4%) than participants in the cat condition (44.8%; b = .76, SE = .31, χ2 = 6.20, p = .013, Exp (B) = 2.14). Study 3cTwo hundred twenty-five MTurk workers completed Study 3c in exchange for a small financial compensation (Mage = 38 years; 49% female). The study featured the same two-cell (pet exposure: dog vs. cat) between-subjects design and manipulated pet exposure by asking participants to view a series of four print ads, one per screen, that featured either dogs or cats as the spokescharacter (see Web Appendix J) and to provide their thoughts and feelings after viewing the ads. We then measured participants' regulatory orientation using the same financial decision-making task ([65]) as in Study 2.A logistic regression showed a marginally significant effect of exposure to pets (cat = 0, dog = 1) on investment choice, such that participants in the dog condition were more likely to choose to invest in the riskier stock option (29.8%) than participants in the cat condition (18.9%; b = .60, SE = .32, χ2 = 3.57, p = .059, Exp(B) = 1.82). Similarly, a one-way ANOVA revealed a significant effect of exposure to pets on money allocation. Participants in the dog condition allocated more money to the stock option (Mdog = $790.35, SD = 514) than those in the cat condition (Mcat = $613.06, SD = 429; F( 1, 223) = 7.86, p = .005,  ηp2   = .03). Study 3dOne hundred fifty-seven MTurk workers completed Study 3d in exchange for a small financial compensation (Mage = 42 years; 61% female). Study 3d employed the same two-cell (pet exposure: dog vs. cat) between-subjects design. To manipulate pet exposure, participants watched a short video featuring either dogs or cats. Both videos had the same theme—pets ""shopping"" around in a store (see Web Appendix K).After participants watched the video, we presented them with a choice scenario. Specifically, we asked them to imagine that they were buying vitamins and that two brands were available (Zhou and Pham 2004). Brand A was rich in vitamin C and iron and could promote high energy. Brand B was rich in antioxidants and could reduce the risk of cancer and heart disease. A separate pretest confirmed that Brand A (Brand B) was perceived as more promotion- (prevention-) focused (see Web Appendix K).After viewing the two brands, participants then indicated their preference for one of the two brands on three seven-point scales (1 = ""definitely/certainly/for sure Brand A,"" and 7 = ""definitely/certainly/for sure Brand B""). We created a preference index by averaging and reverse coding participants' responses to the three items (α = .99; a higher rating indicating a stronger preference for Brand A, the promotion-focused brand).A one-way ANOVA revealed a significant effect of exposure to pets on brand preference. As expected, participants in the dog condition indicated a stronger preference for the promotion-focused brand (Mdog = 3.96, SD = 2.34) than those in the cat condition (Mcat = 3.19, SD = 2.02; F( 1, 155) = 4.93, p = .028,  ηp2   = .03). DiscussionUsing a variety of methods to manipulate exposure to pets (i.e., pet knowledge, viewing print ads featuring pets, watching a short pet video, and recalling the experience of interacting with a pet), Studies 3a–3d provide converging support for H1 and show that exposure to dogs can lead to behaviors consistent with a promotion-focused mindset, whereas exposure to cats can prompt behavior patterns more aligned with a prevention-focused mindset. Specifically, in Study 3a, consumers preferred the ad with promotion-focused (prevention-focused) benefits for a dog (cat) toothpaste product. In Studies 3b–3d, we extended this finding to pet-unrelated domains. In Studies 3b and 3c, consumers exposed to dogs (vs. cats) were more willing to take risks in their decisions. In Study 3d, exposure to dogs (vs. cats) prompted consumers to prefer a vitamin brand with promotion-focused benefits. These findings provide converging support for our basic prediction that in both pet-related and pet-unrelated contexts, exposure to dogs can activate more of a promotion-focused mindset, whereas exposure to cats can activate more of a prevention-focused mindset.Importantly, in Studies 3a–3d, we found no systematic differences between the dog- and cat-exposure conditions in terms of mood, age, gender, ethnicity, income, and pet ownership. In addition, including these variables as control variables does not change our results anyway; thus, we do not discuss these variables further. Although some stimuli used in these studies may not be completely balanced (e.g., the pet pictures used in Study 3c may differ on certain dimensions, and the energy-boosting benefits of the vitamin seem less consequential than the cancer-risk-reducing benefits of the product in Study 3d), these studies taken together show convergent evidence for our main hypothesis and suggest that the proposed effect is robust across different contexts. Study 4: Moderating Role of Pet StereotypicalityThe primary purpose of Study 4 is to examine the moderating effect of pet stereotypicality on the activation of regulatory-focus mindsets (H2). We predict that making nonstereotypical information (i.e., pets that do not possess the stereotypical characteristics of their species) available to consumers will attenuate the effect of pet exposure on regulatory-focus mindsets. MethodThree hundred eighty MTurk participants (57% female; Mage = 40.3 years) completed the study for a small monetary compensation. Study 4 featured a 2 (exposure to pets: dog vs. cat) × 2 (pet stereotypicality: stereotypical vs. nonstereotypical) between-subjects factorial design.We used a recall task similar to Study 3a. Specifically, we told participants that we were interested in consumers' experience with a pet. Participants then read that some dogs (cats) possess stereotypical characteristics of a dog (cat) and some of them do not. To manipulate pet stereotypicality, in the stereotypical conditions, participants were asked to describe an experience interacting with a dog (cat) that reminds them of the stereotypical characteristics of a dog (cat) (i.e., with personality, temperament, and behavior like a stereotypical dog [cat]). In the nonstereotypical conditions, participants were asked to describe an experience interacting with a dog (cat) that does not have the stereotypical characteristics of a dog (cat) (i.e., with personality, temperament, and behavior unlike a stereotypical dog [cat]). (See Web Appendix L for the detailed manipulation).After the recall task, participants completed the financial decision scenario used in Study 2 and Study 3c. Specifically, participants imagined they had $2,000 and considered investing in two assets: a stock and a mutual fund. They indicated their preference between the two options on a nine-point scale (1 = ""stock,"" and 9 = ""mutual fund""; reverse-coded with a higher rating indicating a stronger preference for the promotion-focused option [i.e., stock]) and then indicated the amount of money they would invest. Results PreferenceA 2 × 2 ANOVA revealed a significant two-way interaction of pet exposure with pet stereotypicality on investment preference (F( 1, 376) = 8.11, p = .005,  ηp2   = .021). Planned contrasts showed that, after pets were described as consistent with their stereotypes, the prior findings were replicated. That is, participants in the dog condition demonstrated higher preference for the stock (Mdog = 4.21, SD = 2.69) than those in the cat condition (Mcat = 3.06, SD = 2.14; F( 1, 376) = 11.72, p <.001,  ηp2   = .03). However, after pets were described as inconsistent with their stereotypes, the prior findings of pet exposure disappeared in that participant did not show preferences for the stock (Mdog = 3.40, SD = 2.47; Mcat = 3.66, SD = 2.36; F( 1, 376) = .53, p = .466). Money allocation to the stockA 2 × 2 ANOVA on money allocation to the stock also revealed a significant two-way interaction (F( 1, 376) = 4.72, p = .031,  ηp2   = .012). Planned contrasts showed that, after pets were described as consistent with their stereotypes, the prior findings were again replicated such that participants in the dog condition allocated more money to the stock option (Mdog = $765.82, SD = 523.89) than those in the cat condition (Mcat = $623.39, SD = 466.25; F( 1, 376) = 4.20, p = .041,  ηp2   = .011). However, after pets were described as inconsistent with their stereotypes, the prior findings of pet exposure disappeared in that the amount of money allocated to the stock option was not statistically different between the dog and cat conditions (Mdog = $606.80, SD = $477.79; Mcat = $688.04, SD = $527.19; F( 1, 376) = 1.14, p = .286). Thus, the results support H2. DiscussionProviding support for our theorizing that associations triggered by pet exposure evoke different regulatory motivational mindsets, Study 4 shows that information related to pet stereotypicality moderates the effect of pet exposure on the activation of regulatory-focus mindsets. Specifically, Study 4 demonstrates that exposing participants to pet information consistent with their stereotype replicated the findings in the previous studies; by contrast, exposure to pets inconsistent with their stereotype nullified the effect of pet exposure on the activation of regulatory-focus mindsets.Having established the basic effect of pet exposure on consumers' regulatory motivational mindsets, in subsequent studies we aim to further examine the downstream effects of pet exposure on consumer behavior, including product evaluations, purchase intentions, and real incentive-compatible behaviors. Specifically, as we predict in H3, which is based on the regulatory fit between pet exposure and ad frames, because exposure to dogs (cats) activates a promotion (prevention) regulatory mindset among consumers, they should form more favorable evaluations and show more purchase intentions of products framed with promotion-focused (prevention-focused) benefits. Study 5: Pet Exposure, Product Feature Frames, and Bidding BehaviorStudy 5 aims to provide evidence for H3, which predicts that there is an interaction between pet exposure and the regulatory focus of an ad on consumers' evaluations of the advertised product by using incentive-compatible behaviors. The study also uses a different ad to further augment the robustness of our findings. Two hundred eighty-three undergraduate students from a large midwestern U.S. university participated in the study for partial course credit (45.9% female; Mage = 20.5 years). Study 5 employed a 2 (pet exposure: dog vs. cat) × 2 (regulatory focus: promotion vs. prevention) between-subject factorial design.Similar to the previous studies, to manipulate pet exposure, under the cover story that we wanted to understand consumers' day-to-day experiences, we first asked participants to recall a past experience in which they interacted with a dog or a cat and to write it down. We then told participants that they would read a message from a local massage center. We varied the message to accentuate either a promotion or a prevention focus (see Web Appendix M). The promotion-focused message emphasized that massages performed by therapists help people increase metabolism, boost immunity, and build a rejuvenated body. The prevention-focused message indicated that massages performed by therapists help soothe body aches, relieve tensions, and reduce stress from school and work. We conducted a separate pretest to confirm the success of our regulatory focus manipulation (see Web Appendix M).Next, we told participants that the local massage center would offer $50 gift cards to several survey participants. They were asked to bid on the gift cards and were told that the top bidders would be contacted later and offered the gift cards at the bidding price (though later the top bidders received the gift cards for free). Participants were then instructed to write down the dollar amount they were willing to bid on a $50 gift card.A 2 × 2 ANOVA on participants' bidding amount revealed only a significant interaction between regulatory focus and pet exposure (F( 1, 279) = 8.91, p = .003,  ηp2   = .03). Planned contrasts showed that after exposure to the promotion-focused version of the ad message, participants in the dog condition bid significantly higher (Mdog = $20.31, SD = $14.57) than those in the cat condition (Mcat = $14.98, SD = $13.20; F( 1, 279) = 4.77, p = .030,  ηp2   = .017). By contrast, after exposure to the prevention-focused version of the ad message, participants in the cat condition placed significantly higher bids (Mcat = $20.51, SD = $15.35) than those in the dog condition (Mdog = $15.67, SD = $13.68; F( 1, 279) = 4.14, p = .043,  ηp2   = .02). DiscussionUsing a behavioral study with an incentive-compatible measure, Study 5 confirmed the robustness of our findings that exposure to pets activates different regulatory mindsets among consumers. After viewing the promotion-focused version of an ad promoting a local massage center, participants who recalled exposure to a dog placed higher bids on the gift card; by contrast, after viewing the prevention-focused version, participants who recalled exposure to a cat placed higher bids. These results provide support for H3. Study 6: Pet Exposure and Product Frames Induce Regulatory Fit and PersuasionThe goal of Study 6 is threefold. First, Study 6 aims to augment robustness for H3 by conceptually replicating the findings of Study 5 using a different context (bidding for a product). Second, Study 6 aims to test H4, which predicts that regulatory fit will mediate the interaction of exposure to pets with ads' regulatory focus on consumer behavior. Third, it uses a new method to manipulate exposure to pets, such that dogs (cats) are directly incorporated into the stimuli as an integral part of the ad message. MethodTwo hundred sixty-four undergraduate students from a large southeastern U.S. university participated in the study in exchange for course credit (Mage = 20.2 years; 52% female). Study 6 featured a 2 (pet exposure: dog vs. cat) × 2 (regulatory focus: promotion vs. prevention) between-subjects factorial design.The experimental procedure was similar to Study 5 except that a new ad with a new product (sneaker) was employed and that pets (dogs or cats) were referenced in the ad. We told participants that they would review a message from a sneaker brand. Dependent on the assigned condition, participants next viewed one of the four versions of the ad (adapted from [22]]). The promotion-focused version of the ad read, ""Be a dog (cat) person! Reach your health goal with eagerness. Our sneakers feature H-Ergy synthetic material, which improves breathability of the shoes and promotes strong support for your feet."" The prevention-focused version of the ad read, ""Be a dog (cat) person! Reach your health goal with caution. Our sneakers feature N-Ergy synthetic material, which is anti-skid and reduces the possibility of foot pain."" We conducted a separate pretest to confirm the success of our regulatory focus manipulation and the believability of the stimuli.Two hundred sixty undergraduate students (Mage = 20.3 years; 56% female) were randomly assigned to one of the four conditions. Participants first indicated the extent to which the advertised sneakers had benefits that could help people attain something positive and the extent to which the advertised sneakers had benefits that could help people avoid something negative (adapted from Mogilner, Aaker, and Pennington [2008]; 1 = ""strongly disagree,"" and 9 = ""strongly agree""). Participants then rated the extent to which the message was reasonable/appropriate/believable as an ad (1 = ""strongly disagree,"" and 9 = ""strongly agree""; α = .91; averaged to form a believability index). The ANOVA on manipulation check measures revealed only main effects, such that the promotion-focused message (Mpromotion = 7.53, SD = 1.19) was deemed as having benefits that helped people attain something positive to a greater extent than the prevention-focused message (Mprevention = 6.71, SD = 1.51; F( 1, 256) = 23.76, p <.001,  ηp2   = .09), and that the prevention-focused message (Mprevention = 6.46, SD = 1.93) was perceived as having benefits that helped people avoid something negative to a greater extent than the promotion-focused message (Mpromotion = 5.56, SD = 2.51; F( 1, 256) = 10.58, p = .001,  ηp2   = .04). A one-sample t-test on the believability index revealed a significant difference against the mid-point of the scale (5; M = 5.40; t(259) = 3.10, p = .002; d = .19) such that participants perceived the ad message they viewed as believable. There was no significant difference on the believability index across conditions (F = .03, n.s.).Next, after revealing that the suggested retail price for the sneakers was $50, we asked participants to bid on the advertised sneakers and told them that the top bidders would be offered the sneakers based on the price they bid. After entering the bidding amount, participants then responded to regulatory fit measures (Lee and Aaker 2004) on two nine-point scales (""It was easy to process the message"" and ""It was difficult to understand the message (reverse coded)""; r = .84) ResultsA 2 × 2 ANOVA on participants' bidding amount revealed only a significant interaction between regulatory focus and pet exposure (F( 1, 260) = 8.38, p = .004,  ηp2   = .03). Planned contrasts showed that after exposure to the promotion-focused version of the ad message, participants in the dog condition bid significantly higher (Mdog= $33.74, SD = $11.81) than those in the cat condition (Mcat = $28.23, SD = $15.61; F( 1, 260) = 5.49, p = .020,  ηp2   = .02). By contrast, after exposure to the prevention-focused version of the ad message, participants in the cat condition placed significantly higher bids (Mcat = $32.45, SD = $13.61) than those in the dog condition (Mdog = $28.40, SD = $12.66; F( 1, 260) = 3.05, p = .082,  ηp2   = .012).A 2 × 2 ANOVA on regulatory fit revealed only a significant interaction between exposure to pets and regulatory focus (F( 1, 260) = 9.80, p = .002,  ηp2   = .036). Specifically, planned contrasts showed that for the promotion-focused ad, the dog version elicited higher regulatory fit (Mdog = 5.21, SD = 2.53) than the cat version (Mcat = 4.19, SD = 2.37; F( 1, 260) = 5.37, p = .021,  ηp2   = .02). By contrast, for the prevention-focused ad, the cat version elicited higher regulatory fit (Mcat = 5.15, SD = 2.40) than the dog version (Mdog = 4.24, SD = 2.62; F( 1, 260) = 4.45, p = .036,  ηp2   = .017).To test the mediation prediction in H4, we conducted a moderated mediation analysis using 5,000 bootstrapped samples (PROCESS Model 8; [24]), with exposure to pets as the independent variable, regulatory fit as the mediator, regulatory focus as the moderator, and bidding amount as the dependent variable. The index of moderated mediation was significant (b = 1.68, 95% confidence interval [CI]: [.30, 3.70]). Specifically, for the promotion-focused ad, the conditional indirect effect of pet exposure on bidding amount through regulatory fit was positive and significant (b = .89, 95% CI: [.05, 2.11]). By contrast, for the prevention-focused ad, the conditional indirect effect of pet exposure on bidding amount through regulatory fit was negative and significant (b = −.80, 95% CI: [−2.05, −.003]). Thus, the data support H4. DiscussionUsing a new method (featuring a dog/cat as an integral part of the ad) to manipulate exposure to pets, we provide further evidence for our theorizing of an interactive effect between exposure to pets and regulatory focus of an ad on consumer responses. For sneaker ads featuring promotion-focused (prevention-focused) claims, consumers exposed to dogs (cats) formed higher bidding amount than those exposed to cats (dogs). Importantly, the findings of Study 6 also provide support for H4, such that the influence of exposure to pets on bidding amount was mediated by the regulatory fit between the activated regulatory mindset and the regulatory focus of the ad claim. We also conducted an additional study to conceptually replicate this study in the financial decision-making context (for details, see Web Appendix P). Study 7: Pet Exposure and Product Frames Induce Regulatory Fit and ChoiceStudy 7 has two objectives. First, it aims to lend additional support to H3 and H4 using a within-subject design. Second, to augment the robustness of our findings, we employ a different product category (toothpaste) and a new manipulation of pet exposure (pet pictures). MethodTwo hundred thirty-seven undergraduate students from a large southeastern U.S. university completed the study in exchange for partial course credit (Mage = 20 years; 45% female). The study featured a 2 (pet exposure: dog vs. cat) × 2 (products' regulatory focus: promotion- vs. prevention-focused) mixed ANOVA design, with pet exposure a between-subjects factor and products' regulatory focus a within-subjects factor.Participants were randomly assigned to one of two conditions (pet exposure: dog vs. cat). We told participants that an online calendar company was interested in people's feedback on several dog (cat) pictures that it planned to incorporate into a dog- (cat-) themed calendar. Participants then were shown a series of dog (cat) pictures, one on each screen (order counterbalanced). Afterward, participants were again shown all the pictures they had seen on one screen and were instructed to pick one of the pets to imagine interacting with. (For the stimuli used, see Web Appendix N.)Next, in an ostensibly different task, we presented participants with the descriptions of two toothpaste products (Wang and Lee 2006): Toothpaste A had strong promotion but weak prevention product claims, and Toothpaste B had strong prevention but weak promotion product claims (see Web Appendix O). We counterbalanced the order of the two toothpaste products across all participants. We then asked participants to evaluate each of the two products on four nine-point scales (1 = ""dislike very much/very unfavorable/very unattractive/very bad,"" and 9 = ""like very much/very favorable/very attractive/very good"").Afterward, we presented participants with all the strong feature claims and asked them to evaluate each of the features on a nine-point scale (1 = ""not at all attractive,"" and 9 = ""very attractive""). The features were evaluated as generic features of the product category, rather than as the features of a specific brand, and served as measures of regulatory fit (Wang and Lee 2006). That is, if participants relied more on (promotion or prevention) features that fit their regulatory orientations in their evaluation, they should find strong feature claims consistent with their regulatory orientations more attractive than claims that do not fit their orientations. ResultsWe averaged participants' evaluations of the toothpaste products on the four items to form a brand attitude index for each product (αA = .94, αB = .95). We expected that participants assigned to the dog exposure condition would evaluate Toothpaste A (with the strong promotion claims) more favorably than those assigned to the cat exposure condition. By contrast, participants assigned to the cat exposure condition would evaluate Toothpaste B (with the strong prevention claims) more favorably than those assigned to the dog exposure condition. A mixed ANOVA with toothpaste attitudes as the within-subject variable and pet exposure as the between-subjects variable first revealed a significant main effect of toothpaste attitudes (F( 1, 235) = 4.47, p = .036,  ηp2   = .02), such that, overall, participants evaluated Toothpaste A (with the strong promotion claims; M = 6.94) more positively than Toothpaste B (with the strong prevention claims; M = 6.61). Importantly, the predicted interaction also emerged (F( 1, 235) = 29.76, p <.001,  ηp2   = .11). We found that participants who were exposed to dogs evaluated Toothpaste A more positively (M = 7.41, SD = 1.25) than Toothpaste B (M = 6.29, SD = 1.74, t(235) = 5.74, p <.001; d = .52). By contrast, participants who were exposed to cats formed more positive attitudes toward Toothpaste B (M = 6.94, SD = 1.63) than Toothpaste A (M = 6.45, SD = 1.73, t(235) = −2.21, p = .029; d = –.21).We then analyzed participants' feature attractiveness ratings, which served as our regulatory fit measure. Because both feature type and toothpaste were measured within subject, we calculated a relative feature attractiveness index by dividing participants' attractiveness ratings of promotion features by their ratings of prevention features. With the index as the dependent variable, a one-way ANOVA revealed that participants in the dog exposure condition (Mdog = 1.27, SD = .83) perceived the promotion features as more attractive than participants in the cat exposure condition (Mcat = 1.09, SD = .35; F( 1, 235) = 4.66, p = .032,  ηp2   = .02).To further investigate the mediating role of regulatory fit, we conducted mediation analyses to examine whether perceived attractiveness of the product features mediated the effect of pet exposure on product evaluation. Given the within-subjects design, we used MEMORE macro developed by Montoya and Hayes (2017). With 5,000 bootstrapped samples, the analysis revealed a significant indirect effect of feature attractiveness (b = .41, SE = .09, 95% CI: [.23,.59]), indicating that regulatory fit mediated the effect of pet exposure on product evaluation. DiscussionUsing a within-subject design and a new product category for the dependent variable, Study 7 conceptually replicates the previous studies. Moreover, it provides additional support for H3 in that exposure to pets interacted with an ad's regulatory focus to influence consumer responses and for H4 in that such an effect is driven by regulatory fit. General DiscussionAcross 11 studies, employing different methods (secondary data, lab experiments, and real behavior), different operationalizations of pet exposure (pet ownership; viewing pet-featuring pictures, videos, or ads; and recalled experience with pets), and different measures and contexts of regulatory focus (e.g., financial, service, consumer products), we find converging evidence that pet exposure influences consumer judgments and behaviors through regulatory focus and regulatory fit. Specifically, we show that pet exposure fosters divergent regulatory orientations among consumers, such that exposure to dogs activates a promotion-focused motivational mindset while exposure to cats activates a prevention-focused motivational mindset (Studies 1–3). In Study 4, we further show that this effect is moderated by pet stereotypicality, such that the effects of pet exposure on regulatory mindsets dissipate when consumers are reminded of a pet that is inconsistent with the stereotypes of the species. These regulatory mindsets, when activated by pet exposure, carry over to influence downstream pet-unrelated consumer judgments, purchase decisions, and behaviors through regulatory fit (Studies 5–7), even in pet-unrelated consumption contexts. Theoretical ContributionsOur research contributes to the literature in several ways. First, it recognizes pets as an important source of social influence on consumers' judgments and behaviors (Cavanaugh, Leonard, and Scammon 2008; Hirschman 1994; Holbrook and Woodside 2008). Prior research on marketplace social influence has examined the contexts of consumer-to-consumer and marketer-to-consumer interactions (e.g., Argo, White, and Dahl 2006; Chan and Sengupta 2010; Duclos, Wan, and Jiang 2013; Lee and Shrum 2012; Mead et al. 2011; White and Argo 2011; White and Dahl 2006, [75]). As examples of consumer-to-consumer interactions, Duclos, Wan, and Jiang (2013) show that social exclusion prompts riskier financial decisions because interpersonal rejection heightens the instrumentality of money to obtain benefits in life; Lee and Shrum (2012) find that social exclusion can lead to conspicuous consumption or prosocial behavior. As an example of marketer-to-consumer interactions, Chan and Sengupta (2010) find that consumers who receive insincere flattery from marketers still form favorable implicit evaluations of the marketer, but their explicit evaluations of the marketer are negative. Beyond these human-to-human contexts, our research suggests that the context of pet–human interactions as a source of social influence can similarly affect consumers' motivations, judgments, and behaviors.Second, our research diverges from extant literature on human–animal relationships by going beyond the immediate context of pet ownership and investigating how pet exposure affects the way people render subsequent pet-unrelated judgments and decisions. Prior research on the effects of human–animal relationships has mainly focused on the immediate context of pets and their owners, such as pet owners' health and psychological well-being ([ 2]), their awareness and protection of animal rights (Kidd, Kidd, and Zasloff 1995), and their relationship satisfaction with pets (Cavanaugh, Leonard, and Scammon 2008). Going beyond this immediate context between pets and their owners, we posit and find that exposure to pets or pet-featuring stimuli fosters divergent regulatory orientations, which in turn influence downstream pet-unrelated judgments and decisions in the consumption domain. We hope that this research will stimulate further research to gain a more nuanced understanding of the impact of human–animal relationships.Third, prior research identifies several antecedent variables of regulatory focus, such as one's cultural background (Lee, Aaker, and Gardner 2000), impulse purchase ([65]), and choosing for oneself versus others ([59]). Most of these factors, however, are intrapersonal. Only recently have researchers begun exploring the interpersonal drivers of regulatory focus (e.g., social exclusion [Park and Baumeister 2015], reminders of friends vs. family members [Fei, You, and Yang 2020]). Contributing to this stream of research, we uncover a novel social source of regulatory orientation: exposure to pets. We find that dog exposure is associated with a promotion orientation, whereas cat exposure is associated with a prevention orientation. Additional research is necessary to further explore the social and interpersonal impact on people's regulatory orientation. Practical ImplicationsOur findings also offer novel implications to marketers. First, marketers should consider crafting their advertising messages differently or recommending different products and services when they target consumers depending on their pet exposure situations. For example, to enhance the effectiveness of their advertising appeals or communication messages, marketers should emphasize promotion-focused goals such as gains and nongains if they are targeting dog owners or after consumers are exposed to dogs or dog-featuring stimuli (e.g., after just watching an ad about dogs). Conversely, they should focus on prevention-focused goals such as losses and nonlosses if they are pursuing cat owners or after consumers who are exposed to cats or cat-featuring stimuli. Importantly, our findings show that this advice holds even when the advertised product or service has nothing to do with pets or pet products.Second, our findings offer important insights into how to incorporate pets into marketing communications. Dogs and cats frequently appear in advertisements and marketing campaigns. For example, Subaru has been running the ""Dog Tested, Dog Approved"" campaign since 2009 (Subaru 2020), and Sainsbury's ""Mog the Cat"" campaign raised a great amount of attention during the 2015 Christmas season ([25]). One consideration factor, according to our findings, is the type of products or services being advertised. For products or services mainly perceived as promotion-focused (e.g., stock investment, sports cars), featuring dogs in the ad is likely to increase the ad's persuasiveness. For products or services deemed more prevention-focused (e.g., mutual fund investment, insurance), featuring cats may increase the ad's appeal. According to the findings of the pet stereotypicality study, a caveat is that marketers should ensure that stereotypical pet temperaments are made salient in the message (e.g., the eagerness [cautiousness] aspect of the dog [cat] should be highlighted). Otherwise, the intended effects of featuring pets in the ad may not be achieved.Third, pet-related marketing strategies are especially relevant in today's big-data era, in which marketers likely know more about consumers, including which types of pets they own or interact with. For example, marketers could obtain consumers' pet exposure information from the pet-related products purchased, the YouTube pet videos watched, or the Instagram pet photos posted and use this data to decide what type of marketing information to highlight. Alternatively, census information can provide marketers with an aggregated level of information about pets, as certain states or zip codes may have a higher concentration of dog (cat) owners. Marketers could use this information to determine the design of regional marketing campaigns.Lastly, our findings that pets and pet ownership are potentially related to COVID-19 transmission rate and prevention behaviors could shed new light on policies related to prevention of COVID-19 and potentially other infectious diseases. For example, policy makers in states with more dog owners could design more customized educational programs and materials related to the diseases. Alternatively, when designing ads to prevent the transmission of COVID-19 and other infectious diseases, cats could be incorporated as a spokesperson and/or their temperaments can be referenced in the message to enhance the effectiveness of the ad. Future Research DirectionsThis research indicates that pets can exert a strong social influence on consumers. Future research could examine the strength of the influence of pets versus people on consumer behavior. On the one hand, one may argue that pets' influence on consumers is weaker than other human influences, because people are more influenced by similar others ([ 3]; Bandura 2002). On the other hand, it is also possible that pets may occasionally exert a stronger influence, especially considering that many pet owners admit that they prefer spending time with their pets over other people ([56]).Another possible research direction is to examine other differences between dog and cat exposure. For example, exposure to dogs may be more likely to lead to conspicuous consumption than cat exposure. This follows because many people likely perceive dogs (vs. cats) as less seclusive and more social. Along similar lines, research could examine the different effects of dog versus cat stimuli. For example, using a dog (cat) as a spokescharacter can increase a brand's perceived excitement. Although our research focuses on examining the effect of pet exposure on consumers' regulatory orientations, future research could investigate the reverse relationship of whether promotion- (prevention-) focused consumers are more likely to adopt a dog (cat) or prefer dog- (cat-) related stimuli.Research could also examine additional moderators for our findings. One example is pet anthropomorphism, or people's tendency to assign human characteristics to pets ([ 2]). One prediction is that our findings that dog (cat) exposure evokes a promotion (prevention) orientation would be more salient among consumers who engage in pet anthropomorphism, because these consumers are more likely to be influenced by the pets. Another possible moderator is the role of culture. We conducted our studies primarily with American participants, and the United States is a culture in which the majority of people treat their pets as friends and family members (Sanders and Hirschman 1996). In many other cultures however, pets are not elevated to the same level, and thus consumers may treat their pets as possessions or servants ([ 9]). Future research could examine whether our identified findings still hold in such cultures. Supplemental Materialsj-pdf-1-jmx-10.1177_00222429221078036 - Supplemental material for The Pet Exposure Effect: Exploring the Differential Impact of Dogs Versus Cats on Consumer MindsetsSupplemental material, sj-pdf-1-jmx-10.1177_00222429221078036 for The Pet Exposure Effect: Exploring the Differential Impact of Dogs Versus Cats on Consumer Mindsets by Lei Jia, Xiaojing Yang and Yuwei Jiang in Journal of Marketing  "
19,"The Pet Exposure Effect: Exploring the Differential Impact of Dogs Versus Cats on Consumer Mindsets Despite the ubiquity of pets in consumers' lives, scant research has examined how exposure to them (e.g., recalling past interactions with dogs and cats, viewing ads featuring a dog or a cat) influences consumer behavior. The authors demonstrate that exposure to dogs (cats) reminds consumers of the stereotypical temperaments and behaviors of the pet species, which activates a promotion- (prevention-) focused motivational mindset among consumers. Using secondary data, Study 1 shows that people in states with a higher percentage of dog (cat) owners Google more promotion- (prevention-) focused words and report a higher COVID-19 transmission rate. Using multiple products, Studies 2 and 3 demonstrate that these regulatory mindsets, when activated by pet exposure, carry over to influence downstream consumer judgments, purchase intentions, and behaviors, even in pet-unrelated consumption contexts. Study 4 shows that pet stereotypicality moderates the proposed effect such that the relationship between pet exposure and regulatory orientations persists to the extent consumers are reminded of the stereotypical temperaments and behaviors of the pet species. Studies 5–7 examine the role of regulatory fit and evince that exposure to dogs (cats) leads to more favorable responses toward advertising messages featuring promotion- (prevention-) focused appeals.Keywords: pets; regulatory orientation; advertising; COVID-19Pets are prevalent and play important roles in consumers' daily lives ([ 2]; Cavanaugh, Leonard, and Scammon 2008; Hirschman 1994; Holbrook and Woodside 2008; Serpell and Paul 2011). According to the survey of the American Pet Products Association ([ 4]), 68% of U.S. households, or 84.6 million homes, own a pet. Dogs and cats are the most popular pets, with 48% of U.S. households (60 million homes) owning at least one dog and 37% of U.S. households (47 million homes) owning at least one cat. Pet adoption rates have climbed significantly, with about one in five households having acquired a dog or cat since the outbreak of the COVID-19 pandemic (American Society for the Prevention of Cruelty to Animals [ASPCA] 2021). Pets also frequently appear in popular culture, mass media, and marketing communications. For example, Target uses a dog as its brand mascot, Microsoft features dogs in its 2020 holiday commercial to inspire people to find joy, and Wells Fargo uses a cat in its commercial to advertise its suspicious card activity alert services.Despite the significance of pets in people's lives and in mass media, popular culture, and marketing communications, scant research has examined how pets may influence consumers' judgments, decisions, and behaviors. Existing research on pet–human relationships largely revolves around examining how owning a pet influences the owner's pet-related judgments and behaviors. For example, attesting to a strong tie between owners and their pets ([ 2]; Cavanaugh, Leonard, and Scammon 2008), this stream of literature suggests that pets provide not only companionship but also a sense of safety and belongingness for their owners ([79]). Pet owners have significantly greater physical and psychological well-being than non-pet owners ([ 2]) and are more likely to endorse causes protecting animal rights (Kidd, Kidd, and Zasloff 1995).Research that examines how pets may influence consumer behavior beyond the immediate context of pet ownership is lacking, however. Such knowledge would provide novel and important insights to marketers and allow them to develop marketing strategies based on pet exposure situations. For example, marketers might choose to recommend more fitting products or services or craft appropriate communication messages to effectively target consumers depending on the type of pets to which they are exposed. Consider the following scenario: A newly opened massage center is pondering the language to use in direct mail to potential customers and whether to focus on how its therapies help reduce fatigue and stress or how its therapies promote metabolism and energy levels. Which strategy might be more effective if the company features a cat (dog) figure in its advertisement? This article provides a theory-based answer to this question.In our research, we focus on the effects of exposure to pets (e.g., recalling consumers' past interactions with dogs or cats, viewing ads featuring a dog or a cat) and examine how such experience influences consumers' judgments and decision making through the lens of regulatory focus theory. Drawing from literature on human–animal relationships, research on regulatory orientation, and work on mindset, we suggest that exposure to pets will remind consumers of the stereotypical personality traits, temperaments, and behaviors of the pets and thus will evoke different regulatory mindsets among consumers. Specifically, we predict and find that exposure to dogs fosters a more promotion-focused motivational mindset whereas exposure to cats activates a more prevention-focused motivational mindset. We further identify pet stereotypicality as a moderator for our findings, such that our results on the relationship between pet exposure and regulatory orientations persist only when consumers are reminded of the stereotypical temperaments and behaviors of the pet species, and that our main proposed effects dissipate when consumers are reminded of information inconsistent with the pet stereotypes. Moreover, we show that these regulatory mindsets, when activated by pet exposure, carry over to influence downstream consumer judgments, purchase intentions, and behaviors, even in pet-unrelated consumption contexts. Theoretical Framework Regulatory Orientation and Its Social OriginRegulatory focus theory suggests that consumer judgments, decisions, and behaviors are motivated by two regulatory orientations: promotion and prevention focus (Higgins 1997; Lee, Aaker, and Gardner 2000; Pham and Avnet 2004; Wang and Lee 2006). A promotion focus in self-regulation reflects consumers' motivations to attain growth and nurturance in an effort to align their actual selves with their ideal selves (achieving accomplishments and fulfilling aspirations; Higgins 1987, [28]). Promotion-focused consumers are characterized by an eagerness regulatory system during behavioral regulation (Crowe and Higgins 1997; Lee, Keller, and Sternthal 2010; Pham and Chang 2010; Wang and Lee 2006). For example, they are sensitive to the presence or absence of positive outcomes (gains and successes; Higgins 1997), concerned about reducing errors of omission (Croe and Higgins 1997), and more risk seeking when processing information and rendering decisions (Zhou and Pham 2004). By contrast, self-regulation with a prevention focus reflects consumers' motivations to attain safety and security in an attempt to bring their actual selves into alignment with their ""ought"" selves (fulfilling duties and obligations; Higgins 1987, [28]). Thus, prevention-focused consumers are more vigilant and cautious during behavior regulation (Crowe and Higgins 1997; Lee, Keller, and Sternthal 2010; Pham and Chang 2010; Wang and Lee 2006). In this system, consumers are sensitive to the presence or absence of negative outcomes (losses and failures; Higgins 1997), concerned about reducing errors of commission (Crowe and Higgins 1997), and more risk averse (Zhou and Pham 2004) when processing information and making decisions.More germane to our research is the finding that social influences play a pivotal role in shaping people's regulatory orientations. For example, interactions with childhood caretakers and parents' parenting style can influence the formulation of consumers' regulatory orientations during the socialization process (Crowe and Higgins 1997; Higgins 1996). Social exclusion causes a shift toward prevention motivation (Park and Baumeister 2015), whereas making choices for other people instigates a promotion focus ([59]). In addition, distinct social relationships can activate alternative regulatory orientations, such that reminders of friends activate a promotion focus while reminders of family members engender a prevention focus ([22]). Similarly, positive role models induce a promotion focus, whereas negative role models instigate a prevention focus (Lockwood, Jordan, and Kunda 2002).Given pets' prevalence in consumers' daily lives, we posit that consumers' interactions with pets are also an important part of socialization that can influence their regulatory orientations. These socialization activities can involve direct or indirect interactions with pets (e.g., observing pets' interactions with other people). Indeed, research on human–animal relationships evinces that pets play an important part in people's socialization process, influencing the development of various cognitive and social abilities (e.g., worldviews, empathy; [ 2]; Myers 1999; Purewal et al. 2017). Exposure to Pets and Regulatory OrientationsAs we have mentioned, dogs and cats are two primary types of pets ([ 4]). Despite within-species breed differences, research on animal behavior has identified systematic cross-species differences between domesticated dogs and cats ([11], [12]; Jardim-Messeder et al. 2017). This stream of research suggests that a promotion-oriented eagerness system better captures dogs' temperaments and behavioral characteristics, whereas a prevention-focused cautious system better describes cats' temperaments and behavioral characteristics. For example, on a temperament level, dogs tend to be open and expressive, while cats are elusive and cautious ([11], [12]; Potter and Mills 2015). Consistent with the promotion orientation's receptivity to change (Boldero and Higgins 2011; [43]), dogs (vs. cats) cope better with and adapt quicker to changes in the environment, such as moving into a new house or having a new person in the household ([11], [12]; Langenfeld 2020). In line with the prevention orientation's preference for the status quo ([10]; Chernev 2004), cats (vs. dogs) appear more concerned with the protection their owners provide and the consistency and stability of their social and physical surroundings ([11], [12]). Similarly, consistent with the eagerness prediction of a promotion regulatory system ([15]), dogs are more responsive to rewards (e.g., food, praise, petting) than cats and thus are easier to train ([49]).When interacting with human beings and other pets, dogs are more eager to please their owners and socialize with other dogs, whereas cats are more cautious, suspicious, boundary setting, and anxious when surrounded by unfamiliar people or other cats ([11], [12]; Potter and Mills 2015). Indeed, research has shown that dogs are more attentive and responsive to human's social cues (e.g., gestures) than cats ([49]; Wynne, Udell, and Lord 2008). Dogs' eagerness can be exemplified by the spike in oxytocin (a hormone mammals release when they feel love or affection for someone) when their owners are around ([53]). A study conducted by scientists at BBC shows that dogs produce five times more oxytocin than cats upon seeing their owners ([21]).We further predict that through repeated socialization episodes with pets (through either direct or indirect interaction with pets), the traits and motivational characteristics of dogs (cats) are gradually associated with a promotion-focused (prevention-focused) eagerness (cautiousness) system. These learned associations are brought to mind and thus accessible when consumers interact with pets or encounter stimuli featuring pets (e.g., ads) in their daily lives. To confirm the associations of dogs and cats with promotion and prevention orientations, we conducted a pilot study, which found that participants indeed associated promotion-focused words with dogs and prevention-focused words with cats (for details of the pilot study, see Web Appendix A). HypothesesDrawing from research on motivational mindset, which we review next, we further predict that exposure to dogs (cats) or stimuli featuring them (e.g., ads) will remind consumers of the temperaments and behaviors of the dogs (cats), which will in turn activate a promotion-focused (prevention-focused) mindset among consumers and guide their subsequent judgment and decision making. Mindset reflects ""the activation and use of a procedure that is stored in memory as part of declarative knowledge"" ([76], p. 110). That is, engaging in a particular operation when pursuing a goal in a prior task may give rise to a mindset (e.g., a promotion-focused mindset) that remains accessible in consumers' memory and, in turn, guides their pursuit of a different goal in a subsequent, unrelated context.A growing body of literature has found considerable evidence of the role of mindset across a wide range of information-processing activities, from comprehension, to judgment, to decision making (Ma and Roese 2014; Wyer and Xu 2010; Xu et al. 2020). In some situations, mindsets involve cognitive procedures induced by engaging in a prior task that spills over to influence a subsequent, unrelated context. For example, Xu et al. (2020) show that managers during election years are more likely to adopt a comparative mindset due to the omnipresence of comparative political advertisements. Accordingly, they spend more money on their managerial decisions because the comparative mindset accentuates ""which option to spend money on"" and forgoes the ""whether or not to spend"" consideration. More germane to our theorizing, mindsets may also be based on motivation ([76]), such that the motivational mindset induced by pursuing a goal in a prior task will guide consumers' subsequent behavior in an unrelated context (e.g., pursuit of a different goal). For example, Wyer and Xu (2010) assert that the promotion (prevention) regulatory mindset can be induced procedurally by, for example, making salient participants' desire to achieve their ideal (ought) self. When activated, the promotion (prevention) regulatory mindset produces a cross-domain effect, making consumers, for example, more likely to approach positive (avoid negative) consequences in their decision making.Drawing on this stream of literature, we posit that exposure to pets (e.g., recalling an interaction with a pet, viewing ads featuring a pet as the spokescharacter) in a prior task may render different regulatory mindsets salient. Specifically, because the stereotypical personality traits, temperaments, and behaviors of dogs (cats) brought to mind by the pet exposure are associated with eagerness (cautiousness) strategies commonly employed by a promotion (prevention) orientation, consumers' different regulatory orientations (promotion vs. prevention) will be activated. When evoked, these motivational regulatory mindsets will carry over to influence consumers' subsequent, unrelated judgments and decision making, rendering them more eager (cautious) during behavioral regulation, leading them to pursue promotion- (prevention-) focused goals such as growth and advancement (safety and stability), and making them more risk seeking (more risk averse) in decision making. Thus,H1: Pet exposure activates different regulatory motivational mindsets among consumers, such that (a) exposure to dogs or dog-featuring stimuli (vs. cats or cat-featuring stimuli) activates a more promotion-focused mindset and (b) exposure to cats or cat-featuring (vs. dogs or dog-featuring) stimuli activates a more prevention-focused mindset.A key premise of our theorizing that exposure to pets activates different regulatory mindsets is that such exposure will remind consumers of the stereotypical temperaments and behavioral characteristics of dogs (cats), giving rise to a promotion-focused (prevention-focused) mindset. In other words, through repeated socialization, consumers have developed preestablished mental connections between dogs' (cats') typical temperaments and behaviors and the promotion (prevention) focus, and exposure to pets or pet-featuring stimuli can render these stereotypical associations accessible, thus activating the corresponding regulatory-focus mindset among consumers. Prior research has shown that established mental associations are likely to be temporarily weakened, nullified, or even reversed when presented with information inconsistent with the original associations. For example, Gorn, Jiang, and Johar (2008) reversed the association between baby-faceness and unintentionality by presenting counterassociation information about a baby-faced person intentionally harming others. Thus, if our reasoning that mental associations between dogs (cats) and promotion- (prevention-) focused mindsets is right, our proposed effects should persist to the extent consumers are reminded of the stereotypical behaviors and temperaments of the pet species; when consumers are reminded of pet information inconsistent with the stereotypes of the species (e.g., dogs [cats] unlike a stereotypical dog [cat]), we are likely to show that our results are attenuated. More formally,H2: Pet stereotypicality moderates the effect of pet exposure on regulatory mindsets, such that the effect dissipates when consumers are exposed to pets that are inconsistent with the stereotypes of the species.We expect that the impact of pet exposure on consumers' motivational mindsets will carry over to influence downstream variables, including ad evaluation, purchase intention, and real purchasing behavior, even in pet-unrelated consumption contexts. We anticipate that the effects of pet exposure on these variables will stem from the activation of a regulatory mindset and regulatory fit. Regulatory fit occurs when the regulatory strategies individuals employ during goal pursuit are compatible with their regulatory orientations (Higgins 2000; Hong and Lee 2007); it usually results in favorable effects on downstream consumer responses, such as enhanced value of the product ([ 7]), brand attitudes ([37]), self-regulation (Hong and Lee 2007), and decision making (Zhou and Pham 2004).Therefore, in accordance with this literature, we anticipate that consumers who are exposed to dogs or dog-featuring stimuli will experience higher regulatory fit and develop more favorable product evaluations when presented with ad messages featuring promotion-focused claims. By contrast, consumers who are exposed to cats or cat-featuring stimuli will experience higher regulatory fit and develop more favorable product evaluations when presented with ad messages featuring prevention-focused claims. Thus,H3: There is an interaction between pet exposure and the regulatory focus of an ad on consumers' evaluations of the advertised product, such that (a) when exposed to ads with promotion-focused claims, exposure to dogs or dog-featuring stimuli (vs. cats or cat-featuring stimuli) leads consumers to form more favorable product evaluations, and (b) when exposed to ads with prevention-focused claims, exposure to cats or cat-featuring stimuli (vs. dogs or dog-featuring stimuli) leads consumers to form more favorable product evaluations.H4: Regulatory fit mediates the interaction between pet exposure and ads' regulatory focus proposed in H3 on product evaluations. Overview of StudiesStudies 1 and 2 provide initial evidence for our prediction by showing that long-term exposure to dogs (cats) is associated with a promotion (prevention) focus. Specifically, using secondary data gathered from the American Veterinary Medical Association, Google Trends, and Centers for Disease Control and Prevention (CDC), Study 1 finds that people in states with a higher percentage of dog (cat) owners search more promotion- (prevention-) focused words (Study 1a) and show a higher COVID-19 transmission rate (Study 1b). Study 2 shows that dog (vs. cat) owners are more likely to invest in stocks and are less likely to invest in mutual funds in financial decision making. Studies 3a–3d establish the basic effect that exposure to dogs (cats) activates a promotion- (prevention-) focused motivational mindset by employing multiple experimental manipulations of pet exposure and different measures of regulatory orientation in both pet-related and pet-unrelated contexts, including incentive-compatible choices. Study 4 explores a moderating effect for our findings, showing that our hypothesized effects will dissipate when consumers are exposed to pet information inconsistent with the stereotypes of the pet species. Study 5 examines the downstream effects of pet exposure on consumers' incentive-compatible behaviors, showing that consumers exposed to dogs (cats) bid higher for products framed with a promotion (prevention) focus. Studies 6 and 7 provide additional support for our theorizing by examining the mediation effect of regulatory fit. Table 1 in Web Appendix B provides a summary of all studies.GraphTable 1. Study 1a: Results from the Multiple Regression Model. btpPet ownership indexa.362.62.012Median household income−.15−.63.534Per capita GDP−.25−1.26.214Political orientationb−.05−.29.774 1 a Higher scores indicate more dog (vs. cat) owners.2 b 1 = Democratic, 2 = Republican.3 Notes: Dependent variable = Regulatory-focus index: high scores indicate more promotion- (vs. prevention-) focused. Study 1: Pet Ownership and Regulatory Orientations—Evidence from Google Trends and COVID-19 C...Relying on secondary data and operationalizing pet exposure as pet ownership, Study 1 aims to provide preliminary evidence for our prediction that exposure to dogs and cats is associated with different regulatory mindsets. We collected aggregated state-level data on pet owner statistics, public interest in promotion- versus prevention-oriented behaviors, and per capita COVID-19 cases during the pandemic. We expect that at the state level, having a relatively higher dog-owning (cat-owning) population will be related to more search interests in promotion-oriented (prevention-oriented) behaviors in general (Study 1a) and more per capita COVID-19 cases during the pandemic (Study 1b). Study 1a: Pet Ownership and Search Interests in Regulatory BehaviorsFor pet ownership, we obtained the latest (2016) state-level pet ownership data set (n = 49) from the U.S. Pet Ownership and Demographics Sourcebook released by the [ 1]. This data set provides the most complete data on pet population demographics, covering the 48 U.S. continental states and the District of Columbia (excluding Alaska and Hawaii). For each state, we divided the percentage of dog-owning households by the percentage of cat-owning households to obtain a pet-owning index, with a higher score indicating more dog-owning (vs. cat-owning) households in the state.To obtain a proxy for citizens' public interest in regulatory-oriented behaviors in each state, we examined the search interest scores data from Google Trends (Du, Hu, and Damangir 2015; Kozinets, Patterson, and Ashman 2017). Google is the most often-used internet search engine in the United States (accounting for 88% of the market share; Schultheiß and Lewandowski 2021), and Google Trends counts how often a particular search term is entered relative to the total search volume across various geographic regions. After a search term, period, and interested geographic area are entered, Google Trends displays how often that search term appears on Google in that geographic area and in that period relative to the total search volume on a standardized scale ranging from 0 (lowest search volume) to 100 (highest search volume). Given its viable role in monitoring public interests, Google Trends has become an increasingly used data source for research in psychology ([46]), political sciences (Mellon 2013; Weeks and Southwell 2010), and marketing (Du, Hu, and Damangir 2015; Kozinets, Patterson, and Ashman 2017).To build the state-level regulatory orientation index, we first selected ten representative promotion-focused words (i.e., ""growth,"" ""gain,"" ""achievement,"" ""aspiration,"" ""pleasure,"" ""proud,"" ""hope,"" ""earn,"" ""win,"" and ""spontaneous"") and ten representative prevention-focused words (i.e., ""privacy,"" ""safety,"" ""loss,"" ""prevention,"" ""pain,"" ""stable,"" ""saving,"" ""frugal,"" ""rules,"" and ""risky""), in line with literature on regulatory focus (Higgins 1997, [29], [30]; Scholer, Cornwell, and Higgins 2019). We then obtained search interest scores of these words on Google Trends from January 1, 2016, to December 31, 2020, across the 48 continental states plus the District of Columbia. For each state, we calculated the average search interest score for the ten promotion-focused words (α = .85) and the ten prevention-focused words (α = .74). Finally, we built a regulatory orientation index for each state by dividing the promotion search interest score by the prevention search interest score (i.e., higher numbers indicate a higher promotion focus).To demonstrate the ecological validity of our findings, we also controlled for state-level microeconomic influence (income), macroeconomic influence (gross domestic product [GDP]), and political orientations. Specifically, we included the (state-level) covariates median household income in 2016 (U.S. Bureau of the Census 2017), per capita GDP in 2016 (U.S. Bureau of Economic Analysis 2019), and political orientation based on the 2016 presidential election results (The New York Times 2017).We conducted a multiple linear regression with the pet ownership index as the independent variable, regulatory-orientation index as the dependent variable, and median household income, per capita GDP, and political orientation as covariates. Table 1 shows the results. The results reveal that our regression model was significant (F( 4, 44) = 4.86, p = .002), suggesting that the independent variables significantly explained the variance in regulatory orientation. More importantly, after controlling for the covariates, the pet ownership index (b = .36, t = 2.62, p = .012) significantly predicted the regulatory-orientation index, showing that at the state level, a relatively higher dog-owning (cat-owning) population is associated with more search interests in promotion-oriented (prevention-oriented) behaviors in general (for additional analyses, see Web Appendix C). Study 1b: Pet Ownership and Per Capita COVID-19 Cases During the PandemicStudy 1b uses the same pet-ownership data from Study 1a but focuses on per capita COVID-19 cases (CDC 2020) as a proxy for regulation-related behaviors. Considering the findings that promotion- (prevention-) focused people are more risk seeking (risk averse; Zhou and Pham 2004), we expect that dog owners will have an increased probability to engage in promotion-focused, relatively risky behaviors that may result in COVID-19 transmission (e.g., more willing to dine in restaurants, letting their guard down when following social distancing); by contrast, cat owners will have an increased probability to be more cautious and engage in less risky, prevention behaviors (e.g., behaving extra cautiously, practicing social distancing, wearing face masks). Accordingly, we predict that states with more dog (cat) owners will report a higher (lower) number of per capita COVID-19 cases.To obtain a state-level proxy for regulatory-oriented behavior, we examined each state's COVID-19 cases per 100,000 people reported to the CDC from January 21, 2020 (the earliest available date) to November 1, 2020 (the date Study 1b was conducted). As of November 1, 2020, the 48 continental states and the District of Columbia had reported 2,819 COVID-19 cases per 100,000 people, on average, with Vermont being the lowest (348 per 100,000) and North Dakota the highest ( 6,054 per 100,000).We performed a linear regression on COVID-19 cases (per 100,000), with the pet ownership index as the independent variable, and controlled for the same covariates as in Study 1a. The results reveal that our regression model was significant (F( 4, 44) = 8.93, p <.001), suggesting that the independent variables significantly explained the variance in COVID-19 cases. As Table 2 shows, the pet ownership index was significantly related to an increase in COVID-19 cases per 100,000 people (b = .38, t = 3.15, p = .003), suggesting that, at the state level, a relatively higher dog-owning (cat-owning) population was associated with more reported per capita COVID-19 cases. Consistent with this finding, our ancillary analyses (for detailed analyses and results, see Web Appendix D) also suggest that the pet ownership index (higher scores indicating more dog [vs. cat] owners) significantly increased search interests (per Google Trends during the same period as the COVID-19 data) in promotion-focused behaviors, such as dining in, but significantly reduced search interest in prevention-focused behaviors, such as face mask and social distancing.GraphTable 2. Study 1b: Results from the Multiple Regression Model. btpPet ownership indexa.383.15.003Median household income.09.56.579Per capita GDP.13.97.336Political orientationb.533.71.001 4 a Higher scores indicate more dog (vs. cat) owners.5 b 1 = Democratic, 2 = Republican.6 Notes: Dependent variable = number of COVID-19 cases per 100,000 people. DiscussionUsing aggregated state-level data across different data sources, Study 1 provides support for our prediction of a significant association between long-term pet exposure and people's regulatory orientations, such that dog (cat) exposure is associated with a promotion (prevention) focus. Specifically, we find that, at the state level, a relatively higher dog-owning (cat-owning) population is associated with more search interests in promotion- (prevention-) focused behaviors in general (Study 1a) and more reported per capita COVID-19 cases (Study 1b). In subsequent studies, we use individual-level data to provide additional support for our prediction. Study 2: Pet Ownership and the Choice Between Stocks and Mutual FundsStudy 2 also operationalizes pet exposure as pet ownership and examines whether consumers' pet-owning situations are associated with different regulatory mindsets. Unlike Study 1, which used aggregate, state-level data, Study 2 relies on individual-level pet ownership data. In addition, we used an established measure of regulatory orientation ([65]), which involved participants in a financial decision-making task choosing between two investment options: stock (a proxy for promotion focus) and mutual fund (a proxy for prevention focus).We recruited 145 pet owners from Amazon Mechanical Turk (MTurk) (Mage = 35.3 years; 53.1% female; 53% dog owners and 47% cat owners). We recruited only participants who own dogs only or cats only; owners of both dogs and cats were excluded (for the screening criteria, see Web Appendix E). We asked participants to partake in a financial decision-making task, which served as our measure of regulatory orientation (Zhou and Pham 2004). We first gave them basic definitions of stocks and mutual funds and told them that stock investments were typically associated with a higher level of risk, whereas mutual fund investments were typically associated with a lower level of risk and therefore more conservative. Next, we asked participants to imagine that they had $2,000 and were considering investing in two assets: a stock and a mutual fund. Afterward, we asked them to indicate which asset they would invest in if they could choose only one asset and then to indicate the amount of money they would invest. Given that prior research has shown that the activation of a promotion- (vs. prevention-) focused mindset entails greater risk taking (vs. risk aversion; Zhou and Pham 2004), we expected that dog (vs. cat) owners would be more willing to take risks in their financial investments and choose the stock option. After the financial decision-making task, participants completed measures of their mood using PANAS (Positive Affect Negative Affect Schedule; Watson, Clark, and Tellegen 1988) and a few demographic measures, including their age, gender, ethnicity, and income level (Web Appendix F presents the measures used). ResultsA logistic regression showed a significant effect of pet ownership (cat owners = 0, dog owners = 1) on investment choice, such that dog owners (36.4%) were more likely to choose to invest in the riskier stock option than cat owners (20.6%; b = .79, SE = .38, χ2 = 4.73, p = .039, Exp (B) = 2.20). Similarly, a one-way analysis of variance (ANOVA) revealed a significant effect of pet ownership on money allocations. As we expected, dog owners allocated more money to the stock option (Mdog = $796.10, SD = $524.45) than cat owners (Mcat = $603.69, SD = $474.90; F( 1, 143) = 5.31, p = .023,  ηp2   = .04).To rule out possible alternative explanations that participants' mood, gender, age, ethnicity, or income level accounted for our findings, we controlled for these variables simultaneously. Our results for both choice (χ2 = 5.47, p = .019) and money allocations (F = 5.74, p = .018) remained significant even after we controlled for these covariates. DiscussionStudy 2's findings show that dog (vs. cat) owners were more likely to take risks in their financial decisions, showing more preference for stock investment. Importantly, incorporating the demographic variables age, ethnicity, gender, and income as covariates did not change the results. Taken together, using pet ownership as an operationalization, Studies 1 and 2 provide initial support for our prediction that exposure to pets is associated with different regulatory mindsets. However, despite the extra steps taken, such as controlling for demographic variables (e.g., income) to rule out alternative explanations, Studies 1 and 2 were correlational in nature. To provide stronger causal evidence for our prediction, in the subsequent studies, we manipulate exposure to pets in various ways. Study 3: Effects of Pet Exposure on Consumers' Regulatory OrientationsThe purpose of Study 3 is twofold. First, the study aims to establish causality between pet exposure and the formation of regulatory motivation mindsets by using multiple manipulations of pet exposure. Second, the study operationalizes regulatory orientations in various ways and across different (pet-related and pet-unrelated) contexts.In a pet-related domain, Study 3a shows that participants exposed to dogs (vs. cats) will be more likely to prefer a pet toothpaste ad with promotion- (vs. prevention-) focused benefits. Studies 3b–3d test the effect in pet-unrelated domains. Consistent with prior research showing that the activation of a promotion- (prevention) focused mindset entails greater risk taking (risk aversion) (Zhou and Pham 2004), participants who are exposed to dogs (vs. cats) will be more willing to take risks to participate in a lottery (an incentive-compatible behavior; Study 3b) and in their financial investment decisions (Study 3c). In a health product context, Study 3d demonstrates that participants who are exposed to dogs (vs. cats) will be more likely to prefer a vitamin product with promotion- (vs. prevention-) focused benefits.In this and subsequent studies, participants completed mood measures and demographic measures, including pet ownership, gender, age, income level, and ethnicity, at the end of study. Incorporating these variables as covariates did not influence our results (for the exact measures used, see Web Appendix F; for results pertaining to the impact of pet ownership across studies, see Web Appendix G), and thus we do not discuss them further. Study 3aIn Study 3a, we examine our prediction in the context of pet-related decisions: pet toothpaste choice. One hundred eighty-three participants recruited from MTurk completed the study, which featured a two-cell (pet exposure: dog vs. cat) between-subjects design, for a small financial compensation (Mage = 37.4 years; 54.6% female). To manipulate pet exposure (dog vs. cat), under the cover story that we wanted to examine consumers' day-to-day experiences, participants were asked to recall and write down a past experience in which they interacted with a dog or cat (for details of the recall instructions, see Web Appendix H).Afterward, participants were told that a pet toothpaste brand was testing advertisements for its new product and needed their opinions on two ad versions (adapted from Wang and Lee [2006]). Corresponding to their assigned pet exposure condition, participants in the dog (cat) condition viewed dog (cat) toothpaste ads. Ad A, which featured a promotion-focused claim, read, ""Our product helps your dog [cat] freshen breath and strengthen tooth enamel!"" Ad B, which emphasized the prevention-focused benefits of the product, read, ""Our product helps your dog [cat] prevent gingivitis and fight plaque buildup!"" A separate pretest confirmed that Ad A (B) was indeed perceived as more promotion (prevention) focused (Web Appendix H).After viewing the two ads, participants indicated their preference for one of the two ads on three seven-point scales (1 = ""definitely/for sure/certainly Ad A,"" and 7 = ""definitely/for sure/certainly Ad B""). We created a preference index by averaging participants' responses to the three items (α = .99), with higher scores indicating a preference for Ad B, the prevention-focused version.As we expected, a one-way ANOVA revealed a significant effect of exposure to pets on ad preference. Specifically, participants in the dog condition indicated a stronger preference for the promotion-focused ad (Mdog = 4.10, SD = 2.06) than those in the cat condition (Mcat = 4.85, SD = 1.83; F( 1, 181) = 6.78, p = .010,  ηp2   = .04). Study 3bStudy 3b aims to examine our prediction using an incentive-compatible behavior in a pet-unrelated domain. One hundred eighty MTurk workers completed the study, which featured a two-cell (pet exposure: dog vs. cat) between-subjects design, in exchange for a small financial compensation (Mage = 39.5 years; 60% female). We told participants that the study was about people's general knowledge about pets and their past experiences with pets. We randomly assigned them to one of the two conditions (dog vs. cat). Participants first answered five quiz questions about dogs (cats; see Web Appendix I) and then recalled a past experience interacting with a dog (cat) and wrote it down (following the same instructions as in Study 3a).We next told participants that they could participate in a lottery and explained the options they had as follows. If they chose not to participate in the lottery, they would still get paid the initial amount ($.40) as described in the study, so there was nothing to lose. If they chose to participate in the lottery, they had a 50% chance to receive a bonus ($.20) in addition to the base pay; however, they also had a 50% chance to lose half the base pay ($.20). A separate pretest confirmed that the lottery participation (nonparticipation) option was indeed perceived as more promotion- (prevention-) focused (Web Appendix I).Because the promotion (prevention) focus prompts people to focus more on gains (losses) and thus be more risk seeking and open to change (risk averse and status quo oriented) (Liberman et al. 1999; Zhou and Pham 2004), we expected participants who were exposed to dogs to be more likely to participate in the lottery than their counterparts who were exposed to cats. A logistic regression showed a significant effect of pet exposure (cat = 0, dog = 1) on lottery participation, such that participants in the dog condition showed a higher likelihood to take part in the lottery (63.4%) than participants in the cat condition (44.8%; b = .76, SE = .31, χ2 = 6.20, p = .013, Exp (B) = 2.14). Study 3cTwo hundred twenty-five MTurk workers completed Study 3c in exchange for a small financial compensation (Mage = 38 years; 49% female). The study featured the same two-cell (pet exposure: dog vs. cat) between-subjects design and manipulated pet exposure by asking participants to view a series of four print ads, one per screen, that featured either dogs or cats as the spokescharacter (see Web Appendix J) and to provide their thoughts and feelings after viewing the ads. We then measured participants' regulatory orientation using the same financial decision-making task ([65]) as in Study 2.A logistic regression showed a marginally significant effect of exposure to pets (cat = 0, dog = 1) on investment choice, such that participants in the dog condition were more likely to choose to invest in the riskier stock option (29.8%) than participants in the cat condition (18.9%; b = .60, SE = .32, χ2 = 3.57, p = .059, Exp(B) = 1.82). Similarly, a one-way ANOVA revealed a significant effect of exposure to pets on money allocation. Participants in the dog condition allocated more money to the stock option (Mdog = $790.35, SD = 514) than those in the cat condition (Mcat = $613.06, SD = 429; F( 1, 223) = 7.86, p = .005,  ηp2   = .03). Study 3dOne hundred fifty-seven MTurk workers completed Study 3d in exchange for a small financial compensation (Mage = 42 years; 61% female). Study 3d employed the same two-cell (pet exposure: dog vs. cat) between-subjects design. To manipulate pet exposure, participants watched a short video featuring either dogs or cats. Both videos had the same theme—pets ""shopping"" around in a store (see Web Appendix K).After participants watched the video, we presented them with a choice scenario. Specifically, we asked them to imagine that they were buying vitamins and that two brands were available (Zhou and Pham 2004). Brand A was rich in vitamin C and iron and could promote high energy. Brand B was rich in antioxidants and could reduce the risk of cancer and heart disease. A separate pretest confirmed that Brand A (Brand B) was perceived as more promotion- (prevention-) focused (see Web Appendix K).After viewing the two brands, participants then indicated their preference for one of the two brands on three seven-point scales (1 = ""definitely/certainly/for sure Brand A,"" and 7 = ""definitely/certainly/for sure Brand B""). We created a preference index by averaging and reverse coding participants' responses to the three items (α = .99; a higher rating indicating a stronger preference for Brand A, the promotion-focused brand).A one-way ANOVA revealed a significant effect of exposure to pets on brand preference. As expected, participants in the dog condition indicated a stronger preference for the promotion-focused brand (Mdog = 3.96, SD = 2.34) than those in the cat condition (Mcat = 3.19, SD = 2.02; F( 1, 155) = 4.93, p = .028,  ηp2   = .03). DiscussionUsing a variety of methods to manipulate exposure to pets (i.e., pet knowledge, viewing print ads featuring pets, watching a short pet video, and recalling the experience of interacting with a pet), Studies 3a–3d provide converging support for H1 and show that exposure to dogs can lead to behaviors consistent with a promotion-focused mindset, whereas exposure to cats can prompt behavior patterns more aligned with a prevention-focused mindset. Specifically, in Study 3a, consumers preferred the ad with promotion-focused (prevention-focused) benefits for a dog (cat) toothpaste product. In Studies 3b–3d, we extended this finding to pet-unrelated domains. In Studies 3b and 3c, consumers exposed to dogs (vs. cats) were more willing to take risks in their decisions. In Study 3d, exposure to dogs (vs. cats) prompted consumers to prefer a vitamin brand with promotion-focused benefits. These findings provide converging support for our basic prediction that in both pet-related and pet-unrelated contexts, exposure to dogs can activate more of a promotion-focused mindset, whereas exposure to cats can activate more of a prevention-focused mindset.Importantly, in Studies 3a–3d, we found no systematic differences between the dog- and cat-exposure conditions in terms of mood, age, gender, ethnicity, income, and pet ownership. In addition, including these variables as control variables does not change our results anyway; thus, we do not discuss these variables further. Although some stimuli used in these studies may not be completely balanced (e.g., the pet pictures used in Study 3c may differ on certain dimensions, and the energy-boosting benefits of the vitamin seem less consequential than the cancer-risk-reducing benefits of the product in Study 3d), these studies taken together show convergent evidence for our main hypothesis and suggest that the proposed effect is robust across different contexts. Study 4: Moderating Role of Pet StereotypicalityThe primary purpose of Study 4 is to examine the moderating effect of pet stereotypicality on the activation of regulatory-focus mindsets (H2). We predict that making nonstereotypical information (i.e., pets that do not possess the stereotypical characteristics of their species) available to consumers will attenuate the effect of pet exposure on regulatory-focus mindsets. MethodThree hundred eighty MTurk participants (57% female; Mage = 40.3 years) completed the study for a small monetary compensation. Study 4 featured a 2 (exposure to pets: dog vs. cat) × 2 (pet stereotypicality: stereotypical vs. nonstereotypical) between-subjects factorial design.We used a recall task similar to Study 3a. Specifically, we told participants that we were interested in consumers' experience with a pet. Participants then read that some dogs (cats) possess stereotypical characteristics of a dog (cat) and some of them do not. To manipulate pet stereotypicality, in the stereotypical conditions, participants were asked to describe an experience interacting with a dog (cat) that reminds them of the stereotypical characteristics of a dog (cat) (i.e., with personality, temperament, and behavior like a stereotypical dog [cat]). In the nonstereotypical conditions, participants were asked to describe an experience interacting with a dog (cat) that does not have the stereotypical characteristics of a dog (cat) (i.e., with personality, temperament, and behavior unlike a stereotypical dog [cat]). (See Web Appendix L for the detailed manipulation).After the recall task, participants completed the financial decision scenario used in Study 2 and Study 3c. Specifically, participants imagined they had $2,000 and considered investing in two assets: a stock and a mutual fund. They indicated their preference between the two options on a nine-point scale (1 = ""stock,"" and 9 = ""mutual fund""; reverse-coded with a higher rating indicating a stronger preference for the promotion-focused option [i.e., stock]) and then indicated the amount of money they would invest. Results PreferenceA 2 × 2 ANOVA revealed a significant two-way interaction of pet exposure with pet stereotypicality on investment preference (F( 1, 376) = 8.11, p = .005,  ηp2   = .021). Planned contrasts showed that, after pets were described as consistent with their stereotypes, the prior findings were replicated. That is, participants in the dog condition demonstrated higher preference for the stock (Mdog = 4.21, SD = 2.69) than those in the cat condition (Mcat = 3.06, SD = 2.14; F( 1, 376) = 11.72, p <.001,  ηp2   = .03). However, after pets were described as inconsistent with their stereotypes, the prior findings of pet exposure disappeared in that participant did not show preferences for the stock (Mdog = 3.40, SD = 2.47; Mcat = 3.66, SD = 2.36; F( 1, 376) = .53, p = .466). Money allocation to the stockA 2 × 2 ANOVA on money allocation to the stock also revealed a significant two-way interaction (F( 1, 376) = 4.72, p = .031,  ηp2   = .012). Planned contrasts showed that, after pets were described as consistent with their stereotypes, the prior findings were again replicated such that participants in the dog condition allocated more money to the stock option (Mdog = $765.82, SD = 523.89) than those in the cat condition (Mcat = $623.39, SD = 466.25; F( 1, 376) = 4.20, p = .041,  ηp2   = .011). However, after pets were described as inconsistent with their stereotypes, the prior findings of pet exposure disappeared in that the amount of money allocated to the stock option was not statistically different between the dog and cat conditions (Mdog = $606.80, SD = $477.79; Mcat = $688.04, SD = $527.19; F( 1, 376) = 1.14, p = .286). Thus, the results support H2. DiscussionProviding support for our theorizing that associations triggered by pet exposure evoke different regulatory motivational mindsets, Study 4 shows that information related to pet stereotypicality moderates the effect of pet exposure on the activation of regulatory-focus mindsets. Specifically, Study 4 demonstrates that exposing participants to pet information consistent with their stereotype replicated the findings in the previous studies; by contrast, exposure to pets inconsistent with their stereotype nullified the effect of pet exposure on the activation of regulatory-focus mindsets.Having established the basic effect of pet exposure on consumers' regulatory motivational mindsets, in subsequent studies we aim to further examine the downstream effects of pet exposure on consumer behavior, including product evaluations, purchase intentions, and real incentive-compatible behaviors. Specifically, as we predict in H3, which is based on the regulatory fit between pet exposure and ad frames, because exposure to dogs (cats) activates a promotion (prevention) regulatory mindset among consumers, they should form more favorable evaluations and show more purchase intentions of products framed with promotion-focused (prevention-focused) benefits. Study 5: Pet Exposure, Product Feature Frames, and Bidding BehaviorStudy 5 aims to provide evidence for H3, which predicts that there is an interaction between pet exposure and the regulatory focus of an ad on consumers' evaluations of the advertised product by using incentive-compatible behaviors. The study also uses a different ad to further augment the robustness of our findings. Two hundred eighty-three undergraduate students from a large midwestern U.S. university participated in the study for partial course credit (45.9% female; Mage = 20.5 years). Study 5 employed a 2 (pet exposure: dog vs. cat) × 2 (regulatory focus: promotion vs. prevention) between-subject factorial design.Similar to the previous studies, to manipulate pet exposure, under the cover story that we wanted to understand consumers' day-to-day experiences, we first asked participants to recall a past experience in which they interacted with a dog or a cat and to write it down. We then told participants that they would read a message from a local massage center. We varied the message to accentuate either a promotion or a prevention focus (see Web Appendix M). The promotion-focused message emphasized that massages performed by therapists help people increase metabolism, boost immunity, and build a rejuvenated body. The prevention-focused message indicated that massages performed by therapists help soothe body aches, relieve tensions, and reduce stress from school and work. We conducted a separate pretest to confirm the success of our regulatory focus manipulation (see Web Appendix M).Next, we told participants that the local massage center would offer $50 gift cards to several survey participants. They were asked to bid on the gift cards and were told that the top bidders would be contacted later and offered the gift cards at the bidding price (though later the top bidders received the gift cards for free). Participants were then instructed to write down the dollar amount they were willing to bid on a $50 gift card.A 2 × 2 ANOVA on participants' bidding amount revealed only a significant interaction between regulatory focus and pet exposure (F( 1, 279) = 8.91, p = .003,  ηp2   = .03). Planned contrasts showed that after exposure to the promotion-focused version of the ad message, participants in the dog condition bid significantly higher (Mdog = $20.31, SD = $14.57) than those in the cat condition (Mcat = $14.98, SD = $13.20; F( 1, 279) = 4.77, p = .030,  ηp2   = .017). By contrast, after exposure to the prevention-focused version of the ad message, participants in the cat condition placed significantly higher bids (Mcat = $20.51, SD = $15.35) than those in the dog condition (Mdog = $15.67, SD = $13.68; F( 1, 279) = 4.14, p = .043,  ηp2   = .02). DiscussionUsing a behavioral study with an incentive-compatible measure, Study 5 confirmed the robustness of our findings that exposure to pets activates different regulatory mindsets among consumers. After viewing the promotion-focused version of an ad promoting a local massage center, participants who recalled exposure to a dog placed higher bids on the gift card; by contrast, after viewing the prevention-focused version, participants who recalled exposure to a cat placed higher bids. These results provide support for H3. Study 6: Pet Exposure and Product Frames Induce Regulatory Fit and PersuasionThe goal of Study 6 is threefold. First, Study 6 aims to augment robustness for H3 by conceptually replicating the findings of Study 5 using a different context (bidding for a product). Second, Study 6 aims to test H4, which predicts that regulatory fit will mediate the interaction of exposure to pets with ads' regulatory focus on consumer behavior. Third, it uses a new method to manipulate exposure to pets, such that dogs (cats) are directly incorporated into the stimuli as an integral part of the ad message. MethodTwo hundred sixty-four undergraduate students from a large southeastern U.S. university participated in the study in exchange for course credit (Mage = 20.2 years; 52% female). Study 6 featured a 2 (pet exposure: dog vs. cat) × 2 (regulatory focus: promotion vs. prevention) between-subjects factorial design.The experimental procedure was similar to Study 5 except that a new ad with a new product (sneaker) was employed and that pets (dogs or cats) were referenced in the ad. We told participants that they would review a message from a sneaker brand. Dependent on the assigned condition, participants next viewed one of the four versions of the ad (adapted from [22]]). The promotion-focused version of the ad read, ""Be a dog (cat) person! Reach your health goal with eagerness. Our sneakers feature H-Ergy synthetic material, which improves breathability of the shoes and promotes strong support for your feet."" The prevention-focused version of the ad read, ""Be a dog (cat) person! Reach your health goal with caution. Our sneakers feature N-Ergy synthetic material, which is anti-skid and reduces the possibility of foot pain."" We conducted a separate pretest to confirm the success of our regulatory focus manipulation and the believability of the stimuli.Two hundred sixty undergraduate students (Mage = 20.3 years; 56% female) were randomly assigned to one of the four conditions. Participants first indicated the extent to which the advertised sneakers had benefits that could help people attain something positive and the extent to which the advertised sneakers had benefits that could help people avoid something negative (adapted from Mogilner, Aaker, and Pennington [2008]; 1 = ""strongly disagree,"" and 9 = ""strongly agree""). Participants then rated the extent to which the message was reasonable/appropriate/believable as an ad (1 = ""strongly disagree,"" and 9 = ""strongly agree""; α = .91; averaged to form a believability index). The ANOVA on manipulation check measures revealed only main effects, such that the promotion-focused message (Mpromotion = 7.53, SD = 1.19) was deemed as having benefits that helped people attain something positive to a greater extent than the prevention-focused message (Mprevention = 6.71, SD = 1.51; F( 1, 256) = 23.76, p <.001,  ηp2   = .09), and that the prevention-focused message (Mprevention = 6.46, SD = 1.93) was perceived as having benefits that helped people avoid something negative to a greater extent than the promotion-focused message (Mpromotion = 5.56, SD = 2.51; F( 1, 256) = 10.58, p = .001,  ηp2   = .04). A one-sample t-test on the believability index revealed a significant difference against the mid-point of the scale (5; M = 5.40; t(259) = 3.10, p = .002; d = .19) such that participants perceived the ad message they viewed as believable. There was no significant difference on the believability index across conditions (F = .03, n.s.).Next, after revealing that the suggested retail price for the sneakers was $50, we asked participants to bid on the advertised sneakers and told them that the top bidders would be offered the sneakers based on the price they bid. After entering the bidding amount, participants then responded to regulatory fit measures (Lee and Aaker 2004) on two nine-point scales (""It was easy to process the message"" and ""It was difficult to understand the message (reverse coded)""; r = .84) ResultsA 2 × 2 ANOVA on participants' bidding amount revealed only a significant interaction between regulatory focus and pet exposure (F( 1, 260) = 8.38, p = .004,  ηp2   = .03). Planned contrasts showed that after exposure to the promotion-focused version of the ad message, participants in the dog condition bid significantly higher (Mdog= $33.74, SD = $11.81) than those in the cat condition (Mcat = $28.23, SD = $15.61; F( 1, 260) = 5.49, p = .020,  ηp2   = .02). By contrast, after exposure to the prevention-focused version of the ad message, participants in the cat condition placed significantly higher bids (Mcat = $32.45, SD = $13.61) than those in the dog condition (Mdog = $28.40, SD = $12.66; F( 1, 260) = 3.05, p = .082,  ηp2   = .012).A 2 × 2 ANOVA on regulatory fit revealed only a significant interaction between exposure to pets and regulatory focus (F( 1, 260) = 9.80, p = .002,  ηp2   = .036). Specifically, planned contrasts showed that for the promotion-focused ad, the dog version elicited higher regulatory fit (Mdog = 5.21, SD = 2.53) than the cat version (Mcat = 4.19, SD = 2.37; F( 1, 260) = 5.37, p = .021,  ηp2   = .02). By contrast, for the prevention-focused ad, the cat version elicited higher regulatory fit (Mcat = 5.15, SD = 2.40) than the dog version (Mdog = 4.24, SD = 2.62; F( 1, 260) = 4.45, p = .036,  ηp2   = .017).To test the mediation prediction in H4, we conducted a moderated mediation analysis using 5,000 bootstrapped samples (PROCESS Model 8; [24]), with exposure to pets as the independent variable, regulatory fit as the mediator, regulatory focus as the moderator, and bidding amount as the dependent variable. The index of moderated mediation was significant (b = 1.68, 95% confidence interval [CI]: [.30, 3.70]). Specifically, for the promotion-focused ad, the conditional indirect effect of pet exposure on bidding amount through regulatory fit was positive and significant (b = .89, 95% CI: [.05, 2.11]). By contrast, for the prevention-focused ad, the conditional indirect effect of pet exposure on bidding amount through regulatory fit was negative and significant (b = −.80, 95% CI: [−2.05, −.003]). Thus, the data support H4. DiscussionUsing a new method (featuring a dog/cat as an integral part of the ad) to manipulate exposure to pets, we provide further evidence for our theorizing of an interactive effect between exposure to pets and regulatory focus of an ad on consumer responses. For sneaker ads featuring promotion-focused (prevention-focused) claims, consumers exposed to dogs (cats) formed higher bidding amount than those exposed to cats (dogs). Importantly, the findings of Study 6 also provide support for H4, such that the influence of exposure to pets on bidding amount was mediated by the regulatory fit between the activated regulatory mindset and the regulatory focus of the ad claim. We also conducted an additional study to conceptually replicate this study in the financial decision-making context (for details, see Web Appendix P). Study 7: Pet Exposure and Product Frames Induce Regulatory Fit and ChoiceStudy 7 has two objectives. First, it aims to lend additional support to H3 and H4 using a within-subject design. Second, to augment the robustness of our findings, we employ a different product category (toothpaste) and a new manipulation of pet exposure (pet pictures). MethodTwo hundred thirty-seven undergraduate students from a large southeastern U.S. university completed the study in exchange for partial course credit (Mage = 20 years; 45% female). The study featured a 2 (pet exposure: dog vs. cat) × 2 (products' regulatory focus: promotion- vs. prevention-focused) mixed ANOVA design, with pet exposure a between-subjects factor and products' regulatory focus a within-subjects factor.Participants were randomly assigned to one of two conditions (pet exposure: dog vs. cat). We told participants that an online calendar company was interested in people's feedback on several dog (cat) pictures that it planned to incorporate into a dog- (cat-) themed calendar. Participants then were shown a series of dog (cat) pictures, one on each screen (order counterbalanced). Afterward, participants were again shown all the pictures they had seen on one screen and were instructed to pick one of the pets to imagine interacting with. (For the stimuli used, see Web Appendix N.)Next, in an ostensibly different task, we presented participants with the descriptions of two toothpaste products (Wang and Lee 2006): Toothpaste A had strong promotion but weak prevention product claims, and Toothpaste B had strong prevention but weak promotion product claims (see Web Appendix O). We counterbalanced the order of the two toothpaste products across all participants. We then asked participants to evaluate each of the two products on four nine-point scales (1 = ""dislike very much/very unfavorable/very unattractive/very bad,"" and 9 = ""like very much/very favorable/very attractive/very good"").Afterward, we presented participants with all the strong feature claims and asked them to evaluate each of the features on a nine-point scale (1 = ""not at all attractive,"" and 9 = ""very attractive""). The features were evaluated as generic features of the product category, rather than as the features of a specific brand, and served as measures of regulatory fit (Wang and Lee 2006). That is, if participants relied more on (promotion or prevention) features that fit their regulatory orientations in their evaluation, they should find strong feature claims consistent with their regulatory orientations more attractive than claims that do not fit their orientations. ResultsWe averaged participants' evaluations of the toothpaste products on the four items to form a brand attitude index for each product (αA = .94, αB = .95). We expected that participants assigned to the dog exposure condition would evaluate Toothpaste A (with the strong promotion claims) more favorably than those assigned to the cat exposure condition. By contrast, participants assigned to the cat exposure condition would evaluate Toothpaste B (with the strong prevention claims) more favorably than those assigned to the dog exposure condition. A mixed ANOVA with toothpaste attitudes as the within-subject variable and pet exposure as the between-subjects variable first revealed a significant main effect of toothpaste attitudes (F( 1, 235) = 4.47, p = .036,  ηp2   = .02), such that, overall, participants evaluated Toothpaste A (with the strong promotion claims; M = 6.94) more positively than Toothpaste B (with the strong prevention claims; M = 6.61). Importantly, the predicted interaction also emerged (F( 1, 235) = 29.76, p <.001,  ηp2   = .11). We found that participants who were exposed to dogs evaluated Toothpaste A more positively (M = 7.41, SD = 1.25) than Toothpaste B (M = 6.29, SD = 1.74, t(235) = 5.74, p <.001; d = .52). By contrast, participants who were exposed to cats formed more positive attitudes toward Toothpaste B (M = 6.94, SD = 1.63) than Toothpaste A (M = 6.45, SD = 1.73, t(235) = −2.21, p = .029; d = –.21).We then analyzed participants' feature attractiveness ratings, which served as our regulatory fit measure. Because both feature type and toothpaste were measured within subject, we calculated a relative feature attractiveness index by dividing participants' attractiveness ratings of promotion features by their ratings of prevention features. With the index as the dependent variable, a one-way ANOVA revealed that participants in the dog exposure condition (Mdog = 1.27, SD = .83) perceived the promotion features as more attractive than participants in the cat exposure condition (Mcat = 1.09, SD = .35; F( 1, 235) = 4.66, p = .032,  ηp2   = .02).To further investigate the mediating role of regulatory fit, we conducted mediation analyses to examine whether perceived attractiveness of the product features mediated the effect of pet exposure on product evaluation. Given the within-subjects design, we used MEMORE macro developed by Montoya and Hayes (2017). With 5,000 bootstrapped samples, the analysis revealed a significant indirect effect of feature attractiveness (b = .41, SE = .09, 95% CI: [.23,.59]), indicating that regulatory fit mediated the effect of pet exposure on product evaluation. DiscussionUsing a within-subject design and a new product category for the dependent variable, Study 7 conceptually replicates the previous studies. Moreover, it provides additional support for H3 in that exposure to pets interacted with an ad's regulatory focus to influence consumer responses and for H4 in that such an effect is driven by regulatory fit. General DiscussionAcross 11 studies, employing different methods (secondary data, lab experiments, and real behavior), different operationalizations of pet exposure (pet ownership; viewing pet-featuring pictures, videos, or ads; and recalled experience with pets), and different measures and contexts of regulatory focus (e.g., financial, service, consumer products), we find converging evidence that pet exposure influences consumer judgments and behaviors through regulatory focus and regulatory fit. Specifically, we show that pet exposure fosters divergent regulatory orientations among consumers, such that exposure to dogs activates a promotion-focused motivational mindset while exposure to cats activates a prevention-focused motivational mindset (Studies 1–3). In Study 4, we further show that this effect is moderated by pet stereotypicality, such that the effects of pet exposure on regulatory mindsets dissipate when consumers are reminded of a pet that is inconsistent with the stereotypes of the species. These regulatory mindsets, when activated by pet exposure, carry over to influence downstream pet-unrelated consumer judgments, purchase decisions, and behaviors through regulatory fit (Studies 5–7), even in pet-unrelated consumption contexts. Theoretical ContributionsOur research contributes to the literature in several ways. First, it recognizes pets as an important source of social influence on consumers' judgments and behaviors (Cavanaugh, Leonard, and Scammon 2008; Hirschman 1994; Holbrook and Woodside 2008). Prior research on marketplace social influence has examined the contexts of consumer-to-consumer and marketer-to-consumer interactions (e.g., Argo, White, and Dahl 2006; Chan and Sengupta 2010; Duclos, Wan, and Jiang 2013; Lee and Shrum 2012; Mead et al. 2011; White and Argo 2011; White and Dahl 2006, [75]). As examples of consumer-to-consumer interactions, Duclos, Wan, and Jiang (2013) show that social exclusion prompts riskier financial decisions because interpersonal rejection heightens the instrumentality of money to obtain benefits in life; Lee and Shrum (2012) find that social exclusion can lead to conspicuous consumption or prosocial behavior. As an example of marketer-to-consumer interactions, Chan and Sengupta (2010) find that consumers who receive insincere flattery from marketers still form favorable implicit evaluations of the marketer, but their explicit evaluations of the marketer are negative. Beyond these human-to-human contexts, our research suggests that the context of pet–human interactions as a source of social influence can similarly affect consumers' motivations, judgments, and behaviors.Second, our research diverges from extant literature on human–animal relationships by going beyond the immediate context of pet ownership and investigating how pet exposure affects the way people render subsequent pet-unrelated judgments and decisions. Prior research on the effects of human–animal relationships has mainly focused on the immediate context of pets and their owners, such as pet owners' health and psychological well-being ([ 2]), their awareness and protection of animal rights (Kidd, Kidd, and Zasloff 1995), and their relationship satisfaction with pets (Cavanaugh, Leonard, and Scammon 2008). Going beyond this immediate context between pets and their owners, we posit and find that exposure to pets or pet-featuring stimuli fosters divergent regulatory orientations, which in turn influence downstream pet-unrelated judgments and decisions in the consumption domain. We hope that this research will stimulate further research to gain a more nuanced understanding of the impact of human–animal relationships.Third, prior research identifies several antecedent variables of regulatory focus, such as one's cultural background (Lee, Aaker, and Gardner 2000), impulse purchase ([65]), and choosing for oneself versus others ([59]). Most of these factors, however, are intrapersonal. Only recently have researchers begun exploring the interpersonal drivers of regulatory focus (e.g., social exclusion [Park and Baumeister 2015], reminders of friends vs. family members [Fei, You, and Yang 2020]). Contributing to this stream of research, we uncover a novel social source of regulatory orientation: exposure to pets. We find that dog exposure is associated with a promotion orientation, whereas cat exposure is associated with a prevention orientation. Additional research is necessary to further explore the social and interpersonal impact on people's regulatory orientation. Practical ImplicationsOur findings also offer novel implications to marketers. First, marketers should consider crafting their advertising messages differently or recommending different products and services when they target consumers depending on their pet exposure situations. For example, to enhance the effectiveness of their advertising appeals or communication messages, marketers should emphasize promotion-focused goals such as gains and nongains if they are targeting dog owners or after consumers are exposed to dogs or dog-featuring stimuli (e.g., after just watching an ad about dogs). Conversely, they should focus on prevention-focused goals such as losses and nonlosses if they are pursuing cat owners or after consumers who are exposed to cats or cat-featuring stimuli. Importantly, our findings show that this advice holds even when the advertised product or service has nothing to do with pets or pet products.Second, our findings offer important insights into how to incorporate pets into marketing communications. Dogs and cats frequently appear in advertisements and marketing campaigns. For example, Subaru has been running the ""Dog Tested, Dog Approved"" campaign since 2009 (Subaru 2020), and Sainsbury's ""Mog the Cat"" campaign raised a great amount of attention during the 2015 Christmas season ([25]). One consideration factor, according to our findings, is the type of products or services being advertised. For products or services mainly perceived as promotion-focused (e.g., stock investment, sports cars), featuring dogs in the ad is likely to increase the ad's persuasiveness. For products or services deemed more prevention-focused (e.g., mutual fund investment, insurance), featuring cats may increase the ad's appeal. According to the findings of the pet stereotypicality study, a caveat is that marketers should ensure that stereotypical pet temperaments are made salient in the message (e.g., the eagerness [cautiousness] aspect of the dog [cat] should be highlighted). Otherwise, the intended effects of featuring pets in the ad may not be achieved.Third, pet-related marketing strategies are especially relevant in today's big-data era, in which marketers likely know more about consumers, including which types of pets they own or interact with. For example, marketers could obtain consumers' pet exposure information from the pet-related products purchased, the YouTube pet videos watched, or the Instagram pet photos posted and use this data to decide what type of marketing information to highlight. Alternatively, census information can provide marketers with an aggregated level of information about pets, as certain states or zip codes may have a higher concentration of dog (cat) owners. Marketers could use this information to determine the design of regional marketing campaigns.Lastly, our findings that pets and pet ownership are potentially related to COVID-19 transmission rate and prevention behaviors could shed new light on policies related to prevention of COVID-19 and potentially other infectious diseases. For example, policy makers in states with more dog owners could design more customized educational programs and materials related to the diseases. Alternatively, when designing ads to prevent the transmission of COVID-19 and other infectious diseases, cats could be incorporated as a spokesperson and/or their temperaments can be referenced in the message to enhance the effectiveness of the ad. Future Research DirectionsThis research indicates that pets can exert a strong social influence on consumers. Future research could examine the strength of the influence of pets versus people on consumer behavior. On the one hand, one may argue that pets' influence on consumers is weaker than other human influences, because people are more influenced by similar others ([ 3]; Bandura 2002). On the other hand, it is also possible that pets may occasionally exert a stronger influence, especially considering that many pet owners admit that they prefer spending time with their pets over other people ([56]).Another possible research direction is to examine other differences between dog and cat exposure. For example, exposure to dogs may be more likely to lead to conspicuous consumption than cat exposure. This follows because many people likely perceive dogs (vs. cats) as less seclusive and more social. Along similar lines, research could examine the different effects of dog versus cat stimuli. For example, using a dog (cat) as a spokescharacter can increase a brand's perceived excitement. Although our research focuses on examining the effect of pet exposure on consumers' regulatory orientations, future research could investigate the reverse relationship of whether promotion- (prevention-) focused consumers are more likely to adopt a dog (cat) or prefer dog- (cat-) related stimuli.Research could also examine additional moderators for our findings. One example is pet anthropomorphism, or people's tendency to assign human characteristics to pets ([ 2]). One prediction is that our findings that dog (cat) exposure evokes a promotion (prevention) orientation would be more salient among consumers who engage in pet anthropomorphism, because these consumers are more likely to be influenced by the pets. Another possible moderator is the role of culture. We conducted our studies primarily with American participants, and the United States is a culture in which the majority of people treat their pets as friends and family members (Sanders and Hirschman 1996). In many other cultures however, pets are not elevated to the same level, and thus consumers may treat their pets as possessions or servants ([ 9]). Future research could examine whether our identified findings still hold in such cultures. Supplemental Materialsj-pdf-1-jmx-10.1177_00222429221078036 - Supplemental material for The Pet Exposure Effect: Exploring the Differential Impact of Dogs Versus Cats on Consumer MindsetsSupplemental material, sj-pdf-1-jmx-10.1177_00222429221078036 for The Pet Exposure Effect: Exploring the Differential Impact of Dogs Versus Cats on Consumer Mindsets by Lei Jia, Xiaojing Yang and Yuwei Jiang in Journal of Marketing  "
20,"Why Salespeople Avoid Big-Whale Sales Opportunities Contrary to the intuition that salespeople gravitate toward big-whale sales opportunities, in reality they often avoid them. To study this phenomenon, the authors integrate contingent decision-making and conservation-of-resources theories to develop and test a framework of salespeople's decision making when prospecting. Study 1 reveals that the performance impact of salesperson initial judgment of opportunity magnitude follows an inverted U-shape, indicating that salespeople's avoidance of large opportunities results from rational benefit–cost analyses due to their conservation of resources. Interestingly, salespeople use a calibration decision-making strategy (i.e., calculating expected benefits by accounting for conversion uncertainty) at the portfolio rather than prospect level, in solution- but not product-selling contexts. Ignoring this calibration effect can lead to under- or overestimation of conversion rates of up to 100%. Study 2 shows that salespeople's past performance success and experience bias this calibration. Simulations reveal that when high performers or inexperienced salespeople believe their portfolio magnitude is large and conversion uncertainty low, they are less concerned about resource conservation and improve their quota attainment by 50%. Study 3 confirms the theoretical mechanism. These findings shed new light on salespeople's decision making and suggest ways for sales professionals to improve effectiveness when prospecting.Keywords: salesperson judgment; uncertainty; solution selling; prospecting; conservation-of-resources theoryCentral to a firm's customer acquisition is salesperson prospecting, which involves identifying sales opportunities among potential customers. Prior research on salesperson prospecting has underscored its importance not only for firms' customer relationship management (CRM) but also for salesperson performance (e.g., [48]). However, more than 40% of salespeople report that prospecting is challenging and full of uncertainty, taking on average 25% of their time ([ 8]). Whereas some practitioners emphasize the pursuit of large prospects because these ""big whales"" help firms and salespeople achieve rapid sales growth, others warn against prioritizing such prospects because they can easily drain salesperson and company resources ([32]). Moreover, ""in the time it takes to land one major deal, [the salesperson] could have closed five smaller deals"" ([19]). Although practitioners appear to recognize salespeople's benefit–cost trade-offs when prospecting, academic research has not systematically examined this important phenomenon.Marketing research on salesperson prospecting has developed along two major streams. One stream focuses on salesperson judgment of market opportunities, such as market demand for a new brand, customer needs, and expected performance (e.g., [27]; [31]; [66]). This research stream shows a linear positive relationship between customer demand judgment and salesperson performance. The other stream emphasizes the role of salespeople as decision makers in dealing with various types of uncertainty, such as salespeople's general risk aversion, context-specific uncertainty, or salesperson idiosyncratic characteristics (e.g., [ 2]; [10]; [41]; [55]; [60]). Although these research streams provide useful insights into the information salespeople use in their decision making, three important research gaps exist.First, when prospecting, salespeople typically identify multiple potential opportunities but pursue only some of them. However, research is scant on the potentially curvilinear impact of salesperson judgment of opportunity magnitude—defined as a salesperson's judgment of the size of an opportunity—on sales performance. Although anecdotal evidence suggests that salespeople focus on large opportunities, other sources allude to major drawbacks in pursuing them. Opening this black box can be useful for improving companies' prospecting effectiveness. Second, there is a lack of understanding of how conversion uncertainty affects salespeople's decision making when prospecting. A focus on conversion uncertainty is important, because prospecting is costly for the firm and for salespeople. Third, little is known about how such decision making varies between salespeople and selling contexts. Knowledge of these contingencies help sales managers to effectively manage salesperson prospecting behavior.To address these gaps, we seek answers to three key questions. First, what is salespeople's benefit–cost trade-off after they form an initial judgment of opportunity magnitude, and how does this affect their sales performance? The focus on initial judgments is based on prior research that underscores the importance of a primacy effect in both decision making and salesperson–customer interactions ([17]; [27]). Second, how does a salesperson's initial judgments of opportunity conversion uncertainty change the sales performance outcome of the benefit–cost analysis? Given that salespeople's compensation generally depends on conversion, understanding the effect of opportunity conversion uncertainty, or a salesperson's initial judgment of the likelihood to convert an opportunity into a deal, is important. Third, what are important boundary conditions of the effects of these initial judgments? We focus on two sets of moderators: ( 1) the selling context (i.e., product vs. solution selling) and ( 2) key salesperson characteristics (i.e., past performance success and salesperson experience). In doing so, we also explore the role of information level (i.e., prospect and portfolio levels) in salesperson decision making.To answer our questions, we develop and test a contingency framework of salespeople's decision making when prospecting for market opportunities in a sequence of three studies. For theoretical foundation, we integrate research on contingency decision making ([46]) and conservation of resources (COR) ([30]). We augment these theories with field notes from in-depth interviews with sales professionals. While Studies 1 and 2 rely on multisource field data, Study 3 is a scenario-based experiment to provide evidence of the benefit–cost analysis as the underlying mechanism. Together, this multimethod approach allows us to rigorously triangulate the effects and unpack the theoretical mechanisms.This research makes several contributions. First, we contribute to the emerging literature on salesperson judgment and decision making when prospecting by unpacking the underlying decision process. We provide theoretical arguments and strong empirical evidence that explains why salespeople avoid big-whale prospects. Specifically, we show that, based on their initial judgment of opportunity magnitude, salespeople conduct a benefit–cost analysis under resource constraints to decide which opportunity to pursue. This analysis results in an inverted U-shaped relationship between magnitude and performance. Spotlight analyses show that a one-standard-deviation (SD) increase in opportunity magnitude lowers salespeople's conversion rate by 10%.Second, we provide insights into the effect of conversion uncertainty on the salesperson decision-making process when prospecting. The results show that when selling solutions, salespeople use a calibration decision-making strategy, in which the effects of opportunity magnitude are conditional on conversion uncertainty. However, this strategy occurs only at the portfolio level, underscoring the role of salesperson portfolio as a decision-making context. Ignoring the calibration effect in estimating performance outcomes may lead to under- or overestimation of conversion rates of up to 100%. When selling products, salespeople use a compensatory decision-making strategy that accounts for the effects of portfolio magnitude and conversion uncertainty in an additive manner. These findings extend prior work (e.g., [60]) on uncertainty in personal selling and salesperson decision making.Third, we provide empirical evidence for how, in a solution-selling context, the calibration effect varies depending on salesperson past performance success and experience. The results suggest that when faced with high levels of conversion uncertainty, high performers and inexperienced salespeople perform much worse because their resource-conserving tendency makes them more sensitive to the cost increases associated with uncertainty. Simulations reveal that their quota attainment can suffer by as much as 50%. These insights extend prior research focusing on the salesperson–customer dyad in business-to-business (B2B) marketing and retail encounters (e.g., [27]; [43]). Background Literature and Conceptual FrameworkSalespeople are generally assigned to a territory or a customer segment, and their sales opportunities can be self-generated or assigned ([48]). Within a given period, they move these sales opportunities through a funnel from prospects to closed sales deals. At any given time, salespeople form a judgment of the magnitude of specific sales prospects, with a certain level of conversion uncertainty. Prior research on salesperson prospecting provides useful insights into why salespeople fail to follow sales leads, how their judgment of opportunities linearly influences their performance, and how they deal with uncertainty. However, it has not examined why and when salespeople pursue or avoid big opportunities. To shed light on these issues, we view salesperson prospecting as decision making under resource constraints. In this section, we first briefly review the relevant literature and then present our conceptual framework. Prospecting as Decision Making Under Resource Constraints Decision-making frameworksTwo major decision-making frameworks are the benefit–cost framework ([ 6]) and perceptual frameworks, such as prospect theory ([59]). In their review of these two frameworks, [46] posit that the former provides insights into rational decision making under multiple alternatives while the latter is useful in explaining cognitive biases and heuristics in decision making. In their review, they also underscore task and individual characteristics, such as willingness to bear uncertainty, as important contingencies of individual decision making. COR theoryUnlike the majority of general decision-making frameworks that assume away any resource constraint, COR theory emphasizes that people ""strive to retain, protect, and build resources and that what is threatening to them is the potential or actual loss of these valued resources"" ([30], p. 513). Furthermore, people must invest resources to gain resources, and those who experience a lack of resources attempt to conserve remaining resources ([26]). We argue that COR theory is particularly relevant in the context of B2B salespeople's prospecting for three reasons. First, unlike simple, low-effort choices between two lotteries, the pursuit of a prospect is costly—salespeople need to invest their time, effort, and resources in converting prospects into sales ([48]). Second, uncertainty in prospecting brings salespeople's resource constraints to the fore. Unlike gambling, which can be replayed, a forgone sales opportunity might be gone for good, and a failure to convert opportunities represents a loss of resources. Thus, salespeople need to balance between risk seeking and COR. Third, salespeople differ in terms of resource constraints ([48]). Contingency Framework of Salesperson Decision Making When ProspectingWe integrate decision-making frameworks with COR theory to propose a contingency framework of salesperson decision making when prospecting. Our framework focuses on the initial judgments of sales opportunities in terms of magnitude and conversion uncertainty. First, although salespeople encounter multiple market opportunities, they only invest their resources into converting some of them. The benefit–cost framework suggests that this decision is based on rational benefit–cost analyses of opportunity magnitude before action ([ 6]). COR theory offers a similar explanation that the pursuit of an opportunity is a trade-off between resource acquisition (e.g., the expected benefits of a sale) and resource conservation (e.g., the costs of resources expended on pursuing the opportunity). Second, salespeople make this decision under uncertainty. In line with contingency decision-making frameworks and COR theory, we expect that salespeople's benefit–cost analysis of opportunity magnitude is contingent on conversion uncertainty. This is because uncertainty prevents action by obfuscating ""whether the potential reward of action is worth the potential costs"" ([40], p. 139; see also [26]; [46]).Third, task and personal factors represent additional contingencies that distort the rational benefit–cost analyses. We focus on two sets of contingencies. The first is the selling context (i.e., product vs. solution selling), in which a product denotes a physical object that can be sold in a transactional way (e.g., lamps) and a solution refers to a product-service system (e.g., smart lighting) that requires a relational process and tailoring. Solution selling represents a more uncertain task than product selling ([57]; [60]). Examining the selling context is important because many firms that shift from product to solution selling often struggle to cope with the inherent greater uncertainty (e.g., [18]; [60]). The second set includes two salesperson characteristics related to the propensity to conserve resources under uncertainty. Past performance success refers to salesperson quota attainment in the previous quota period. Salesperson experience refers to a salesperson's time in the sales territory, with the company, and in the sales profession ([ 2]). We focus on these two moderators because prior research suggests these factors are related to how salespeople deal with uncertainty and conserve resources ([26]; [30]; [48]). Overview of StudiesTo test our conceptual framework, we conducted three studies using multiple methods. We provide an overview of our conceptual framework, hypotheses, and the studies in Figure 1. Study 1 focuses on how salesperson initial judgments of opportunity magnitude determine the actual conversion of a prospect into a sale, which in the aggregate influences the salesperson conversion rate at the portfolio level. In doing so, we also investigate how salespeople calibrate opportunity magnitude for opportunity conversion uncertainty and whether such calibration differs between product and solution selling. In Study 2, we examine the heterogeneity of such calibration effect, with a focus on salespeople's past performance success and experience. The dependent variable in Study 2 is salesperson quota achievement, which is theoretically connected with the conversion rate examined in Study 1. In Study 3, a scenario-based experiment, we elucidate the underlying benefit–cost mechanism and the role of resource slack. Table 1 summarizes the key concepts in our framework and corresponding operational measures.Graph: Figure 1. A contingency framework of salesperson decision making when prospecting and overview of three studies.GraphTable 1. Overview of Key Concepts and Operationalizations in Studies 1 and 2. Key ConceptsDescription and Conceptual MeaningConceptual FoundationsOperationalizationStudy 1Study 2Representative StudiesPerformance OutcomeSales performanceDegree to which the salesperson obtains a desired outcomeAhearne et al. (2010)Prospect-level performance: A binary measure, where 0 reflects no deal and 1 reflects that a deal has been made (objective likelihood of conversion)✓Smith, Gopalakrishna, and Chatterjee (2006); Mayberry, Boles, and Donthu (2018)Portfolio-level performance: The ratio of prospects that are successfully turned into deals in a salesperson's portfolio (objective conversion rate)✓Own operationalizationSalesperson performance: Percentage of quota attainment✓Ahearne et al. (2010)Initial Cues and Judgment FormationOpportunity magnitudeThe size of a potential sales optionKumar, Petersen, and Leone (2013)Prospect magnitude: Log-transformed salesperson initial judgment of revenue for a prospect (i.e., deal magnitude in $)✓Mayberry, Boles, and Donthu (2018)Portfolio baseline magnitude: The mean of all prospects' magnitude in a salesperson' portfolio✓Own operationalizationPortfolio magnitude: Reflective four-item construct capturing a solution-selling salesperson's initial estimates in terms of the size of order intake, sales volume, revenue, and profits of their entire portfolio.✓Van der Borgh, De Jong, and Nijssen (2019)Opportunity conversion uncertaintyThe subjective likelihood of being able to convert an opportunity into a desired sales outcomeMcMullen and Shepherd (2006)Prospect conversion uncertainty: Categorical measure differentiating among high, medium, and low levels of likelihood to convert a prospect into a paying customer within six months (−1, 0, and 1), based on salesperson initial judgment.✓Own operationalizationPortfolio baseline conversion uncertainty: The average of prospect conversion uncertainty across all the prospects in a salesperson's portfolio✓Own operationalizationPortfolio conversion uncertainty: Reflective four-item construct capturing a salesperson's initial judgment of uncertainty for realizing anticipated outcomes for solution selling for their entire portfolio (in terms of size of order intake, sales volume, revenue, and profits)✓Own operationalizationContingenciesSalesperson characteristicsDifferences in motivation, attitude, or risk propensity that determine whether a salesperson is willing to bear uncertainty or notMcMullen and Shepherd (2006); Payne, Bettman, and Johnson (1992)Past performance success: The percentage of quota achieved in the previous quota cycle✓Mayberry, Boles, and Donthu (2018)Salesperson experience: Composite measure consisting of three measures of sales experience (i.e., time in sales territory, time with the company, and time in the sales profession). We z-scored these scores and averaged them to form an overall experience index.✓Ahearne et al. (2010)Task characteristicsVarious dimensions, descriptors, or attributes of a particular organizational position that determine task execution and/or outcomesPayne, Bettman, and Johnson (1992)Binary measure indicating whether a prospect requires product selling or solution selling✓Mayberry, Boles, and Donthu (2018)Information levelsDenotes the reference class of judgments, distinguishing between judgment about the specific case or the aggregate of multiple casesSniezek and Buckley (1995)Data are separated into information at the single case level (prospect information) and aggregate level (portfolio baseline information)✓Own operationalization  Study 1: Understanding the Interplay Between Opportunity Magnitude and Conversion UncertaintyStudy 1 examines the interaction effect between opportunity magnitude and conversion uncertainty in product- and solution-selling contexts at both the prospect and portfolio levels. We supplement our theoretical development for this study with verbatim quotes from a qualitative study of seven salespeople (for a description of respondents, see Web Appendix W1). Study 1 Hypothesis Development Benefit–cost analysis of opportunity magnitudeWe predict that the effect of opportunity magnitude on salesperson performance follows an inverted U-shaped relationship. This is due to two countervailing underlying mechanisms: a linear positive effect from potential benefits of pursuing an opportunity and a curvilinear effect from potential costs of such pursuit. We follow [24] recommendation to visually summarize this benefit–cost analysis in Figure 2, Panel A. On the one hand, the higher the magnitude of the opportunity, the greater the extrinsic and intrinsic benefits of pursuing a sizable opportunity. Extrinsic benefits take the form of potential compensation and recognition from the selling firm, whereas intrinsic benefits include the potential enjoyment in pursuing sizable opportunities and the learning associated with the pursuit ([ 7]; [15]).Graph: Figure 2. Illustration of theoretical arguments for the inverted U-shaped effect and the calibration effect.On the other hand, pursuing a sizable opportunity entails substantial explicit and implicit costs. Salespeople incur explicit costs when pursing an opportunity because they need to invest resources, such as time and effort ([48]). Implicit costs refer to the opportunity costs of such pursuit because when pursuing an opportunity, salespeople must forgo other opportunities ([54]). As the opportunity magnitude increases, both explicit and implicit costs accelerate significantly because salespeople are constrained by limited resources and information-processing capacity ([30]; [48]; [55]). One senior salesperson of a software company explained this issue succinctly:Big opportunities? Um, the pros. The prospect of earning a load of money.... Second, obviously it's also more satisfying or fulfilling.... So, it's more complex, which is also like a nice challenge. Plus, you learn the most from complex deals and bigger deals.... But it costs a lot of time and whatever time you spend on one deal you cannot reinvest anymore in smaller deals. So, there's always a trade-off.Therefore, when both potential benefits and costs are considered, the effect of opportunity magnitude on salesperson performance will incrementally increase at first, but after a threshold, the costs to act on moderate to large opportunities outweigh their benefits. Thus, H1:  All else being equal, salesperson judgment of opportunity magnitude has a curvilinear, inverted U-shaped effect on salesperson performance. Opportunity magnitude–conversion uncertainty calibrationBy itself, conversion uncertainty can be a source of benefits for salespeople because uncertainty stimulates positive feelings and excitement ([50]). However, conversion uncertainty also increases costs of pursuing—both explicit costs (i.e., salespeople need to exert greater effort to convert highly uncertain opportunities) and implicit costs (i.e., highly uncertain opportunities carry higher opportunity costs). Under the compensatory decision-making strategy, salespeople assess the benefits and costs of opportunity magnitude and conversion uncertainty in an additive manner. However, our interviews suggest that salespeople at times calibrate for conversion uncertainty in a multiplicative rather than additive manner. For example, one solution-selling salesperson was very clear on his decision-making strategy to deal with conversion uncertainty:When I take a look at a deal that has a very high certainty, so say you've a 90% closing chance, but it's very small in size, and you have a very big deal that has a small closing chance. Yeah, I can just multiply it [size by uncertainty] and see where I get the most buck for my uncertainty, so to speak. Even though it's very simple, it's pretty effective.We refer to this multiplicative strategy as the calibration hypothesis, such that the inverted U-shaped relationship between opportunity magnitude and salesperson performance in H1 is contingent on conversion uncertainty ([40]). We again follow [24] recommendation to visually summarize how conversion uncertainty influences the benefit and cost functions in our arguments in Figure 2, Panel B.In terms of the benefit function, the (previously noted) solution-selling salesperson's calculus is consistent with both expectancy theory and COR theory ([30]; [64]). These theories suggest that salespeople calculate the expected benefits of an action by multiplying its benefits by the success odds (i.e., expected benefits = magnitude-based benefits × conversion uncertainty). Thus, when conversion uncertainty is high, the expected benefits of pursuing a large opportunity are lower, making it less motivating to pursue. Importantly, this calculus is universal, without evoking any individual characteristics as contingencies. Therefore, conversion uncertainty weakens the slope of the benefit line, shifting the inverted U-shaped effect of opportunity magnitude on salesperson performance to the left.How do salespeople calibrate for conversion uncertainty in assessing the costs of pursuing an opportunity? As mentioned previously, conversion uncertainty can be positively stimulating for risk seekers but harmfully costly for people who want to conserve resources. In this regard, prior research indicates that salespeople are heterogeneous in their risk-seeking behavior for various reasons, such as their past performance success and their capability (e.g., [42]). Given this heterogeneity, the cost function can swing in either direction, and thus we predict that, in the aggregate, opportunity conversion uncertainty may appear as not having an influence on the cost function. For the leftward shifting effect to occur, opportunity conversion uncertainty only needs to shift the benefit function downward and does not need to change the shape of the cost function ([24]). Thus, H2:  Opportunity conversion uncertainty moderates the effect of opportunity magnitude on salesperson performance, such that it shifts the inverted U-shaped effect of opportunity magnitude on salesperson performance to the left. Solution- versus product-selling taskPrior research on decision making suggests that, under high levels of situational uncertainty, people search for a relevant reference class to calibrate their judgments ([23]; [33]). However, [60] suggest that, beyond outcome uncertainty, such as conversion uncertainty, solution selling has a higher level of need and process uncertainty than product selling. We argue that it is this difference in overall situational uncertainty that causes salespeople to calibrate for conversion uncertainty differently when selling products versus solutions. Specifically, because need and process uncertainties are higher in solution selling, salespeople do not have a reliable frame of reference to count on. By contrast, because need and process uncertainties are lower in product selling, salespeople can confidently draw from their knowledge of customer needs and requirements, the sales process, and product configurations to deal with conversion uncertainty. Therefore, compared with product-selling salespeople, solution-selling salespeople are more sensitive to conversion uncertainty and tend to calibrate for this uncertainty more intensely. Thus, we expect the weakening effect of conversion uncertainty on the benefits function predicted in H2 to be stronger in solution- than product-selling contexts. H3:  The leftward shifting effect of opportunity conversion uncertainty on the inverted U-shaped relationship between opportunity magnitude and salesperson performance is stronger in solution- than product-selling contexts. Institutional ContextWe collected data from a Fortune Global 500 firm, a market leader in lighting products and solutions for enterprise customers; at the time of data collection, the company generated more than $25 billion annually in total revenue. The company provides a broad portfolio of lighting offerings, ranging from products (e.g., luminaires, lighting electronics, horticulture lighting) to system solutions (e.g., connected, smart luminaires; lighting management software). Customers come from various industries, such as food and fashion retail, health care, education, sports, municipalities, hospitality, infrastructure, and manufacturing. For its field-based sales approach, the company relies primarily on direct sales, and salespeople are subject to the same compensation and incentive scheme. Salespeople obtain a fixed yearly salary plus commission (maximum 30% of the fixed salary). To explore the impact of initial judgments of opportunity magnitude and conversion uncertainty, we gathered archival data from the company's sales force automation (SFA) system for all prospects within one market. For every prospect, we obtained transaction-level records from January 2016 to May 2017, including initial estimates of opportunity magnitude (i.e., prospect deal size) and conversion uncertainty, updated estimates, and the final sales outcome. The SFA data cover 12,988 B2B prospects, handled by 173 salespeople, who logged 110,278 events in total. We provide supplemental information about the research context and the SFA data in Web Appendix W2. Manifest VariablesBecause we are interested in the performance impact of a salesperson's initial judgments, we aggregated the event-level data to the prospect level. This approach allows us to estimate a two-level model in which prospect-level data (case-specific; within salesperson) are nested within portfolio-level data (baseline; between salesperson). Focal variablesA unique feature of Study 1 is that we leverage the company's SFA data to operationalize the key variables. We measure opportunity magnitude as the salesperson's initial point estimate of a prospect in terms of revenue. Following [62], we log-transform the measure to correct for right-skewness. We measure opportunity conversion uncertainty as a categorical measure that captures the probability of converting a prospect into a deal within six months. We coded these categories as −1 (low), 0 (medium), and 1 (high) to facilitate interpretation and enhance model parsimony ([16]). We measure prospect-level performance as a binary measure that indicates the actual conversion of a prospect at the end of the sales cycle (0 = no deal, 1 = a deal). Portfolio-level performance indicates the salesperson's portfolio-level conversion rate, aggregated from their prospect-level actual conversion. Control variablesTo obtain unbiased estimates, we control for the nonlinear effects of uncertainty by including a square term ([20]). To zero in on the effects of initial judgments, we control for several time-related dynamics. Specifically, we control for five (e.g., [ 1]). First, we control for duration of a sales cycle by including the sales cycle length ([39]). Second, we control for frequency of leads by including workload ([48]), measured as the total number of leads under a salesperson's wing during the assessment. Previous studies have shown that workload affects judgments (e.g., [22]). Third, we control for the frequency of uncertainty updates (i.e., process uncertainty), which reflects the total number of changes a salesperson has made after the initial uncertainty estimate. It reflects the doubt inherent in the sales process and filters out variation in the dependent variable after salespeople made their initial judgments, which is the focus of the article. Fourth, we control for accuracy and recency effects by including the difference between the initial and final estimates for opportunity magnitude and uncertainty (magnitude accuracy = [last estimate − first estimate]; conversion uncertainty accuracy = [first estimate − last estimate]). Fifth, we control for timing- and sequence-related dynamics by including time fixed effects. We use dummy variables to account for the prospect's industry (i.e., public, office, retail, and other). We also control for the potentially curvilinear effect of uncertainty because prior research suggests that people respond more rigorously to two ends of the uncertainty continuum than to moderate levels of uncertainty ([ 2]; [55]; [67]). Web Appendix W3 provides sample descriptives and the correlation matrix. Empirical Strategy Levels of analysis and centeringAs is true in many B2B selling contexts, salespeople are responsible for a portfolio of prospects. In the ""Study 1 Hypothesis Development"" subsection, we use the term ""opportunity"" without specifying whether this opportunity is at the prospect or portfolio level. However, previous studies in decision making (e.g., [33]; [58], [59]) show that people ( 1) can leverage two levels of information (case-specific and base-rate) and ( 2) use a reference point. From a multilevel perspective, portfolio baseline judgments are essentially salesperson-level constructs that capture between-salesperson variation, serving the function of the base-rate information about the sales territory. Prospect-level judgments reflect within-salesperson judgment about specific prospects relative to each salesperson's portfolio baseline judgments ([12]). Because assuming that an effect existing at a higher level will generalize to a lower level (or vice versa) can be erroneous ([13]), we employ multilevel modeling techniques to estimate the impact of a salesperson's initial judgments of opportunity magnitude and conversion uncertainty on performance and test the effect at the prospect (case-specific) and portfolio (baseline) levels. Conceptually, people tend to rely heavily on the mean value in their decision making ([28])—akin to a salesperson's baseline. Therefore, in Study 1, we examine the portfolio average magnitude and average conversion uncertainty as the reference points at the portfolio level and refer to these as portfolio baseline magnitude and conversion uncertainty.We specify a multilevel model using Mplus 8.3 ([44]). To allow for unbiased estimates at the between and within levels, we decompose the manifest variables into uncorrelated latent ""between"" and ""within"" components ([47]). Latent means of focal variables are estimated at the between level, while ""pure"" within-person effects are estimated in the within-level portion of the model. This specification is necessary for teasing apart prospect- and portfolio-level effects ([47]). EstimationThe complexity of the model and the use of a binary dependent variable did not allow use of robust maximum likelihood estimation techniques because of a lack of model convergence. As an alternative, we employed Bayesian estimation techniques and therefore specified a Bayesian multilevel probit model. We estimate two sets of models for the pooled (no separation of selling contexts), product-selling, and solution-selling data, respectively. Models 1–3 are main-effects-only models, and Models 4–6 are the interaction-effects models. All the manifest variables are standardized to aid in interpretation, with the exception of conversion uncertainty. We provide details on the model specification and estimation in Web Appendix W4. Endogeneity considerationsThe effect of opportunity magnitude and conversion uncertainty on performance may be endogenous, because common unobserved factors may influence both predictors and outcomes in our model (due to, e.g., simultaneity, measurement error, omitted variables). Following prior studies ([22]; [49]; [63]), we address endogeneity in three ways: ( 1) by adopting a rich data-modeling approach, ( 2) by controlling for endogeneity due to omitted variables, and ( 3) by checking for endogeneity due to selection bias. Because we consider a multilevel setting, we need to address endogeneity at each level ([37]). We correct for Level 1 endogeneity using a control function procedure ([49]) and check for robustness with an instrument-free Gaussian copula approach ([45]). We control for Level 2 (cross-level) endogeneity in our multilevel latent covariate model by allowing a correlation between random intercepts and slopes ([ 3]). A direct test of the random-effects assumption (Wald  χ12   = 2.226, p = .136) indicates that Level 2 endogeneity is not a concern ([ 3]). Web Appendix W4 provides further details of model-free evidence of inverted U-shaped relationship, robustness checks of adding higher-order terms and seasonal variation to the empirical model, and endogeneity corrections. ResultsWe present the results of our tests for H1 and H2 for solution selling at the portfolio level before discussing the differences between solution selling and product selling (H3). In the ""Discussion"" section, we explore differences across portfolio and prospect levels. Benefit–cost analysis of opportunity magnitudeTable 2 shows the results of the analyses. To test H1, we follow a rigorous three-step procedure ([24]). First, we find a significant, negative effect of (opportunity magnitude)2 on salesperson performance for solution selling (Model 3: γ02 = −.142, p < .001). We plot this effect in Panel A of Figure 3, which shows an inverted U-shape. Second, we formally test that the marginal effects on the left side of the turning point of the inverted U-shape are positive and significant and those on the right side of the turning point are negative and significant. Mathematically, we test whether γ01 + 2γ02XL is positive and significant and γ01 + 2γ02XH is negative and significant, where XL and XH represent low and high values of opportunity magnitude within the data range, respectively. For portfolio baseline magnitude, the results confirm this pattern (for details, see Web Appendix W5). Third, we examine whether the turning point (i.e., X) is located within the data range. Taking the first derivative of the Level 2 equation specified for Model 3 and setting it to zero yields a turning point X of −γ01/2γ02. We found that the turning point is 2.52 SD below the mean value and within the data range (Xsolution = −2.52 SD; 95% confidence interval [CI] = [−4.15, −1.38]). Overall, these results confirm an inverted U-shaped relationship between opportunity magnitude and salesperson performance for solution selling, in support of H1.Graph: Figure 3. Study 1: inverted U-shape and the moderating effect of opportunity conversion uncertainty in solution selling.GraphTable 2. Study 1—Results of Multilevel Probit Analyses: Effect of Initial Judgment on Performance Outcomes. Step 1Step 2aHyp.Model 1: PooledModel 2: ProductsModel 3: SolutionsModel 4: PooledModel 5: ProductsModel 6: SolutionsbSDbSDbSDbSDbSDbSDL2: DV = Portfolio-Level PerformanceMagnitude (γ01)−.536***.149−.301*.162−.716**.232−.562***.178−.377*.198−.240.303Magnitude2 (γ02)−.264***.056−.225**.066−.142***.045−.261***.056−.234***.069−.161***.048H1Uncertainty (γ04)−1.136***.249−1.026***.269−1.541**.481−1.248***.262−1.086***.279−1.556***.466Magnitude × Uncertainty (γ03)——————.051.158.108.175−.663**.227H2/H3L1: DV = Prospect-Level PerformanceMagnitude (β1j)−.456***.075−.410***.062−.536***.155−.393***.052−.414***.065−.484***.159Magnitude2 (β2j)−.142***.026−.158***.022−.098**.038−.123***.019−.157***.022−.129***.043Uncertainty (β4j)−.932***.112−.877***.105−.571*.258−.911***.094−.885***.104−.510***.259Magnitude × Uncertainty (β3j)——————−.017.040.011.045−.053.086ControlsUncertainty2 (L2).323.275.249.3041.100*.515.394***.273.251.309.929*.508Uncertainty2 (L1)−.093.080−.129.083−.167.228−.058.077−.126.079−.235.237Process uncertainty−.270***.016−.288***.017−.204***.040−.276***.016−.290***.017−.207***.040Sales cycle length.092***.021.088***.023.015.142.087***.020.091***.022.069.144Magnitude accuracy−.047***.012−.026*.013−.095***.035−.047***.013−.027*.013−.096**.035Uncertainty accuracy.820***.028.835***.031.675***.073.819***.028.834***.031.684***.073Workload−.004.008.002.008−.008.019−.002.006.003.007−.010.019ε^Magnitudeb.020.039−.006.038.168.119−.023.032−.008.039.114.124Magnitude × ε^Magnitude−.006.015−.011*.016.096*.058−.038**.016−.012.017.149**.066ε^Uncertainty.062*.030.015.028−.095.070.000.027.015.029−.112.071Uncertainty × ε^Uncertainty.109***.021.106*.018.191***.047.095***.016.103***.020.194***.048IMR−1.291***.084−1.416***.088−2.292***.593−1.027***.079−1.423***.087−2.112***.592Industry fixed effectsYesYesYesYesYesYesTime fixed effectsYesYesYesYesYesYesConstant (γ00)1.364***.2021.282***.1611.465.8331.199***.1561.220***.1731.101.866Pseudo-R2: L2/L1.795/.554.645/.545.805.716.801/.509.678/.543.831/.713n (prospects)/N (salespeople)12,988/17310,991/1661,997/12112,988/17310,991/1661,997/121 5 *p < .05.1 **p < .01.2 ***p < .001; unstandardized coefficients.6 a To determine the extent of bias in our estimates ([24]), we also tested an extension of Models 4–6, in which we included Magnitude2 × Uncertainty at L2 (γ05) and L1 (β5j). The results for these models showed nonsignificant effects. The results show that neither interaction is significant (  γ05   = .064, p > .05;  β5j   = .002, p > .05).7 b We also tested the robustness of our findings by controlling for endogeneity using the instrument-free Gaussian copula approach. Findings are similar when compared with the control function approach reported in this table. For details of the results of these additional analyses, see Web Appendix W4.8 Notes: L2 = portfolio level; L1 = prospect level. Magnitude = opportunity magnitude; Uncertainty = opportunity conversion uncertainty. IMR = inverse Mills ratio. Opportunity magnitude–conversion uncertainty calibration hypothesisTo test H2, we add the interaction term (magnitude × uncertainty)ij to the equation. The results of Model 6 in Table 2 confirm that the interaction is significant and negative in the solution-selling context (γ03 = −.663, p <.01). However, we cannot determine significance from the estimated interaction term alone in a nonlinear model (probit) with nonlinear interaction terms ([65]). Therefore, we formally test how the turning point changes as conversion uncertainty changes. To do so, we derive the turning point ""magnitude*"" (X*) by setting the first derivative of Model 6's equation with respect to X to zero. Then, we take the derivative of the turning point with respect to conversion uncertainty (Z) to show how the turning point changes as conversion uncertainty changes, yielding δX*/δZ = (−γ02 γ05)/[2(γ02)2]. We find that this term is significant and negative (b = −2.032, p <.01), in support of H2. Solution- versus product-selling task hypothesisTo test H3, we compare the results for the two selling tasks (see Table 2). At the portfolio level, in contrast with the significant and negative interaction in the solution-selling context (Model 6: γ03 = −.663, p <.01), we find no interaction effect of opportunity magnitude and conversion uncertainty on salesperson performance for product selling (Model 5: γ03 = .108, p >.10). Testing the difference between solution versus product selling reveals significant differences at the portfolio level (Δ[γ03_Solution; γ03_Product] = .771, p <.01), in support of H3. DiscussionStudy 1 provides evidence of an inverted U-shaped relationship between opportunity magnitude and sales performance, across levels and selling context. However, we only find evidence of the calibration hypothesis for the solution-selling context at the portfolio level. This suggests that salespeople respond differently to opportunities of different magnitude, depending on the baseline conversion uncertainty of their portfolio of solution prospects. To illustrate the impact of this effect, we ran a simple counterfactual analysis and compared the calibration model with a compensatory model (i.e., fixing the interaction coefficient γ03 to zero). The results show that under certain conditions the compensatory model over- or underestimates conversion rates by almost 100% (e.g., predict 100% conversion, 0% ""true"" value). For large opportunity magnitude (>∼$26.750), the compensatory model overestimates conversion rates by up to 30% for high conversion uncertainty but underestimates conversion rates by up to 90% for low conversion uncertainty. Overall, these results highlight the importance of the calibration model.We also found that the information level (i.e., prospect- and portfolio-level) matters. While prior research on decision making generally focuses on two simple choices, it might be cognitively impossible for salespeople to constantly make decisions at the prospect level while juggling a portfolio of prospects in their sales funnel. Consistent with this notion, our findings in Table 2 suggest that salespeople adopt a simpler compensatory decision-making strategy at the prospect level (i.e., that accounts for conversion uncertainty in an additive/subtractive manner) but rely on a more complex decision-making strategy at the portfolio level (i.e., a calibration strategy that accounts for conversion uncertainty in an interactive manner, using portfolio baseline information) for solution selling. In practice, salespeople generally have an idea about this so-called portfolio baseline in their assigned territory, such as the average magnitude and uncertainty of their portfolio. They then assess individual opportunities relative to this baseline and prioritize accordingly ([53]; [58]). Study 2: Examining the Heterogeneity of the Calibration Effect in Solution SellingAlthough Study 1 shows that salespeople calibrate for opportunity conversion uncertainty when selling solutions at the portfolio level, it does not investigate how salespeople differ in their calibration. Study 2 focuses on salespeople's past performance success and experience as two key boundary conditions that bias their rational calibration of benefit–cost analyses. Empirically, a test of these contingencies is a three-way interaction test of the two-way interaction in H2. Study 2 Hypothesis Development Past performance successWe first focus on how past performance success influences the way salespeople calibrate their benefit function for conversion uncertainty differently. Compared with salespeople with low past performance, those with high past performance have a higher sense of competence. As a result, they are more risk-seeking and view uncertain opportunities as challenging and intrinsically motivating ([42]; [56]). To these high performers, the intrinsic benefits associated with high conversion uncertainty may outweigh the potential loss of extrinsic benefits. By contrast, salespeople with low past performance repulse opportunities that have high conversion uncertainty. This aversion arises because these highly uncertain opportunities not only threaten their potential extrinsic benefits (e.g., losing compensation and rewards) but also represent an unreliable path to achieve intrinsic benefits (e.g., bolstering their lack of competence; see [15]). Thus, the downward shift of the benefit function created by opportunity conversion uncertainty (predicted in H2) is weaker among salespeople whose past performance success is high (vs. low).In terms of the cost function, being successful in the prior period induces high performers to be more sensitive to opportunity conversion uncertainty for two reasons. First, consistent with COR theory, they are likely to slow down to conserve their resources to reduce stress ([30]). Empirical evidence shows that people tend to hold back after achieving a goal before working on the next goal (e.g., a resetting period; [11]; [34]). Second, high performers are more sensitive to high implicit costs associated with high conversion uncertainty because opportunities that can be converted with certainty allow them to maintain their status (e.g., [38]). Therefore, for high performers, opportunity conversion uncertainty is likely to shift their cost function upward more strongly. This upward shift is especially strong when opportunity magnitude is large because large opportunities require them to invest much more resources. By contrast, while salespeople whose past performance was less successful are also sensitive to the opportunity costs associated with high conversion uncertainty, their main concern is to prove themselves to the firm. Therefore, these poor performers need to exert greater efforts and cannot afford to conserve their resources. As a result, poor performers' cost function shifts upward less strongly when opportunity conversion uncertainty is high.Taken together, compared with salespeople who are low past performers, high performers view opportunities with high conversion uncertainty as more beneficial but also more costly. In prospecting, although all salespeople have limited resources ([48]), high performers are more inclined to conserve their resources than poor performers. Thus, for high past performers, we predict that the upward shifting effect of conversion uncertainty on the cost function will outweigh its downward yet weaker shifting effect on the benefit function. As a result, the expected net benefits of pursuing an opportunity will be lower when conversion uncertainty is high, causing the inverted U-shaped relationship between opportunity magnitude and salesperson performance to shift more strongly to the left ([24]). H4:  The greater salesperson past performance success, the stronger is the leftward shifting effect of opportunity conversion uncertainty on the inverted U-shaped relationship between opportunity magnitude and salesperson performance. Salesperson experienceWe argue that because experienced salespeople differ from less experienced salespeople in terms of resources, they calibrate their benefits and costs under conversion uncertainty differently. First, they have better network-based resources in the form of relationships they have built over time. Second, they are more knowledgeable about various aspects of the sales process (e.g., customers, the market, the competition, the company), another resource critical for success in prospecting ([48]).In terms of the benefit function, the resources accumulated over time make experienced salespeople believe they are more capable, resulting in more risk-seeking behavior ([42]; [56]). For them, the challenge associated with uncertain opportunities can be intrinsically motivating. Because experienced salespeople are more strongly motivated by intrinsic than extrinsic benefits (e.g., [14]), the intrinsic benefits associated with high conversion uncertainty may outweigh the potential loss of extrinsic benefits. By contrast, the lack of capability and resources makes inexperienced salespeople more concerned about potential losses of both intrinsic and extrinsic benefits at high levels of opportunity conversion uncertainty ([42]). Therefore, the downward shifting effect created by opportunity conversion uncertainty on the benefit function is stronger among inexperienced salespeople than experienced ones.In terms of the cost function, the abundance of aforementioned resources makes experienced salespeople less concerned about COR when pursuing opportunities with high conversion uncertainty. Conversely, given their lack of resources, inexperienced salespeople are more concerned about conserving their limited resources and are more sensitive to the costs associated with high conversion uncertainty ([26]; [30]). Thus, opportunity conversion uncertainty is likely to create a weaker upward shift of the cost function among experienced than inexperienced salespeople. Taking the benefit and cost effects together, experienced salespeople expect greater net benefits when conversion uncertainty is high. For them, the inverted U-shaped relationship between opportunity magnitude and salesperson performance shifts less strongly to the left ([24]). H5:  The greater salesperson experience, the weaker is the leftward shifting effect of opportunity conversion uncertainty on the inverted U-shaped relationship between opportunity magnitude and salesperson performance. Institutional ContextIn Study 2, we corroborate Study 1's findings and examine the postulated boundary conditions of salespeople's calibration for conversion uncertainty in solution selling at the portfolio level. We collected data from the sales organization of a large firm ($16.4 billion in total revenue per year). The firm, which operates in the B2B market, provides information and technology solutions (e.g., workspace systems, data center solutions, managed services, security) to customers in industries such as finance, government, education, transport, service, retail, and media. Field-based salespeople are grouped according to the industries the firm serves, with each assigned a territory. All the salespeople are subject to the same compensation and incentive scheme and obtain a fixed yearly salary plus commission (with a progressive plan for all sales beyond quota). Using a survey instrument, we collected information about the salespeople's perceptions of their portfolios. Of the 248 salespeople, 211 completed the questionnaire (85% response rate). Consistent with the length of the average sales cycle, we collected objective salesperson performance from the firm's records six months after the survey. MeasuresIn Study 2, we examine the portfolio magnitude and conversion uncertainty in the aggregate at the portfolio level. Thus, portfolio magnitude corresponds to opportunity magnitude, and portfolio conversion uncertainty corresponds to opportunity conversion uncertainty. Focal variablesTo measure portfolio magnitude, we use the expected customer demand scale from [61]. The scale has four items that cover salespeople's judgment of the opportunity magnitude in terms of order intake, sales volume, revenue, and profits for the solutions in their portfolio. To measure portfolio conversion uncertainty, we developed a new scale that asks salespeople to assess their degree of (un)certainty about the portfolio magnitude. We inversely coded the scores to obtain uncertainty scores. We obtained salesperson past performance success (in the previous quota cycle) and salesperson performance from company databases. We used the percentage of quota achievement, as previous studies indicate that it accurately captures measurable task performance output while accounting for situational factors ([ 2]). Following previous studies (e.g., [ 2]), we operationalize salesperson experience as a composite measure consisting of three separate measures of experience: time in sales territory, time with the company, and time in the sales profession. Control variablesWe control for nonlinear effects of uncertainty by including a squared term ([20]). Dummy variables account for salespeople's industry. We also account for individual characteristics that may influence their judgments (i.e., age and workload). Finally, we control for salesperson trait competitiveness, measured with a scale from [ 9]. Web Appendix W6 provides measurement scales and descriptives of Study 2. Empirical Strategy Validation of measurement modelA confirmatory factor analysis of the measures indicated good model fit (  χ412   = 88.108, p < .01; comparative fit index = .950; Tucker–Lewis index = .933; root mean square error of approximation = .074; square root mean residual = .045; [ 5]). The scales achieved sufficient reliability, with composite reliabilities between.77 and.90 and average variances extracted exceeding.50 for all constructs, indicating reliability. The average variance extracted of each construct exceeds the average variance shared with any other construct, providing evidence of discriminant validity. In addition, all factor loadings are significant (p < .01) and have standardized values ranging from.65 to.91, thus demonstrating convergent validity of the constructs. To examine the effects of opportunity magnitude, conversion uncertainty, past performance success, and salesperson experience on salesperson performance, we specified a multilevel model in Mplus 8.3 ([44]) to control for the nesting of the data. We provide the model specification in Web Appendix W7. Endogeneity considerationsThe effect of salesperson opportunity magnitude and conversion uncertainty on sales performance may be spurious as a result of omitted variables (e.g., group-level factors) and correlation between independent variables and the error terms. For example, a sales manager's and coworkers' judgments may influence a salesperson's judgments and performance outcomes. To control for possible endogeneity in our analyses, we adopted [21] control function procedure. Web Appendix W7 provides further details. ResultsWe present the results of our retests of the main effects of the opportunity magnitude (H1) and calibration (H2) hypotheses for solution selling at the portfolio level. We then report the findings regarding the boundary conditions of past performance and salesperson experience (H4 and H5). Main effect of opportunity magnitudeWe report the results in Table 3. To retest H1 about the inverted U-shaped effect of opportunity magnitude on salesperson performance, we again rely on the three-step approach. First, in line with our results from Study 1, we find that opportunity magnitude2 has a significant, negative effect (Model 7: ζ2 = −.059, p < .05). Second, we formally test marginal effects. We find that the slope is positive and significant for low values of opportunity magnitude and negative and significant for high values (see Web Appendix W5). Third, we calculate the turning point. The estimated turning point is just above the average of the opportunity magnitude scale (i.e., mean + .36 = 3.55), with an estimated CI well within the data range (95% CIraw score = [3.02, 4.78]). These results confirm the inverted U-shaped relationship between opportunity magnitude and salesperson performance, corroborating H1.GraphTable 3. Study 2: Results (Solution-Selling Context). PortfolioRobust Maximum Likelihood EstimatesHyp.Model 7Model 8Model 9Model 10bSDbSDbSDbSDMagnitude (ζ1).042.043.040.043.057.041.098**.038Magnitude2 (ζ2)−.059*.026−.074**.029−.075**.028−.116***.030H1Uncertainty (ζ3)−.087**.037−.088*.038−.121*.056−.098.059Past performance success (ζ4).040.027.042.028.037.039.034.037Salesperson experience (ζ5).015.078.029.078.071.108.035.109Moderation EffectsMagnitude × Uncertainty (ζ6)——−.044*a.025−.044.027−.066*.028H2Magnitude2 × Uncertainty (ζ7)————.026.016.018.013Magnitude × Past perf. success (ζ8)————−.051.037−.026.034Uncertainty × Past perf. success (ζ9)————−.054.037−.012.043Magnitude2 × Past perf. success (ζ10)————.007.026−.023.032Magn. × Uncert. × Past perf. success (ζ11)——————−.058*.027H4Magn.2 × Uncert. × Past perf. success (ζ12)——————−.022.016Magnitude × Salesperson exp. (ζ13)————−.107*.064−.142*.059Uncertainty × Salesperson exp. (ζ14)————.041.039.150*.079Magnitude2 × Salesperson exp. (ζ15)————.024.041.075*.033Magn. × Uncert. × Salesperson exp. (ζ16)——————.016.044H5Magn.2 × Uncert. × Salesperson exp. (ζ17)——————−.070**.024ControlsAge.019.049.017.048.015.051.014.056Workload−.123***.038−.119***.037−.113**.038−.101**.038Trait competitiveness.029.041.037.040.033.040.054.040Dummy Govt. & Edu.−.018.147−.020.148−.049.123−.010.142Dummy Industry & Transport−.074.110−.075.112−.134.107−.102.130Dummy Services, Retail & Media.014.110.006.112−.031.105.044.136Uncertainty2.054**.020.041*.020.035.022.027.022Constant1.826***.0871.841***.0891.890***.0791.876***.100Pseudo-R2.161.171.208.250Log-likelihood−800.078−798.029−793.395−788.232Log-likelihood χ2 diff. test (d.f.)b—5.07(1)*11.33(7)344.20(4) ***cN (salespeople)211211211211 9 *p < .05.3 **p < .01.4 ***p < .001; unstandardized coefficients.10 a To determine the extent of bias in our estimates ([24]), we also examined an extension of Model 8 in which we added Magnitude2 × Uncertainty (ζ7) to our equation. The results show that the coefficient of this interaction is not statistically different from zero (ζ7 = .018, p > .10) and does not improve model fit (χ21 = .660, p > .10). See also Web Appendix W7.11 b When using the MLR estimator in Mplus, a log-likelihood difference test statistic is calculated using log-likelihoods and scaling correction factors for the null and alternative models.12 c Model fit of Model 10 is also significantly better than that of Model 8 (χ21 = 41.97, p < .001).13 Notes: Magnitude = opportunity magnitude; uncertainty = opportunity conversion uncertainty. Moderating role of conversion uncertaintyTo retest H2, we add ζ6(magnitude × uncertainty)jh to the equation and test its significance. Model 8 in Table 3 shows that the interaction is significant and negative (ζ6 = −.044, p < .05). We estimate the change in the turning point using the same approach we reported in Study 1. The result again confirms H2, as the change in the turning point is negative and significant (b = −.301, p < .05). Salesperson characteristics as moderatorsTo test the moderating role of past performance success and salesperson experience, we extend Model 8's equation and test ζ11 and ζ16. Model 10 in Table 3 shows that ζ11 is negative and significant (ζ11 = −.058, p < .05), in support of H4. By contrast, ζ16 is not significant (ζ16 = .016, p > .10) and thus does not support a shifting effect, as postulated in H5. Instead, we find a significant, negative curvilinear moderating effect of salesperson experience (ζ17 = −.070, p < .01), suggesting a flipping effect.We performed several additional robustness checks of our results. First, we used the Wilcoxon rank-sum test (p = .270) to compare respondents and nonrespondents. The tests showed no significant differences between respondents and nonrespondents, alleviating concerns about self-selection bias in our sample. Second, results from Ramsey's regression error specification test (RESET) (χ2 = 1.14, p = .285) alleviate concerns about omitted variables. Third, the maximum variance inflation factors is 3.62, well below the threshold value of 10 ([25]), indicating no multicollinearity issues. Fourth, to test heteroskedasticity we conducted the Cameron–Trivedi test (p = .337) and Breusch–Pagan test (p = .269), neither of which was significant, thus alleviating concerns about heteroskedasticity in our results. DiscussionThe results of Study 2 not only corroborate the key findings of Study 1 in a different context but also reveal the boundary conditions of salesperson calibration. Figure 4, Panel A, reveals that high past performance success triggers COR, whereas low past performance success provokes more risk-seeking behavior. Salespeople with high past performance success perform best under low uncertainty, whereas those with low past performance success do better under high uncertainty (see right-hand and left-hand sides, respectively). Figure 4, Panel B, shows that highly experienced salespeople perform best under the most challenging situations (low/moderate opportunity magnitude; high conversion uncertainty). However, the left-hand side shows that for inexperienced salespeople, high conversion uncertainty dampens quota achievement significantly. These findings suggest that less experienced salespeople tend to conserve resources under high conversion uncertainty, whereas highly experienced salespeople are more willing to bear uncertainty because they have more resources available.Graph: Figure 4. Study 2: three-way moderating effects (opportunity magnitude × opportunity conversion uncertainty × salesperson characteristics). Study 3: Unpacking the Benefit–Cost Mechanisms in ProspectingStudy 3, a scenario-based experiment, has three objectives. First, we replicate the inverted U-shaped effect of opportunity magnitude on sales performance in a controlled setting. Second, we unpack the underlying benefit–cost mechanism of this effect assumed in Studies 1 and 2. Third, we examine the role of resource slack to show the appropriateness of using COR theory. Study 3 Hypothesis DevelopmentAccording to COR theory, salespeople with limited resources are more likely to conserve them than those with abundant resources ([26]; [30]). For the former, small opportunities do not provide significant resources, whereas large opportunities are prohibitively resource straining. Thus, the indirect negative effect of opportunity magnitude on willingness to pursue an opportunity through costs is amplified when salesperson resource slack is limited. For these salespeople, the total indirect effect of opportunity magnitude through benefits and costs follows an inverted U-shape. By contrast, salespeople with high resource slack are motivated to pursue larger opportunities because they have the resources and, by mobilizing them, can gain even more resources ([26]). For these salespeople, the total indirect effect of opportunity magnitude through benefits and costs is convex. Thus, H6:  Salesperson resource slack buffers the negative effect of costs on salesperson willingness to pursue an opportunity. As a corollary, the total indirect effect of opportunity magnitude on salesperson willingness to pursue an opportunity follows an inverted U-shape only when salesperson resource slack is low. Method and Results Sample and procedureGiven the consistent findings of an inverted U-shape across levels, in Study 3 we focus on the prospect level. We partnered with a prominent market research firm to access a diverse panel of salespeople from various industries. The research firm randomly recruited 216 experienced salespeople (64% 36–45 years of age, 62% male, 53% in the information technology industry) for our between-subjects experiment.We then randomly assigned them to one of five scenarios. Each scenario informed participants that they were assigned a territory where the typical revenue of a prospect was $50,000. We included this portfolio baseline information to ensure the design matches with Study 1 and real-life selling contexts. They identified a new sales lead (Prospect A) with a specific opportunity magnitude. In line with data from Study 1, we set the opportunity magnitude at five levels: $1,000, $10,000, $50,000, $250,000, and $1,000,000. After participants read the scenario, we assessed their willingness to pursue Prospect A, anticipated costs, and anticipated benefits on a seven-point scale (1 = ""strongly disagree,"" and 7 = ""strongly agree"") and their resource slack for prospecting activities. We used the natural variation of salespeople's resource slack in their jobs, as previous research shows that resource slack affects people's framing of costs and benefits in decision making ([68]). Post hoc tests indicated that resource slack was not differently distributed between treatment groups (F = .30; p > .10), thereby providing evidence that the manipulation itself did not affect participants' perceptions of resource slack. We included the manipulation check, attention and realism checks and demographic questions. ResultsAnalysis of variance revealed significant between group differences in willingness to pursue the prospect (F( 4, 211) = 2.835, p = .025). Specifically, willingness to pursue is significantly greater (p < .05) in the $50,000 condition (5.50) than in the small ($1,000; 4.75) or large ($1,000,000; 4.96) conditions. Thus, we replicate the inverted U-shaped effect of opportunity magnitude found in Studies 1 and 2. We then specified the path model of the Study 3 panel in Figure 1. We find that the effect of costs on willingness to pursue is contingent on resource slack (b = .242, p < .01). To test the mediating benefit–cost mechanism and the COR effect, we examined the ""instantaneous conditional indirect effect"" of opportunity magnitude on willingness to pursue, with salesperson resource slack as the moderator ([29]). The results show that when resource slack is low, the total indirect effect of opportunity magnitude through the two mediators is only significant at moderate levels of opportunity magnitude (θopp.mag=2 = .111, p < .05). When resource slack is high, moderate to high levels of opportunity magnitude translate significantly into willingness to pursue (θopp.mag=5 = .202, p < .01). These results lend support to H6 and our contention that, under resource constraints, the inverted U-shaped effect of opportunity magnitude operates through the benefit–cost mechanism. For further details, see Web Appendix W8. General DiscussionIntegrating decision-making and COR theories, we develop and test a framework of salesperson decision making when prospecting in three multimethod studies. Together, the empirical evidence explains why salespeople avoid big-whale sales opportunities. Theoretical ContributionsOur research stems from the idea that salespeople differ from participants in studies that focus on low-effort, constraint-free, and repeatable choices ([36]). First, the potential benefits—extrinsic and/or intrinsic—of salespeople's decisions are consequential rather than trivial. Second, given their resource constraints and the ephemeral nature of sales opportunities, their costs—explicit and/or implicit—are not negligible. Third, their decision-making context abounds with uncertainties. Our findings confirm and provide novel insights into the theoretical importance of these differences for research on salespeople's decision making, especially when prospecting. Benefit–cost analysis in salesperson prospectingWe provide strong empirical evidence that in deciding on which opportunities to pursue, salespeople conduct a benefit–cost analysis based on their initial judgment of opportunity magnitude. We show that the relationship between initial judgment of opportunity magnitude and actual conversion follows an inverted U-shape, regardless of selling task (product vs. solution selling) and information level (prospect vs. portfolio). This finding debunks the intuition that salespeople gravitate toward big-whale opportunities, an insight that extends current understanding of salesperson prospecting behavior. Our result also confirms that salespeople's initial judgment of opportunity magnitude exerts a strong impact on their subsequent behavior and performance, even after controlling for transient phases. This finding complements prior research on salesperson intuition ([27]) and on primacy and anchoring effects ([58]). Salesperson calibration for conversion uncertaintyWe also found that solution-selling salespeople take into consideration opportunity conversion uncertainty in their benefit–cost analysis. Due to this calibration, the inverted U-shaped relationship between opportunity magnitude and performance shifts to the left. This shift implies that salespeople are generally more risk-seeking when opportunity magnitude ranges from small to moderate and risk-averse when opportunity magnitude is large. The counterfactual analyses we conducted show that the calibration effect reduces misspecification of conversion rates by up to 100%, when compared with the estimates from a compensatory decision strategy in which uncertainty is simply factored in as an extra cost. This finding provides a more nuanced understanding of the differences between salespeople's decision-making strategies (i.e., calibration vs. compensatory) when prospecting. Furthermore, it joins two separate streams of research on salesperson decision making, one focusing on salesperson judgment of demands and the other on uncertainty. Selling contexts and calibrationCompared with product selling, solution selling is full of uncertainties (e.g., need, process, outcome; [60]). Although these uncertainties are likely to influence salesperson behavior, salesperson behavior and decision making in solution selling has not received much academic research. We contribute to the literature by showing that salespeople indeed use different decision-making strategies in solution selling versus product selling. Specifically, they rely on a calibration decision-making strategy only in solution selling and only at the portfolio level. At the prospect level, regardless of the selling context, salespeople assess each individual opportunity relative to their portfolio baseline in terms of magnitude and conversion uncertainty using a compensatory strategy in which a large magnitude can make up for high uncertainty (and vice versa). Salesperson characteristics and calibration in solution sellingWe find that salesperson past performance success and salesperson experience are important contingencies of salespeople's decision-making process under uncertainty (i.e., calibration). The interaction plots (Figure 4) suggest that salespeople who have achieved past performance success and/or have low experience tend to conserve their resources and become more risk averse when selling solutions. They perform better under low or average than high conversion uncertainty conditions. Experienced salespeople are able to overcome this cautious approach.Our findings also address the contrast between the COR perspective ([30]) and the risk-seeking perspective based on research on risky choice, such as gambling ([56]). While the latter perspective is not specific to the selling context, the COR argument is uniquely relevant to the personal selling context, as it accounts for the notions that ( 1) salespeople are subject to resource constraints and their efforts are costly and ( 2) salespeople need to conserve resources to avoid stress in the long run ([48]). Therefore, researchers who apply decision-making theories to the context of personal selling will benefit from accounting for the uniqueness of salespeople as decision makers. Information level and calibrationOur findings highlight a dual information-processing framework in salesperson decision making when prospecting for solution-selling opportunities ([51]; [58]). Under these conditions, we find that salesperson performance is a function of two processes. At the prospect level, salespeople rely on a simple compensatory model in their decision making, such that a large magnitude can make up for high uncertainty (and vice versa). At the portfolio level, they integrate information about both the magnitude and uncertainty of the prospects they targeted in a more complex calibration model. In this decision-making strategy, conversion uncertainty interacts with opportunity magnitude in driving salesperson portfolio performance. This insight is a meaningful step toward a better understanding of salespeople's prioritization of resources and the importance of considering salesperson characteristics in prospecting. It also sheds first light on potential differences between findings at the prospect level and those at the portfolio level (i.e., a lack of homology) and calls for additional multilevel research of this kind. Managerial ImplicationsOur findings provide both managers and salespeople with several new insights into salesperson decision making when prospecting. We underscore key performance implications of our findings by simulating several what-if analyses using the parameters from our results. Managing salespeople's avoidance of large opportunitiesThe results from three studies consistently show that, all else being equal, salespeople are likely to gravitate toward medium-sized opportunities, leaving smaller and larger opportunities unattended. Using simulated data from Study 1, we find that a salesperson with a prospect whose magnitude equals their baseline opportunity magnitude of ∼$26,750 will have an 86.5% probability of successful conversion. Yet receiving a new prospect that is 1 SD larger in terms of magnitude (∼$143,250) will decrease the conversion odds by more than 15%. This effect is due to the propensity to conserve resources when there are constraints, as Study 3 further shows that the anticipated costs only affect salespeople's pursuit of large prospects when operating under resource constraints.Thus, to assuage salespeople's avoidance of big-whale deals, managers can leverage their firms' CRM databases. Specifically, a manager can use historical CRM data to calculate the baseline estimates of opportunity magnitude (and conversion uncertainty) for each salesperson. Then, the manager can use this information to match marketing-generated prospects with a salesperson's portfolio baseline, because a large difference in opportunity magnitude between a new opportunity and the salesperson baseline is demotivating and decreases conversion success. Furthermore, when necessary, managers should alter salespeople's benefit–cost calculus when prospecting. For example, they should provide salespeople who work on relatively large opportunities with extra benefits (both extrinsic and intrinsic) and additional resources (to relax the resource constraints), thereby increasing the likelihood of conversion. In addition, managers could pair salespeople with peers with larger portfolio baselines to create ad hoc sales teams to follow up. Such a temporary arrangement can reduce the costs for the focal salespeople. Salesperson calibration for conversion uncertainty in solution sellingOur findings show that salespeople's decision-making strategy differs between solution and product selling. For product selling, salespeople rely on a compensatory decision-making strategy at both prospect and portfolio levels. For solution selling, however, they rely on a calibration decision-making strategy, which is noncompensatory in nature, with conversion uncertainty acting as the calibrator of opportunity magnitude at the portfolio level. This calibration effect underscores the important role of portfolio-level information, in terms of both magnitude and conversion uncertainty, in salesperson decision making for solutions. Thus, sales managers should pay close attention to this important ""between-salespeople"" difference when reallocating prospects for maximum effect. Continuing with the previous example from the simulated data, a sales manager could intuitively decide to allocate the solution-selling prospect of ∼$143,250 to a salesperson with a portfolio baseline magnitude of about the same size (i.e., ∼$143,250). However, if this salesperson's portfolio baseline conversion uncertainty is 1 SD higher, the probability of closing the deal decreases by 36.2%. To reduce conversion uncertainty, managers can play an active role by, for example, using more behavior-based control to curtail salespeople's pursuit of highly uncertain opportunities and providing them with more frequent feedback. Firms can also leverage advanced sales analytics capabilities to decrease uncertainty in opportunity costs and train salespeople how to use the information in their prospecting decisions. Boundary conditions of conversion uncertainty calibration for solution sellingOur results indicate that past performance success and experience can alter the way salespeople calibrate for conversion uncertainty. Thus, these two variables are important for managers as well as salespeople. In a post hoc analysis, we used Study 2's results to predict salesperson quota achievement under various combinations of levels of salespeople's past performance success and experience (±1 SD as high and low values). Drawing on the results summarized in Table 4, Panel A, we derive the most effective managerial actions for managing salesperson prospecting in Table 4, Panel B. Three insights are worth noting. First, regardless of past performance success, salespeople's quota attainment is the worst when they gravitate toward highly certain but small opportunities. Second, salespeople who performed well in the past are most likely to ""hit"" quota again when the portfolio opportunity is large and conversion uncertainty is low (96%). Nevertheless, these high performers become average performers when conversion uncertainty and portfolio magnitude are average (ranging from 50% to 79%). Therefore, an effective way to manage high performers' prospecting is to give them a large portfolio but also help them reduce conversion uncertainty. This combination allows them to conserve resources while also maintaining high levels of performance. By contrast, salespeople who performed poorly in the past can achieve a quota attainment as high as 83% when they have a large portfolio and conversion uncertainty is high. The increase in opportunity magnitude is more motivating to these poor performers—they are willing to exert greater efforts without conserving resources to prove themselves. Thus, an effective, but perhaps counterintuitive, way to manage poor performers' prospecting is to give them a larger, more uncertain portfolio to challenge them.GraphTable 4. Study 2: Managerial Insights into Boundary Conditions of Salesperson Prospecting in Solution Selling. A: Post Hoc Analysis for Study 2Moderating Role of Salesperson Past Performance Success (H4)Low Past Performance Success (−1 SD)High Past Performance Success (+1 SD)Conversion UncertaintyConversion UncertaintyMagnitude1 SD LowerAverage1 SD Higher1 SD LowerAverage1 SD Higher1 SD lower36%40%48%39%50%64%Average68%67%71%84%81%79%1 SD higher67%76%83%96%70%50% Table 4. Study 2: Managerial Insights into Boundary Conditions of Salesperson Prospecting in Solution Selling. Moderating Role of Salesperson Experience (H5)Low Salesperson Experience (−1 SD)High Salesperson Experience (+1 SD)Conversion UncertaintyConversion UncertaintyMagnitude1 SD LowerAverage1 SD Higher1 SD LowerAverage1 SD Higher1 SD lower29%28%27%54%76%106%Average106%75%53%53%75%106%1 SD higher118%84%60%60%62%64% Table 4. Study 2: Managerial Insights into Boundary Conditions of Salesperson Prospecting in Solution Selling. B: Managerial TakeawaysObservation from DataSuggested Managerial ActionRegardless of past performance success, salespeople's quota attainment is worst when they gravitate toward highly certain but small opportunities.Point out the importance of ""bread-and-butter"" prospects, as salespeople may ignore them while such prospects could be of strategic importance.Salespeople with greater past performance success perform relatively well when portfolio opportunity is large and conversion uncertainty is low.Give these salespeople a large portfolio but also help them reduce conversion uncertainty (to help them conserve resources).Salespeople with lower past performance success tend to perform better for relatively larger and more uncertain portfolios.Give these salespeople a larger, more uncertain portfolio to challenge them, while also providing opportunities to recuperate from poor past performance.Inexperienced salespeople perform better for relatively larger, but certain portfolios.Reduce conversion uncertainty (e.g., via information provision, training) and provide them with larger portfolios.Experienced salespeople tend to perform especially well for relatively small to moderate portfolio magnitude with relatively high conversion uncertainty.Challenge these salespeople with opportunities that have high conversion uncertainty, while ensuring the portfolio itself is not too large. 14 Notes: 100% = on-target performance. Shaded boxes reflect higher levels of quota achievement.Third, when conversion uncertainty is reduced, inexperienced salespeople who handle a large portfolio can go from zero to hero, as their quota attainment increases from 60% to 118%. Inexperienced salespeople also achieve low quota (under 30%) when their portfolio is small, regardless of conversion uncertainty. Therefore, an effective way to manage inexperienced salespeople's prospecting is to reduce conversion uncertainty and provide them with ample opportunities. By contrast, experienced salespeople do not perform well when their portfolio opportunity is large, regardless of conversion uncertainty (range: 60%–64%). However, they thrive under high conversion uncertainty and when their portfolio is moderate in size, with a quota attainment exceeding 100%. Thus, an effective way to manage experienced salespeople is to challenge them with opportunities that have high conversion uncertainty, while ensuring the portfolio opportunity itself is not too large.What do these results mean for salespeople? Our results show that salespeople need to be cognizant of potential biases created by their past performance success and experience. This is because these biases can significantly improve or impair their sales performance, as indicated by the aforementioned potential gains and losses in quota attainment. By changing the benefit–cost analysis and reducing factors that drive conversion uncertainty (e.g., learn from peers, ask managers for support, form ad hoc teams), salespeople can become more effective in closing big-whale deals and hitting their targets despite conversion uncertainty. Limitations and Future Research DirectionsWhile our research covers three empirical contexts and our data came from multiple sources, this article has several limitations. First, although opportunity magnitude and conversion uncertainty are two of the most important factors in salesperson decision making, they are by no means the only factors. In our research, we included several contingencies and control variables to account for heterogeneity. Nevertheless, we urge further research to consider other aspects as contingencies of salesperson calibration, such as salesperson perceptual accuracy in forming judgments of opportunity magnitude and uncertainty, customer characteristics, competition, and the source of the sales leads ([46]; [48]). Future research could also explore how managers can influence salesperson calibration (e.g., through incentives, by changing baseline conversion uncertainty via altering the composition of self-generated and assigned leads). Moreover, the nature of uncertainty itself and how it affects judgment and decision making could be further explored. Second, we focus on portfolio baseline magnitude and conversion uncertainty as the frames of reference for how salespeople form relative comparisons of prospects in their portfolios. Although this focus is both theoretically and empirically justified, further research could examine other reference points as suggested in the judgment–decision making literature (e.g., sales goals, status quo, minimum requirement). Other measures of magnitude, such as customer lifetime value, could also be examined.Third, we control for several time-related effects in Study 2 but did not examine the dynamics. Although the first impression generally serves as the anchor point, people adjust their anchors as they receive new information ([58]). Further research could examine how salespeople update their judgment of uncertainty over time by exploring how this effect manifests itself in salesperson prospecting. Fourth, we focus on salesperson past performance success and salesperson experience as moderators, but other moderators may exist, such as control systems and training. Finally, although theoretical arguments exist in support of the moderating role of salespeople's past performance success and experience, future research could explicitly test how these contingencies influence the underlying benefit–costs analysis. Supplemental Materialsj-pdf-1-jmx-10.1177_00222429211037336 - Supplemental material for Why Salespeople Avoid Big-Whale Sales OpportunitiesSupplemental material, sj-pdf-1-jmx-10.1177_00222429211037336 for Why Salespeople Avoid Big-Whale Sales Opportunities by Juan Xu, Michel van der Borgh, Edwin J. Nijssen, and Son K. Lam in Journal of Marketing  "
20,"Why Salespeople Avoid Big-Whale Sales Opportunities Contrary to the intuition that salespeople gravitate toward big-whale sales opportunities, in reality they often avoid them. To study this phenomenon, the authors integrate contingent decision-making and conservation-of-resources theories to develop and test a framework of salespeople's decision making when prospecting. Study 1 reveals that the performance impact of salesperson initial judgment of opportunity magnitude follows an inverted U-shape, indicating that salespeople's avoidance of large opportunities results from rational benefit–cost analyses due to their conservation of resources. Interestingly, salespeople use a calibration decision-making strategy (i.e., calculating expected benefits by accounting for conversion uncertainty) at the portfolio rather than prospect level, in solution- but not product-selling contexts. Ignoring this calibration effect can lead to under- or overestimation of conversion rates of up to 100%. Study 2 shows that salespeople's past performance success and experience bias this calibration. Simulations reveal that when high performers or inexperienced salespeople believe their portfolio magnitude is large and conversion uncertainty low, they are less concerned about resource conservation and improve their quota attainment by 50%. Study 3 confirms the theoretical mechanism. These findings shed new light on salespeople's decision making and suggest ways for sales professionals to improve effectiveness when prospecting.Keywords: salesperson judgment; uncertainty; solution selling; prospecting; conservation-of-resources theoryCentral to a firm's customer acquisition is salesperson prospecting, which involves identifying sales opportunities among potential customers. Prior research on salesperson prospecting has underscored its importance not only for firms' customer relationship management (CRM) but also for salesperson performance (e.g., [48]). However, more than 40% of salespeople report that prospecting is challenging and full of uncertainty, taking on average 25% of their time ([ 8]). Whereas some practitioners emphasize the pursuit of large prospects because these ""big whales"" help firms and salespeople achieve rapid sales growth, others warn against prioritizing such prospects because they can easily drain salesperson and company resources ([32]). Moreover, ""in the time it takes to land one major deal, [the salesperson] could have closed five smaller deals"" ([19]). Although practitioners appear to recognize salespeople's benefit–cost trade-offs when prospecting, academic research has not systematically examined this important phenomenon.Marketing research on salesperson prospecting has developed along two major streams. One stream focuses on salesperson judgment of market opportunities, such as market demand for a new brand, customer needs, and expected performance (e.g., [27]; [31]; [66]). This research stream shows a linear positive relationship between customer demand judgment and salesperson performance. The other stream emphasizes the role of salespeople as decision makers in dealing with various types of uncertainty, such as salespeople's general risk aversion, context-specific uncertainty, or salesperson idiosyncratic characteristics (e.g., [ 2]; [10]; [41]; [55]; [60]). Although these research streams provide useful insights into the information salespeople use in their decision making, three important research gaps exist.First, when prospecting, salespeople typically identify multiple potential opportunities but pursue only some of them. However, research is scant on the potentially curvilinear impact of salesperson judgment of opportunity magnitude—defined as a salesperson's judgment of the size of an opportunity—on sales performance. Although anecdotal evidence suggests that salespeople focus on large opportunities, other sources allude to major drawbacks in pursuing them. Opening this black box can be useful for improving companies' prospecting effectiveness. Second, there is a lack of understanding of how conversion uncertainty affects salespeople's decision making when prospecting. A focus on conversion uncertainty is important, because prospecting is costly for the firm and for salespeople. Third, little is known about how such decision making varies between salespeople and selling contexts. Knowledge of these contingencies help sales managers to effectively manage salesperson prospecting behavior.To address these gaps, we seek answers to three key questions. First, what is salespeople's benefit–cost trade-off after they form an initial judgment of opportunity magnitude, and how does this affect their sales performance? The focus on initial judgments is based on prior research that underscores the importance of a primacy effect in both decision making and salesperson–customer interactions ([17]; [27]). Second, how does a salesperson's initial judgments of opportunity conversion uncertainty change the sales performance outcome of the benefit–cost analysis? Given that salespeople's compensation generally depends on conversion, understanding the effect of opportunity conversion uncertainty, or a salesperson's initial judgment of the likelihood to convert an opportunity into a deal, is important. Third, what are important boundary conditions of the effects of these initial judgments? We focus on two sets of moderators: ( 1) the selling context (i.e., product vs. solution selling) and ( 2) key salesperson characteristics (i.e., past performance success and salesperson experience). In doing so, we also explore the role of information level (i.e., prospect and portfolio levels) in salesperson decision making.To answer our questions, we develop and test a contingency framework of salespeople's decision making when prospecting for market opportunities in a sequence of three studies. For theoretical foundation, we integrate research on contingency decision making ([46]) and conservation of resources (COR) ([30]). We augment these theories with field notes from in-depth interviews with sales professionals. While Studies 1 and 2 rely on multisource field data, Study 3 is a scenario-based experiment to provide evidence of the benefit–cost analysis as the underlying mechanism. Together, this multimethod approach allows us to rigorously triangulate the effects and unpack the theoretical mechanisms.This research makes several contributions. First, we contribute to the emerging literature on salesperson judgment and decision making when prospecting by unpacking the underlying decision process. We provide theoretical arguments and strong empirical evidence that explains why salespeople avoid big-whale prospects. Specifically, we show that, based on their initial judgment of opportunity magnitude, salespeople conduct a benefit–cost analysis under resource constraints to decide which opportunity to pursue. This analysis results in an inverted U-shaped relationship between magnitude and performance. Spotlight analyses show that a one-standard-deviation (SD) increase in opportunity magnitude lowers salespeople's conversion rate by 10%.Second, we provide insights into the effect of conversion uncertainty on the salesperson decision-making process when prospecting. The results show that when selling solutions, salespeople use a calibration decision-making strategy, in which the effects of opportunity magnitude are conditional on conversion uncertainty. However, this strategy occurs only at the portfolio level, underscoring the role of salesperson portfolio as a decision-making context. Ignoring the calibration effect in estimating performance outcomes may lead to under- or overestimation of conversion rates of up to 100%. When selling products, salespeople use a compensatory decision-making strategy that accounts for the effects of portfolio magnitude and conversion uncertainty in an additive manner. These findings extend prior work (e.g., [60]) on uncertainty in personal selling and salesperson decision making.Third, we provide empirical evidence for how, in a solution-selling context, the calibration effect varies depending on salesperson past performance success and experience. The results suggest that when faced with high levels of conversion uncertainty, high performers and inexperienced salespeople perform much worse because their resource-conserving tendency makes them more sensitive to the cost increases associated with uncertainty. Simulations reveal that their quota attainment can suffer by as much as 50%. These insights extend prior research focusing on the salesperson–customer dyad in business-to-business (B2B) marketing and retail encounters (e.g., [27]; [43]). Background Literature and Conceptual FrameworkSalespeople are generally assigned to a territory or a customer segment, and their sales opportunities can be self-generated or assigned ([48]). Within a given period, they move these sales opportunities through a funnel from prospects to closed sales deals. At any given time, salespeople form a judgment of the magnitude of specific sales prospects, with a certain level of conversion uncertainty. Prior research on salesperson prospecting provides useful insights into why salespeople fail to follow sales leads, how their judgment of opportunities linearly influences their performance, and how they deal with uncertainty. However, it has not examined why and when salespeople pursue or avoid big opportunities. To shed light on these issues, we view salesperson prospecting as decision making under resource constraints. In this section, we first briefly review the relevant literature and then present our conceptual framework. Prospecting as Decision Making Under Resource Constraints Decision-making frameworksTwo major decision-making frameworks are the benefit–cost framework ([ 6]) and perceptual frameworks, such as prospect theory ([59]). In their review of these two frameworks, [46] posit that the former provides insights into rational decision making under multiple alternatives while the latter is useful in explaining cognitive biases and heuristics in decision making. In their review, they also underscore task and individual characteristics, such as willingness to bear uncertainty, as important contingencies of individual decision making. COR theoryUnlike the majority of general decision-making frameworks that assume away any resource constraint, COR theory emphasizes that people ""strive to retain, protect, and build resources and that what is threatening to them is the potential or actual loss of these valued resources"" ([30], p. 513). Furthermore, people must invest resources to gain resources, and those who experience a lack of resources attempt to conserve remaining resources ([26]). We argue that COR theory is particularly relevant in the context of B2B salespeople's prospecting for three reasons. First, unlike simple, low-effort choices between two lotteries, the pursuit of a prospect is costly—salespeople need to invest their time, effort, and resources in converting prospects into sales ([48]). Second, uncertainty in prospecting brings salespeople's resource constraints to the fore. Unlike gambling, which can be replayed, a forgone sales opportunity might be gone for good, and a failure to convert opportunities represents a loss of resources. Thus, salespeople need to balance between risk seeking and COR. Third, salespeople differ in terms of resource constraints ([48]). Contingency Framework of Salesperson Decision Making When ProspectingWe integrate decision-making frameworks with COR theory to propose a contingency framework of salesperson decision making when prospecting. Our framework focuses on the initial judgments of sales opportunities in terms of magnitude and conversion uncertainty. First, although salespeople encounter multiple market opportunities, they only invest their resources into converting some of them. The benefit–cost framework suggests that this decision is based on rational benefit–cost analyses of opportunity magnitude before action ([ 6]). COR theory offers a similar explanation that the pursuit of an opportunity is a trade-off between resource acquisition (e.g., the expected benefits of a sale) and resource conservation (e.g., the costs of resources expended on pursuing the opportunity). Second, salespeople make this decision under uncertainty. In line with contingency decision-making frameworks and COR theory, we expect that salespeople's benefit–cost analysis of opportunity magnitude is contingent on conversion uncertainty. This is because uncertainty prevents action by obfuscating ""whether the potential reward of action is worth the potential costs"" ([40], p. 139; see also [26]; [46]).Third, task and personal factors represent additional contingencies that distort the rational benefit–cost analyses. We focus on two sets of contingencies. The first is the selling context (i.e., product vs. solution selling), in which a product denotes a physical object that can be sold in a transactional way (e.g., lamps) and a solution refers to a product-service system (e.g., smart lighting) that requires a relational process and tailoring. Solution selling represents a more uncertain task than product selling ([57]; [60]). Examining the selling context is important because many firms that shift from product to solution selling often struggle to cope with the inherent greater uncertainty (e.g., [18]; [60]). The second set includes two salesperson characteristics related to the propensity to conserve resources under uncertainty. Past performance success refers to salesperson quota attainment in the previous quota period. Salesperson experience refers to a salesperson's time in the sales territory, with the company, and in the sales profession ([ 2]). We focus on these two moderators because prior research suggests these factors are related to how salespeople deal with uncertainty and conserve resources ([26]; [30]; [48]). Overview of StudiesTo test our conceptual framework, we conducted three studies using multiple methods. We provide an overview of our conceptual framework, hypotheses, and the studies in Figure 1. Study 1 focuses on how salesperson initial judgments of opportunity magnitude determine the actual conversion of a prospect into a sale, which in the aggregate influences the salesperson conversion rate at the portfolio level. In doing so, we also investigate how salespeople calibrate opportunity magnitude for opportunity conversion uncertainty and whether such calibration differs between product and solution selling. In Study 2, we examine the heterogeneity of such calibration effect, with a focus on salespeople's past performance success and experience. The dependent variable in Study 2 is salesperson quota achievement, which is theoretically connected with the conversion rate examined in Study 1. In Study 3, a scenario-based experiment, we elucidate the underlying benefit–cost mechanism and the role of resource slack. Table 1 summarizes the key concepts in our framework and corresponding operational measures.Graph: Figure 1. A contingency framework of salesperson decision making when prospecting and overview of three studies.GraphTable 1. Overview of Key Concepts and Operationalizations in Studies 1 and 2. Key ConceptsDescription and Conceptual MeaningConceptual FoundationsOperationalizationStudy 1Study 2Representative StudiesPerformance OutcomeSales performanceDegree to which the salesperson obtains a desired outcomeAhearne et al. (2010)Prospect-level performance: A binary measure, where 0 reflects no deal and 1 reflects that a deal has been made (objective likelihood of conversion)✓Smith, Gopalakrishna, and Chatterjee (2006); Mayberry, Boles, and Donthu (2018)Portfolio-level performance: The ratio of prospects that are successfully turned into deals in a salesperson's portfolio (objective conversion rate)✓Own operationalizationSalesperson performance: Percentage of quota attainment✓Ahearne et al. (2010)Initial Cues and Judgment FormationOpportunity magnitudeThe size of a potential sales optionKumar, Petersen, and Leone (2013)Prospect magnitude: Log-transformed salesperson initial judgment of revenue for a prospect (i.e., deal magnitude in $)✓Mayberry, Boles, and Donthu (2018)Portfolio baseline magnitude: The mean of all prospects' magnitude in a salesperson' portfolio✓Own operationalizationPortfolio magnitude: Reflective four-item construct capturing a solution-selling salesperson's initial estimates in terms of the size of order intake, sales volume, revenue, and profits of their entire portfolio.✓Van der Borgh, De Jong, and Nijssen (2019)Opportunity conversion uncertaintyThe subjective likelihood of being able to convert an opportunity into a desired sales outcomeMcMullen and Shepherd (2006)Prospect conversion uncertainty: Categorical measure differentiating among high, medium, and low levels of likelihood to convert a prospect into a paying customer within six months (−1, 0, and 1), based on salesperson initial judgment.✓Own operationalizationPortfolio baseline conversion uncertainty: The average of prospect conversion uncertainty across all the prospects in a salesperson's portfolio✓Own operationalizationPortfolio conversion uncertainty: Reflective four-item construct capturing a salesperson's initial judgment of uncertainty for realizing anticipated outcomes for solution selling for their entire portfolio (in terms of size of order intake, sales volume, revenue, and profits)✓Own operationalizationContingenciesSalesperson characteristicsDifferences in motivation, attitude, or risk propensity that determine whether a salesperson is willing to bear uncertainty or notMcMullen and Shepherd (2006); Payne, Bettman, and Johnson (1992)Past performance success: The percentage of quota achieved in the previous quota cycle✓Mayberry, Boles, and Donthu (2018)Salesperson experience: Composite measure consisting of three measures of sales experience (i.e., time in sales territory, time with the company, and time in the sales profession). We z-scored these scores and averaged them to form an overall experience index.✓Ahearne et al. (2010)Task characteristicsVarious dimensions, descriptors, or attributes of a particular organizational position that determine task execution and/or outcomesPayne, Bettman, and Johnson (1992)Binary measure indicating whether a prospect requires product selling or solution selling✓Mayberry, Boles, and Donthu (2018)Information levelsDenotes the reference class of judgments, distinguishing between judgment about the specific case or the aggregate of multiple casesSniezek and Buckley (1995)Data are separated into information at the single case level (prospect information) and aggregate level (portfolio baseline information)✓Own operationalization  Study 1: Understanding the Interplay Between Opportunity Magnitude and Conversion UncertaintyStudy 1 examines the interaction effect between opportunity magnitude and conversion uncertainty in product- and solution-selling contexts at both the prospect and portfolio levels. We supplement our theoretical development for this study with verbatim quotes from a qualitative study of seven salespeople (for a description of respondents, see Web Appendix W1). Study 1 Hypothesis Development Benefit–cost analysis of opportunity magnitudeWe predict that the effect of opportunity magnitude on salesperson performance follows an inverted U-shaped relationship. This is due to two countervailing underlying mechanisms: a linear positive effect from potential benefits of pursuing an opportunity and a curvilinear effect from potential costs of such pursuit. We follow [24] recommendation to visually summarize this benefit–cost analysis in Figure 2, Panel A. On the one hand, the higher the magnitude of the opportunity, the greater the extrinsic and intrinsic benefits of pursuing a sizable opportunity. Extrinsic benefits take the form of potential compensation and recognition from the selling firm, whereas intrinsic benefits include the potential enjoyment in pursuing sizable opportunities and the learning associated with the pursuit ([ 7]; [15]).Graph: Figure 2. Illustration of theoretical arguments for the inverted U-shaped effect and the calibration effect.On the other hand, pursuing a sizable opportunity entails substantial explicit and implicit costs. Salespeople incur explicit costs when pursing an opportunity because they need to invest resources, such as time and effort ([48]). Implicit costs refer to the opportunity costs of such pursuit because when pursuing an opportunity, salespeople must forgo other opportunities ([54]). As the opportunity magnitude increases, both explicit and implicit costs accelerate significantly because salespeople are constrained by limited resources and information-processing capacity ([30]; [48]; [55]). One senior salesperson of a software company explained this issue succinctly:Big opportunities? Um, the pros. The prospect of earning a load of money.... Second, obviously it's also more satisfying or fulfilling.... So, it's more complex, which is also like a nice challenge. Plus, you learn the most from complex deals and bigger deals.... But it costs a lot of time and whatever time you spend on one deal you cannot reinvest anymore in smaller deals. So, there's always a trade-off.Therefore, when both potential benefits and costs are considered, the effect of opportunity magnitude on salesperson performance will incrementally increase at first, but after a threshold, the costs to act on moderate to large opportunities outweigh their benefits. Thus, H1:  All else being equal, salesperson judgment of opportunity magnitude has a curvilinear, inverted U-shaped effect on salesperson performance. Opportunity magnitude–conversion uncertainty calibrationBy itself, conversion uncertainty can be a source of benefits for salespeople because uncertainty stimulates positive feelings and excitement ([50]). However, conversion uncertainty also increases costs of pursuing—both explicit costs (i.e., salespeople need to exert greater effort to convert highly uncertain opportunities) and implicit costs (i.e., highly uncertain opportunities carry higher opportunity costs). Under the compensatory decision-making strategy, salespeople assess the benefits and costs of opportunity magnitude and conversion uncertainty in an additive manner. However, our interviews suggest that salespeople at times calibrate for conversion uncertainty in a multiplicative rather than additive manner. For example, one solution-selling salesperson was very clear on his decision-making strategy to deal with conversion uncertainty:When I take a look at a deal that has a very high certainty, so say you've a 90% closing chance, but it's very small in size, and you have a very big deal that has a small closing chance. Yeah, I can just multiply it [size by uncertainty] and see where I get the most buck for my uncertainty, so to speak. Even though it's very simple, it's pretty effective.We refer to this multiplicative strategy as the calibration hypothesis, such that the inverted U-shaped relationship between opportunity magnitude and salesperson performance in H1 is contingent on conversion uncertainty ([40]). We again follow [24] recommendation to visually summarize how conversion uncertainty influences the benefit and cost functions in our arguments in Figure 2, Panel B.In terms of the benefit function, the (previously noted) solution-selling salesperson's calculus is consistent with both expectancy theory and COR theory ([30]; [64]). These theories suggest that salespeople calculate the expected benefits of an action by multiplying its benefits by the success odds (i.e., expected benefits = magnitude-based benefits × conversion uncertainty). Thus, when conversion uncertainty is high, the expected benefits of pursuing a large opportunity are lower, making it less motivating to pursue. Importantly, this calculus is universal, without evoking any individual characteristics as contingencies. Therefore, conversion uncertainty weakens the slope of the benefit line, shifting the inverted U-shaped effect of opportunity magnitude on salesperson performance to the left.How do salespeople calibrate for conversion uncertainty in assessing the costs of pursuing an opportunity? As mentioned previously, conversion uncertainty can be positively stimulating for risk seekers but harmfully costly for people who want to conserve resources. In this regard, prior research indicates that salespeople are heterogeneous in their risk-seeking behavior for various reasons, such as their past performance success and their capability (e.g., [42]). Given this heterogeneity, the cost function can swing in either direction, and thus we predict that, in the aggregate, opportunity conversion uncertainty may appear as not having an influence on the cost function. For the leftward shifting effect to occur, opportunity conversion uncertainty only needs to shift the benefit function downward and does not need to change the shape of the cost function ([24]). Thus, H2:  Opportunity conversion uncertainty moderates the effect of opportunity magnitude on salesperson performance, such that it shifts the inverted U-shaped effect of opportunity magnitude on salesperson performance to the left. Solution- versus product-selling taskPrior research on decision making suggests that, under high levels of situational uncertainty, people search for a relevant reference class to calibrate their judgments ([23]; [33]). However, [60] suggest that, beyond outcome uncertainty, such as conversion uncertainty, solution selling has a higher level of need and process uncertainty than product selling. We argue that it is this difference in overall situational uncertainty that causes salespeople to calibrate for conversion uncertainty differently when selling products versus solutions. Specifically, because need and process uncertainties are higher in solution selling, salespeople do not have a reliable frame of reference to count on. By contrast, because need and process uncertainties are lower in product selling, salespeople can confidently draw from their knowledge of customer needs and requirements, the sales process, and product configurations to deal with conversion uncertainty. Therefore, compared with product-selling salespeople, solution-selling salespeople are more sensitive to conversion uncertainty and tend to calibrate for this uncertainty more intensely. Thus, we expect the weakening effect of conversion uncertainty on the benefits function predicted in H2 to be stronger in solution- than product-selling contexts. H3:  The leftward shifting effect of opportunity conversion uncertainty on the inverted U-shaped relationship between opportunity magnitude and salesperson performance is stronger in solution- than product-selling contexts. Institutional ContextWe collected data from a Fortune Global 500 firm, a market leader in lighting products and solutions for enterprise customers; at the time of data collection, the company generated more than $25 billion annually in total revenue. The company provides a broad portfolio of lighting offerings, ranging from products (e.g., luminaires, lighting electronics, horticulture lighting) to system solutions (e.g., connected, smart luminaires; lighting management software). Customers come from various industries, such as food and fashion retail, health care, education, sports, municipalities, hospitality, infrastructure, and manufacturing. For its field-based sales approach, the company relies primarily on direct sales, and salespeople are subject to the same compensation and incentive scheme. Salespeople obtain a fixed yearly salary plus commission (maximum 30% of the fixed salary). To explore the impact of initial judgments of opportunity magnitude and conversion uncertainty, we gathered archival data from the company's sales force automation (SFA) system for all prospects within one market. For every prospect, we obtained transaction-level records from January 2016 to May 2017, including initial estimates of opportunity magnitude (i.e., prospect deal size) and conversion uncertainty, updated estimates, and the final sales outcome. The SFA data cover 12,988 B2B prospects, handled by 173 salespeople, who logged 110,278 events in total. We provide supplemental information about the research context and the SFA data in Web Appendix W2. Manifest VariablesBecause we are interested in the performance impact of a salesperson's initial judgments, we aggregated the event-level data to the prospect level. This approach allows us to estimate a two-level model in which prospect-level data (case-specific; within salesperson) are nested within portfolio-level data (baseline; between salesperson). Focal variablesA unique feature of Study 1 is that we leverage the company's SFA data to operationalize the key variables. We measure opportunity magnitude as the salesperson's initial point estimate of a prospect in terms of revenue. Following [62], we log-transform the measure to correct for right-skewness. We measure opportunity conversion uncertainty as a categorical measure that captures the probability of converting a prospect into a deal within six months. We coded these categories as −1 (low), 0 (medium), and 1 (high) to facilitate interpretation and enhance model parsimony ([16]). We measure prospect-level performance as a binary measure that indicates the actual conversion of a prospect at the end of the sales cycle (0 = no deal, 1 = a deal). Portfolio-level performance indicates the salesperson's portfolio-level conversion rate, aggregated from their prospect-level actual conversion. Control variablesTo obtain unbiased estimates, we control for the nonlinear effects of uncertainty by including a square term ([20]). To zero in on the effects of initial judgments, we control for several time-related dynamics. Specifically, we control for five (e.g., [ 1]). First, we control for duration of a sales cycle by including the sales cycle length ([39]). Second, we control for frequency of leads by including workload ([48]), measured as the total number of leads under a salesperson's wing during the assessment. Previous studies have shown that workload affects judgments (e.g., [22]). Third, we control for the frequency of uncertainty updates (i.e., process uncertainty), which reflects the total number of changes a salesperson has made after the initial uncertainty estimate. It reflects the doubt inherent in the sales process and filters out variation in the dependent variable after salespeople made their initial judgments, which is the focus of the article. Fourth, we control for accuracy and recency effects by including the difference between the initial and final estimates for opportunity magnitude and uncertainty (magnitude accuracy = [last estimate − first estimate]; conversion uncertainty accuracy = [first estimate − last estimate]). Fifth, we control for timing- and sequence-related dynamics by including time fixed effects. We use dummy variables to account for the prospect's industry (i.e., public, office, retail, and other). We also control for the potentially curvilinear effect of uncertainty because prior research suggests that people respond more rigorously to two ends of the uncertainty continuum than to moderate levels of uncertainty ([ 2]; [55]; [67]). Web Appendix W3 provides sample descriptives and the correlation matrix. Empirical Strategy Levels of analysis and centeringAs is true in many B2B selling contexts, salespeople are responsible for a portfolio of prospects. In the ""Study 1 Hypothesis Development"" subsection, we use the term ""opportunity"" without specifying whether this opportunity is at the prospect or portfolio level. However, previous studies in decision making (e.g., [33]; [58], [59]) show that people ( 1) can leverage two levels of information (case-specific and base-rate) and ( 2) use a reference point. From a multilevel perspective, portfolio baseline judgments are essentially salesperson-level constructs that capture between-salesperson variation, serving the function of the base-rate information about the sales territory. Prospect-level judgments reflect within-salesperson judgment about specific prospects relative to each salesperson's portfolio baseline judgments ([12]). Because assuming that an effect existing at a higher level will generalize to a lower level (or vice versa) can be erroneous ([13]), we employ multilevel modeling techniques to estimate the impact of a salesperson's initial judgments of opportunity magnitude and conversion uncertainty on performance and test the effect at the prospect (case-specific) and portfolio (baseline) levels. Conceptually, people tend to rely heavily on the mean value in their decision making ([28])—akin to a salesperson's baseline. Therefore, in Study 1, we examine the portfolio average magnitude and average conversion uncertainty as the reference points at the portfolio level and refer to these as portfolio baseline magnitude and conversion uncertainty.We specify a multilevel model using Mplus 8.3 ([44]). To allow for unbiased estimates at the between and within levels, we decompose the manifest variables into uncorrelated latent ""between"" and ""within"" components ([47]). Latent means of focal variables are estimated at the between level, while ""pure"" within-person effects are estimated in the within-level portion of the model. This specification is necessary for teasing apart prospect- and portfolio-level effects ([47]). EstimationThe complexity of the model and the use of a binary dependent variable did not allow use of robust maximum likelihood estimation techniques because of a lack of model convergence. As an alternative, we employed Bayesian estimation techniques and therefore specified a Bayesian multilevel probit model. We estimate two sets of models for the pooled (no separation of selling contexts), product-selling, and solution-selling data, respectively. Models 1–3 are main-effects-only models, and Models 4–6 are the interaction-effects models. All the manifest variables are standardized to aid in interpretation, with the exception of conversion uncertainty. We provide details on the model specification and estimation in Web Appendix W4. Endogeneity considerationsThe effect of opportunity magnitude and conversion uncertainty on performance may be endogenous, because common unobserved factors may influence both predictors and outcomes in our model (due to, e.g., simultaneity, measurement error, omitted variables). Following prior studies ([22]; [49]; [63]), we address endogeneity in three ways: ( 1) by adopting a rich data-modeling approach, ( 2) by controlling for endogeneity due to omitted variables, and ( 3) by checking for endogeneity due to selection bias. Because we consider a multilevel setting, we need to address endogeneity at each level ([37]). We correct for Level 1 endogeneity using a control function procedure ([49]) and check for robustness with an instrument-free Gaussian copula approach ([45]). We control for Level 2 (cross-level) endogeneity in our multilevel latent covariate model by allowing a correlation between random intercepts and slopes ([ 3]). A direct test of the random-effects assumption (Wald  χ12   = 2.226, p = .136) indicates that Level 2 endogeneity is not a concern ([ 3]). Web Appendix W4 provides further details of model-free evidence of inverted U-shaped relationship, robustness checks of adding higher-order terms and seasonal variation to the empirical model, and endogeneity corrections. ResultsWe present the results of our tests for H1 and H2 for solution selling at the portfolio level before discussing the differences between solution selling and product selling (H3). In the ""Discussion"" section, we explore differences across portfolio and prospect levels. Benefit–cost analysis of opportunity magnitudeTable 2 shows the results of the analyses. To test H1, we follow a rigorous three-step procedure ([24]). First, we find a significant, negative effect of (opportunity magnitude)2 on salesperson performance for solution selling (Model 3: γ02 = −.142, p < .001). We plot this effect in Panel A of Figure 3, which shows an inverted U-shape. Second, we formally test that the marginal effects on the left side of the turning point of the inverted U-shape are positive and significant and those on the right side of the turning point are negative and significant. Mathematically, we test whether γ01 + 2γ02XL is positive and significant and γ01 + 2γ02XH is negative and significant, where XL and XH represent low and high values of opportunity magnitude within the data range, respectively. For portfolio baseline magnitude, the results confirm this pattern (for details, see Web Appendix W5). Third, we examine whether the turning point (i.e., X) is located within the data range. Taking the first derivative of the Level 2 equation specified for Model 3 and setting it to zero yields a turning point X of −γ01/2γ02. We found that the turning point is 2.52 SD below the mean value and within the data range (Xsolution = −2.52 SD; 95% confidence interval [CI] = [−4.15, −1.38]). Overall, these results confirm an inverted U-shaped relationship between opportunity magnitude and salesperson performance for solution selling, in support of H1.Graph: Figure 3. Study 1: inverted U-shape and the moderating effect of opportunity conversion uncertainty in solution selling.GraphTable 2. Study 1—Results of Multilevel Probit Analyses: Effect of Initial Judgment on Performance Outcomes. Step 1Step 2aHyp.Model 1: PooledModel 2: ProductsModel 3: SolutionsModel 4: PooledModel 5: ProductsModel 6: SolutionsbSDbSDbSDbSDbSDbSDL2: DV = Portfolio-Level PerformanceMagnitude (γ01)−.536***.149−.301*.162−.716**.232−.562***.178−.377*.198−.240.303Magnitude2 (γ02)−.264***.056−.225**.066−.142***.045−.261***.056−.234***.069−.161***.048H1Uncertainty (γ04)−1.136***.249−1.026***.269−1.541**.481−1.248***.262−1.086***.279−1.556***.466Magnitude × Uncertainty (γ03)——————.051.158.108.175−.663**.227H2/H3L1: DV = Prospect-Level PerformanceMagnitude (β1j)−.456***.075−.410***.062−.536***.155−.393***.052−.414***.065−.484***.159Magnitude2 (β2j)−.142***.026−.158***.022−.098**.038−.123***.019−.157***.022−.129***.043Uncertainty (β4j)−.932***.112−.877***.105−.571*.258−.911***.094−.885***.104−.510***.259Magnitude × Uncertainty (β3j)——————−.017.040.011.045−.053.086ControlsUncertainty2 (L2).323.275.249.3041.100*.515.394***.273.251.309.929*.508Uncertainty2 (L1)−.093.080−.129.083−.167.228−.058.077−.126.079−.235.237Process uncertainty−.270***.016−.288***.017−.204***.040−.276***.016−.290***.017−.207***.040Sales cycle length.092***.021.088***.023.015.142.087***.020.091***.022.069.144Magnitude accuracy−.047***.012−.026*.013−.095***.035−.047***.013−.027*.013−.096**.035Uncertainty accuracy.820***.028.835***.031.675***.073.819***.028.834***.031.684***.073Workload−.004.008.002.008−.008.019−.002.006.003.007−.010.019ε^Magnitudeb.020.039−.006.038.168.119−.023.032−.008.039.114.124Magnitude × ε^Magnitude−.006.015−.011*.016.096*.058−.038**.016−.012.017.149**.066ε^Uncertainty.062*.030.015.028−.095.070.000.027.015.029−.112.071Uncertainty × ε^Uncertainty.109***.021.106*.018.191***.047.095***.016.103***.020.194***.048IMR−1.291***.084−1.416***.088−2.292***.593−1.027***.079−1.423***.087−2.112***.592Industry fixed effectsYesYesYesYesYesYesTime fixed effectsYesYesYesYesYesYesConstant (γ00)1.364***.2021.282***.1611.465.8331.199***.1561.220***.1731.101.866Pseudo-R2: L2/L1.795/.554.645/.545.805.716.801/.509.678/.543.831/.713n (prospects)/N (salespeople)12,988/17310,991/1661,997/12112,988/17310,991/1661,997/121 5 *p < .05.1 **p < .01.2 ***p < .001; unstandardized coefficients.6 a To determine the extent of bias in our estimates ([24]), we also tested an extension of Models 4–6, in which we included Magnitude2 × Uncertainty at L2 (γ05) and L1 (β5j). The results for these models showed nonsignificant effects. The results show that neither interaction is significant (  γ05   = .064, p > .05;  β5j   = .002, p > .05).7 b We also tested the robustness of our findings by controlling for endogeneity using the instrument-free Gaussian copula approach. Findings are similar when compared with the control function approach reported in this table. For details of the results of these additional analyses, see Web Appendix W4.8 Notes: L2 = portfolio level; L1 = prospect level. Magnitude = opportunity magnitude; Uncertainty = opportunity conversion uncertainty. IMR = inverse Mills ratio. Opportunity magnitude–conversion uncertainty calibration hypothesisTo test H2, we add the interaction term (magnitude × uncertainty)ij to the equation. The results of Model 6 in Table 2 confirm that the interaction is significant and negative in the solution-selling context (γ03 = −.663, p <.01). However, we cannot determine significance from the estimated interaction term alone in a nonlinear model (probit) with nonlinear interaction terms ([65]). Therefore, we formally test how the turning point changes as conversion uncertainty changes. To do so, we derive the turning point ""magnitude*"" (X*) by setting the first derivative of Model 6's equation with respect to X to zero. Then, we take the derivative of the turning point with respect to conversion uncertainty (Z) to show how the turning point changes as conversion uncertainty changes, yielding δX*/δZ = (−γ02 γ05)/[2(γ02)2]. We find that this term is significant and negative (b = −2.032, p <.01), in support of H2. Solution- versus product-selling task hypothesisTo test H3, we compare the results for the two selling tasks (see Table 2). At the portfolio level, in contrast with the significant and negative interaction in the solution-selling context (Model 6: γ03 = −.663, p <.01), we find no interaction effect of opportunity magnitude and conversion uncertainty on salesperson performance for product selling (Model 5: γ03 = .108, p >.10). Testing the difference between solution versus product selling reveals significant differences at the portfolio level (Δ[γ03_Solution; γ03_Product] = .771, p <.01), in support of H3. DiscussionStudy 1 provides evidence of an inverted U-shaped relationship between opportunity magnitude and sales performance, across levels and selling context. However, we only find evidence of the calibration hypothesis for the solution-selling context at the portfolio level. This suggests that salespeople respond differently to opportunities of different magnitude, depending on the baseline conversion uncertainty of their portfolio of solution prospects. To illustrate the impact of this effect, we ran a simple counterfactual analysis and compared the calibration model with a compensatory model (i.e., fixing the interaction coefficient γ03 to zero). The results show that under certain conditions the compensatory model over- or underestimates conversion rates by almost 100% (e.g., predict 100% conversion, 0% ""true"" value). For large opportunity magnitude (>∼$26.750), the compensatory model overestimates conversion rates by up to 30% for high conversion uncertainty but underestimates conversion rates by up to 90% for low conversion uncertainty. Overall, these results highlight the importance of the calibration model.We also found that the information level (i.e., prospect- and portfolio-level) matters. While prior research on decision making generally focuses on two simple choices, it might be cognitively impossible for salespeople to constantly make decisions at the prospect level while juggling a portfolio of prospects in their sales funnel. Consistent with this notion, our findings in Table 2 suggest that salespeople adopt a simpler compensatory decision-making strategy at the prospect level (i.e., that accounts for conversion uncertainty in an additive/subtractive manner) but rely on a more complex decision-making strategy at the portfolio level (i.e., a calibration strategy that accounts for conversion uncertainty in an interactive manner, using portfolio baseline information) for solution selling. In practice, salespeople generally have an idea about this so-called portfolio baseline in their assigned territory, such as the average magnitude and uncertainty of their portfolio. They then assess individual opportunities relative to this baseline and prioritize accordingly ([53]; [58]). Study 2: Examining the Heterogeneity of the Calibration Effect in Solution SellingAlthough Study 1 shows that salespeople calibrate for opportunity conversion uncertainty when selling solutions at the portfolio level, it does not investigate how salespeople differ in their calibration. Study 2 focuses on salespeople's past performance success and experience as two key boundary conditions that bias their rational calibration of benefit–cost analyses. Empirically, a test of these contingencies is a three-way interaction test of the two-way interaction in H2. Study 2 Hypothesis Development Past performance successWe first focus on how past performance success influences the way salespeople calibrate their benefit function for conversion uncertainty differently. Compared with salespeople with low past performance, those with high past performance have a higher sense of competence. As a result, they are more risk-seeking and view uncertain opportunities as challenging and intrinsically motivating ([42]; [56]). To these high performers, the intrinsic benefits associated with high conversion uncertainty may outweigh the potential loss of extrinsic benefits. By contrast, salespeople with low past performance repulse opportunities that have high conversion uncertainty. This aversion arises because these highly uncertain opportunities not only threaten their potential extrinsic benefits (e.g., losing compensation and rewards) but also represent an unreliable path to achieve intrinsic benefits (e.g., bolstering their lack of competence; see [15]). Thus, the downward shift of the benefit function created by opportunity conversion uncertainty (predicted in H2) is weaker among salespeople whose past performance success is high (vs. low).In terms of the cost function, being successful in the prior period induces high performers to be more sensitive to opportunity conversion uncertainty for two reasons. First, consistent with COR theory, they are likely to slow down to conserve their resources to reduce stress ([30]). Empirical evidence shows that people tend to hold back after achieving a goal before working on the next goal (e.g., a resetting period; [11]; [34]). Second, high performers are more sensitive to high implicit costs associated with high conversion uncertainty because opportunities that can be converted with certainty allow them to maintain their status (e.g., [38]). Therefore, for high performers, opportunity conversion uncertainty is likely to shift their cost function upward more strongly. This upward shift is especially strong when opportunity magnitude is large because large opportunities require them to invest much more resources. By contrast, while salespeople whose past performance was less successful are also sensitive to the opportunity costs associated with high conversion uncertainty, their main concern is to prove themselves to the firm. Therefore, these poor performers need to exert greater efforts and cannot afford to conserve their resources. As a result, poor performers' cost function shifts upward less strongly when opportunity conversion uncertainty is high.Taken together, compared with salespeople who are low past performers, high performers view opportunities with high conversion uncertainty as more beneficial but also more costly. In prospecting, although all salespeople have limited resources ([48]), high performers are more inclined to conserve their resources than poor performers. Thus, for high past performers, we predict that the upward shifting effect of conversion uncertainty on the cost function will outweigh its downward yet weaker shifting effect on the benefit function. As a result, the expected net benefits of pursuing an opportunity will be lower when conversion uncertainty is high, causing the inverted U-shaped relationship between opportunity magnitude and salesperson performance to shift more strongly to the left ([24]). H4:  The greater salesperson past performance success, the stronger is the leftward shifting effect of opportunity conversion uncertainty on the inverted U-shaped relationship between opportunity magnitude and salesperson performance. Salesperson experienceWe argue that because experienced salespeople differ from less experienced salespeople in terms of resources, they calibrate their benefits and costs under conversion uncertainty differently. First, they have better network-based resources in the form of relationships they have built over time. Second, they are more knowledgeable about various aspects of the sales process (e.g., customers, the market, the competition, the company), another resource critical for success in prospecting ([48]).In terms of the benefit function, the resources accumulated over time make experienced salespeople believe they are more capable, resulting in more risk-seeking behavior ([42]; [56]). For them, the challenge associated with uncertain opportunities can be intrinsically motivating. Because experienced salespeople are more strongly motivated by intrinsic than extrinsic benefits (e.g., [14]), the intrinsic benefits associated with high conversion uncertainty may outweigh the potential loss of extrinsic benefits. By contrast, the lack of capability and resources makes inexperienced salespeople more concerned about potential losses of both intrinsic and extrinsic benefits at high levels of opportunity conversion uncertainty ([42]). Therefore, the downward shifting effect created by opportunity conversion uncertainty on the benefit function is stronger among inexperienced salespeople than experienced ones.In terms of the cost function, the abundance of aforementioned resources makes experienced salespeople less concerned about COR when pursuing opportunities with high conversion uncertainty. Conversely, given their lack of resources, inexperienced salespeople are more concerned about conserving their limited resources and are more sensitive to the costs associated with high conversion uncertainty ([26]; [30]). Thus, opportunity conversion uncertainty is likely to create a weaker upward shift of the cost function among experienced than inexperienced salespeople. Taking the benefit and cost effects together, experienced salespeople expect greater net benefits when conversion uncertainty is high. For them, the inverted U-shaped relationship between opportunity magnitude and salesperson performance shifts less strongly to the left ([24]). H5:  The greater salesperson experience, the weaker is the leftward shifting effect of opportunity conversion uncertainty on the inverted U-shaped relationship between opportunity magnitude and salesperson performance. Institutional ContextIn Study 2, we corroborate Study 1's findings and examine the postulated boundary conditions of salespeople's calibration for conversion uncertainty in solution selling at the portfolio level. We collected data from the sales organization of a large firm ($16.4 billion in total revenue per year). The firm, which operates in the B2B market, provides information and technology solutions (e.g., workspace systems, data center solutions, managed services, security) to customers in industries such as finance, government, education, transport, service, retail, and media. Field-based salespeople are grouped according to the industries the firm serves, with each assigned a territory. All the salespeople are subject to the same compensation and incentive scheme and obtain a fixed yearly salary plus commission (with a progressive plan for all sales beyond quota). Using a survey instrument, we collected information about the salespeople's perceptions of their portfolios. Of the 248 salespeople, 211 completed the questionnaire (85% response rate). Consistent with the length of the average sales cycle, we collected objective salesperson performance from the firm's records six months after the survey. MeasuresIn Study 2, we examine the portfolio magnitude and conversion uncertainty in the aggregate at the portfolio level. Thus, portfolio magnitude corresponds to opportunity magnitude, and portfolio conversion uncertainty corresponds to opportunity conversion uncertainty. Focal variablesTo measure portfolio magnitude, we use the expected customer demand scale from [61]. The scale has four items that cover salespeople's judgment of the opportunity magnitude in terms of order intake, sales volume, revenue, and profits for the solutions in their portfolio. To measure portfolio conversion uncertainty, we developed a new scale that asks salespeople to assess their degree of (un)certainty about the portfolio magnitude. We inversely coded the scores to obtain uncertainty scores. We obtained salesperson past performance success (in the previous quota cycle) and salesperson performance from company databases. We used the percentage of quota achievement, as previous studies indicate that it accurately captures measurable task performance output while accounting for situational factors ([ 2]). Following previous studies (e.g., [ 2]), we operationalize salesperson experience as a composite measure consisting of three separate measures of experience: time in sales territory, time with the company, and time in the sales profession. Control variablesWe control for nonlinear effects of uncertainty by including a squared term ([20]). Dummy variables account for salespeople's industry. We also account for individual characteristics that may influence their judgments (i.e., age and workload). Finally, we control for salesperson trait competitiveness, measured with a scale from [ 9]. Web Appendix W6 provides measurement scales and descriptives of Study 2. Empirical Strategy Validation of measurement modelA confirmatory factor analysis of the measures indicated good model fit (  χ412   = 88.108, p < .01; comparative fit index = .950; Tucker–Lewis index = .933; root mean square error of approximation = .074; square root mean residual = .045; [ 5]). The scales achieved sufficient reliability, with composite reliabilities between.77 and.90 and average variances extracted exceeding.50 for all constructs, indicating reliability. The average variance extracted of each construct exceeds the average variance shared with any other construct, providing evidence of discriminant validity. In addition, all factor loadings are significant (p < .01) and have standardized values ranging from.65 to.91, thus demonstrating convergent validity of the constructs. To examine the effects of opportunity magnitude, conversion uncertainty, past performance success, and salesperson experience on salesperson performance, we specified a multilevel model in Mplus 8.3 ([44]) to control for the nesting of the data. We provide the model specification in Web Appendix W7. Endogeneity considerationsThe effect of salesperson opportunity magnitude and conversion uncertainty on sales performance may be spurious as a result of omitted variables (e.g., group-level factors) and correlation between independent variables and the error terms. For example, a sales manager's and coworkers' judgments may influence a salesperson's judgments and performance outcomes. To control for possible endogeneity in our analyses, we adopted [21] control function procedure. Web Appendix W7 provides further details. ResultsWe present the results of our retests of the main effects of the opportunity magnitude (H1) and calibration (H2) hypotheses for solution selling at the portfolio level. We then report the findings regarding the boundary conditions of past performance and salesperson experience (H4 and H5). Main effect of opportunity magnitudeWe report the results in Table 3. To retest H1 about the inverted U-shaped effect of opportunity magnitude on salesperson performance, we again rely on the three-step approach. First, in line with our results from Study 1, we find that opportunity magnitude2 has a significant, negative effect (Model 7: ζ2 = −.059, p < .05). Second, we formally test marginal effects. We find that the slope is positive and significant for low values of opportunity magnitude and negative and significant for high values (see Web Appendix W5). Third, we calculate the turning point. The estimated turning point is just above the average of the opportunity magnitude scale (i.e., mean + .36 = 3.55), with an estimated CI well within the data range (95% CIraw score = [3.02, 4.78]). These results confirm the inverted U-shaped relationship between opportunity magnitude and salesperson performance, corroborating H1.GraphTable 3. Study 2: Results (Solution-Selling Context). PortfolioRobust Maximum Likelihood EstimatesHyp.Model 7Model 8Model 9Model 10bSDbSDbSDbSDMagnitude (ζ1).042.043.040.043.057.041.098**.038Magnitude2 (ζ2)−.059*.026−.074**.029−.075**.028−.116***.030H1Uncertainty (ζ3)−.087**.037−.088*.038−.121*.056−.098.059Past performance success (ζ4).040.027.042.028.037.039.034.037Salesperson experience (ζ5).015.078.029.078.071.108.035.109Moderation EffectsMagnitude × Uncertainty (ζ6)——−.044*a.025−.044.027−.066*.028H2Magnitude2 × Uncertainty (ζ7)————.026.016.018.013Magnitude × Past perf. success (ζ8)————−.051.037−.026.034Uncertainty × Past perf. success (ζ9)————−.054.037−.012.043Magnitude2 × Past perf. success (ζ10)————.007.026−.023.032Magn. × Uncert. × Past perf. success (ζ11)——————−.058*.027H4Magn.2 × Uncert. × Past perf. success (ζ12)——————−.022.016Magnitude × Salesperson exp. (ζ13)————−.107*.064−.142*.059Uncertainty × Salesperson exp. (ζ14)————.041.039.150*.079Magnitude2 × Salesperson exp. (ζ15)————.024.041.075*.033Magn. × Uncert. × Salesperson exp. (ζ16)——————.016.044H5Magn.2 × Uncert. × Salesperson exp. (ζ17)——————−.070**.024ControlsAge.019.049.017.048.015.051.014.056Workload−.123***.038−.119***.037−.113**.038−.101**.038Trait competitiveness.029.041.037.040.033.040.054.040Dummy Govt. & Edu.−.018.147−.020.148−.049.123−.010.142Dummy Industry & Transport−.074.110−.075.112−.134.107−.102.130Dummy Services, Retail & Media.014.110.006.112−.031.105.044.136Uncertainty2.054**.020.041*.020.035.022.027.022Constant1.826***.0871.841***.0891.890***.0791.876***.100Pseudo-R2.161.171.208.250Log-likelihood−800.078−798.029−793.395−788.232Log-likelihood χ2 diff. test (d.f.)b—5.07(1)*11.33(7)344.20(4) ***cN (salespeople)211211211211 9 *p < .05.3 **p < .01.4 ***p < .001; unstandardized coefficients.10 a To determine the extent of bias in our estimates ([24]), we also examined an extension of Model 8 in which we added Magnitude2 × Uncertainty (ζ7) to our equation. The results show that the coefficient of this interaction is not statistically different from zero (ζ7 = .018, p > .10) and does not improve model fit (χ21 = .660, p > .10). See also Web Appendix W7.11 b When using the MLR estimator in Mplus, a log-likelihood difference test statistic is calculated using log-likelihoods and scaling correction factors for the null and alternative models.12 c Model fit of Model 10 is also significantly better than that of Model 8 (χ21 = 41.97, p < .001).13 Notes: Magnitude = opportunity magnitude; uncertainty = opportunity conversion uncertainty. Moderating role of conversion uncertaintyTo retest H2, we add ζ6(magnitude × uncertainty)jh to the equation and test its significance. Model 8 in Table 3 shows that the interaction is significant and negative (ζ6 = −.044, p < .05). We estimate the change in the turning point using the same approach we reported in Study 1. The result again confirms H2, as the change in the turning point is negative and significant (b = −.301, p < .05). Salesperson characteristics as moderatorsTo test the moderating role of past performance success and salesperson experience, we extend Model 8's equation and test ζ11 and ζ16. Model 10 in Table 3 shows that ζ11 is negative and significant (ζ11 = −.058, p < .05), in support of H4. By contrast, ζ16 is not significant (ζ16 = .016, p > .10) and thus does not support a shifting effect, as postulated in H5. Instead, we find a significant, negative curvilinear moderating effect of salesperson experience (ζ17 = −.070, p < .01), suggesting a flipping effect.We performed several additional robustness checks of our results. First, we used the Wilcoxon rank-sum test (p = .270) to compare respondents and nonrespondents. The tests showed no significant differences between respondents and nonrespondents, alleviating concerns about self-selection bias in our sample. Second, results from Ramsey's regression error specification test (RESET) (χ2 = 1.14, p = .285) alleviate concerns about omitted variables. Third, the maximum variance inflation factors is 3.62, well below the threshold value of 10 ([25]), indicating no multicollinearity issues. Fourth, to test heteroskedasticity we conducted the Cameron–Trivedi test (p = .337) and Breusch–Pagan test (p = .269), neither of which was significant, thus alleviating concerns about heteroskedasticity in our results. DiscussionThe results of Study 2 not only corroborate the key findings of Study 1 in a different context but also reveal the boundary conditions of salesperson calibration. Figure 4, Panel A, reveals that high past performance success triggers COR, whereas low past performance success provokes more risk-seeking behavior. Salespeople with high past performance success perform best under low uncertainty, whereas those with low past performance success do better under high uncertainty (see right-hand and left-hand sides, respectively). Figure 4, Panel B, shows that highly experienced salespeople perform best under the most challenging situations (low/moderate opportunity magnitude; high conversion uncertainty). However, the left-hand side shows that for inexperienced salespeople, high conversion uncertainty dampens quota achievement significantly. These findings suggest that less experienced salespeople tend to conserve resources under high conversion uncertainty, whereas highly experienced salespeople are more willing to bear uncertainty because they have more resources available.Graph: Figure 4. Study 2: three-way moderating effects (opportunity magnitude × opportunity conversion uncertainty × salesperson characteristics). Study 3: Unpacking the Benefit–Cost Mechanisms in ProspectingStudy 3, a scenario-based experiment, has three objectives. First, we replicate the inverted U-shaped effect of opportunity magnitude on sales performance in a controlled setting. Second, we unpack the underlying benefit–cost mechanism of this effect assumed in Studies 1 and 2. Third, we examine the role of resource slack to show the appropriateness of using COR theory. Study 3 Hypothesis DevelopmentAccording to COR theory, salespeople with limited resources are more likely to conserve them than those with abundant resources ([26]; [30]). For the former, small opportunities do not provide significant resources, whereas large opportunities are prohibitively resource straining. Thus, the indirect negative effect of opportunity magnitude on willingness to pursue an opportunity through costs is amplified when salesperson resource slack is limited. For these salespeople, the total indirect effect of opportunity magnitude through benefits and costs follows an inverted U-shape. By contrast, salespeople with high resource slack are motivated to pursue larger opportunities because they have the resources and, by mobilizing them, can gain even more resources ([26]). For these salespeople, the total indirect effect of opportunity magnitude through benefits and costs is convex. Thus, H6:  Salesperson resource slack buffers the negative effect of costs on salesperson willingness to pursue an opportunity. As a corollary, the total indirect effect of opportunity magnitude on salesperson willingness to pursue an opportunity follows an inverted U-shape only when salesperson resource slack is low. Method and Results Sample and procedureGiven the consistent findings of an inverted U-shape across levels, in Study 3 we focus on the prospect level. We partnered with a prominent market research firm to access a diverse panel of salespeople from various industries. The research firm randomly recruited 216 experienced salespeople (64% 36–45 years of age, 62% male, 53% in the information technology industry) for our between-subjects experiment.We then randomly assigned them to one of five scenarios. Each scenario informed participants that they were assigned a territory where the typical revenue of a prospect was $50,000. We included this portfolio baseline information to ensure the design matches with Study 1 and real-life selling contexts. They identified a new sales lead (Prospect A) with a specific opportunity magnitude. In line with data from Study 1, we set the opportunity magnitude at five levels: $1,000, $10,000, $50,000, $250,000, and $1,000,000. After participants read the scenario, we assessed their willingness to pursue Prospect A, anticipated costs, and anticipated benefits on a seven-point scale (1 = ""strongly disagree,"" and 7 = ""strongly agree"") and their resource slack for prospecting activities. We used the natural variation of salespeople's resource slack in their jobs, as previous research shows that resource slack affects people's framing of costs and benefits in decision making ([68]). Post hoc tests indicated that resource slack was not differently distributed between treatment groups (F = .30; p > .10), thereby providing evidence that the manipulation itself did not affect participants' perceptions of resource slack. We included the manipulation check, attention and realism checks and demographic questions. ResultsAnalysis of variance revealed significant between group differences in willingness to pursue the prospect (F( 4, 211) = 2.835, p = .025). Specifically, willingness to pursue is significantly greater (p < .05) in the $50,000 condition (5.50) than in the small ($1,000; 4.75) or large ($1,000,000; 4.96) conditions. Thus, we replicate the inverted U-shaped effect of opportunity magnitude found in Studies 1 and 2. We then specified the path model of the Study 3 panel in Figure 1. We find that the effect of costs on willingness to pursue is contingent on resource slack (b = .242, p < .01). To test the mediating benefit–cost mechanism and the COR effect, we examined the ""instantaneous conditional indirect effect"" of opportunity magnitude on willingness to pursue, with salesperson resource slack as the moderator ([29]). The results show that when resource slack is low, the total indirect effect of opportunity magnitude through the two mediators is only significant at moderate levels of opportunity magnitude (θopp.mag=2 = .111, p < .05). When resource slack is high, moderate to high levels of opportunity magnitude translate significantly into willingness to pursue (θopp.mag=5 = .202, p < .01). These results lend support to H6 and our contention that, under resource constraints, the inverted U-shaped effect of opportunity magnitude operates through the benefit–cost mechanism. For further details, see Web Appendix W8. General DiscussionIntegrating decision-making and COR theories, we develop and test a framework of salesperson decision making when prospecting in three multimethod studies. Together, the empirical evidence explains why salespeople avoid big-whale sales opportunities. Theoretical ContributionsOur research stems from the idea that salespeople differ from participants in studies that focus on low-effort, constraint-free, and repeatable choices ([36]). First, the potential benefits—extrinsic and/or intrinsic—of salespeople's decisions are consequential rather than trivial. Second, given their resource constraints and the ephemeral nature of sales opportunities, their costs—explicit and/or implicit—are not negligible. Third, their decision-making context abounds with uncertainties. Our findings confirm and provide novel insights into the theoretical importance of these differences for research on salespeople's decision making, especially when prospecting. Benefit–cost analysis in salesperson prospectingWe provide strong empirical evidence that in deciding on which opportunities to pursue, salespeople conduct a benefit–cost analysis based on their initial judgment of opportunity magnitude. We show that the relationship between initial judgment of opportunity magnitude and actual conversion follows an inverted U-shape, regardless of selling task (product vs. solution selling) and information level (prospect vs. portfolio). This finding debunks the intuition that salespeople gravitate toward big-whale opportunities, an insight that extends current understanding of salesperson prospecting behavior. Our result also confirms that salespeople's initial judgment of opportunity magnitude exerts a strong impact on their subsequent behavior and performance, even after controlling for transient phases. This finding complements prior research on salesperson intuition ([27]) and on primacy and anchoring effects ([58]). Salesperson calibration for conversion uncertaintyWe also found that solution-selling salespeople take into consideration opportunity conversion uncertainty in their benefit–cost analysis. Due to this calibration, the inverted U-shaped relationship between opportunity magnitude and performance shifts to the left. This shift implies that salespeople are generally more risk-seeking when opportunity magnitude ranges from small to moderate and risk-averse when opportunity magnitude is large. The counterfactual analyses we conducted show that the calibration effect reduces misspecification of conversion rates by up to 100%, when compared with the estimates from a compensatory decision strategy in which uncertainty is simply factored in as an extra cost. This finding provides a more nuanced understanding of the differences between salespeople's decision-making strategies (i.e., calibration vs. compensatory) when prospecting. Furthermore, it joins two separate streams of research on salesperson decision making, one focusing on salesperson judgment of demands and the other on uncertainty. Selling contexts and calibrationCompared with product selling, solution selling is full of uncertainties (e.g., need, process, outcome; [60]). Although these uncertainties are likely to influence salesperson behavior, salesperson behavior and decision making in solution selling has not received much academic research. We contribute to the literature by showing that salespeople indeed use different decision-making strategies in solution selling versus product selling. Specifically, they rely on a calibration decision-making strategy only in solution selling and only at the portfolio level. At the prospect level, regardless of the selling context, salespeople assess each individual opportunity relative to their portfolio baseline in terms of magnitude and conversion uncertainty using a compensatory strategy in which a large magnitude can make up for high uncertainty (and vice versa). Salesperson characteristics and calibration in solution sellingWe find that salesperson past performance success and salesperson experience are important contingencies of salespeople's decision-making process under uncertainty (i.e., calibration). The interaction plots (Figure 4) suggest that salespeople who have achieved past performance success and/or have low experience tend to conserve their resources and become more risk averse when selling solutions. They perform better under low or average than high conversion uncertainty conditions. Experienced salespeople are able to overcome this cautious approach.Our findings also address the contrast between the COR perspective ([30]) and the risk-seeking perspective based on research on risky choice, such as gambling ([56]). While the latter perspective is not specific to the selling context, the COR argument is uniquely relevant to the personal selling context, as it accounts for the notions that ( 1) salespeople are subject to resource constraints and their efforts are costly and ( 2) salespeople need to conserve resources to avoid stress in the long run ([48]). Therefore, researchers who apply decision-making theories to the context of personal selling will benefit from accounting for the uniqueness of salespeople as decision makers. Information level and calibrationOur findings highlight a dual information-processing framework in salesperson decision making when prospecting for solution-selling opportunities ([51]; [58]). Under these conditions, we find that salesperson performance is a function of two processes. At the prospect level, salespeople rely on a simple compensatory model in their decision making, such that a large magnitude can make up for high uncertainty (and vice versa). At the portfolio level, they integrate information about both the magnitude and uncertainty of the prospects they targeted in a more complex calibration model. In this decision-making strategy, conversion uncertainty interacts with opportunity magnitude in driving salesperson portfolio performance. This insight is a meaningful step toward a better understanding of salespeople's prioritization of resources and the importance of considering salesperson characteristics in prospecting. It also sheds first light on potential differences between findings at the prospect level and those at the portfolio level (i.e., a lack of homology) and calls for additional multilevel research of this kind. Managerial ImplicationsOur findings provide both managers and salespeople with several new insights into salesperson decision making when prospecting. We underscore key performance implications of our findings by simulating several what-if analyses using the parameters from our results. Managing salespeople's avoidance of large opportunitiesThe results from three studies consistently show that, all else being equal, salespeople are likely to gravitate toward medium-sized opportunities, leaving smaller and larger opportunities unattended. Using simulated data from Study 1, we find that a salesperson with a prospect whose magnitude equals their baseline opportunity magnitude of ∼$26,750 will have an 86.5% probability of successful conversion. Yet receiving a new prospect that is 1 SD larger in terms of magnitude (∼$143,250) will decrease the conversion odds by more than 15%. This effect is due to the propensity to conserve resources when there are constraints, as Study 3 further shows that the anticipated costs only affect salespeople's pursuit of large prospects when operating under resource constraints.Thus, to assuage salespeople's avoidance of big-whale deals, managers can leverage their firms' CRM databases. Specifically, a manager can use historical CRM data to calculate the baseline estimates of opportunity magnitude (and conversion uncertainty) for each salesperson. Then, the manager can use this information to match marketing-generated prospects with a salesperson's portfolio baseline, because a large difference in opportunity magnitude between a new opportunity and the salesperson baseline is demotivating and decreases conversion success. Furthermore, when necessary, managers should alter salespeople's benefit–cost calculus when prospecting. For example, they should provide salespeople who work on relatively large opportunities with extra benefits (both extrinsic and intrinsic) and additional resources (to relax the resource constraints), thereby increasing the likelihood of conversion. In addition, managers could pair salespeople with peers with larger portfolio baselines to create ad hoc sales teams to follow up. Such a temporary arrangement can reduce the costs for the focal salespeople. Salesperson calibration for conversion uncertainty in solution sellingOur findings show that salespeople's decision-making strategy differs between solution and product selling. For product selling, salespeople rely on a compensatory decision-making strategy at both prospect and portfolio levels. For solution selling, however, they rely on a calibration decision-making strategy, which is noncompensatory in nature, with conversion uncertainty acting as the calibrator of opportunity magnitude at the portfolio level. This calibration effect underscores the important role of portfolio-level information, in terms of both magnitude and conversion uncertainty, in salesperson decision making for solutions. Thus, sales managers should pay close attention to this important ""between-salespeople"" difference when reallocating prospects for maximum effect. Continuing with the previous example from the simulated data, a sales manager could intuitively decide to allocate the solution-selling prospect of ∼$143,250 to a salesperson with a portfolio baseline magnitude of about the same size (i.e., ∼$143,250). However, if this salesperson's portfolio baseline conversion uncertainty is 1 SD higher, the probability of closing the deal decreases by 36.2%. To reduce conversion uncertainty, managers can play an active role by, for example, using more behavior-based control to curtail salespeople's pursuit of highly uncertain opportunities and providing them with more frequent feedback. Firms can also leverage advanced sales analytics capabilities to decrease uncertainty in opportunity costs and train salespeople how to use the information in their prospecting decisions. Boundary conditions of conversion uncertainty calibration for solution sellingOur results indicate that past performance success and experience can alter the way salespeople calibrate for conversion uncertainty. Thus, these two variables are important for managers as well as salespeople. In a post hoc analysis, we used Study 2's results to predict salesperson quota achievement under various combinations of levels of salespeople's past performance success and experience (±1 SD as high and low values). Drawing on the results summarized in Table 4, Panel A, we derive the most effective managerial actions for managing salesperson prospecting in Table 4, Panel B. Three insights are worth noting. First, regardless of past performance success, salespeople's quota attainment is the worst when they gravitate toward highly certain but small opportunities. Second, salespeople who performed well in the past are most likely to ""hit"" quota again when the portfolio opportunity is large and conversion uncertainty is low (96%). Nevertheless, these high performers become average performers when conversion uncertainty and portfolio magnitude are average (ranging from 50% to 79%). Therefore, an effective way to manage high performers' prospecting is to give them a large portfolio but also help them reduce conversion uncertainty. This combination allows them to conserve resources while also maintaining high levels of performance. By contrast, salespeople who performed poorly in the past can achieve a quota attainment as high as 83% when they have a large portfolio and conversion uncertainty is high. The increase in opportunity magnitude is more motivating to these poor performers—they are willing to exert greater efforts without conserving resources to prove themselves. Thus, an effective, but perhaps counterintuitive, way to manage poor performers' prospecting is to give them a larger, more uncertain portfolio to challenge them.GraphTable 4. Study 2: Managerial Insights into Boundary Conditions of Salesperson Prospecting in Solution Selling. A: Post Hoc Analysis for Study 2Moderating Role of Salesperson Past Performance Success (H4)Low Past Performance Success (−1 SD)High Past Performance Success (+1 SD)Conversion UncertaintyConversion UncertaintyMagnitude1 SD LowerAverage1 SD Higher1 SD LowerAverage1 SD Higher1 SD lower36%40%48%39%50%64%Average68%67%71%84%81%79%1 SD higher67%76%83%96%70%50% Table 4. Study 2: Managerial Insights into Boundary Conditions of Salesperson Prospecting in Solution Selling. Moderating Role of Salesperson Experience (H5)Low Salesperson Experience (−1 SD)High Salesperson Experience (+1 SD)Conversion UncertaintyConversion UncertaintyMagnitude1 SD LowerAverage1 SD Higher1 SD LowerAverage1 SD Higher1 SD lower29%28%27%54%76%106%Average106%75%53%53%75%106%1 SD higher118%84%60%60%62%64% Table 4. Study 2: Managerial Insights into Boundary Conditions of Salesperson Prospecting in Solution Selling. B: Managerial TakeawaysObservation from DataSuggested Managerial ActionRegardless of past performance success, salespeople's quota attainment is worst when they gravitate toward highly certain but small opportunities.Point out the importance of ""bread-and-butter"" prospects, as salespeople may ignore them while such prospects could be of strategic importance.Salespeople with greater past performance success perform relatively well when portfolio opportunity is large and conversion uncertainty is low.Give these salespeople a large portfolio but also help them reduce conversion uncertainty (to help them conserve resources).Salespeople with lower past performance success tend to perform better for relatively larger and more uncertain portfolios.Give these salespeople a larger, more uncertain portfolio to challenge them, while also providing opportunities to recuperate from poor past performance.Inexperienced salespeople perform better for relatively larger, but certain portfolios.Reduce conversion uncertainty (e.g., via information provision, training) and provide them with larger portfolios.Experienced salespeople tend to perform especially well for relatively small to moderate portfolio magnitude with relatively high conversion uncertainty.Challenge these salespeople with opportunities that have high conversion uncertainty, while ensuring the portfolio itself is not too large. 14 Notes: 100% = on-target performance. Shaded boxes reflect higher levels of quota achievement.Third, when conversion uncertainty is reduced, inexperienced salespeople who handle a large portfolio can go from zero to hero, as their quota attainment increases from 60% to 118%. Inexperienced salespeople also achieve low quota (under 30%) when their portfolio is small, regardless of conversion uncertainty. Therefore, an effective way to manage inexperienced salespeople's prospecting is to reduce conversion uncertainty and provide them with ample opportunities. By contrast, experienced salespeople do not perform well when their portfolio opportunity is large, regardless of conversion uncertainty (range: 60%–64%). However, they thrive under high conversion uncertainty and when their portfolio is moderate in size, with a quota attainment exceeding 100%. Thus, an effective way to manage experienced salespeople is to challenge them with opportunities that have high conversion uncertainty, while ensuring the portfolio opportunity itself is not too large.What do these results mean for salespeople? Our results show that salespeople need to be cognizant of potential biases created by their past performance success and experience. This is because these biases can significantly improve or impair their sales performance, as indicated by the aforementioned potential gains and losses in quota attainment. By changing the benefit–cost analysis and reducing factors that drive conversion uncertainty (e.g., learn from peers, ask managers for support, form ad hoc teams), salespeople can become more effective in closing big-whale deals and hitting their targets despite conversion uncertainty. Limitations and Future Research DirectionsWhile our research covers three empirical contexts and our data came from multiple sources, this article has several limitations. First, although opportunity magnitude and conversion uncertainty are two of the most important factors in salesperson decision making, they are by no means the only factors. In our research, we included several contingencies and control variables to account for heterogeneity. Nevertheless, we urge further research to consider other aspects as contingencies of salesperson calibration, such as salesperson perceptual accuracy in forming judgments of opportunity magnitude and uncertainty, customer characteristics, competition, and the source of the sales leads ([46]; [48]). Future research could also explore how managers can influence salesperson calibration (e.g., through incentives, by changing baseline conversion uncertainty via altering the composition of self-generated and assigned leads). Moreover, the nature of uncertainty itself and how it affects judgment and decision making could be further explored. Second, we focus on portfolio baseline magnitude and conversion uncertainty as the frames of reference for how salespeople form relative comparisons of prospects in their portfolios. Although this focus is both theoretically and empirically justified, further research could examine other reference points as suggested in the judgment–decision making literature (e.g., sales goals, status quo, minimum requirement). Other measures of magnitude, such as customer lifetime value, could also be examined.Third, we control for several time-related effects in Study 2 but did not examine the dynamics. Although the first impression generally serves as the anchor point, people adjust their anchors as they receive new information ([58]). Further research could examine how salespeople update their judgment of uncertainty over time by exploring how this effect manifests itself in salesperson prospecting. Fourth, we focus on salesperson past performance success and salesperson experience as moderators, but other moderators may exist, such as control systems and training. Finally, although theoretical arguments exist in support of the moderating role of salespeople's past performance success and experience, future research could explicitly test how these contingencies influence the underlying benefit–costs analysis. Supplemental Materialsj-pdf-1-jmx-10.1177_00222429211037336 - Supplemental material for Why Salespeople Avoid Big-Whale Sales OpportunitiesSupplemental material, sj-pdf-1-jmx-10.1177_00222429211037336 for Why Salespeople Avoid Big-Whale Sales Opportunities by Juan Xu, Michel van der Borgh, Edwin J. Nijssen, and Son K. Lam in Journal of Marketing  "
